[
  {
    "objectID": "labs/seurat/seurat_01_qc.html",
    "href": "labs/seurat/seurat_01_qc.html",
    "title": " Quality Control",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified."
  },
  {
    "objectID": "labs/seurat/seurat_01_qc.html#meta-qc_data",
    "href": "labs/seurat/seurat_01_qc.html#meta-qc_data",
    "title": " Quality Control",
    "section": "1 Get data",
    "text": "1 Get data\nIn this tutorial, we will run all tutorials with a set of 8 PBMC 10x datasets from 4 covid-19 patients and 4 healthy controls, the samples have been subsampled to 1500 cells per sample. We can start by defining our paths.\n\n# download pre-computed annotation\nfetch_annotation &lt;- TRUE\n\n# url for source and intermediate data\npath_data &lt;- \"https://nextcloud.dc.scilifelab.se/public.php/webdav\"\ncurl_upass &lt;- \"-u zbC5fr2LbEZ9rSE:scRNAseq2025\"\n\npath_covid &lt;- \"./data/covid/raw\"\nif (!dir.exists(path_covid)) dir.create(path_covid, recursive = T)\n\npath_results &lt;- \"./data/covid/results\"\nif (!dir.exists(path_results)) dir.create(path_results, recursive = T)\n\n\nfile_list &lt;- c(\n    \"normal_pbmc_13.h5\", \"normal_pbmc_14.h5\", \"normal_pbmc_19.h5\", \"normal_pbmc_5.h5\",\n    \"ncov_pbmc_15.h5\", \"ncov_pbmc_16.h5\", \"ncov_pbmc_17.h5\", \"ncov_pbmc_1.h5\"\n)\n\nfor (i in file_list) {\n    path_file &lt;- file.path(path_covid, i)\n    if (!file.exists(path_file)) {\n        download.file(url = file.path(file.path(path_data, \"covid/raw\"), i),\n              destfile = path_file, method = \"curl\", extra = curl_upass)\n    }\n}\n\nWith data in place, now we can start loading libraries we will use in this tutorial.\n\nsuppressPackageStartupMessages({\n    library(Seurat)\n    library(Matrix)\n    library(ggplot2)\n    library(patchwork)\n    # remotes::install_github(\"chris-mcginnis-ucsf/DoubletFinder\", upgrade = FALSE, dependencies = TRUE)\n    library(DoubletFinder)\n})\n\nWe can first load the data individually by reading directly from HDF5 file format (.h5).\n\ncov.15 &lt;- Seurat::Read10X_h5(\n    filename = file.path(path_covid, \"ncov_pbmc_15.h5\"),\n    use.names = T\n)\ncov.1 &lt;- Seurat::Read10X_h5(\n    filename = file.path(path_covid, \"ncov_pbmc_1.h5\"),\n    use.names = T\n)\ncov.16 &lt;- Seurat::Read10X_h5(\n    filename = file.path(path_covid, \"ncov_pbmc_16.h5\"),\n    use.names = T\n)\ncov.17 &lt;- Seurat::Read10X_h5(\n    filename = file.path(path_covid, \"ncov_pbmc_17.h5\"),\n    use.names = T\n)\n\nctrl.5 &lt;- Seurat::Read10X_h5(\n    filename = file.path(path_covid, \"normal_pbmc_5.h5\"),\n    use.names = T\n)\nctrl.13 &lt;- Seurat::Read10X_h5(\n    filename = file.path(path_covid, \"normal_pbmc_13.h5\"),\n    use.names = T\n)\nctrl.14 &lt;- Seurat::Read10X_h5(\n    filename = file.path(path_covid, \"normal_pbmc_14.h5\"),\n    use.names = T\n)\nctrl.19 &lt;- Seurat::Read10X_h5(\n    filename = file.path(path_covid, \"normal_pbmc_19.h5\"),\n    use.names = T\n)"
  },
  {
    "objectID": "labs/seurat/seurat_01_qc.html#meta-qc_collate",
    "href": "labs/seurat/seurat_01_qc.html#meta-qc_collate",
    "title": " Quality Control",
    "section": "2 Collate",
    "text": "2 Collate\nWe can now merge them objects into a single object. Each analysis workflow (Seurat, Scater, Scanpy, etc) has its own way of storing data. We will add dataset labels as cell.ids just in case you have overlapping barcodes between the datasets. After that we add a column type in the metadata to define covid and ctrl samples.\nBut first, we need to create Seurat objects using each of the expression matrices we loaded. We define each sample in the project slot, so in each object, the sample id can be found in the metadata slot orig.ident.\n\nsdata.cov1 &lt;- CreateSeuratObject(cov.1, project = \"covid_1\")\nsdata.cov15 &lt;- CreateSeuratObject(cov.15, project = \"covid_15\")\nsdata.cov17 &lt;- CreateSeuratObject(cov.17, project = \"covid_17\")\nsdata.cov16 &lt;- CreateSeuratObject(cov.16, project = \"covid_16\")\nsdata.ctrl5 &lt;- CreateSeuratObject(ctrl.5, project = \"ctrl_5\")\nsdata.ctrl13 &lt;- CreateSeuratObject(ctrl.13, project = \"ctrl_13\")\nsdata.ctrl14 &lt;- CreateSeuratObject(ctrl.14, project = \"ctrl_14\")\nsdata.ctrl19 &lt;- CreateSeuratObject(ctrl.19, project = \"ctrl_19\")\n\n\n# add metadata\nsdata.cov1$type &lt;- \"Covid\"\nsdata.cov15$type &lt;- \"Covid\"\nsdata.cov16$type &lt;- \"Covid\"\nsdata.cov17$type &lt;- \"Covid\"\n\nsdata.ctrl5$type &lt;- \"Ctrl\"\nsdata.ctrl13$type &lt;- \"Ctrl\"\nsdata.ctrl14$type &lt;- \"Ctrl\"\nsdata.ctrl19$type &lt;- \"Ctrl\"\n\n# Merge datasets into one single seurat object\nalldata &lt;- merge(sdata.cov1, c(sdata.cov15, sdata.cov16, sdata.cov17, sdata.ctrl5, sdata.ctrl13, sdata.ctrl14, sdata.ctrl19), add.cell.ids = c(\"covid_1\", \"covid_15\", \"covid_16\", \"covid_17\", \"ctrl_5\", \"ctrl_13\", \"ctrl_14\", \"ctrl_19\"))\n\nIn Seurat v5, merging creates a single object, but keeps the expression information split into different layers for integration. If not proceeding with integration, rejoin the layers after merging.\n\nalldata &lt;- JoinLayers(alldata)\nalldata\n\nAn object of class Seurat \n33538 features across 12000 samples within 1 assay \nActive assay: RNA (33538 features, 0 variable features)\n 1 layer present: counts\n\n\nOnce you have created the merged Seurat object, the count matrices and individual count matrices and objects are not needed anymore. It is a good idea to remove them and run garbage collect to free up some memory.\n\n# remove all objects that will not be used.\nrm(cov.1, cov.15, cov.16, cov.17, ctrl.5, ctrl.13, ctrl.14, ctrl.19, sdata.cov1, sdata.cov15, sdata.cov16, sdata.cov17, sdata.ctrl5, sdata.ctrl13, sdata.ctrl14, sdata.ctrl19)\n# run garbage collect to free up memory\ngc()\n\n           used  (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb)\nNcells  3593577 192.0    6788372  362.6         NA   6788372  362.6\nVcells 32552103 248.4  162125710 1237.0      65536 201327045 1536.1\n\n\nHere is how the count matrix and the metadata look like for every cell.\n\nalldata[[\"RNA\"]]$counts[1:10, 1:4] \n\n10 x 4 sparse Matrix of class \"dgCMatrix\"\n            covid_1_AGGTAGGTCGTTGTTT-1 covid_1_TAGAGTCGTCCTCCAT-1\nMIR1302-2HG                          .                          .\nFAM138A                              .                          .\nOR4F5                                .                          .\nAL627309.1                           .                          .\nAL627309.3                           .                          .\nAL627309.2                           .                          .\nAL627309.4                           .                          .\nAL732372.1                           .                          .\nOR4F29                               .                          .\nAC114498.1                           .                          .\n            covid_1_CCCTGATAGCGAACTG-1 covid_1_TCATCATTCCACGTAA-1\nMIR1302-2HG                          .                          .\nFAM138A                              .                          .\nOR4F5                                .                          .\nAL627309.1                           .                          .\nAL627309.3                           .                          .\nAL627309.2                           .                          .\nAL627309.4                           .                          .\nAL732372.1                           .                          .\nOR4F29                               .                          .\nAC114498.1                           .                          .\n\nhead(alldata@meta.data, 10)"
  },
  {
    "objectID": "labs/seurat/seurat_01_qc.html#meta-qc_calqc",
    "href": "labs/seurat/seurat_01_qc.html#meta-qc_calqc",
    "title": " Quality Control",
    "section": "3 Calculate QC",
    "text": "3 Calculate QC\nHaving the data in a suitable format, we can start calculating some quality metrics. We can for example calculate the percentage of mitochondrial and ribosomal genes per cell and add to the metadata. The proportion of hemoglobin genes can give an indication of red blood cell contamination, but in some tissues it can also be the case that some celltypes have higher content of hemoglobin. This will be helpful to visualize them across different metadata parameters (i.e. datasetID and chemistry version). There are several ways of doing this. The QC metrics are finally added to the metadata table.\nCiting from Simple Single Cell workflows (Lun, McCarthy & Marioni, 2017): High proportions are indicative of poor-quality cells (Islam et al. 2014; Ilicic et al. 2016), possibly because of loss of cytoplasmic RNA from perforated cells. The reasoning is that mitochondria are larger than individual transcript molecules and less likely to escape through tears in the cell membrane.\n\n# method 1: doing it using Seurat function\nalldata &lt;- PercentageFeatureSet(alldata, \"^MT-\", col.name = \"percent_mito\")\n\n\n# Ribosomal\nalldata &lt;- PercentageFeatureSet(alldata, \"^RP[SL]\", col.name = \"percent_ribo\")\n# Percentage hemoglobin genes - includes all genes starting with HB except HBP.\nalldata &lt;- PercentageFeatureSet(alldata, \"^HB[^(P|E|S)]\", col.name = \"percent_hb\")\n# Percentage for some platelet markers\nalldata &lt;- PercentageFeatureSet(alldata, \"PECAM1|PF4\", col.name = \"percent_plat\")\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAlternatively, percentage expression can be calculated manually. Here is an example. Do not run this script now.\n\n# Do not run now!\ntotal_counts_per_cell &lt;- colSums(alldata@assays$RNA@counts)\nmito_genes &lt;- rownames(alldata)[grep(\"^MT-\", rownames(alldata))]\nalldata$percent_mito2 &lt;- colSums(alldata@assays$RNA@counts[mito_genes, ]) / total_counts_per_cell\n\n\n\n\nNow you can see that we have additional data in the metadata slot.\n\nhead(alldata@meta.data)"
  },
  {
    "objectID": "labs/seurat/seurat_01_qc.html#meta-qc_plotqc",
    "href": "labs/seurat/seurat_01_qc.html#meta-qc_plotqc",
    "title": " Quality Control",
    "section": "4 Plot QC",
    "text": "4 Plot QC\nNow we can plot some of the QC variables as violin plots.\n\nfeats &lt;- c(\"nFeature_RNA\", \"nCount_RNA\", \"percent_mito\", \"percent_ribo\", \"percent_hb\", \"percent_plat\")\nVlnPlot(alldata, group.by = \"orig.ident\", split.by = \"type\", features = feats, pt.size = 0.1, ncol = 3)\n\n\n\n\n\n\n\n\nAs you can see, there is quite some difference in quality for these samples, with for instance the covid_15 and covid_16 samples having cells with fewer detected genes and more mitochondrial content. As the ribosomal proteins are highly expressed they will make up a larger proportion of the transcriptional landscape when fewer of the lowly expressed genes are detected. We can also plot the different QC-measures as scatter plots.\n\nFeatureScatter(alldata, \"nCount_RNA\", \"nFeature_RNA\", group.by = \"orig.ident\", pt.size = .5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nPlot additional QC stats that we have calculated as scatter plots. How are the different measures correlated? Can you explain why?"
  },
  {
    "objectID": "labs/seurat/seurat_01_qc.html#meta-qc_filter",
    "href": "labs/seurat/seurat_01_qc.html#meta-qc_filter",
    "title": " Quality Control",
    "section": "5 Filtering",
    "text": "5 Filtering\n\n5.1 Detection-based filtering\nA standard approach is to filter cells with low number of reads as well as genes that are present in at least a given number of cells. Here we will only consider cells with at least 200 detected genes and genes need to be expressed in at least 3 cells. Please note that those values are highly dependent on the library preparation method used.\n\nselected_c &lt;- WhichCells(alldata, expression = nFeature_RNA &gt; 200)\nselected_f &lt;- rownames(alldata)[Matrix::rowSums(alldata[[\"RNA\"]]$counts) &gt; 3]\n\ndata.filt &lt;- subset(alldata, features = selected_f, cells = selected_c)\ndim(data.filt)\n\n[1] 18877 10656\n\ntable(data.filt$orig.ident)\n\n\n covid_1 covid_15 covid_16 covid_17  ctrl_13  ctrl_14  ctrl_19   ctrl_5 \n    1254     1283     1127     1371     1417     1399     1434     1371 \n\n\nExtremely high number of detected genes could indicate doublets. However, depending on the cell type composition in your sample, you may have cells with higher number of genes (and also higher counts) from one cell type. In this case, we will run doublet prediction further down, so we will skip this step now, but the code below is an example of how it can be run:\n\n# skip and run DoubletFinder instead\n# data.filt &lt;- subset(data.filt, cells=WhichCells(data.filt, expression = nFeature_RNA &lt; 4100))\n\nAdditionally, we can also see which genes contribute the most to such reads. We can for instance plot the percentage of counts per gene.\n\n# Compute the proportion of counts of each gene per cell\n# Use sparse matrix operations, if your dataset is large, doing matrix devisions the regular way will take a very long time.\n\nC &lt;- data.filt[[\"RNA\"]]$counts\nC@x &lt;- C@x / rep.int(colSums(C), diff(C@p)) * 100\nmost_expressed &lt;- order(Matrix::rowSums(C), decreasing = T)[20:1]\nboxplot(as.matrix(t(C[most_expressed, ])),\n    cex = 0.1, las = 1, xlab = \"Percent counts per cell\",\n    col = (scales::hue_pal())(20)[20:1], horizontal = TRUE\n)\n\n\n\n\n\n\n\n\nAs you can see, MALAT1 constitutes up to 30% of the UMIs from a single cell and the other top genes are mitochondrial and ribosomal genes. It is quite common that nuclear lincRNAs have correlation with quality and mitochondrial reads, so high detection of MALAT1 may be a technical issue. Let us assemble some information about such genes, which are important for quality control and downstream filtering.\n\n\n5.2 Mito/Ribo filtering\nWe also have quite a lot of cells with high proportion of mitochondrial and low proportion of ribosomal reads. It would be wise to remove those cells, if we have enough cells left after filtering. Another option would be to either remove all mitochondrial reads from the dataset and hope that the remaining genes still have enough biological signal. A third option would be to just regress out the percent_mito variable during scaling. In this case we had as much as 99.7% mitochondrial reads in some of the cells, so it is quite unlikely that there is much cell type signature left in those. Looking at the plots, make reasonable decisions on where to draw the cutoff. In this case, the bulk of the cells are below 20% mitochondrial reads and that will be used as a cutoff. We will also remove cells with less than 5% ribosomal reads.\n\ndata.filt &lt;- subset(data.filt, percent_mito &lt; 20 & percent_ribo &gt; 5)\ndim(data.filt)\n\n[1] 18877  7431\n\ntable(data.filt$orig.ident)\n\n\n covid_1 covid_15 covid_16 covid_17  ctrl_13  ctrl_14  ctrl_19   ctrl_5 \n     900      599      373     1101     1173     1063     1170     1052 \n\n\nAs you can see, a large proportion of sample covid_15 is filtered out. Also, there is still quite a lot of variation in percent_mito, so it will have to be dealt with in the data analysis step. We can also notice that the percent_ribo are also highly variable, but that is expected since different cell types have different proportions of ribosomal content, according to their function.\n\n\n5.3 Plot filtered QC\nLets plot the same QC-stats once more.\n\nfeats &lt;- c(\"nFeature_RNA\", \"nCount_RNA\", \"percent_mito\", \"percent_ribo\", \"percent_hb\")\nVlnPlot(data.filt, group.by = \"orig.ident\", features = feats, pt.size = 0.1, ncol = 3) + NoLegend()\n\n\n\n\n\n\n\n\n\n\n5.4 Filter genes\nAs the level of expression of mitochondrial and MALAT1 genes are judged as mainly technical, it can be wise to remove them from the dataset before any further analysis. In this case we will also remove the HB genes.\n\ndim(data.filt)\n\n[1] 18877  7431\n\n# Filter MALAT1\ndata.filt &lt;- data.filt[!grepl(\"MALAT1\", rownames(data.filt)), ]\n\n# Filter Mitocondrial\ndata.filt &lt;- data.filt[!grepl(\"^MT-\", rownames(data.filt)), ]\n\n# Filter Ribossomal gene (optional if that is a problem on your data)\n# data.filt &lt;- data.filt[ ! grepl(\"^RP[SL]\", rownames(data.filt)), ]\n\n# Filter Hemoglobin gene (optional if that is a problem on your data)\ndata.filt &lt;- data.filt[!grepl(\"^HB[^(P|E|S)]\", rownames(data.filt)), ]\n\ndim(data.filt)\n\n[1] 18854  7431"
  },
  {
    "objectID": "labs/seurat/seurat_01_qc.html#meta-qc_sex",
    "href": "labs/seurat/seurat_01_qc.html#meta-qc_sex",
    "title": " Quality Control",
    "section": "6 Sample sex",
    "text": "6 Sample sex\nWhen working with human or animal samples, you should ideally constrain your experiments to a single sex to avoid including sex bias in the conclusions. However this may not always be possible. By looking at reads from chromosomeY (males) and XIST (X-inactive specific transcript) expression (mainly female) it is quite easy to determine per sample which sex it is. It can also be a good way to detect if there has been any mislabelling in which case, the sample metadata sex does not agree with the computational predictions.\nTo get chromosome information for all genes, you should ideally parse the information from the gtf file that you used in the mapping pipeline as it has the exact same annotation version/gene naming. However, it may not always be available, as in this case where we have downloaded public data. R package biomaRt can be used to fetch annotation information. The code to run biomaRt is provided. As the biomart instances are quite often unresponsive, we will download and use a file that was created in advance.\n\n\n\n\n\n\nTip\n\n\n\n\n\nHere is the code to download annotation data from Ensembl using biomaRt. We will not run this now and instead use a pre-computed file in the step below.\n\n# fetch_annotation is defined at the top of this document\nif (!fetch_annotation) {\n  suppressMessages(library(biomaRt))\n\n  # initialize connection to mart, may take some time if the sites are unresponsive.\n  mart &lt;- useMart(\"ENSEMBL_MART_ENSEMBL\", dataset = \"hsapiens_gene_ensembl\")\n\n  # fetch chromosome info plus some other annotations\n  genes_table &lt;- try(biomaRt::getBM(attributes = c(\n    \"ensembl_gene_id\", \"external_gene_name\",\n    \"description\", \"gene_biotype\", \"chromosome_name\", \"start_position\"\n  ), mart = mart, useCache = F))\n\n  write.csv(genes_table, file = \"data/covid/results/genes_table.csv\")\n}\n\n\n\n\nDownload precomputed data.\n\n# fetch_annotation is defined at the top of this document\nif (fetch_annotation) {\n  genes_file &lt;- file.path(path_results, \"genes_table.csv\")\n  if (!file.exists(genes_file)) download.file(file.path(path_data, \"covid/results_seurat/genes_table.csv\"), destfile = genes_file,\n                                              method = \"curl\", extra = curl_upass)\n}\n\n\ngenes.table &lt;- read.csv(genes_file)\ngenes.table &lt;- genes.table[genes.table$external_gene_name %in% rownames(data.filt), ]\n\nNow that we have the chromosome information, we can calculate the proportion of reads that comes from chromosome Y per cell.But first we have to remove all genes in the pseudoautosmal regions of chrY that are: * chromosome:GRCh38:Y:10001 - 2781479 is shared with X: 10001 - 2781479 (PAR1) * chromosome:GRCh38:Y:56887903 - 57217415 is shared with X: 155701383 - 156030895 (PAR2)\n\npar1 = c(10001, 2781479)\npar2 = c(56887903, 57217415)\np1.gene = genes.table$external_gene_name[genes.table$start_position &gt; par1[1] & genes.table$start_position &lt; par1[2] & genes.table$chromosome_name == \"Y\"]\np2.gene = genes.table$external_gene_name[genes.table$start_position &gt; par2[1] & genes.table$start_position &lt; par2[2] & genes.table$chromosome_name == \"Y\"]\n\nchrY.gene &lt;- genes.table$external_gene_name[genes.table$chromosome_name == \"Y\"]\nchrY.gene = setdiff(chrY.gene, c(p1.gene, p2.gene))\n\ndata.filt &lt;- PercentageFeatureSet(data.filt, features = chrY.gene, col.name = \"pct_chrY\")\n\nThen plot XIST expression vs chrY proportion. As you can see, the samples are clearly on either side, even if some cells do not have detection of either.\n\nFeatureScatter(data.filt, feature1 = \"XIST\", feature2 = \"pct_chrY\", slot = \"counts\")\n\n\n\n\n\n\n\n\nPlot as violins.\n\nVlnPlot(data.filt, features = c(\"XIST\", \"pct_chrY\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nHere, we can see clearly that we have three males and five females, can you see which samples they are? Do you think this will cause any problems for downstream analysis? Discuss with your group: what would be the best way to deal with this type of sex bias?"
  },
  {
    "objectID": "labs/seurat/seurat_01_qc.html#meta-qc_cellcycle",
    "href": "labs/seurat/seurat_01_qc.html#meta-qc_cellcycle",
    "title": " Quality Control",
    "section": "7 Cell cycle state",
    "text": "7 Cell cycle state\nWe here perform cell cycle scoring. To score a gene list, the algorithm calculates the difference of mean expression of the given list and the mean expression of reference genes. To build the reference, the function randomly chooses a bunch of genes matching the distribution of the expression of the given list. Cell cycle scoring adds three slots in the metadata, a score for S phase, a score for G2M phase and the predicted cell cycle phase.\n\n# Before running CellCycleScoring the data need to be normalized and logtransformed.\ndata.filt &lt;- NormalizeData(data.filt)\ndata.filt &lt;- CellCycleScoring(\n    object = data.filt,\n    g2m.features = cc.genes$g2m.genes,\n    s.features = cc.genes$s.genes\n)\n\nWe can now create a violin plot for the cell cycle scores as well.\n\nVlnPlot(data.filt, features = c(\"S.Score\", \"G2M.Score\"), group.by = \"orig.ident\", ncol = 3, pt.size = .1)\n\n\n\n\n\n\n\n\nIn this case it looks like we only have a few cycling cells in these datasets.\nSeurat does an automatic prediction of cell cycle phase with a default cutoff of the scores at zero. As you can see this does not fit this data very well, so be cautious with using these predictions. Instead we suggest that you look at the scores.\n\nFeatureScatter(data.filt, \"S.Score\", \"G2M.Score\", group.by = \"Phase\")"
  },
  {
    "objectID": "labs/seurat/seurat_01_qc.html#meta-qc_doublet",
    "href": "labs/seurat/seurat_01_qc.html#meta-qc_doublet",
    "title": " Quality Control",
    "section": "8 Predict doublets",
    "text": "8 Predict doublets\nDoublets/Multiples of cells in the same well/droplet is a common issue in scRNAseq protocols. Especially in droplet-based methods with overloading of cells. In a typical 10x experiment the proportion of doublets is linearly dependent on the amount of loaded cells. As indicated from the Chromium user guide, doublet rates are about as follows:\n\nMost doublet detectors simulates doublets by merging cell counts and predicts doublets as cells that have similar embeddings as the simulated doublets. Most such packages need an assumption about the number/proportion of expected doublets in the dataset. The data you are using is subsampled, but the original datasets contained about 5 000 cells per sample, hence we can assume that they loaded about 9 000 cells and should have a doublet rate at about 4%.\n\n\n\n\n\n\nCaution\n\n\n\nIdeally doublet prediction should be run on each sample separately, especially if your samples have different proportions of cell types. In this case, the data is subsampled so we have very few cells per sample and all samples are sorted PBMCs, so it is okay to run them together.\n\n\nHere, we will use DoubletFinder to predict doublet cells. But before doing doublet detection we need to run scaling, variable gene selection and PCA, as well as UMAP for visualization. These steps will be explored in more detail in coming exercises.\n\ndata.filt &lt;- FindVariableFeatures(data.filt, verbose = F)\ndata.filt &lt;- ScaleData(data.filt, vars.to.regress = c(\"nFeature_RNA\", \"percent_mito\"), verbose = F)\ndata.filt &lt;- RunPCA(data.filt, verbose = F, npcs = 20)\ndata.filt &lt;- RunUMAP(data.filt, dims = 1:10, verbose = F)\n\nThen we run doubletFinder, selecting first 10 PCs and a pK value of 0.9. To optimize the parameters, you can run the paramSweep function in the package.\n\nsuppressMessages(library(DoubletFinder))\n# Can run parameter optimization with paramSweep\n\n# sweep.res &lt;- paramSweep_v3(data.filt)\n# sweep.stats &lt;- summarizeSweep(sweep.res, GT = FALSE)\n# bcmvn &lt;- find.pK(sweep.stats)\n# barplot(bcmvn$BCmetric, names.arg = bcmvn$pK, las=2)\n\n# define the expected number of doublet cellscells.\nnExp &lt;- round(ncol(data.filt) * 0.04) # expect 4% doublets\ndata.filt &lt;- doubletFinder(data.filt, pN = 0.25, pK = 0.09, nExp = nExp, PCs = 1:10)\n\n[1] \"Creating 2477 artificial doublets...\"\n[1] \"Creating Seurat object...\"\n[1] \"Normalizing Seurat object...\"\n[1] \"Finding variable genes...\"\n[1] \"Scaling data...\"\n[1] \"Running PCA...\"\n[1] \"Calculating PC distance matrix...\"\n[1] \"Computing pANN...\"\n[1] \"Classifying doublets..\"\n\n\n\n# name of the DF prediction can change, so extract the correct column name.\nDF.name &lt;- colnames(data.filt@meta.data)[grepl(\"DF.classification\", colnames(data.filt@meta.data))]\n\nwrap_plots(\n    DimPlot(data.filt, group.by = \"orig.ident\") + NoAxes(),\n    DimPlot(data.filt, group.by = DF.name) + NoAxes(),\n    ncol = 2\n)\n\n\n\n\n\n\n\n\nWe should expect that two cells have more detected genes than a single cell, lets check if our predicted doublets also have more detected genes in general.\n\nVlnPlot(data.filt, features = \"nFeature_RNA\", group.by = DF.name, pt.size = .1)\n\n\n\n\n\n\n\n\nNow, lets remove all predicted doublets from our data.\n\ndata.filt &lt;- data.filt[, data.filt@meta.data[, DF.name] == \"Singlet\"]\ndim(data.filt)\n\n[1] 18854  7134\n\n\nTo summarize, lets check how many cells we have removed per sample, we started with 1500 cells per sample. Looking back at the intitial QC plots does it make sense that some samples have much fewer cells now?\n\ntable(alldata$orig.ident)\n\n\n covid_1 covid_15 covid_16 covid_17  ctrl_13  ctrl_14  ctrl_19   ctrl_5 \n    1500     1500     1500     1500     1500     1500     1500     1500 \n\ntable(data.filt$orig.ident)\n\n\n covid_1 covid_15 covid_16 covid_17  ctrl_13  ctrl_14  ctrl_19   ctrl_5 \n     875      549      357     1057     1126      996     1141     1033"
  },
  {
    "objectID": "labs/seurat/seurat_01_qc.html#meta-qc_save",
    "href": "labs/seurat/seurat_01_qc.html#meta-qc_save",
    "title": " Quality Control",
    "section": "9 Save data",
    "text": "9 Save data\nFinally, lets save the QC-filtered data for further analysis. Create output directory data/covid/results and save data to that folder. This will be used in downstream labs.\n\nsaveRDS(data.filt, file.path(path_results, \"seurat_covid_qc.rds\"))"
  },
  {
    "objectID": "labs/seurat/seurat_01_qc.html#meta-session",
    "href": "labs/seurat/seurat_01_qc.html#meta-session",
    "title": " Quality Control",
    "section": "10 Session info",
    "text": "10 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] KernSmooth_2.23-24  fields_16.3         viridisLite_0.4.2  \n [4] spam_2.10-0         DoubletFinder_2.0.4 patchwork_1.2.0    \n [7] ggplot2_3.5.1       Matrix_1.6-5        Seurat_5.1.0       \n[10] SeuratObject_5.0.2  sp_2.1-4           \n\nloaded via a namespace (and not attached):\n  [1] deldir_2.0-4           pbapply_1.7-2          gridExtra_2.3         \n  [4] rlang_1.1.4            magrittr_2.0.3         RcppAnnoy_0.0.22      \n  [7] spatstat.geom_3.2-9    matrixStats_1.4.1      ggridges_0.5.6        \n [10] compiler_4.3.3         maps_3.4.2             png_0.1-8             \n [13] vctrs_0.6.5            reshape2_1.4.4         hdf5r_1.3.11          \n [16] stringr_1.5.1          pkgconfig_2.0.3        fastmap_1.2.0         \n [19] labeling_0.4.3         utf8_1.2.4             promises_1.3.0        \n [22] rmarkdown_2.28         ggbeeswarm_0.7.2       bit_4.0.5             \n [25] purrr_1.0.2            xfun_0.47              jsonlite_1.8.8        \n [28] goftest_1.2-3          later_1.3.2            spatstat.utils_3.1-0  \n [31] irlba_2.3.5.1          parallel_4.3.3         cluster_2.1.6         \n [34] R6_2.5.1               ica_1.0-3              stringi_1.8.4         \n [37] RColorBrewer_1.1-3     spatstat.data_3.1-2    reticulate_1.39.0     \n [40] parallelly_1.38.0      lmtest_0.9-40          scattermore_1.2       \n [43] Rcpp_1.0.13            knitr_1.48             tensor_1.5            \n [46] future.apply_1.11.2    zoo_1.8-12             sctransform_0.4.1     \n [49] httpuv_1.6.15          splines_4.3.3          igraph_2.0.3          \n [52] tidyselect_1.2.1       abind_1.4-5            yaml_2.3.10           \n [55] spatstat.random_3.2-3  codetools_0.2-20       miniUI_0.1.1.1        \n [58] spatstat.explore_3.2-6 listenv_0.9.1          lattice_0.22-6        \n [61] tibble_3.2.1           plyr_1.8.9             withr_3.0.1           \n [64] shiny_1.9.1            ROCR_1.0-11            ggrastr_1.0.2         \n [67] evaluate_0.24.0        Rtsne_0.17             future_1.34.0         \n [70] fastDummies_1.7.4      survival_3.7-0         polyclip_1.10-7       \n [73] fitdistrplus_1.2-1     pillar_1.9.0           plotly_4.10.4         \n [76] generics_0.1.3         RcppHNSW_0.6.0         munsell_0.5.1         \n [79] scales_1.3.0           globals_0.16.3         xtable_1.8-4          \n [82] glue_1.7.0             lazyeval_0.2.2         tools_4.3.3           \n [85] data.table_1.15.4      RSpectra_0.16-2        RANN_2.6.2            \n [88] leiden_0.4.3.1         dotCall64_1.1-1        cowplot_1.1.3         \n [91] grid_4.3.3             tidyr_1.3.1            colorspace_2.1-1      \n [94] nlme_3.1-165           beeswarm_0.4.0         vipor_0.4.7           \n [97] cli_3.6.3              spatstat.sparse_3.1-0  fansi_1.0.6           \n[100] dplyr_1.1.4            uwot_0.1.16            gtable_0.3.5          \n[103] digest_0.6.37          progressr_0.14.0       ggrepel_0.9.6         \n[106] farver_2.1.2           htmlwidgets_1.6.4      htmltools_0.5.8.1     \n[109] lifecycle_1.0.4        httr_1.4.7             mime_0.12             \n[112] bit64_4.0.5            MASS_7.3-60.0.1"
  },
  {
    "objectID": "labs/seurat/seurat_02_dimred.html",
    "href": "labs/seurat/seurat_02_dimred.html",
    "title": " Dimensionality Reduction",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified."
  },
  {
    "objectID": "labs/seurat/seurat_02_dimred.html#meta-dimred_prep",
    "href": "labs/seurat/seurat_02_dimred.html#meta-dimred_prep",
    "title": " Dimensionality Reduction",
    "section": "1 Data preparation",
    "text": "1 Data preparation\nFirst, let’s load all necessary libraries and the QC-filtered dataset from the previous step.\n\n# Activate conda environment to get the correct python path\nreticulate::use_condaenv(\"/Users/asabjor/miniconda3/envs/seurat5\", required = TRUE)\n\nsuppressPackageStartupMessages({\n    library(Seurat)\n    library(ggplot2) # plotting\n    library(patchwork) # combining figures\n    library(scran)\n})\n\n\n# download pre-computed data if missing or long compute\nfetch_data &lt;- TRUE\n\n# url for source and intermediate data\npath_data &lt;- \"https://nextcloud.dc.scilifelab.se/public.php/webdav\"\ncurl_upass &lt;- \"-u zbC5fr2LbEZ9rSE:scRNAseq2025\"\n\npath_file &lt;- \"data/covid/results/seurat_covid_qc.rds\"\nif (!dir.exists(dirname(path_file))) dir.create(dirname(path_file), recursive = TRUE)\nif (fetch_data && !file.exists(path_file)) download.file(url = file.path(path_data, \"covid/results_seurat/seurat_covid_qc.rds\"), destfile = path_file, method = \"curl\", extra = curl_upass)\n\nalldata &lt;- readRDS(path_file)"
  },
  {
    "objectID": "labs/seurat/seurat_02_dimred.html#meta-dimred_fs",
    "href": "labs/seurat/seurat_02_dimred.html#meta-dimred_fs",
    "title": " Dimensionality Reduction",
    "section": "2 Feature selection",
    "text": "2 Feature selection\nWe first need to define which features/genes are important in our dataset to distinguish cell types. For this purpose, we need to find genes that are highly variable across cells, which in turn will also provide a good separation of the cell clusters.\n\nsuppressWarnings(suppressMessages(alldata &lt;- FindVariableFeatures(alldata, selection.method = \"vst\", nfeatures = 2000, verbose = FALSE, assay = \"RNA\")))\ntop20 &lt;- head(VariableFeatures(alldata), 20)\n\nLabelPoints(plot = VariableFeaturePlot(alldata), points = top20, repel = TRUE)"
  },
  {
    "objectID": "labs/seurat/seurat_02_dimred.html#meta-dimred_zs",
    "href": "labs/seurat/seurat_02_dimred.html#meta-dimred_zs",
    "title": " Dimensionality Reduction",
    "section": "3 Z-score transformation",
    "text": "3 Z-score transformation\nNow that the genes have been selected, we now proceed with PCA. Since each gene has a different expression level, it means that genes with higher expression values will naturally have higher variation that will be captured by PCA. This means that we need to somehow give each gene a similar weight when performing PCA (see below). The common practice is to center and scale each gene before performing PCA. This exact scaling called Z-score normalization is very useful for PCA, clustering and plotting heatmaps. Additionally, we can use regression to remove any unwanted sources of variation from the dataset, such as cell cycle, sequencing depth, percent mitochondria etc. This is achieved by doing a generalized linear regression using these parameters as co-variates in the model. Then the residuals of the model are taken as the regressed data. Although perhaps not in the best way, batch effect regression can also be done here. By default, variables are scaled in the PCA step and is not done separately. But it could be achieved by running the commands below:\n\nalldata &lt;- ScaleData(alldata, vars.to.regress = c(\"percent_mito\", \"nFeature_RNA\"), assay = \"RNA\")"
  },
  {
    "objectID": "labs/seurat/seurat_02_dimred.html#meta-dimred_pca",
    "href": "labs/seurat/seurat_02_dimred.html#meta-dimred_pca",
    "title": " Dimensionality Reduction",
    "section": "4 PCA",
    "text": "4 PCA\nPerforming PCA has many useful applications and interpretations, which much depends on the data used. In the case of single-cell data, we want to segregate samples based on gene expression patterns in the data.\nTo run PCA, you can use the function RunPCA().\n\nalldata &lt;- RunPCA(alldata, npcs = 50, verbose = F)\n\nWe then plot the first principal components.\n\nwrap_plots(\n    DimPlot(alldata, reduction = \"pca\", group.by = \"orig.ident\", dims = 1:2),\n    DimPlot(alldata, reduction = \"pca\", group.by = \"orig.ident\", dims = 3:4),\n    DimPlot(alldata, reduction = \"pca\", group.by = \"orig.ident\", dims = 5:6),\n    ncol = 3\n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nTo identify which genes (Seurat) or metadata parameters (Scater/Scran) contribute the most to each PC, one can retrieve the loading matrix information. Unfortunately, this is not implemented in Scater/Scran, so you will need to compute PCA using logcounts.\n\nVizDimLoadings(alldata, dims = 1:5, reduction = \"pca\", ncol = 5, balanced = T)\n\n\n\n\n\n\n\n\nWe can also plot the amount of variance explained by each PC.\n\nElbowPlot(alldata, reduction = \"pca\", ndims = 50)\n\n\n\n\n\n\n\n\nBased on this plot, we can see that the top 8 PCs retain a lot of information, while other PCs contain progressively less. However, it is still advisable to use more PCs since they might contain information about rare cell types (such as platelets and DCs in this dataset)\nWith the scater package we can check how different metadata variables contribute to each PCs. This can be important to look at to understand different biases you may have in your data.\n\nscater::plotExplanatoryPCs(as.SingleCellExperiment(alldata), nvars_to_plot = 15, npcs_to_plot = 20)\n\n\n\n\n\n\n\n\nClearly, orig,ident (and ident which is created by SingleCellExperiment) clearly contributes to many of the PCs and PC7 is trongly influenced by cell cycle,"
  },
  {
    "objectID": "labs/seurat/seurat_02_dimred.html#meta-dimred_tsne",
    "href": "labs/seurat/seurat_02_dimred.html#meta-dimred_tsne",
    "title": " Dimensionality Reduction",
    "section": "5 tSNE",
    "text": "5 tSNE\nWe will now run BH-tSNE.\n\nalldata &lt;- RunTSNE(\n    alldata,\n    reduction = \"pca\", dims = 1:30,\n    perplexity = 30,\n    max_iter = 1000,\n    theta = 0.5,\n    eta = 200,\n    num_threads = 0\n)\n# see ?Rtsne and ?RunTSNE for more info\n\nWe plot the tSNE scatterplot colored by dataset. We can clearly see the effect of batches present in the dataset.\n\nDimPlot(alldata, reduction = \"tsne\", group.by = \"orig.ident\")"
  },
  {
    "objectID": "labs/seurat/seurat_02_dimred.html#meta-dimred_umap",
    "href": "labs/seurat/seurat_02_dimred.html#meta-dimred_umap",
    "title": " Dimensionality Reduction",
    "section": "6 UMAP",
    "text": "6 UMAP\nWe can now run UMAP for cell embeddings.\n\nalldata &lt;- RunUMAP(\n    alldata,\n    reduction = \"pca\",\n    dims = 1:30,\n    n.components = 2,\n    n.neighbors = 30,\n    n.epochs = 200,\n    min.dist = 0.3,\n    learning.rate = 1,\n    spread = 1\n)\n# see ?RunUMAP for more info\n\nA feature of UMAP is that it is not limited by the number of dimensions the data cen be reduced into (unlike tSNE). We can simply reduce the dimentions altering the n.components parameter. So here we will create a UMAP with 10 dimensions.\nIn Seurat, we can add in additional reductions, by default they are named “pca”, “umap”, “tsne” etc. depending on the function you run. Here we will specify an alternative name for the umap with the reduction.name parameter.\n\nalldata &lt;- RunUMAP(\n    alldata,\n    reduction.name = \"UMAP10_on_PCA\",\n    reduction = \"pca\",\n    dims = 1:30,\n    n.components = 10,\n    n.neighbors = 30,\n    n.epochs = 200,\n    min.dist = 0.3,\n    learning.rate = 1,\n    spread = 1\n)\n# see ?RunUMAP for more info\n\nUMAP is plotted colored per dataset. Although less distinct as in the tSNE, we still see quite an effect of the different batches in the data.\n\nwrap_plots(\n    DimPlot(alldata, reduction = \"umap\", group.by = \"orig.ident\") + ggplot2::ggtitle(label = \"UMAP_on_PCA\"),\n    DimPlot(alldata, reduction = \"UMAP10_on_PCA\", group.by = \"orig.ident\", dims = 1:2) + ggplot2::ggtitle(label = \"UMAP10_on_PCA\"),\n    DimPlot(alldata, reduction = \"UMAP10_on_PCA\", group.by = \"orig.ident\", dims = 3:4) + ggplot2::ggtitle(label = \"UMAP10_on_PCA\"),\n    ncol = 3\n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nWe can now plot PCA, UMAP and tSNE side by side for comparison. Have a look at the UMAP and tSNE. What similarities/differences do you see? Can you explain the differences based on what you learned during the lecture? Also, we can conclude from the dimensionality reductions that our dataset contains a batch effect that needs to be corrected before proceeding to clustering and differential gene expression analysis.\n\nwrap_plots(\n    DimPlot(alldata, reduction = \"pca\", group.by = \"orig.ident\"),\n    DimPlot(alldata, reduction = \"tsne\", group.by = \"orig.ident\"),\n    DimPlot(alldata, reduction = \"umap\", group.by = \"orig.ident\"),\n    ncol = 3\n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nWe have now done Variable gene selection, PCA and UMAP with the settings we selected for you. Test a few different ways of selecting variable genes, number of PCs for UMAP and check how it influences your embedding."
  },
  {
    "objectID": "labs/seurat/seurat_02_dimred.html#meta-dimred_zsg",
    "href": "labs/seurat/seurat_02_dimred.html#meta-dimred_zsg",
    "title": " Dimensionality Reduction",
    "section": "7 Z-scores & DR graphs",
    "text": "7 Z-scores & DR graphs\nAlthough running a second dimensionality reduction (i.e tSNE or UMAP) on PCA would be a standard approach (because it allows higher computation efficiency), the options are actually limitless. Below we will show a couple of other common options such as running directly on the scaled data (z-scores) (which was used for PCA) or on a graph built from scaled data. We will only work with UMAPs, but the same applies for tSNE.\n\n7.1 UMAP from z-scores\nTo run tSNE or UMAP on the scaled data, one first needs to select the number of variables to use. This is because including dimensions that do contribute to the separation of your cell types will in the end mask those differences. Another reason for it is because running with all genes/features also will take longer or might be computationally unfeasible. Therefore we will use the scaled data of the highly variable genes.\n\nalldata &lt;- RunUMAP(\n    alldata,\n    reduction.name = \"UMAP_on_ScaleData\",\n    features = VariableFeatures(alldata),\n    assay = \"RNA\",\n    n.components = 2,\n    n.neighbors = 30,\n    n.epochs = 200,\n    min.dist = 0.3,\n    learning.rate = 1,\n    spread = 1\n)\n\n\n\n7.2 UMAP from graph\nTo run tSNE or UMAP on the a graph, we first need to build a graph from the data. In fact, both tSNE and UMAP first build a graph from the data using a specified distance matrix and then optimize the embedding. Since a graph is just a matrix containing distances from cell to cell and as such, you can run either UMAP or tSNE using any other distance metric desired. Euclidean and Correlation are usually the most commonly used.\n\n#OBS! Skip for now, known issue with later version of umap-learn in Seurat5\n# have 0.5.6 now, tested downgrading to 0.5.4 or 0.5.3 but still have same error.\n\n# Build Graph\nalldata &lt;- FindNeighbors(alldata,\n    reduction = \"pca\",\n    assay = \"RNA\",\n    k.param = 20,\n    features = VariableFeatures(alldata)\n)\n\nalldata &lt;- RunUMAP(alldata,\n    reduction.name = \"UMAP_on_Graph\",\n    umap.method = \"umap-learn\",\n    graph = \"RNA_snn\",\n    n.epochs = 200,\n    assay = \"RNA\"\n)\n\nWe can now plot the UMAP comparing both on PCA vs ScaledSata vs Graph.\n\np1 &lt;- DimPlot(alldata, reduction = \"umap\", group.by = \"orig.ident\") + ggplot2::ggtitle(label = \"UMAP_on_PCA\")\np2 &lt;- DimPlot(alldata, reduction = \"UMAP_on_ScaleData\", group.by = \"orig.ident\") + ggplot2::ggtitle(label = \"UMAP_on_ScaleData\")\np3 &lt;- DimPlot(alldata, reduction = \"UMAP_on_Graph\", group.by = \"orig.ident\") + ggplot2::ggtitle(label = \"UMAP_on_Graph\")\nwrap_plots(p1, p2, p3, ncol = 3) + plot_layout(guides = \"collect\")"
  },
  {
    "objectID": "labs/seurat/seurat_02_dimred.html#meta-dimred_plotgenes",
    "href": "labs/seurat/seurat_02_dimred.html#meta-dimred_plotgenes",
    "title": " Dimensionality Reduction",
    "section": "8 Genes of interest",
    "text": "8 Genes of interest\nLet’s plot some marker genes for different cell types onto the embedding.\n\n\n\nMarkers\nCell Type\n\n\n\n\nCD3E\nT cells\n\n\nCD3E CD4\nCD4+ T cells\n\n\nCD3E CD8A\nCD8+ T cells\n\n\nGNLY, NKG7\nNK cells\n\n\nMS4A1\nB cells\n\n\nCD14, LYZ, CST3, MS4A7\nCD14+ Monocytes\n\n\nFCGR3A, LYZ, CST3, MS4A7\nFCGR3A+ Monocytes\n\n\nFCER1A, CST3\nDCs\n\n\n\n\nmyfeatures &lt;- c(\"CD3E\", \"CD4\", \"CD8A\", \"NKG7\", \"GNLY\", \"MS4A1\", \"CD14\", \"LYZ\", \"MS4A7\", \"FCGR3A\", \"CST3\", \"FCER1A\")\nFeaturePlot(alldata, reduction = \"umap\", dims = 1:2, features = myfeatures, ncol = 4, order = T) +\n    NoLegend() + NoAxes() + NoGrid()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nSelect some of your dimensionality reductions and plot some of the QC stats that were calculated in the previous lab. Can you see if some of the separation in your data is driven by quality of the cells?\n\n\n\nmyfeatures &lt;- c(\"nCount_RNA\",\"nFeature_RNA\", \"percent_mito\",\"percent_ribo\",\"percent_hb\",\"percent_plat\")\nFeaturePlot(alldata, reduction = \"umap\", dims = 1:2, features = myfeatures, ncol = 3, order = T) +\n    NoLegend() + NoAxes() + NoGrid()"
  },
  {
    "objectID": "labs/seurat/seurat_02_dimred.html#meta-dimred_save",
    "href": "labs/seurat/seurat_02_dimred.html#meta-dimred_save",
    "title": " Dimensionality Reduction",
    "section": "9 Save data",
    "text": "9 Save data\nWe can finally save the object for use in future steps.\n\nsaveRDS(alldata, \"data/covid/results/seurat_covid_qc_dr.rds\")"
  },
  {
    "objectID": "labs/seurat/seurat_02_dimred.html#meta-session",
    "href": "labs/seurat/seurat_02_dimred.html#meta-session",
    "title": " Dimensionality Reduction",
    "section": "10 Session info",
    "text": "10 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] scran_1.30.0                scuttle_1.12.0             \n [3] SingleCellExperiment_1.24.0 SummarizedExperiment_1.32.0\n [5] Biobase_2.62.0              GenomicRanges_1.54.1       \n [7] GenomeInfoDb_1.38.1         IRanges_2.36.0             \n [9] S4Vectors_0.40.2            BiocGenerics_0.48.1        \n[11] MatrixGenerics_1.14.0       matrixStats_1.4.1          \n[13] patchwork_1.2.0             ggplot2_3.5.1              \n[15] Seurat_5.1.0                SeuratObject_5.0.2         \n[17] sp_2.1-4                   \n\nloaded via a namespace (and not attached):\n  [1] RcppAnnoy_0.0.22          splines_4.3.3            \n  [3] later_1.3.2               bitops_1.0-8             \n  [5] tibble_3.2.1              polyclip_1.10-7          \n  [7] fastDummies_1.7.4         lifecycle_1.0.4          \n  [9] edgeR_4.0.16              globals_0.16.3           \n [11] lattice_0.22-6            MASS_7.3-60.0.1          \n [13] magrittr_2.0.3            limma_3.58.1             \n [15] plotly_4.10.4             rmarkdown_2.28           \n [17] yaml_2.3.10               metapod_1.10.0           \n [19] httpuv_1.6.15             sctransform_0.4.1        \n [21] spam_2.10-0               spatstat.sparse_3.1-0    \n [23] reticulate_1.39.0         cowplot_1.1.3            \n [25] pbapply_1.7-2             RColorBrewer_1.1-3       \n [27] abind_1.4-5               zlibbioc_1.48.0          \n [29] Rtsne_0.17                purrr_1.0.2              \n [31] RCurl_1.98-1.16           GenomeInfoDbData_1.2.11  \n [33] ggrepel_0.9.6             irlba_2.3.5.1            \n [35] listenv_0.9.1             spatstat.utils_3.1-0     \n [37] goftest_1.2-3             RSpectra_0.16-2          \n [39] spatstat.random_3.2-3     dqrng_0.3.2              \n [41] fitdistrplus_1.2-1        parallelly_1.38.0        \n [43] DelayedMatrixStats_1.24.0 leiden_0.4.3.1           \n [45] codetools_0.2-20          DelayedArray_0.28.0      \n [47] tidyselect_1.2.1          farver_2.1.2             \n [49] viridis_0.6.5             ScaledMatrix_1.10.0      \n [51] spatstat.explore_3.2-6    jsonlite_1.8.8           \n [53] BiocNeighbors_1.20.0      progressr_0.14.0         \n [55] ggridges_0.5.6            survival_3.7-0           \n [57] scater_1.30.1             tools_4.3.3              \n [59] ica_1.0-3                 Rcpp_1.0.13              \n [61] glue_1.7.0                gridExtra_2.3            \n [63] SparseArray_1.2.2         xfun_0.47                \n [65] dplyr_1.1.4               withr_3.0.1              \n [67] fastmap_1.2.0             bluster_1.12.0           \n [69] fansi_1.0.6               digest_0.6.37            \n [71] rsvd_1.0.5                R6_2.5.1                 \n [73] mime_0.12                 colorspace_2.1-1         \n [75] scattermore_1.2           tensor_1.5               \n [77] spatstat.data_3.1-2       utf8_1.2.4               \n [79] tidyr_1.3.1               generics_0.1.3           \n [81] data.table_1.15.4         httr_1.4.7               \n [83] htmlwidgets_1.6.4         S4Arrays_1.2.0           \n [85] uwot_0.1.16               pkgconfig_2.0.3          \n [87] gtable_0.3.5              lmtest_0.9-40            \n [89] XVector_0.42.0            htmltools_0.5.8.1        \n [91] dotCall64_1.1-1           scales_1.3.0             \n [93] png_0.1-8                 knitr_1.48               \n [95] reshape2_1.4.4            nlme_3.1-165             \n [97] zoo_1.8-12                stringr_1.5.1            \n [99] KernSmooth_2.23-24        vipor_0.4.7              \n[101] parallel_4.3.3            miniUI_0.1.1.1           \n[103] pillar_1.9.0              grid_4.3.3               \n[105] vctrs_0.6.5               RANN_2.6.2               \n[107] promises_1.3.0            BiocSingular_1.18.0      \n[109] beachmat_2.18.0           xtable_1.8-4             \n[111] cluster_2.1.6             beeswarm_0.4.0           \n[113] evaluate_0.24.0           cli_3.6.3                \n[115] locfit_1.5-9.9            compiler_4.3.3           \n[117] rlang_1.1.4               crayon_1.5.3             \n[119] future.apply_1.11.2       labeling_0.4.3           \n[121] plyr_1.8.9                ggbeeswarm_0.7.2         \n[123] stringi_1.8.4             viridisLite_0.4.2        \n[125] deldir_2.0-4              BiocParallel_1.36.0      \n[127] munsell_0.5.1             lazyeval_0.2.2           \n[129] spatstat.geom_3.2-9       Matrix_1.6-5             \n[131] RcppHNSW_0.6.0            sparseMatrixStats_1.14.0 \n[133] future_1.34.0             statmod_1.5.0            \n[135] shiny_1.9.1               ROCR_1.0-11              \n[137] igraph_2.0.3"
  },
  {
    "objectID": "labs/seurat/seurat_03_integration.html",
    "href": "labs/seurat/seurat_03_integration.html",
    "title": " Data Integration",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified.\nIn this tutorial we will look at different ways of integrating multiple single cell RNA-seq datasets. We will explore a few different methods to correct for batch effects across datasets. Seurat uses the data integration method presented in Comprehensive Integration of Single Cell Data, while Scran and Scanpy use a mutual Nearest neighbour method (MNN). Below you can find a list of some methods for single data integration:"
  },
  {
    "objectID": "labs/seurat/seurat_03_integration.html#meta-int_prep",
    "href": "labs/seurat/seurat_03_integration.html#meta-int_prep",
    "title": " Data Integration",
    "section": "1 Data preparation",
    "text": "1 Data preparation\nLet’s first load necessary libraries and the data saved in the previous lab.\n\nsuppressPackageStartupMessages({\n    library(Seurat)\n    library(ggplot2)\n    library(patchwork)\n    library(basilisk)\n})\n\ncondapath = \"/Users/asabjor/miniconda3/envs/seurat5_u\"\n\n\n# download pre-computed data if missing or long compute\nfetch_data &lt;- TRUE\n\n# url for source and intermediate data\npath_data &lt;- \"https://nextcloud.dc.scilifelab.se/public.php/webdav\"\ncurl_upass &lt;- \"-u zbC5fr2LbEZ9rSE:scRNAseq2025\"\n\npath_file &lt;- \"data/covid/results/seurat_covid_qc_dr.rds\"\nif (!dir.exists(dirname(path_file))) dir.create(dirname(path_file), recursive = TRUE)\nif (fetch_data && !file.exists(path_file)) download.file(url = file.path(path_data, \"covid/results_seurat/seurat_covid_qc_dr.rds\"), destfile = path_file, method = \"curl\", extra = curl_upass)\n\nalldata &lt;- readRDS(path_file)\nprint(names(alldata@reductions))\n\n[1] \"pca\"               \"umap\"              \"tsne\"             \n[4] \"UMAP10_on_PCA\"     \"UMAP_on_ScaleData\"\n\n\nWith Seurat5 we can split the RNA assay into multiple Layers with one count matrix and one data matrix per sample. When we then run FindVariableFeatures on the object it will run it for each of the samples separately, but also compute the overall variable features by combining their ranks.\n\n# get the variable genes from all the datasets without batch information.\nhvgs_old = VariableFeatures(alldata)\n\n# now split the object into layers\nalldata[[\"RNA\"]] &lt;- split(alldata[[\"RNA\"]], f = alldata$orig.ident)\n\n# detect HVGs\nalldata &lt;- FindVariableFeatures(alldata, selection.method = \"vst\", nfeatures = 2000, verbose = FALSE)\n\n# to get the HVGs for each layer we have to fetch them individually\ndata.layers &lt;- Layers(alldata)[grep(\"data.\",Layers(alldata))]\nprint(data.layers)\n\n[1] \"data.covid_1\"  \"data.covid_15\" \"data.covid_16\" \"data.covid_17\"\n[5] \"data.ctrl_5\"   \"data.ctrl_13\"  \"data.ctrl_14\"  \"data.ctrl_19\" \n\nhvgs_per_dataset &lt;- lapply(data.layers, function(x) VariableFeatures(alldata, layer = x) )\nnames(hvgs_per_dataset) = data.layers\n\n# also add in the variable genes that was selected on the whole dataset and the old ones \nhvgs_per_dataset$all &lt;- VariableFeatures(alldata)\nhvgs_per_dataset$old &lt;- hvgs_old\n\ntemp &lt;- unique(unlist(hvgs_per_dataset))\noverlap &lt;- sapply( hvgs_per_dataset , function(x) { temp %in% x } )\npheatmap::pheatmap(t(overlap*1),cluster_rows = F ,\n                   color = c(\"grey90\",\"grey20\"))\n\n\n\n\n\n\n\n\nAs you can see, there are a lot of genes that are variable in just one dataset. There are also some genes in the gene set that was selected using all the data that are not variable in any of the individual datasets. These are most likely genes driven by batch effects.\nA better way to select features for integration is to combine the information on variable genes across the dataset. This is what we have in the all section where the ranks of the variable features in the different datasets is combined.\nFor all downstream integration we will use this set of genes so that it is comparable across the methods. Before doing anything else we need to rerun ScaleData and PCA with that set of genes.\n\nhvgs_all = hvgs_per_dataset$all\n\nalldata = ScaleData(alldata, features = hvgs_all, vars.to.regress = c(\"percent_mito\", \"nFeature_RNA\"))\nalldata = RunPCA(alldata, features = hvgs_all, verbose = FALSE)"
  },
  {
    "objectID": "labs/seurat/seurat_03_integration.html#cca",
    "href": "labs/seurat/seurat_03_integration.html#cca",
    "title": " Data Integration",
    "section": "2 CCA",
    "text": "2 CCA\nIn Seurat v4 we run the integration in two steps, first finding anchors between datasets with FindIntegrationAnchors() and then running the actual integration with IntegrateData(). Since Seurat v5 this is done in a single command using the function IntegrateLayers(), we specify the name for the integration as integrated_cca.\n\nalldata &lt;- IntegrateLayers(object = alldata, \n                           method = CCAIntegration, orig.reduction = \"pca\", \n                           new.reduction = \"integrated_cca\", verbose = FALSE)\n\nWe should now have a new dimensionality reduction slot (integrated_cca) in the object:\n\nnames(alldata@reductions)\n\n[1] \"pca\"               \"umap\"              \"tsne\"             \n[4] \"UMAP10_on_PCA\"     \"UMAP_on_ScaleData\" \"integrated_cca\"   \n\n\nUsing this new integrated dimensionality reduction we can now run UMAP and tSNE on that object, and we again specify the names of the new reductions so that the old UMAP and tSNE are not overwritten.\n\nalldata &lt;- RunUMAP(alldata, reduction = \"integrated_cca\", dims = 1:30, reduction.name = \"umap_cca\")\nalldata &lt;- RunTSNE(alldata, reduction = \"integrated_cca\", dims = 1:30, reduction.name = \"tsne_cca\")\n\nnames(alldata@reductions)\n\n[1] \"pca\"               \"umap\"              \"tsne\"             \n[4] \"UMAP10_on_PCA\"     \"UMAP_on_ScaleData\" \"integrated_cca\"   \n[7] \"umap_cca\"          \"tsne_cca\"         \n\n\nWe can now plot the unintegrated and the integrated space reduced dimensions.\n\nwrap_plots(\n  DimPlot(alldata, reduction = \"pca\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"PCA raw_data\"),\n  DimPlot(alldata, reduction = \"tsne\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"tSNE raw_data\"),\n  DimPlot(alldata, reduction = \"umap\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"UMAP raw_data\"),\n  \n  DimPlot(alldata, reduction = \"integrated_cca\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"CCA integrated\"),\n  DimPlot(alldata, reduction = \"tsne_cca\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"tSNE integrated\"),\n  DimPlot(alldata, reduction = \"umap_cca\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"UMAP integrated\"),\n  ncol = 3\n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n2.1 Marker genes\nLet’s plot some marker genes for different cell types onto the embedding.\n\n\n\nMarkers\nCell Type\n\n\n\n\nCD3E\nT cells\n\n\nCD3E CD4\nCD4+ T cells\n\n\nCD3E CD8A\nCD8+ T cells\n\n\nGNLY, NKG7\nNK cells\n\n\nMS4A1\nB cells\n\n\nCD14, LYZ, CST3, MS4A7\nCD14+ Monocytes\n\n\nFCGR3A, LYZ, CST3, MS4A7\nFCGR3A+ Monocytes\n\n\nFCER1A, CST3\nDCs\n\n\n\n\nmyfeatures &lt;- c(\"CD3E\", \"CD4\", \"CD8A\", \"NKG7\", \"GNLY\", \"MS4A1\", \"CD14\", \"LYZ\", \"MS4A7\", \"FCGR3A\", \"CST3\", \"FCER1A\")\nFeaturePlot(alldata, reduction = \"umap_cca\", dims = 1:2, features = myfeatures, ncol = 4, order = T) + NoLegend() + NoAxes() + NoGrid()"
  },
  {
    "objectID": "labs/seurat/seurat_03_integration.html#harmony",
    "href": "labs/seurat/seurat_03_integration.html#harmony",
    "title": " Data Integration",
    "section": "3 Harmony",
    "text": "3 Harmony\nAn alternative method for integration is Harmony, for more details on the method, please see their paper Nat. Methods.\nThis method runs integration on a dimensionality reduction, in most applications the PCA. So first, we will rerun scaling and PCA with the same set of genes that were used for the CCA integration.\nOBS! Make sure to revert back to the RNA assay.\n\nalldata.int@active.assay = \"RNA\"\nVariableFeatures(alldata.int) = hvgs_all\nalldata.int = ScaleData(alldata.int, vars.to.regress = c(\"percent_mito\", \"nFeature_RNA\"))\nalldata.int = RunPCA(alldata.int, reduction.name = \"pca_harmony\")\n\nNow we are ready to run Harmony.\n\nlibrary(harmony)\n\nalldata.int &lt;- RunHarmony(\n  alldata.int,\n  group.by.vars = \"orig.ident\",\n  reduction.use = \"pca_harmony\",\n  dims.use = 1:50,\n  assay.use = \"RNA\")\n\nHarmony will create another reduction slot in your seurat object with the name harmony, so now we can use that reduction instead of PCA to run UMAP.\n\nalldata.int &lt;- RunUMAP(alldata.int, dims = 1:50, reduction = \"harmony\", reduction.name = \"umap_harmony\")\n\nDimPlot(alldata.int, reduction = \"umap_harmony\", group.by = \"orig.ident\") + NoAxes() + ggtitle(\"Harmony UMAP\")"
  },
  {
    "objectID": "labs/seurat/seurat_03_integration.html#scanorama",
    "href": "labs/seurat/seurat_03_integration.html#scanorama",
    "title": " Data Integration",
    "section": "4 Scanorama",
    "text": "4 Scanorama\nAnother integration method is Scanorama (see Nat. Biotech.). This method is implemented in python, but we can run it through the Reticulate package.\nWe will use the split list of samples and transpose the matrix as scanorama requires rows as samples and columns as genes. We also create a list with the highly variables genes for each sample.\n\nassaylist &lt;- list()\ngenelist &lt;- list()\nfor(i in 1:length(alldata.list)) {\n  assaylist[[i]] &lt;- t(as.matrix(GetAssayData(alldata.list[[i]], \"data\")[hvgs_all,]))\n  genelist[[i]] &lt;- hvgs_all\n}\n\nlapply(assaylist,dim)\n\n[[1]]\n[1]  873 2000\n\n[[2]]\n[1]  557 2000\n\n[[3]]\n[1]  357 2000\n\n[[4]]\n[1] 1050 2000\n\n[[5]]\n[1] 1034 2000\n\n[[6]]\n[1] 1125 2000\n\n[[7]]\n[1]  999 2000\n\n[[8]]\n[1] 1139 2000\n\n\nThen, we use the scanorama function through reticulate. The integrated data is added back into the Seurat object as a new Reduction.\n\n# Activate scanorama Python venv\nscanorama &lt;- reticulate::import(\"scanorama\")\n\nintegrated.data &lt;- scanorama$integrate(datasets_full = assaylist,\n                                       genes_list = genelist )\n\n# Now we create a new dim reduction object in the format that Seurat uses\nintdimred &lt;- do.call(rbind, integrated.data[[1]])\ncolnames(intdimred) &lt;- paste0(\"PC_\", 1:100)\nrownames(intdimred) &lt;- colnames(alldata.int)\n\n# Add standard deviations in order to draw Elbow Plots in Seurat\nstdevs &lt;- apply(intdimred, MARGIN = 2, FUN = sd)\n\n# Create a new dim red object.\nalldata.int[[\"scanorama\"]] &lt;- CreateDimReducObject(\n  embeddings = intdimred,\n  stdev      = stdevs,\n  key        = \"PC_\",\n  assay      = \"RNA\")\n\n\n#Here we use all PCs computed from Scanorama for UMAP calculation\nalldata.int &lt;- RunUMAP(alldata.int, dims = 1:100, reduction = \"scanorama\",reduction.name = \"umap_scanorama\")\n\nDimPlot(alldata.int, reduction = \"umap_scanorama\", group.by = \"orig.ident\") + NoAxes() + ggtitle(\"Harmony UMAP\")"
  },
  {
    "objectID": "labs/seurat/seurat_03_integration.html#overview-all-methods",
    "href": "labs/seurat/seurat_03_integration.html#overview-all-methods",
    "title": " Data Integration",
    "section": "5 Overview all methods",
    "text": "5 Overview all methods\nNow we will plot UMAPS with all three integration methods side by side.\n\np1 &lt;- DimPlot(alldata, reduction = \"umap\", group.by = \"orig.ident\") + ggtitle(\"UMAP raw_data\")\np2 &lt;- DimPlot(alldata, reduction = \"umap_cca\", group.by = \"orig.ident\") + ggtitle(\"UMAP CCA\")\np3 &lt;- DimPlot(alldata, reduction = \"umap_harmony\", group.by = \"orig.ident\") + ggtitle(\"UMAP Harmony\")\np4 &lt;- DimPlot(alldata, reduction = \"umap_scanorama\", group.by = \"orig.ident\")+ggtitle(\"UMAP Scanorama\")\n\nwrap_plots(p1, p2, p3, p4, nrow = 2) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nLook at the different integration results, which one do you think looks the best? How would you motivate selecting one method over the other? How do you think you could best evaluate if the integration worked well?\n\n\nLet’s save the integrated data for further analysis.\n\nsaveRDS(alldata,\"data/covid/results/seurat_covid_qc_dr_int.rds\")"
  },
  {
    "objectID": "labs/seurat/seurat_03_integration.html#extra-task",
    "href": "labs/seurat/seurat_03_integration.html#extra-task",
    "title": " Data Integration",
    "section": "6 Extra task",
    "text": "6 Extra task\nYou have now done the Seurat integration with CCA which is quite slow. There are other options in the FindIntegrationAnchors() function. Try rerunning the integration with rpca and/or rlsi and create a new UMAP. Compare the results."
  },
  {
    "objectID": "labs/seurat/seurat_03_integration.html#meta-session",
    "href": "labs/seurat/seurat_03_integration.html#meta-session",
    "title": " Data Integration",
    "section": "7 Session info",
    "text": "7 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5_u/lib/libopenblasp-r0.3.28.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] basilisk_1.14.1    patchwork_1.3.0    ggplot2_3.5.1      Seurat_5.1.0      \n[5] SeuratObject_5.0.2 sp_2.1-4          \n\nloaded via a namespace (and not attached):\n  [1] deldir_2.0-4           pbapply_1.7-2          gridExtra_2.3         \n  [4] rlang_1.1.4            magrittr_2.0.3         RcppAnnoy_0.0.22      \n  [7] spatstat.geom_3.3-4    matrixStats_1.5.0      ggridges_0.5.6        \n [10] compiler_4.3.3         dir.expiry_1.10.0      png_0.1-8             \n [13] vctrs_0.6.5            reshape2_1.4.4         stringr_1.5.1         \n [16] pkgconfig_2.0.3        fastmap_1.2.0          labeling_0.4.3        \n [19] promises_1.3.2         rmarkdown_2.29         purrr_1.0.2           \n [22] xfun_0.50              jsonlite_1.8.9         goftest_1.2-3         \n [25] later_1.4.1            spatstat.utils_3.1-2   irlba_2.3.5.1         \n [28] parallel_4.3.3         cluster_2.1.8          R6_2.5.1              \n [31] ica_1.0-3              stringi_1.8.4          RColorBrewer_1.1-3    \n [34] spatstat.data_3.1-4    reticulate_1.40.0      parallelly_1.41.0     \n [37] spatstat.univar_3.1-1  lmtest_0.9-40          scattermore_1.2       \n [40] Rcpp_1.0.13-1          knitr_1.49             tensor_1.5            \n [43] future.apply_1.11.2    zoo_1.8-12             sctransform_0.4.1     \n [46] httpuv_1.6.15          Matrix_1.6-5           splines_4.3.3         \n [49] igraph_2.0.3           tidyselect_1.2.1       abind_1.4-5           \n [52] yaml_2.3.10            spatstat.random_3.3-2  codetools_0.2-20      \n [55] miniUI_0.1.1.1         spatstat.explore_3.3-4 listenv_0.9.1         \n [58] lattice_0.22-6         tibble_3.2.1           plyr_1.8.9            \n [61] basilisk.utils_1.14.1  withr_3.0.2            shiny_1.10.0          \n [64] ROCR_1.0-11            evaluate_1.0.1         Rtsne_0.17            \n [67] future_1.34.0          fastDummies_1.7.4      survival_3.8-3        \n [70] polyclip_1.10-7        fitdistrplus_1.2-2     filelock_1.0.3        \n [73] pillar_1.10.1          KernSmooth_2.23-26     plotly_4.10.4         \n [76] generics_0.1.3         RcppHNSW_0.6.0         munsell_0.5.1         \n [79] scales_1.3.0           globals_0.16.3         xtable_1.8-4          \n [82] RhpcBLASctl_0.23-42    glue_1.8.0             pheatmap_1.0.12       \n [85] lazyeval_0.2.2         tools_4.3.3            data.table_1.15.4     \n [88] RSpectra_0.16-2        RANN_2.6.2             leiden_0.4.3.1        \n [91] dotCall64_1.2          cowplot_1.1.3          grid_4.3.3            \n [94] tidyr_1.3.1            colorspace_2.1-1       nlme_3.1-165          \n [97] cli_3.6.3              spatstat.sparse_3.1-0  spam_2.11-0           \n[100] viridisLite_0.4.2      dplyr_1.1.4            uwot_0.1.16           \n[103] gtable_0.3.6           digest_0.6.37          progressr_0.15.1      \n[106] ggrepel_0.9.6          htmlwidgets_1.6.4      farver_2.1.2          \n[109] htmltools_0.5.8.1      lifecycle_1.0.4        httr_1.4.7            \n[112] mime_0.12              harmony_1.2.3          MASS_7.3-60.0.1"
  },
  {
    "objectID": "labs/seurat/seurat_04_clustering.html",
    "href": "labs/seurat/seurat_04_clustering.html",
    "title": " Clustering",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified.\nIn this tutorial, we will continue the analysis of the integrated dataset. We will use the integrated PCA or CCA to perform the clustering. First, we will construct a \\(k\\)-nearest neighbor graph in order to perform a clustering on the graph. We will also show how to perform hierarchical clustering and k-means clustering on the selected space.\nLet’s first load all necessary libraries and also the integrated dataset from the previous step.\nsuppressPackageStartupMessages({\n    library(Seurat)\n    library(patchwork)\n    library(ggplot2)\n    library(pheatmap)\n    library(clustree)\n})\n# download pre-computed data if missing or long compute\nfetch_data &lt;- TRUE\n\n# url for source and intermediate data\npath_data &lt;- \"https://nextcloud.dc.scilifelab.se/public.php/webdav\"\ncurl_upass &lt;- \"-u zbC5fr2LbEZ9rSE:scRNAseq2025\"\n\npath_file &lt;- \"data/covid/results/seurat_covid_qc_dr_int.rds\"\nif (!dir.exists(dirname(path_file))) dir.create(dirname(path_file), recursive = TRUE)\nif (fetch_data && !file.exists(path_file)) download.file(url = file.path(path_data, \"covid/results_seurat/seurat_covid_qc_dr_int.rds\"), destfile = path_file, method = \"curl\", extra = curl_upass)\n\nalldata &lt;- readRDS(path_file)\nprint(names(alldata@reductions))\n\n [1] \"pca\"               \"umap\"              \"tsne\"             \n [4] \"UMAP10_on_PCA\"     \"UMAP_on_ScaleData\" \"integrated_cca\"   \n [7] \"umap_cca\"          \"tsne_cca\"          \"harmony\"          \n[10] \"umap_harmony\"      \"scanorama\"         \"scanoramaC\"       \n[13] \"umap_scanorama\"    \"umap_scanoramaC\""
  },
  {
    "objectID": "labs/seurat/seurat_04_clustering.html#meta-clust_graphclust",
    "href": "labs/seurat/seurat_04_clustering.html#meta-clust_graphclust",
    "title": " Clustering",
    "section": "1 Graph clustering",
    "text": "1 Graph clustering\nThe procedure of clustering on a Graph can be generalized as 3 main steps:\n- Build a kNN graph from the data.\n- Prune spurious connections from kNN graph (optional step). This is a SNN graph.\n- Find groups of cells that maximizes the connections within the group compared other groups.\n\n1.1 Building kNN / SNN graph\nThe first step into graph clustering is to construct a k-nn graph, in case you don’t have one. For this, we will use the PCA space. Thus, as done for dimensionality reduction, we will use ony the top N PCA dimensions for this purpose (the same used for computing UMAP / tSNE).\nAs we can see above, the Seurat function FindNeighbors() already computes both the KNN and SNN graphs, in which we can control the minimal percentage of shared neighbours to be kept. See ?FindNeighbors for additional options.\nThe main options to consider are:\n\ndims - the number of dimensions from the initial reduction to include when calculating distances between cells.\nk.param - the number of neighbors per cell to include in the KNN graph.\nprune.SNN - sets the cutoff for Jaccard index when pruning the graph.\n\n\n# use the CCA integration to create the neighborhood graph.\nalldata &lt;- FindNeighbors(alldata, dims = 1:30, k.param = 60, prune.SNN = 1 / 15, reduction =  \"integrated_cca\")\n\n# check the names for graphs in the object.\nnames(alldata@graphs)\n\n[1] \"RNA_nn\"  \"RNA_snn\"\n\n\nWe can take a look at the kNN and SNN graphs. The kNN graph is a matrix where every connection between cells is represented as \\(1\\)s. This is called a unweighted graph (default in Seurat). In the SNN graph on the other hand, some cell connections have more importance than others, and the graph scales from \\(0\\) to a maximum distance (in this case \\(1\\)). Usually, the smaller the distance, the closer two points are, and stronger is their connection. This is called a weighted graph. Both weighted and unweighted graphs are suitable for clustering, but clustering on unweighted graphs is faster for large datasets (&gt; 100k cells).\n\npheatmap(alldata@graphs$RNA_nn[1:200, 1:200],\n    col = c(\"white\", \"black\"), border_color = \"grey90\", main = \"KNN graph\",\n    legend = F, cluster_rows = F, cluster_cols = F, fontsize = 2\n)\n\n\n\n\n\n\n\npheatmap(alldata@graphs$RNA_snn[1:200, 1:200],\n    col = colorRampPalette(c(\"white\", \"yellow\", \"red\"))(100),\n    border_color = \"grey90\", main = \"SNN graph\",\n    legend = F, cluster_rows = F, cluster_cols = F, fontsize = 2\n)\n\n\n\n\n\n\n\n\n\n\n1.2 Clustering on a graph\nOnce the graph is built, we can now perform graph clustering. The clustering is done respective to a resolution which can be interpreted as how coarse you want your cluster to be. Higher resolution means higher number of clusters.\nIn Seurat, the function FindClusters() will do a graph-based clustering using “Louvain” algorithim by default (algorithm = 1). To use the leiden algorithm, you need to set it to algorithm = 4. See ?FindClusters for additional options.\nBy default it will run clustering on the SNN graph we created in the previous step, but you can also specify different graphs for clustering with graph.name.\n\n# Clustering with louvain (algorithm 1) and a few different resolutions\nfor (res in c(0.1, 0.25, .5, 1, 1.5, 2)) {\n    alldata &lt;- FindClusters(alldata, graph.name = \"RNA_snn\", resolution = res, algorithm = 1)\n}\n\n# each time you run clustering, the data is stored in meta data columns:\n# seurat_clusters - lastest results only\n# RNA_snn_res.XX - for each different resolution you test.\n\n\nwrap_plots(\n    DimPlot(alldata, reduction = \"umap_cca\", group.by = \"RNA_snn_res.0.1\") + ggtitle(\"louvain_0.1\"),\n    DimPlot(alldata, reduction = \"umap_cca\", group.by = \"RNA_snn_res.0.25\") + ggtitle(\"louvain_0.25\"),\n    DimPlot(alldata, reduction = \"umap_cca\", group.by = \"RNA_snn_res.0.5\") + ggtitle(\"louvain_0.5\"),\n    DimPlot(alldata, reduction = \"umap_cca\", group.by = \"RNA_snn_res.1\") + ggtitle(\"louvain_1\"),\n    DimPlot(alldata, reduction = \"umap_cca\", group.by = \"RNA_snn_res.2\") + ggtitle(\"louvain_2\"),\n    ncol = 3\n)\n\n\n\n\n\n\n\n\nWe can now use the clustree package to visualize how cells are distributed between clusters depending on resolution.\n\nsuppressPackageStartupMessages(library(clustree))\nclustree(alldata@meta.data, prefix = \"RNA_snn_res.\")"
  },
  {
    "objectID": "labs/seurat/seurat_04_clustering.html#meta-clust_kmean",
    "href": "labs/seurat/seurat_04_clustering.html#meta-clust_kmean",
    "title": " Clustering",
    "section": "2 K-means clustering",
    "text": "2 K-means clustering\nK-means is a generic clustering algorithm that has been used in many application areas. In R, it can be applied via the kmeans() function. Typically, it is applied to a reduced dimension representation of the expression data (most often PCA, because of the interpretability of the low-dimensional distances). We need to define the number of clusters in advance. Since the results depend on the initialization of the cluster centers, it is typically recommended to run K-means with multiple starting configurations (via the nstart argument).\n\nfor (k in c(5, 7, 10, 12, 15, 17, 20)) {\n    alldata@meta.data[, paste0(\"kmeans_\", k)] &lt;- kmeans(x = Embeddings(alldata, \"integrated_cca\"), centers = k, nstart = 100)$cluster\n}\n\nwrap_plots(\n    DimPlot(alldata, reduction = \"umap_cca\", group.by = \"kmeans_5\") + ggtitle(\"kmeans_5\"),\n    DimPlot(alldata, reduction = \"umap_cca\", group.by = \"kmeans_10\") + ggtitle(\"kmeans_10\"),\n    DimPlot(alldata, reduction = \"umap_cca\", group.by = \"kmeans_15\") + ggtitle(\"kmeans_15\"),\n    ncol = 3\n)\n\n\n\n\n\n\n\n\n\nclustree(alldata@meta.data, prefix = \"kmeans_\")"
  },
  {
    "objectID": "labs/seurat/seurat_04_clustering.html#meta-clust_hier",
    "href": "labs/seurat/seurat_04_clustering.html#meta-clust_hier",
    "title": " Clustering",
    "section": "3 Hierarchical clustering",
    "text": "3 Hierarchical clustering\n\n3.1 Defining distance between cells\nThe base R stats package already contains a function dist that calculates distances between all pairs of samples. Since we want to compute distances between samples, rather than among genes, we need to transpose the data before applying it to the dist function. This can be done by simply adding the transpose function t() to the data. The distance methods available in dist are: ‘euclidean’, ‘maximum’, ‘manhattan’, ‘canberra’, ‘binary’ or ‘minkowski’.\n\nd &lt;- dist(Embeddings(alldata, \"integrated_cca\"), method = \"euclidean\")\n\nAs you might have realized, correlation is not a method implemented in the dist() function. However, we can create our own distances and transform them to a distance object. We can first compute sample correlations using the cor function.\nAs you already know, correlation range from -1 to 1, where 1 indicates that two samples are closest, -1 indicates that two samples are the furthest and 0 is somewhat in between. This, however, creates a problem in defining distances because a distance of 0 indicates that two samples are closest, 1 indicates that two samples are the furthest and distance of -1 is not meaningful. We thus need to transform the correlations to a positive scale (a.k.a. adjacency):\n\\[adj = \\frac{1- cor}{2}\\]\nOnce we transformed the correlations to a 0-1 scale, we can simply convert it to a distance object using as.dist() function. The transformation does not need to have a maximum of 1, but it is more intuitive to have it at 1, rather than at any other number.\n\n# Compute sample correlations\nsample_cor &lt;- cor(Matrix::t(Embeddings(alldata, \"integrated_cca\")))\n\n# Transform the scale from correlations\nsample_cor &lt;- (1 - sample_cor) / 2\n\n# Convert it to a distance object\nd2 &lt;- as.dist(sample_cor)\n\n\n\n3.2 Clustering cells\nAfter having calculated the distances between samples, we can now proceed with the hierarchical clustering per-se. We will use the function hclust() for this purpose, in which we can simply run it with the distance objects created above. The methods available are: ‘ward.D’, ‘ward.D2’, ‘single’, ‘complete’, ‘average’, ‘mcquitty’, ‘median’ or ‘centroid’. It is possible to plot the dendrogram for all cells, but this is very time consuming and we will omit for this tutorial.\n\n# euclidean\nh_euclidean &lt;- hclust(d, method = \"ward.D2\")\n\n# correlation\nh_correlation &lt;- hclust(d2, method = \"ward.D2\")\n\nOnce your dendrogram is created, the next step is to define which samples belong to a particular cluster. After identifying the dendrogram, we can now literally cut the tree at a fixed threshold (with cutree) at different levels to define the clusters. We can either define the number of clusters or decide on a height. We can simply try different clustering levels.\n\n# euclidean distance\nalldata$hc_euclidean_5 &lt;- cutree(h_euclidean, k = 5)\nalldata$hc_euclidean_10 &lt;- cutree(h_euclidean, k = 10)\nalldata$hc_euclidean_15 &lt;- cutree(h_euclidean, k = 15)\n\n# correlation distance\nalldata$hc_corelation_5 &lt;- cutree(h_correlation, k = 5)\nalldata$hc_corelation_10 &lt;- cutree(h_correlation, k = 10)\nalldata$hc_corelation_15 &lt;- cutree(h_correlation, k = 15)\n\nwrap_plots(\n    DimPlot(alldata, reduction = \"umap_cca\", group.by = \"hc_euclidean_5\") + ggtitle(\"hc_euc_5\"),\n    DimPlot(alldata, reduction = \"umap_cca\", group.by = \"hc_euclidean_10\") + ggtitle(\"hc_euc_10\"),\n    DimPlot(alldata, reduction = \"umap_cca\", group.by = \"hc_euclidean_15\") + ggtitle(\"hc_euc_15\"),\n    DimPlot(alldata, reduction = \"umap_cca\", group.by = \"hc_corelation_5\") + ggtitle(\"hc_cor_5\"),\n    DimPlot(alldata, reduction = \"umap_cca\", group.by = \"hc_corelation_10\") + ggtitle(\"hc_cor_10\"),\n    DimPlot(alldata, reduction = \"umap_cca\", group.by = \"hc_corelation_15\") + ggtitle(\"hc_cor_15\"),\n    ncol = 3\n) + plot_layout()\n\n\n\n\n\n\n\n\nFinally, lets save the clustered data for further analysis.\n\nsaveRDS(alldata, \"data/covid/results/seurat_covid_qc_dr_int_cl.rds\")"
  },
  {
    "objectID": "labs/seurat/seurat_04_clustering.html#meta-clust_distribution",
    "href": "labs/seurat/seurat_04_clustering.html#meta-clust_distribution",
    "title": " Clustering",
    "section": "4 Distribution of clusters",
    "text": "4 Distribution of clusters\nNow, we can select one of our clustering methods and compare the proportion of samples across the clusters.\nSelect the RNA_snn_res.0.5 and plot proportion of samples per cluster and also proportion covid vs ctrl.\n\np1 &lt;- ggplot(alldata@meta.data, aes(x = RNA_snn_res.0.5, fill = orig.ident)) +\n    geom_bar(position = \"fill\")\np2 &lt;- ggplot(alldata@meta.data, aes(x = RNA_snn_res.0.5, fill = type)) +\n    geom_bar(position = \"fill\")\n\np1 + p2\n\n\n\n\n\n\n\n\nIn this case we have quite good representation of each sample in each cluster. But there are clearly some biases with more cells from one sample in some clusters and also more covid cells in some of the clusters.\nWe can also plot it in the other direction, the proportion of each cluster per sample.\n\nggplot(alldata@meta.data, aes(x = orig.ident, fill = RNA_snn_res.0.5)) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nBy now you should know how to plot different features onto your data. Take the QC metrics that were calculated in the first exercise, that should be stored in your data object, and plot it as violin plots per cluster using the clustering method of your choice. For example, plot number of UMIS, detected genes, percent mitochondrial reads. Then, check carefully if there is any bias in how your data is separated by quality metrics. Could it be explained biologically, or could there be a technical bias there?\n\n\n\nVlnPlot(alldata, group.by = \"RNA_snn_res.0.5\", features = c(\"nFeature_RNA\", \"percent_mito\"))\n\n\n\n\n\n\n\n\nSome clusters that are clearly defined by higher number of genes and counts. These are either doublets or a larger celltype. And some clusters with low values on these metrics that are either low quality cells or a smaller celltype. You will have to explore these clusters in more detail to judge what you believe them to be."
  },
  {
    "objectID": "labs/seurat/seurat_04_clustering.html#meta-session",
    "href": "labs/seurat/seurat_04_clustering.html#meta-session",
    "title": " Clustering",
    "section": "6 Session info",
    "text": "6 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] clustree_0.5.1     ggraph_2.2.1       pheatmap_1.0.12    ggplot2_3.5.1     \n[5] patchwork_1.2.0    Seurat_5.1.0       SeuratObject_5.0.2 sp_2.1-4          \n\nloaded via a namespace (and not attached):\n  [1] RColorBrewer_1.1-3     jsonlite_1.8.8         magrittr_2.0.3        \n  [4] ggbeeswarm_0.7.2       spatstat.utils_3.1-0   farver_2.1.2          \n  [7] rmarkdown_2.28         vctrs_0.6.5            ROCR_1.0-11           \n [10] memoise_2.0.1          spatstat.explore_3.2-6 htmltools_0.5.8.1     \n [13] sctransform_0.4.1      parallelly_1.38.0      KernSmooth_2.23-24    \n [16] htmlwidgets_1.6.4      ica_1.0-3              plyr_1.8.9            \n [19] plotly_4.10.4          zoo_1.8-12             cachem_1.1.0          \n [22] igraph_2.0.3           mime_0.12              lifecycle_1.0.4       \n [25] pkgconfig_2.0.3        Matrix_1.6-5           R6_2.5.1              \n [28] fastmap_1.2.0          fitdistrplus_1.2-1     future_1.34.0         \n [31] shiny_1.9.1            digest_0.6.37          colorspace_2.1-1      \n [34] tensor_1.5             RSpectra_0.16-2        irlba_2.3.5.1         \n [37] labeling_0.4.3         progressr_0.14.0       fansi_1.0.6           \n [40] spatstat.sparse_3.1-0  httr_1.4.7             polyclip_1.10-7       \n [43] abind_1.4-5            compiler_4.3.3         withr_3.0.1           \n [46] backports_1.5.0        viridis_0.6.5          fastDummies_1.7.4     \n [49] ggforce_0.4.2          MASS_7.3-60.0.1        tools_4.3.3           \n [52] vipor_0.4.7            lmtest_0.9-40          beeswarm_0.4.0        \n [55] httpuv_1.6.15          future.apply_1.11.2    goftest_1.2-3         \n [58] glue_1.7.0             nlme_3.1-165           promises_1.3.0        \n [61] grid_4.3.3             checkmate_2.3.2        Rtsne_0.17            \n [64] cluster_2.1.6          reshape2_1.4.4         generics_0.1.3        \n [67] gtable_0.3.5           spatstat.data_3.1-2    tidyr_1.3.1           \n [70] data.table_1.15.4      tidygraph_1.3.0        utf8_1.2.4            \n [73] spatstat.geom_3.2-9    RcppAnnoy_0.0.22       ggrepel_0.9.6         \n [76] RANN_2.6.2             pillar_1.9.0           stringr_1.5.1         \n [79] spam_2.10-0            RcppHNSW_0.6.0         later_1.3.2           \n [82] splines_4.3.3          dplyr_1.1.4            tweenr_2.0.3          \n [85] lattice_0.22-6         survival_3.7-0         deldir_2.0-4          \n [88] tidyselect_1.2.1       miniUI_0.1.1.1         pbapply_1.7-2         \n [91] knitr_1.48             gridExtra_2.3          scattermore_1.2       \n [94] xfun_0.47              graphlayouts_1.1.1     matrixStats_1.4.1     \n [97] stringi_1.8.4          lazyeval_0.2.2         yaml_2.3.10           \n[100] evaluate_0.24.0        codetools_0.2-20       tibble_3.2.1          \n[103] cli_3.6.3              uwot_0.1.16            xtable_1.8-4          \n[106] reticulate_1.39.0      munsell_0.5.1          Rcpp_1.0.13           \n[109] globals_0.16.3         spatstat.random_3.2-3  png_0.1-8             \n[112] ggrastr_1.0.2          parallel_4.3.3         dotCall64_1.1-1       \n[115] listenv_0.9.1          viridisLite_0.4.2      scales_1.3.0          \n[118] ggridges_0.5.6         leiden_0.4.3.1         purrr_1.0.2           \n[121] rlang_1.1.4            cowplot_1.1.3"
  },
  {
    "objectID": "labs/seurat/seurat_05_dge.html",
    "href": "labs/seurat/seurat_05_dge.html",
    "title": " Differential gene expression",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified.\nIn this tutorial we will cover differential gene expression, which comprises an extensive range of topics and methods. In single cell, differential expresison can have multiple functionalities such as identifying marker genes for cell populations, as well as identifying differentially regulated genes across conditions (healthy vs control). We will also cover controlling batch effect in your test.\nWe can first load the data from the clustering session. Moreover, we can already decide which clustering resolution to use. First let’s define using the louvain clustering to identifying differentially expressed genes.\nsuppressPackageStartupMessages({\n    library(Seurat)\n    library(dplyr)\n    library(patchwork)\n    library(ggplot2)\n    library(pheatmap)\n    library(enrichR)\n    library(Matrix)\n    library(edgeR)\n    library(MAST)\n})\n# download pre-computed data if missing or long compute\nfetch_data &lt;- TRUE\n\n# url for source and intermediate data\npath_data &lt;- \"https://nextcloud.dc.scilifelab.se/public.php/webdav\"\ncurl_upass &lt;- \"-u zbC5fr2LbEZ9rSE:scRNAseq2025\"\npath_file &lt;- \"data/covid/results/seurat_covid_qc_dr_int_cl.rds\"\n\nif (!dir.exists(dirname(path_file))) dir.create(dirname(path_file), recursive = TRUE)\nif (fetch_data && !file.exists(path_file)) download.file(url = file.path(path_data, \"covid/results_seurat/seurat_covid_qc_dr_int_cl.rds\"), destfile = path_file, method = \"curl\", extra = curl_upass)\n\nalldata &lt;- readRDS(path_file)\n# Set the identity as louvain with resolution 0.5\nsel.clust &lt;- \"RNA_snn_res.0.5\"\n\nalldata &lt;- SetIdent(alldata, value = sel.clust)\ntable(alldata@active.ident)\n\n\n   0    1    2    3    4    5    6    7    8    9 \n2138 1245 1086  646  532  357  357  334  263  176\n# plot this clustering\nwrap_plots(\n    DimPlot(alldata, reduction = \"umap_cca\", label = T) + NoAxes(),\n    DimPlot(alldata, reduction = \"umap_cca\", group.by = \"orig.ident\") + NoAxes(),\n    DimPlot(alldata, reduction = \"umap_cca\", group.by = \"type\") + NoAxes(),\n    ncol = 3\n)"
  },
  {
    "objectID": "labs/seurat/seurat_05_dge.html#meta-dge_cmg",
    "href": "labs/seurat/seurat_05_dge.html#meta-dge_cmg",
    "title": " Differential gene expression",
    "section": "1 Cell marker genes",
    "text": "1 Cell marker genes\nLet us first compute a ranking for the highly differential genes in each cluster. There are many different tests and parameters to be chosen that can be used to refine your results. When looking for marker genes, we want genes that are positively expressed in a cell type and possibly not expressed in others.\nFor differential expression it is important to use the RNA assay, for most tests we will use the logtransformed counts in the data slot. With Seurat v5 the data may be split into layers depending on what you did with the data beforehand. So first, lets check what we have in our object:\n\nLayers(alldata@assays$RNA)\n\n [1] \"counts.covid_1\"  \"counts.covid_15\" \"counts.covid_16\" \"counts.covid_17\"\n [5] \"counts.ctrl_5\"   \"counts.ctrl_13\"  \"counts.ctrl_14\"  \"counts.ctrl_19\" \n [9] \"scale.data\"      \"data.covid_1\"    \"data.covid_15\"   \"data.covid_16\"  \n[13] \"data.covid_17\"   \"data.ctrl_5\"     \"data.ctrl_13\"    \"data.ctrl_14\"   \n[17] \"data.ctrl_19\"   \n\n\nAs you can see, we have the data split by each sample, so we need to merge them into one single matrix to run the differential expression.\n\nalldata@active.assay = \"RNA\"\nalldata &lt;- JoinLayers(object = alldata, layers = c(\"data\",\"counts\"))\n\nLayers(alldata@assays$RNA)\n\n[1] \"counts\"     \"data\"       \"scale.data\"\n\n\nNow we can run the function FindAllMarkers that will run each of the clusters vs the rest. As you can see, there are some filtering criteria to remov genes that do not have certain log2FC or percent expressed. Here we only test for upregulated genes, so the only.pos parameter is set to TRUE.\n\n# Compute differentiall expression\nmarkers_genes &lt;- FindAllMarkers(\n    alldata,\n    log2FC.threshold = 0.2,\n    test.use = \"wilcox\",\n    min.pct = 0.1,\n    min.diff.pct = 0.2,\n    only.pos = TRUE,\n    max.cells.per.ident = 50,\n    assay = \"RNA\"\n)\n\nWe can now select the top 25 overexpressed genes for plotting.\n\nmarkers_genes %&gt;%\n    group_by(cluster) %&gt;%\n    top_n(-25, p_val_adj) -&gt; top25\nhead(top25)\n\n\n\n  \n\n\n\n\npar(mfrow = c(2, 5), mar = c(4, 6, 3, 1))\nfor (i in unique(top25$cluster)) {\n    barplot(sort(setNames(top25$avg_log2FC, top25$gene)[top25$cluster == i], F),\n        horiz = T, las = 1, main = paste0(i, \" vs. rest\"), border = \"white\", yaxs = \"i\"\n    )\n    abline(v = c(0, 0.25), lty = c(1, 2))\n}\n\n\n\n\n\n\n\n\nWe can visualize them as a heatmap. Here we are selecting the top 5.\n\nmarkers_genes %&gt;%\n    group_by(cluster) %&gt;%\n    slice_min(p_val_adj, n = 5, with_ties = FALSE) -&gt; top5\n# create a scale.data slot for the selected genes\nalldata &lt;- ScaleData(alldata, features = as.character(unique(top5$gene)), assay = \"RNA\")\nDoHeatmap(alldata, features = as.character(unique(top5$gene)), group.by = sel.clust, assay = \"RNA\")\n\n\n\n\n\n\n\n\nAnother way is by representing the overall group expression and detection rates in a dot-plot.\n\nDotPlot(alldata, features = rev(as.character(unique(top5$gene))), group.by = sel.clust, assay = \"RNA\") + coord_flip()\n\n\n\n\n\n\n\n\nWe can also plot a violin plot for each gene.\n\n# take top 3 genes per cluster/\ntop5 %&gt;%\n    group_by(cluster) %&gt;%\n    top_n(-3, p_val) -&gt; top3\n\n# set pt.size to zero if you do not want all the points to hide the violin shapes, or to a small value like 0.1\nVlnPlot(alldata, features = as.character(unique(top3$gene)), ncol = 5, group.by = sel.clust, assay = \"RNA\", pt.size = 0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nTake a screenshot of those results and re-run the same code above with another test: “wilcox” (Wilcoxon Rank Sum test), “bimod” (Likelihood-ratio test), “roc” (Identifies ‘markers’ of gene expression using ROC analysis),“t” (Student’s t-test),“negbinom” (negative binomial generalized linear model),“poisson” (poisson generalized linear model), “LR” (logistic regression), “MAST” (hurdle model), “DESeq2” (negative binomial distribution).\n\n\n\n1.1 DGE with equal amount of cells\nThe number of cells per cluster differ quite a bit in this data\n\ntable(alldata@active.ident)\n\n\n   0    1    2    3    4    5    6    7    8    9 \n2138 1245 1086  646  532  357  357  334  263  176 \n\n\nHence when we run FindAllMarkers one cluster vs rest, the largest cluster (cluster 0) will dominate the “rest” and influence the results the most. So it is often a good idea to subsample the clusters to an equal number of cells before running differential expression for one vs rest. We can select a fixed number of cells per cluster with the function WhichCells and the argument downsample.\n\nsub &lt;- subset(alldata, cells = WhichCells(alldata, downsample = 300))\n\ntable(sub@active.ident)\n\n\n  0   1   2   3   4   5   6   7   8   9 \n300 300 300 300 300 300 300 300 263 176 \n\n\nNow rerun FindAllMarkers with this set and compare the results.\n\nmarkers_genes_sub &lt;- FindAllMarkers(\n    sub,\n    log2FC.threshold = 0.2,\n    test.use = \"wilcox\",\n    min.pct = 0.1,\n    min.diff.pct = 0.2,\n    only.pos = TRUE,\n    max.cells.per.ident = 50,\n    assay = \"RNA\"\n)\n\nThe number of significant genes per cluster has changed, with more for some clusters and less for others.\n\ntable(markers_genes$cluster)\n\n\n  0   1   2   3   4   5   6   7   8   9 \n727 118 114 154  97 776 198  69  69  17 \n\ntable(markers_genes_sub$cluster)\n\n\n  0   1   2   3   4   5   6   7   8   9 \n700 157 114 140 122 991 195  57 112  13 \n\n\n\nmarkers_genes_sub %&gt;%\n    group_by(cluster) %&gt;%\n    slice_min(p_val_adj, n = 5, with_ties = FALSE) -&gt; top5_sub\n\nDotPlot(alldata, features = rev(as.character(unique(top5_sub$gene))), group.by = sel.clust, assay = \"RNA\") + coord_flip()"
  },
  {
    "objectID": "labs/seurat/seurat_05_dge.html#meta-dge_cond",
    "href": "labs/seurat/seurat_05_dge.html#meta-dge_cond",
    "title": " Differential gene expression",
    "section": "2 DGE across conditions",
    "text": "2 DGE across conditions\nThe second way of computing differential expression is to answer which genes are differentially expressed within a cluster. For example, in our case we have libraries comming from patients and controls and we would like to know which genes are influenced the most in a particular cell type. For this end, we will first subset our data for the desired cell cluster, then change the cell identities to the variable of comparison (which now in our case is the type, e.g. Covid/Ctrl).\n\n# select all cells in cluster 3\ncell_selection &lt;- subset(alldata, cells = colnames(alldata)[alldata@meta.data[, sel.clust] == 3])\ncell_selection &lt;- SetIdent(cell_selection, value = \"type\")\n# Compute differentiall expression\nDGE_cell_selection &lt;- FindAllMarkers(cell_selection,\n    log2FC.threshold = 0.2,\n    test.use = \"wilcox\",\n    min.pct = 0.1,\n    min.diff.pct = 0.2,\n    only.pos = TRUE,\n    max.cells.per.ident = 50,\n    assay = \"RNA\"\n)\n\nWe can now plot the expression across the type.\n\nDGE_cell_selection %&gt;%\n    group_by(cluster) %&gt;%\n    top_n(-5, p_val) -&gt; top5_cell_selection\nVlnPlot(cell_selection, features = as.character(unique(top5_cell_selection$gene)), ncol = 5, group.by = \"type\", assay = \"RNA\", pt.size = .1)\n\n\n\n\n\n\n\n\nWe can also plot these genes across all clusters, but split by type, to check if the genes are also over/under expressed in other celltypes.\n\nVlnPlot(alldata,\n    features = as.character(unique(top5_cell_selection$gene)),\n    ncol = 4, split.by = \"type\", assay = \"RNA\", pt.size = 0\n)\n\n\n\n\n\n\n\n\nAs you can see, we have many sex chromosome related genes among the top DE genes. And if you remember from the QC lab, we have unbalanced sex distribution among our subjects, so this may not be related to covid at all.\n\n2.1 Remove sex chromosome genes\nTo remove some of the bias due to unbalanced sex in the subjects, we can remove the sex chromosome related genes.\n\ngenes_file &lt;- file.path(\"data/covid/results/genes_table.csv\")\nif (!file.exists(genes_file)) download.file(file.path(path_data, \"covid/results_seurat/genes_table.csv\"), destfile = genes_file,method = \"curl\", extra = curl_upass)\n\n\ngene.info &lt;- read.csv(genes_file) # was created in the QC exercise\n\nauto.genes &lt;- gene.info$external_gene_name[!(gene.info$chromosome_name %in% c(\"X\", \"Y\"))]\n\ncell_selection@active.assay &lt;- \"RNA\"\nkeep.genes &lt;- intersect(rownames(cell_selection), auto.genes)\ncell_selection &lt;- cell_selection[keep.genes, ]\n\n# then renormalize the data\ncell_selection &lt;- NormalizeData(cell_selection)\n\nRerun differential expression:\n\n# Compute differential expression\nDGE_cell_selection &lt;- FindMarkers(cell_selection,\n    ident.1 = \"Covid\", ident.2 = \"Ctrl\",\n    logfc.threshold = 0.2, test.use = \"wilcox\", min.pct = 0.1,\n    min.diff.pct = 0.2, assay = \"RNA\"\n)\n\n# Define as Covid or Ctrl in the df and add a gene column\nDGE_cell_selection$direction &lt;- ifelse(DGE_cell_selection$avg_log2FC &gt; 0, \"Covid\", \"Ctrl\")\nDGE_cell_selection$gene &lt;- rownames(DGE_cell_selection)\n\n\nDGE_cell_selection %&gt;%\n    group_by(direction) %&gt;%\n    top_n(-5, p_val) %&gt;%\n    arrange(direction) -&gt; top5_cell_selection\n\n\nVlnPlot(cell_selection,\n    features = as.character(unique(top5_cell_selection$gene)),\n    ncol = 5, group.by = \"type\", assay = \"RNA\", pt.size = .1\n)\n\n\n\n\n\n\n\n\nWe can also plot these genes across all clusters, but split by type, to check if the genes are also over/under expressed in other celltypes/clusters.\n\nVlnPlot(alldata,\n    features = as.character(unique(top5_cell_selection$gene)),\n    ncol = 4, split.by = \"type\", assay = \"RNA\", pt.size = 0\n)"
  },
  {
    "objectID": "labs/seurat/seurat_05_dge.html#patient-batch-effects",
    "href": "labs/seurat/seurat_05_dge.html#patient-batch-effects",
    "title": " Differential gene expression",
    "section": "3 Patient Batch effects",
    "text": "3 Patient Batch effects\nWhen we are testing for Covid vs Control, we are running a DGE test for 4 vs 4 individuals. That will be very sensitive to sample differences unless we find a way to control for it. So first, let’s check how the top DEGs are expressed across the individuals within cluster 3:\n\nVlnPlot(cell_selection, group.by = \"orig.ident\", features = as.character(unique(top5_cell_selection$gene)), ncol = 4, assay = \"RNA\", pt.size = 0)\n\n\n\n\n\n\n\n\nAs you can see, many of the genes detected as DGE in Covid are unique to one or 2 patients.\nWe can examine more genes with a DotPlot:\n\nDGE_cell_selection %&gt;%\n    group_by(direction) %&gt;%\n    top_n(-20, p_val) -&gt; top20_cell_selection\nDotPlot(cell_selection, features = rev(as.character(unique(top20_cell_selection$gene))), group.by = \"orig.ident\", assay = \"RNA\") + coord_flip() + RotatedAxis()\n\n\n\n\n\n\n\n\nAs you can see, most of the DGEs are driven by the covid_17 patient. It is also a sample with very high number of cells:\n\ntable(cell_selection$orig.ident)\n\n\n covid_1 covid_15 covid_16 covid_17  ctrl_13  ctrl_14  ctrl_19   ctrl_5 \n      94       32       37      170       65       60       35      153"
  },
  {
    "objectID": "labs/seurat/seurat_05_dge.html#subsample",
    "href": "labs/seurat/seurat_05_dge.html#subsample",
    "title": " Differential gene expression",
    "section": "4 Subsample",
    "text": "4 Subsample\nSo one obvious thing to consider is an equal amount of cells per individual so that the DGE results are not dominated by a single sample.\nWe will use the downsample option in the Seurat function WhichCells() to select 30 cells per cluster:\n\ncell_selection &lt;- SetIdent(cell_selection, value = \"orig.ident\")\nsub_data &lt;- subset(cell_selection, cells = WhichCells(cell_selection, downsample = 30))\n\ntable(sub_data$orig.ident)\n\n\n covid_1 covid_15 covid_16 covid_17  ctrl_13  ctrl_14  ctrl_19   ctrl_5 \n      30       30       30       30       30       30       30       30 \n\n\nAnd now we run DGE analysis again:\n\nsub_data &lt;- SetIdent(sub_data, value = \"type\")\n\n# Compute differentiall expression\nDGE_sub &lt;- FindMarkers(sub_data,\n    ident.1 = \"Covid\", ident.2 = \"Ctrl\",\n    logfc.threshold = 0.2, test.use = \"wilcox\", min.pct = 0.1,\n    min.diff.pct = 0.2, assay = \"RNA\"\n)\n\n# Define as Covid or Ctrl in the df and add a gene column\nDGE_sub$direction &lt;- ifelse(DGE_sub$avg_log2FC &gt; 0, \"Covid\", \"Ctrl\")\nDGE_sub$gene &lt;- rownames(DGE_sub)\n\n\nDGE_sub %&gt;%\n    group_by(direction) %&gt;%\n    top_n(-5, p_val) %&gt;%\n    arrange(direction) -&gt; top5_sub\n\nVlnPlot(sub_data,\n    features = as.character(unique(top5_sub$gene)),\n    ncol = 5, group.by = \"type\", assay = \"RNA\", pt.size = .1\n)\n\n\n\n\n\n\n\n\nPlot as dotplot, but in the full (not subsampled) data, still only showing cluster 3:\n\nDGE_sub %&gt;%\n    group_by(direction) %&gt;%\n    top_n(-20, p_val) -&gt; top20_sub\nDotPlot(cell_selection, features = rev(as.character(unique(top20_sub$gene))), group.by = \"orig.ident\", assay = \"RNA\") +\n    coord_flip() + RotatedAxis()\n\n\n\n\n\n\n\n\nIt looks much better now. But if we look per patient you can see that we still have some genes that are dominated by a single patient.\nWhy do you think this is?"
  },
  {
    "objectID": "labs/seurat/seurat_05_dge.html#pseudobulk",
    "href": "labs/seurat/seurat_05_dge.html#pseudobulk",
    "title": " Differential gene expression",
    "section": "5 Pseudobulk",
    "text": "5 Pseudobulk\nOne option is to treat the samples as pseudobulks and do differential expression for the 4 patients vs 4 controls. You do lose some information about cell variability within each patient, but instead you gain the advantage of mainly looking for effects that are seen in multiple patients.\nHowever, having only 4 patients is perhaps too low, with many more patients it will work better to run pseudobulk analysis.\nFor a fair comparison we should have equal number of cells per sample when we create the pseudobulk with AggregateExpression. For pseudobulk it is recommended to use the subsampled object so that you have similar amount of cells into each pseudobulk calculation. A biased number of cells will strongly influence the differential expression you run with the pseudobulk.\n\npseudobulk = AggregateExpression(sub_data, group.by = \"orig.ident\", assays = \"RNA\")$RNA\n\nThen run edgeR:\n\n# define the groups\nbulk.labels &lt;- c(\"Covid\", \"Covid\", \"Covid\", \"Covid\", \"Ctrl\", \"Ctrl\", \"Ctrl\", \"Ctrl\")\n\ndge.list &lt;- DGEList(counts = pseudobulk, group = factor(bulk.labels))\nkeep &lt;- filterByExpr(dge.list)\ndge.list &lt;- dge.list[keep, , keep.lib.sizes = FALSE]\n\ndge.list &lt;- calcNormFactors(dge.list)\ndesign &lt;- model.matrix(~bulk.labels)\n\ndge.list &lt;- estimateDisp(dge.list, design)\n\nfit &lt;- glmQLFit(dge.list, design)\nqlf &lt;- glmQLFTest(fit, coef = 2)\ntopTags(qlf)\n\nCoefficient:  bulk.labelsCtrl \n           logFC    logCPM         F       PValue       FDR\nWDFY2   1.471776  7.175742 15.099461 0.0009581845 0.4683637\nAHNAK   1.402522  7.845133 14.393278 0.0011846521 0.4683637\nSKIL   -1.570543  6.781652 14.206374 0.0012541140 0.4683637\nCD69   -1.594180 10.341748 13.395898 0.0016123421 0.4683637\nDDIT3  -1.429081  7.306436 13.285757 0.0016692453 0.4683637\nSTAG3  -1.886657  7.459503 13.061054 0.0017923761 0.4683637\nNUDT3   1.311554  6.898319 12.851803 0.0019161578 0.4683637\nSLIRP  -1.193814  6.686989 10.669825 0.0039659763 0.7858307\nIFITM2 -1.252358  8.090788 10.018526 0.0049872285 0.7858307\nSNHG15 -1.273079  6.840242  9.962369 0.0050881698 0.7858307\n\n\nAs you can see, we have very few significant genes. Since we only have 4 vs 4 samples, we should not expect to find many genes with this method.\nAgain as dotplot including top 10 genes:\n\nres.edgeR &lt;- topTags(qlf, 100)$table\nres.edgeR$dir &lt;- ifelse(res.edgeR$logFC &gt; 0, \"Covid\", \"Ctrl\")\nres.edgeR$gene &lt;- rownames(res.edgeR)\n\nres.edgeR %&gt;%\n    group_by(dir) %&gt;%\n    top_n(-10, PValue) %&gt;%\n    arrange(dir) -&gt; top.edgeR\n\nDotPlot(cell_selection,\n    features = as.character(unique(top.edgeR$gene)), group.by = \"orig.ident\",\n    assay = \"RNA\"\n) + coord_flip() + ggtitle(\"EdgeR pseudobulk\") + RotatedAxis()\n\n\n\n\n\n\n\n\nAs you can see, even if we find few genes, they seem to make sense across all the patients."
  },
  {
    "objectID": "labs/seurat/seurat_05_dge.html#mast-random-effect",
    "href": "labs/seurat/seurat_05_dge.html#mast-random-effect",
    "title": " Differential gene expression",
    "section": "6 MAST random effect",
    "text": "6 MAST random effect\nMAST has the option to add a random effect for the patient when running DGE analysis. It is quite slow, even with this small dataset, so it may not be practical for a larger dataset unless you have access to a compute cluster.\nWe will run MAST with and without patient info as random effect and compare the results\nFirst, filter genes in part to speed up the process but also to avoid too many warnings in the model fitting step of MAST. We will keep genes that are expressed with at least 2 reads in 2 covid patients or 2 controls.\n\n# select genes that are expressed in at least 2 patients or 2 ctrls with &gt; 2 reads.\nnPatient &lt;- sapply(unique(cell_selection$orig.ident), function(x) {\n    rowSums(cell_selection@assays$RNA@layers$counts[, cell_selection$orig.ident == x] &gt; 2)\n})\nnCovid &lt;- rowSums(nPatient[, 1:4] &gt; 2)\nnCtrl &lt;- rowSums(nPatient[, 5:8] &gt; 2)\n\nsel &lt;- nCovid &gt;= 2 | nCtrl &gt;= 2\ncell_selection_sub &lt;- cell_selection[sel, ]\n\nSet up the MAST object.\n\n# create the feature data\nfData &lt;- data.frame(primerid = rownames(cell_selection_sub))\nm &lt;- cell_selection_sub@meta.data\nm$wellKey &lt;- rownames(m)\n\n# make sure type and orig.ident are factors\nm$orig.ident &lt;- factor(m$orig.ident)\nm$type &lt;- factor(m$type)\n\nsca &lt;- MAST::FromMatrix(\n    exprsArray = as.matrix(x = cell_selection_sub@assays$RNA@layers$data),\n    check_sanity = FALSE, cData = m, fData = fData\n)\n\nFirst, run the regular MAST analysis without random effects\n\n# takes a while to run, so save a file to tmpdir in case you have to rerun the code\ntmpdir &lt;- \"data/covid/results/tmp_dge\"\ndir.create(tmpdir, showWarnings = F)\n\ntmpfile1 &lt;- file.path(tmpdir, \"mast_bayesglm_cl3.Rds\")\nif (file.exists(tmpfile1)) {\n    fcHurdle1 &lt;- readRDS(tmpfile1)\n} else {\n    zlmCond &lt;- suppressMessages(MAST::zlm(~ type + nFeature_RNA, sca, method = \"bayesglm\", ebayes = T))\n    summaryCond &lt;- suppressMessages(MAST::summary(zlmCond, doLRT = \"typeCtrl\"))\n    summaryDt &lt;- summaryCond$datatable\n    fcHurdle &lt;- merge(summaryDt[summaryDt$contrast == \"typeCtrl\" & summaryDt$component ==\n        \"logFC\", c(1, 7, 5, 6, 8)], summaryDt[summaryDt$contrast == \"typeCtrl\" &\n        summaryDt$component == \"H\", c(1, 4)], by = \"primerid\")\n    fcHurdle1 &lt;- stats::na.omit(as.data.frame(fcHurdle))\n    saveRDS(fcHurdle1, tmpfile1)\n}\n\nThen run MAST with glmer and random effect.\n\nlibrary(lme4)\n\ntmpfile2 &lt;- file.path(tmpdir, \"mast_glme_cl3.Rds\")\nif (file.exists(tmpfile2)) {\n    fcHurdle2 &lt;- readRDS(tmpfile2)\n} else {\n    zlmCond &lt;- suppressMessages(MAST::zlm(~ type + nFeature_RNA + (1 | orig.ident), sca,\n        method = \"glmer\",\n        ebayes = F, strictConvergence = FALSE\n    ))\n\n    summaryCond &lt;- suppressMessages(MAST::summary(zlmCond, doLRT = \"typeCtrl\"))\n    summaryDt &lt;- summaryCond$datatable\n    fcHurdle &lt;- merge(summaryDt[summaryDt$contrast == \"typeCtrl\" & summaryDt$component ==\n        \"logFC\", c(1, 7, 5, 6, 8)], summaryDt[summaryDt$contrast == \"typeCtrl\" &\n        summaryDt$component == \"H\", c(1, 4)], by = \"primerid\")\n    fcHurdle2 &lt;- stats::na.omit(as.data.frame(fcHurdle))\n    saveRDS(fcHurdle2, tmpfile2)\n}\n\nTop genes with normal MAST:\n\ntop1 &lt;- head(fcHurdle1[order(fcHurdle1$`Pr(&gt;Chisq)`), ], 10)\ntop1\n\n\n\n  \n\n\nfcHurdle1$pval &lt;- fcHurdle1$`Pr(&gt;Chisq)`\nfcHurdle1$dir &lt;- ifelse(fcHurdle1$z &gt; 0, \"Ctrl\", \"Covid\")\nfcHurdle1 %&gt;%\n    group_by(dir) %&gt;%\n    top_n(-10, pval) %&gt;%\n    arrange(z) -&gt; mastN\n\nmastN &lt;- mastN$primerid\n\nTop genes with random effect:\n\ntop2 &lt;- head(fcHurdle2[order(fcHurdle2$`Pr(&gt;Chisq)`), ], 10)\ntop2\n\n\n\n  \n\n\nfcHurdle2$pval &lt;- fcHurdle2$`Pr(&gt;Chisq)`\nfcHurdle2$dir &lt;- ifelse(fcHurdle2$z &gt; 0, \"Ctrl\", \"Covid\")\nfcHurdle2 %&gt;%\n    group_by(dir) %&gt;%\n    top_n(-10, pval) %&gt;%\n    arrange(z) -&gt; mastR\n\nmastR &lt;- mastR$primerid\n\nAs you can see, we have lower significance for the genes with the random effect added.\nDotplot for top 10 genes in each direction:\n\np1 &lt;- DotPlot(cell_selection, features = mastN, group.by = \"orig.ident\", assay = \"RNA\") +\n    coord_flip() + RotatedAxis() + ggtitle(\"Regular MAST\")\n\np2 &lt;- DotPlot(cell_selection, features = mastR, group.by = \"orig.ident\", assay = \"RNA\") +\n    coord_flip() + RotatedAxis() + ggtitle(\"With random effect\")\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nYou have now run DGE analysis for Covid vs Ctrl in cluster 3 with several diffent methods. Have a look at the different results. Where did you get more/less significant genes? Which results would you like to present in a paper? Discuss with a neighbor which one you think looks best and why."
  },
  {
    "objectID": "labs/seurat/seurat_05_dge.html#meta-dge_gsa",
    "href": "labs/seurat/seurat_05_dge.html#meta-dge_gsa",
    "title": " Differential gene expression",
    "section": "7 Gene Set Analysis (GSA)",
    "text": "7 Gene Set Analysis (GSA)\n\n7.1 Hypergeometric enrichment test\nHaving a defined list of differentially expressed genes, you can now look for their combined function using hypergeometric test.\nIn this case we will use the DGE from MAST with random effect to run enrichment analysis.\n\n# Load additional packages\nlibrary(enrichR)\n\n# Check available databases to perform enrichment (then choose one)\nenrichR::listEnrichrDbs()\n\n\n\n  \n\n\n# Perform enrichment\nenrich_results &lt;- enrichr(\n    genes     = fcHurdle2$primerid[fcHurdle2$z &lt; 0 & fcHurdle2$pval &lt; 0.05],\n    databases = \"GO_Biological_Process_2017b\"\n)[[1]]\n\nUploading data to Enrichr... Done.\n  Querying GO_Biological_Process_2017b... Done.\nParsing results... Done.\n\n\nSome databases of interest:\nGO_Biological_Process_2017bKEGG_2019_HumanKEGG_2019_MouseWikiPathways_2019_HumanWikiPathways_2019_Mouse\nYou visualize your results using a simple barplot, for example:\n\npar(mfrow = c(1, 1), mar = c(3, 25, 2, 1))\nbarplot(\n    height = -log10(enrich_results$P.value)[10:1],\n    names.arg = enrich_results$Term[10:1],\n    horiz = TRUE,\n    las = 1,\n    border = FALSE,\n    cex.names = .6\n)\nabline(v = c(-log10(0.05)), lty = 2)\nabline(v = 0, lty = 1)"
  },
  {
    "objectID": "labs/seurat/seurat_05_dge.html#meta-dge_gsea",
    "href": "labs/seurat/seurat_05_dge.html#meta-dge_gsea",
    "title": " Differential gene expression",
    "section": "8 Gene Set Enrichment Analysis (GSEA)",
    "text": "8 Gene Set Enrichment Analysis (GSEA)\nBesides the enrichment using hypergeometric test, we can also perform gene set enrichment analysis (GSEA), which scores ranked genes list (usually based on fold changes) and computes permutation test to check if a particular gene set is more present in the Up-regulated genes, among the DOWN_regulated genes or not differentially regulated.\nBefore, we ran FindMarkers() with the default settings for reporting only significantly up/down regulated genes, but now we need statistics on a larger set of genes, so we will have to rerun the test with more lenient cutoffs. We will not use it now but this is how it should be run:\n\n## OBS! No need to run\n\nsub_data &lt;- SetIdent(sub_data, value = \"type\")\n\nDGE_cell_selection2 &lt;- FindMarkers(\n    sub_data,\n    ident.1 = \"Covid\",\n    log2FC.threshold = -Inf,\n    test.use = \"wilcox\",\n    min.pct = 0.05,\n    min.diff.pct = 0,\n    only.pos = FALSE,\n    max.cells.per.ident = 50,\n    assay = \"RNA\"\n)\n\n# Create a gene rank based on the gene expression fold change\ngene_rank &lt;- setNames(DGE_cell_selection2$avg_log2FC, casefold(rownames(DGE_cell_selection2), upper = T))\n\nIn this case we will use the results from MAST with random effect to run GSEA, and we will use the Z-score from MAST to sort the genes.\n\ngene_rank &lt;- setNames(fcHurdle2$z, casefold(fcHurdle2$primerid, upper = T))\n\nOnce our list of genes are sorted, we can proceed with the enrichment itself. We can use the package to get gene set from the Molecular Signature Database (MSigDB) and select KEGG pathways as an example.\n\nlibrary(msigdbr)\n\n# Download gene sets\nmsigdbgmt &lt;- msigdbr::msigdbr(\"Homo sapiens\")\nmsigdbgmt &lt;- as.data.frame(msigdbgmt)\n\n# List available gene sets\nunique(msigdbgmt$gs_subcat)\n\n [1] \"MIR:MIR_Legacy\"  \"TFT:TFT_Legacy\"  \"CGP\"             \"TFT:GTRD\"       \n [5] \"\"                \"VAX\"             \"CP:BIOCARTA\"     \"CGN\"            \n [9] \"GO:BP\"           \"GO:CC\"           \"IMMUNESIGDB\"     \"GO:MF\"          \n[13] \"HPO\"             \"CP:KEGG\"         \"MIR:MIRDB\"       \"CM\"             \n[17] \"CP\"              \"CP:PID\"          \"CP:REACTOME\"     \"CP:WIKIPATHWAYS\"\n\n# Subset which gene set you want to use.\nmsigdbgmt_subset &lt;- msigdbgmt[msigdbgmt$gs_subcat == \"CP:WIKIPATHWAYS\", ]\ngmt &lt;- lapply(unique(msigdbgmt_subset$gs_name), function(x) {\n    msigdbgmt_subset[msigdbgmt_subset$gs_name == x, \"gene_symbol\"]\n})\nnames(gmt) &lt;- unique(paste0(msigdbgmt_subset$gs_name, \"_\", msigdbgmt_subset$gs_exact_source))\n\nNext, we will run GSEA. This will result in a table containing information for several pathways. We can then sort and filter those pathways to visualize only the top ones. You can select/filter them by either p-value or normalized enrichment score (NES).\n\nlibrary(fgsea)\n\n# Perform enrichemnt analysis\nfgseaRes &lt;- fgsea(pathways = gmt, stats = gene_rank, minSize = 15, maxSize = 500)\nfgseaRes &lt;- fgseaRes[order(fgseaRes$pval, decreasing = F), ]\n\n# Filter the results table to show only the top 10 UP or DOWN regulated processes (optional)\ntop10_UP &lt;- fgseaRes$pathway[1:10]\n\n# Nice summary table (shown as a plot)\nplotGseaTable(gmt[top10_UP], gene_rank, fgseaRes, gseaParam = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nWhich KEGG pathways are upregulated in this cluster? Which KEGG pathways are dowregulated in this cluster? Change the pathway source to another gene set (e.g. CP:WIKIPATHWAYS or CP:REACTOME or CP:BIOCARTA or GO:BP) and check the if you get similar results?"
  },
  {
    "objectID": "labs/seurat/seurat_05_dge.html#meta-session",
    "href": "labs/seurat/seurat_05_dge.html#meta-session",
    "title": " Differential gene expression",
    "section": "9 Session info",
    "text": "9 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] fgsea_1.28.0                msigdbr_7.5.1              \n [3] lme4_1.1-35.5               MAST_1.28.0                \n [5] SingleCellExperiment_1.24.0 SummarizedExperiment_1.32.0\n [7] Biobase_2.62.0              GenomicRanges_1.54.1       \n [9] GenomeInfoDb_1.38.1         IRanges_2.36.0             \n[11] S4Vectors_0.40.2            BiocGenerics_0.48.1        \n[13] MatrixGenerics_1.14.0       matrixStats_1.4.1          \n[15] edgeR_4.0.16                limma_3.58.1               \n[17] Matrix_1.6-5                enrichR_3.2                \n[19] pheatmap_1.0.12             ggplot2_3.5.1              \n[21] patchwork_1.2.0             dplyr_1.1.4                \n[23] Seurat_5.1.0                SeuratObject_5.0.2         \n[25] sp_2.1-4                   \n\nloaded via a namespace (and not attached):\n  [1] RcppAnnoy_0.0.22        splines_4.3.3           later_1.3.2            \n  [4] bitops_1.0-8            tibble_3.2.1            polyclip_1.10-7        \n  [7] fastDummies_1.7.4       lifecycle_1.0.4         globals_0.16.3         \n [10] lattice_0.22-6          MASS_7.3-60.0.1         magrittr_2.0.3         \n [13] plotly_4.10.4           rmarkdown_2.28          yaml_2.3.10            \n [16] httpuv_1.6.15           sctransform_0.4.1       spam_2.10-0            \n [19] spatstat.sparse_3.1-0   reticulate_1.39.0       minqa_1.2.8            \n [22] cowplot_1.1.3           pbapply_1.7-2           RColorBrewer_1.1-3     \n [25] abind_1.4-5             zlibbioc_1.48.0         Rtsne_0.17             \n [28] purrr_1.0.2             presto_1.0.0            RCurl_1.98-1.16        \n [31] WriteXLS_6.7.0          GenomeInfoDbData_1.2.11 ggrepel_0.9.6          \n [34] irlba_2.3.5.1           listenv_0.9.1           spatstat.utils_3.1-0   \n [37] goftest_1.2-3           RSpectra_0.16-2         spatstat.random_3.2-3  \n [40] fitdistrplus_1.2-1      parallelly_1.38.0       leiden_0.4.3.1         \n [43] codetools_0.2-20        DelayedArray_0.28.0     tidyselect_1.2.1       \n [46] farver_2.1.2            spatstat.explore_3.2-6  jsonlite_1.8.8         \n [49] progressr_0.14.0        ggridges_0.5.6          survival_3.7-0         \n [52] tools_4.3.3             ica_1.0-3               Rcpp_1.0.13            \n [55] glue_1.7.0              gridExtra_2.3           SparseArray_1.2.2      \n [58] xfun_0.47               withr_3.0.1             fastmap_1.2.0          \n [61] boot_1.3-31             fansi_1.0.6             digest_0.6.37          \n [64] R6_2.5.1                mime_0.12               colorspace_2.1-1       \n [67] scattermore_1.2         tensor_1.5              spatstat.data_3.1-2    \n [70] utf8_1.2.4              tidyr_1.3.1             generics_0.1.3         \n [73] data.table_1.15.4       httr_1.4.7              htmlwidgets_1.6.4      \n [76] S4Arrays_1.2.0          uwot_0.1.16             pkgconfig_2.0.3        \n [79] gtable_0.3.5            lmtest_0.9-40           XVector_0.42.0         \n [82] htmltools_0.5.8.1       dotCall64_1.1-1         scales_1.3.0           \n [85] png_0.1-8               knitr_1.48              reshape2_1.4.4         \n [88] rjson_0.2.21            nloptr_2.1.1            nlme_3.1-165           \n [91] curl_5.2.1              zoo_1.8-12              stringr_1.5.1          \n [94] KernSmooth_2.23-24      parallel_4.3.3          miniUI_0.1.1.1         \n [97] vipor_0.4.7             ggrastr_1.0.2           pillar_1.9.0           \n[100] grid_4.3.3              vctrs_0.6.5             RANN_2.6.2             \n[103] promises_1.3.0          xtable_1.8-4            cluster_2.1.6          \n[106] beeswarm_0.4.0          evaluate_0.24.0         cli_3.6.3              \n[109] locfit_1.5-9.9          compiler_4.3.3          rlang_1.1.4            \n[112] crayon_1.5.3            future.apply_1.11.2     labeling_0.4.3         \n[115] plyr_1.8.9              ggbeeswarm_0.7.2        stringi_1.8.4          \n[118] BiocParallel_1.36.0     viridisLite_0.4.2       deldir_2.0-4           \n[121] babelgene_22.9          munsell_0.5.1           lazyeval_0.2.2         \n[124] spatstat.geom_3.2-9     RcppHNSW_0.6.0          future_1.34.0          \n[127] statmod_1.5.0           shiny_1.9.1             ROCR_1.0-11            \n[130] igraph_2.0.3            fastmatch_1.1-4"
  },
  {
    "objectID": "labs/seurat/seurat_06_celltyping.html",
    "href": "labs/seurat/seurat_06_celltyping.html",
    "title": " Celltype prediction",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified.\nCelltype prediction can either be performed on indiviudal cells where each cell gets a predicted celltype label, or on the level of clusters. All methods are based on similarity to other datasets, single cell or sorted bulk RNAseq, or uses known marker genes for each cell type.\nIdeally celltype predictions should be run on each sample separately and not using the integrated data. In this case we will select one sample from the Covid data, ctrl_13 and predict celltype by cell on that sample.\nSome methods will predict a celltype to each cell based on what it is most similar to, even if that celltype is not included in the reference. Other methods include an uncertainty so that cells with low similarity scores will be unclassified.\nThere are multiple different methods to predict celltypes, here we will just cover a few of those.\nWe will use a reference PBMC dataset from the scPred package which is provided as a Seurat object with counts. Unfortunately scPred has not been updated to run with Seurat v5, so we will not use it here. We will test classification based on Seurat labelTransfer, the SingleR method scPred and using Azimuth. Finally we will use gene set enrichment predict celltype based on the DEGs of each cluster."
  },
  {
    "objectID": "labs/seurat/seurat_06_celltyping.html#meta-ct_read",
    "href": "labs/seurat/seurat_06_celltyping.html#meta-ct_read",
    "title": " Celltype prediction",
    "section": "1 Read data",
    "text": "1 Read data\nFirst, lets load required libraries\n\nsuppressPackageStartupMessages({\n    library(Seurat)\n    library(dplyr)\n    library(patchwork)\n    library(ggplot2)\n    library(pheatmap)\n    library(scPred)\n    library(celldex)\n    library(SingleR)\n    library(SeuratData)\n    library(Azimuth)\n})\n\nLet’s read in the saved Covid-19 data object from the clustering step.\n\n# download pre-computed data if missing or long compute\nfetch_data &lt;- TRUE\n\n# url for source and intermediate data\npath_data &lt;- \"https://nextcloud.dc.scilifelab.se/public.php/webdav\"\ncurl_upass &lt;- \"-u zbC5fr2LbEZ9rSE:scRNAseq2025\"\npath_file &lt;- \"data/covid/results/seurat_covid_qc_dr_int_cl.rds\"\n\nif (!dir.exists(dirname(path_file))) dir.create(dirname(path_file), recursive = TRUE)\nif (fetch_data && !file.exists(path_file)) download.file(url = file.path(path_data, \"covid/results_seurat/seurat_covid_qc_dr_int_cl.rds\"), destfile = path_file, method = \"curl\", extra = curl_upass)\n\nalldata &lt;- readRDS(path_file)\n\nSubset one patient.\n\nctrl &lt;- alldata[, alldata$orig.ident == \"ctrl_13\"]"
  },
  {
    "objectID": "labs/seurat/seurat_06_celltyping.html#meta-ct_ref",
    "href": "labs/seurat/seurat_06_celltyping.html#meta-ct_ref",
    "title": " Celltype prediction",
    "section": "2 Reference data",
    "text": "2 Reference data\nLoad the reference dataset with annotated labels that is provided by the scPred package, it is a subsampled set of cells from human PBMCs.\n\nreference &lt;- scPred::pbmc_1\nreference\n\nAn object of class Seurat \n32838 features across 3500 samples within 1 assay \nActive assay: RNA (32838 features, 0 variable features)\n 2 layers present: counts, data\n\n\nRerun analysis pipeline. Run normalization, feature selection and dimensionality reduction\nHere, we will run all the steps that we did in previous labs in one go using the magittr package with the pipe-operator %&gt;%.\n\nreference &lt;- reference %&gt;%\n    NormalizeData() %&gt;%\n    FindVariableFeatures() %&gt;%\n    ScaleData() %&gt;%\n    RunPCA(verbose = F) %&gt;%\n    RunUMAP(dims = 1:30)\n\n\nDimPlot(reference, group.by = \"cell_type\", label = TRUE, repel = TRUE) + NoAxes()\n\n\n\n\n\n\n\n\nRun all steps of the analysis for the ctrl sample as well. Use the clustering from the integration lab with resolution 0.5.\n\n# Set the identity as louvain with resolution 0.5\nctrl &lt;- SetIdent(ctrl, value = \"RNA_snn_res.0.5\")\n\nctrl &lt;- ctrl %&gt;%\n    NormalizeData() %&gt;%\n    FindVariableFeatures() %&gt;%\n    ScaleData() %&gt;%\n    RunPCA(verbose = F) %&gt;%\n    RunUMAP(dims = 1:30)\n\n\nDimPlot(ctrl, label = TRUE, repel = TRUE) + NoAxes()"
  },
  {
    "objectID": "labs/seurat/seurat_06_celltyping.html#label-transfer",
    "href": "labs/seurat/seurat_06_celltyping.html#label-transfer",
    "title": " Celltype prediction",
    "section": "3 Label transfer",
    "text": "3 Label transfer\nFirst we will run label transfer using a similar method as in the integration exercise. But, instead of CCA, which is the default for the FindTransferAnchors() function, we will use pcaproject, ie; the query dataset is projected onto the PCA of the reference dataset. Then, the labels of the reference data are predicted.\n\ntransfer.anchors &lt;- FindTransferAnchors(\n    reference = reference, query = ctrl,\n    dims = 1:30\n)\npredictions &lt;- TransferData(\n    anchorset = transfer.anchors, refdata = reference$cell_type,\n    dims = 1:30\n)\nctrl &lt;- AddMetaData(object = ctrl, metadata = predictions)\n\n\nDimPlot(ctrl, group.by = \"predicted.id\", label = T, repel = T) + NoAxes()\n\n\n\n\n\n\n\n\nNow plot how many cells of each celltypes can be found in each cluster.\n\nggplot(ctrl@meta.data, aes(x = RNA_snn_res.0.5, fill = predicted.id)) +\n    geom_bar() +\n    theme_classic()"
  },
  {
    "objectID": "labs/seurat/seurat_06_celltyping.html#meta-ct_scpred",
    "href": "labs/seurat/seurat_06_celltyping.html#meta-ct_scpred",
    "title": " Celltype prediction",
    "section": "4 scPred",
    "text": "4 scPred\nscPred will train a classifier based on all principal components. First, getFeatureSpace() will create a scPred object stored in the @misc slot where it extracts the PCs that best separates the different celltypes. Then trainModel() will do the actual training for each celltype.\n\nreference &lt;- getFeatureSpace(reference, \"cell_type\")\n\n●  Extracting feature space for each cell type...\nDONE!\n\nreference &lt;- trainModel(reference)\n\n●  Training models for each cell type...\nmaximum number of iterations reached 0.0001152056 -0.0001143117DONE!\n\n\nWe can then print how well the training worked for the different celltypes by printing the number of PCs used, the ROC value and Sensitivity/Specificity. Which celltypes do you think are harder to classify based on this dataset?\n\nget_scpred(reference)\n\n'scPred' object\n✔  Prediction variable = cell_type \n✔  Discriminant features per cell type\n✔  Training model(s)\nSummary\n\n|Cell type   |    n| Features|Method    |   ROC|  Sens|  Spec|\n|:-----------|----:|--------:|:---------|-----:|-----:|-----:|\n|B cell      |  280|       50|svmRadial | 1.000| 0.964| 1.000|\n|CD4 T cell  | 1620|       50|svmRadial | 0.997| 0.972| 0.975|\n|CD8 T cell  |  945|       50|svmRadial | 0.985| 0.899| 0.978|\n|cDC         |   26|       50|svmRadial | 0.995| 0.547| 1.000|\n|cMono       |  212|       50|svmRadial | 0.994| 0.958| 0.970|\n|ncMono      |   79|       50|svmRadial | 0.998| 0.570| 1.000|\n|NK cell     |  312|       50|svmRadial | 0.999| 0.933| 0.996|\n|pDC         |   20|       50|svmRadial | 1.000| 0.700| 1.000|\n|Plasma cell |    6|       50|svmRadial | 1.000| 0.800| 1.000|\n\n\nYou can optimize parameters for each dataset by chaining parameters and testing different types of models, see more at: https://powellgenomicslab.github.io/scPred/articles/introduction.html. But for now, we will continue with this model. Now, let’s predict celltypes on our data, where scPred will align the two datasets with Harmony and then perform classification.\n\nctrl &lt;- scPredict(ctrl, reference)\n\n●  Matching reference with new dataset...\n     ─ 2000 features present in reference loadings\n     ─ 1786 features shared between reference and new dataset\n     ─ 89.3% of features in the reference are present in new dataset\n●  Aligning new data to reference...\n●  Classifying cells...\nDONE!\n\n\n\nDimPlot(ctrl, group.by = \"scpred_prediction\", label = T, repel = T) + NoAxes()\n\n\n\n\n\n\n\n\nNow plot how many cells of each celltypes can be found in each cluster.\n\nggplot(ctrl@meta.data, aes(x = CCA_snn_res.0.5, fill = scpred_prediction)) +\n    geom_bar() +\n    theme_classic()"
  },
  {
    "objectID": "labs/seurat/seurat_06_celltyping.html#meta-ct_compare",
    "href": "labs/seurat/seurat_06_celltyping.html#meta-ct_compare",
    "title": " Celltype prediction",
    "section": "6 Compare results",
    "text": "6 Compare results\nNow we will compare the output of the two methods using the convenient function in scPred crossTab() that prints the overlap between two metadata slots.\n\ncrossTab(ctrl, \"predicted.id\", \"singler.hpca\")\n\n\n\n  \n\n\n\nWe can also plot all the different predictions side by side\n\nwrap_plots(\n    DimPlot(ctrl, label = T, group.by = \"predicted.id\") + NoAxes() + ggtitle(\"LabelTransfer\"),\n    DimPlot(ctrl, label = T, group.by = \"singler.hpca\") + NoAxes() + ggtitle(\"SingleR HPCA\"),\n    DimPlot(ctrl, label = T, group.by = \"singler.ref\") + NoAxes() + ggtitle(\"SingleR Ref\"),\n    DimPlot(ctrl, label = T, group.by = \"predicted.celltype.l1\") + NoAxes() + ggtitle(\"Azimuth l1\"),\n    ncol = 2\n)"
  },
  {
    "objectID": "labs/seurat/seurat_06_celltyping.html#meta-ct_gsea",
    "href": "labs/seurat/seurat_06_celltyping.html#meta-ct_gsea",
    "title": " Celltype prediction",
    "section": "7 GSEA with celltype markers",
    "text": "7 GSEA with celltype markers\nAnother option, where celltype can be classified on cluster level is to use gene set enrichment among the DEGs with known markers for different celltypes. Similar to how we did functional enrichment for the DEGs in the differential expression exercise. There are some resources for celltype gene sets that can be used. Such as CellMarker, PanglaoDB or celltype gene sets at MSigDB. We can also look at overlap between DEGs in a reference dataset and the dataset you are analyzing.\n\n7.1 DEG overlap\nFirst, lets extract top DEGs for our Covid-19 dataset and the reference dataset. When we run differential expression for our dataset, we want to report as many genes as possible, hence we set the cutoffs quite lenient.\n\n# run differential expression in our dataset, using clustering at resolution 0.5\n# first we need to join the layers\nalldata@active.assay = \"RNA\"\nalldata &lt;- JoinLayers(object = alldata, layers = c(\"data\",\"counts\"))\n\n# set the clustering you want to use as the identity class.\nalldata &lt;- SetIdent(alldata, value = \"RNA_snn_res.0.5\")\nDGE_table &lt;- FindAllMarkers(\n    alldata,\n    logfc.threshold = 0,\n    test.use = \"wilcox\",\n    min.pct = 0.1,\n    min.diff.pct = 0,\n    only.pos = TRUE,\n    max.cells.per.ident = 100,\n    return.thresh = 1,\n    assay = \"RNA\"\n)\n\n# split into a list\nDGE_list &lt;- split(DGE_table, DGE_table$cluster)\n\nunlist(lapply(DGE_list, nrow))\n\n   0    1    2    3    4    5    6    7    8    9 \n3265 4287 3420 2600 2165 3536 2630 2439 2393 3910 \n\n\n\n# Compute differential gene expression in reference dataset (that has cell annotation)\nreference &lt;- SetIdent(reference, value = \"cell_type\")\nreference_markers &lt;- FindAllMarkers(\n    reference,\n    min.pct = .1,\n    min.diff.pct = .2,\n    only.pos = T,\n    max.cells.per.ident = 20,\n    return.thresh = 1\n)\n\n# Identify the top cell marker genes in reference dataset\n# select top 50 with hihgest foldchange among top 100 signifcant genes.\nreference_markers &lt;- reference_markers[order(reference_markers$avg_log2FC, decreasing = T), ]\nreference_markers %&gt;%\n    group_by(cluster) %&gt;%\n    top_n(-100, p_val) %&gt;%\n    top_n(50, avg_log2FC) -&gt; top50_cell_selection\n\n# Transform the markers into a list\nref_list &lt;- split(top50_cell_selection$gene, top50_cell_selection$cluster)\n\nunlist(lapply(ref_list, length))\n\n CD8 T cell  CD4 T cell       cMono      B cell     NK cell         pDC \n         30          15          50          50          50          50 \n     ncMono         cDC Plasma cell \n         50          50          50 \n\n\nNow we can run GSEA for the DEGs from our dataset and check for enrichment of top DEGs in the reference dataset.\n\nsuppressPackageStartupMessages(library(fgsea))\n\n# run fgsea for each of the clusters in the list\nres &lt;- lapply(DGE_list, function(x) {\n    gene_rank &lt;- setNames(x$avg_log2FC, x$gene)\n    fgseaRes &lt;- fgsea(pathways = ref_list, stats = gene_rank, nperm = 10000)\n    return(fgseaRes)\n})\nnames(res) &lt;- names(DGE_list)\n\n# You can filter and resort the table based on ES, NES or pvalue\nres &lt;- lapply(res, function(x) {\n    x[x$pval &lt; 0.1, ]\n})\nres &lt;- lapply(res, function(x) {\n    x[x$size &gt; 2, ]\n})\nres &lt;- lapply(res, function(x) {\n    x[order(x$NES, decreasing = T), ]\n})\nres\n\n$`0`\n   pathway        pval       padj        ES      NES nMoreExtreme  size\n    &lt;char&gt;       &lt;num&gt;      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1:   cMono 0.000099990 0.00029997 0.8657747 2.035407            0    48\n2:     cDC 0.000099990 0.00029997 0.7099219 1.647726            0    39\n3:  ncMono 0.000099990 0.00029997 0.6642744 1.555884            0    45\n4:     pDC 0.009423559 0.02120301 0.6451232 1.431353           93    22\n    leadingEdge\n         &lt;list&gt;\n1: S100A12,....\n2: LYZ, LGA....\n3: SLC11A1,....\n4: MS4A6A, ....\n\n$`1`\n       pathway         pval        padj        ES      NES nMoreExtreme  size\n        &lt;char&gt;        &lt;num&gt;       &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1:     NK cell 0.0001000000 0.000451898 0.9156134 2.529660            0    47\n2:  CD8 T cell 0.0001004218 0.000451898 0.8551651 2.216793            0    25\n3: Plasma cell 0.0032248312 0.009674494 0.6445657 1.647584           31    22\n4:      ncMono 0.0128590314 0.028932821 0.8221468 1.597839          106     5\n5:         pDC 0.0624924889 0.112486480 0.7425491 1.443141          519     5\n    leadingEdge\n         &lt;list&gt;\n1: KLRF1, A....\n2: PRF1, GZ....\n3: CD38, SL....\n4: FCGR3A, RHOC\n5: C12orf75....\n\n$`2`\n       pathway         pval         padj        ES      NES nMoreExtreme  size\n        &lt;char&gt;        &lt;num&gt;        &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1:  CD8 T cell 0.0001001603 0.0006009615 0.8625656 2.159745            0    29\n2:  CD4 T cell 0.0027783952 0.0055567904 0.8295096 1.684067           24     7\n3:     NK cell 0.0004005207 0.0012015620 0.6560595 1.652107            3    31\n4:         pDC 0.0039970022 0.0059955034 0.9035340 1.618793           31     4\n5: Plasma cell 0.0350292752 0.0350292752 0.5965408 1.428551          346    19\n    leadingEdge\n         &lt;list&gt;\n1: CD8B, CD....\n2: CD3D, CD....\n3: XCL2, CC....\n4: PTMS, C1....\n5: FKBP11, ....\n\n$`3`\n       pathway        pval         padj        ES      NES nMoreExtreme  size\n        &lt;char&gt;       &lt;num&gt;        &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1:      B cell 0.000099990 0.0004534919 0.8699471 1.995861            0    46\n2:         pDC 0.000100776 0.0004534919 0.7920478 1.704256            0    19\n3: Plasma cell 0.003393425 0.0076352068 0.8304795 1.630014           31     9\n4:         cDC 0.002737504 0.0076352068 0.7559164 1.597116           26    16\n    leadingEdge\n         &lt;list&gt;\n1: TCL1A, F....\n2: TSPAN13,....\n3: PLPP5, D....\n4: ADAM28, ....\n\n$`4`\n      pathway         pval         padj        ES      NES nMoreExtreme  size\n       &lt;char&gt;        &lt;num&gt;        &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1: CD4 T cell 0.0001018641 0.0005093206 0.8823680 1.854275            0    14\n2: CD8 T cell 0.0007517182 0.0018792955 0.8681207 1.687616            6     8\n    leadingEdge\n         &lt;list&gt;\n1: MAL, IL7....\n2: CD3D, CD....\n\n$`5`\n   pathway       pval         padj        ES      NES nMoreExtreme  size\n    &lt;char&gt;      &lt;num&gt;        &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1:  ncMono 0.00009999 0.0004002802 0.8545233 2.043101            0    49\n2:   cMono 0.00010007 0.0004002802 0.7209933 1.675090            0    32\n3:     cDC 0.00480144 0.0128038412 0.6225434 1.460984           47    37\n4: NK cell 0.04207363 0.0841472577 0.7342381 1.451788          391     8\n    leadingEdge\n         &lt;list&gt;\n1: CDKN1C, ....\n2: AIF1, SE....\n3: LST1, CO....\n4: FCGR3A, ....\n\n$`6`\n       pathway         pval         padj        ES      NES nMoreExtreme  size\n        &lt;char&gt;        &lt;num&gt;        &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1:      B cell 0.0000999900 0.0007999200 0.8344032 1.848025            0    46\n2:         pDC 0.0002017349 0.0008054772 0.8370006 1.729132            1    17\n3: Plasma cell 0.0004027386 0.0008054772 0.8094681 1.680445            3    18\n4:         cDC 0.0004027386 0.0008054772 0.7944045 1.649173            3    18\n    leadingEdge\n         &lt;list&gt;\n1: LINC0178....\n2: SPIB, JC....\n3: TNFRSF13....\n4: ADAM28, ....\n\n$`7`\n      pathway        pval         padj       ES      NES nMoreExtreme  size\n       &lt;char&gt;       &lt;num&gt;        &lt;num&gt;    &lt;num&gt;    &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1: CD4 T cell 0.000102417 0.0006145023 0.828097 1.893825            0    14\n    leadingEdge\n         &lt;list&gt;\n1: TSHZ2, L....\n\n$`8`\nEmpty data.table (0 rows and 8 cols): pathway,pval,padj,ES,NES,nMoreExtreme...\n\n$`9`\n       pathway         pval        padj        ES      NES nMoreExtreme  size\n        &lt;char&gt;        &lt;num&gt;       &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1: Plasma cell 0.0002005817 0.001604653 0.7249438 1.832438            1    27\n2:         pDC 0.0060943575 0.024377430 0.7418881 1.665241           57    11\n    leadingEdge\n         &lt;list&gt;\n1: JCHAIN, ....\n2: MYBL2, J....\n\n\nSelecting top significant overlap per cluster, we can now rename the clusters according to the predicted labels. OBS! Be aware that if you have some clusters that have non-significant p-values for all the gene sets, the cluster label will not be very reliable. Also, the gene sets you are using may not cover all the celltypes you have in your dataset and hence predictions may just be the most similar celltype. Also, some of the clusters have very similar p-values to multiple celltypes, for instance the ncMono and cMono celltypes are equally good for some clusters.\n\nnew.cluster.ids &lt;- unlist(lapply(res, function(x) {\n    as.data.frame(x)[1, 1]\n}))\n\nannot = new.cluster.ids[as.character(alldata@active.ident)]\nnames(annot) = colnames(alldata)\nalldata$ref_gsea &lt;- annot\n\nwrap_plots(\n    DimPlot(alldata, label = T, group.by = \"RNA_snn_res.0.5\") + NoAxes(),\n    DimPlot(alldata, label = T, group.by = \"ref_gsea\") + NoAxes(),\n    ncol = 2\n)\n\n\n\n\n\n\n\n\nCompare the results with the other celltype prediction methods in the ctrl_13 sample.\n\nctrl$ref_gsea &lt;- alldata$ref_gsea[alldata$orig.ident == \"ctrl_13\"]\n\nwrap_plots(\n    DimPlot(ctrl, label = T, group.by = \"ref_gsea\") + NoAxes() + ggtitle(\"GSEA\"),\n    DimPlot(ctrl, label = T, group.by = \"predicted.id\") + NoAxes() + ggtitle(\"LabelTransfer\"),\n    ncol = 2\n)\n\n\n\n\n\n\n\n\n\n\n7.2 With annotated gene sets\nWe have downloaded the celltype gene lists from http://bio-bigdata.hrbmu.edu.cn/CellMarker/CellMarker_download.html and converted the excel file to a csv for you. Read in the gene lists and do some filtering.\n\npath_file &lt;- file.path(\"data/cell_marker_human.csv\")\nif (!file.exists(path_file)) download.file(file.path(path_data, \"misc/cell_marker_human.csv\"), destfile = path_file, method = \"curl\", extra = curl_upass)\n\n\n# Load the human marker table\nmarkers &lt;- read.delim(\"data/cell_marker_human.csv\", sep = \";\")\nmarkers &lt;- markers[markers$species == \"Human\", ]\nmarkers &lt;- markers[markers$cancer_type == \"Normal\", ]\n\n# Filter by tissue (to reduce computational time and have tissue-specific classification)\nsort(unique(markers$tissue_type))\n\n  [1] \"Abdomen\"                        \"Abdominal adipose tissue\"      \n  [3] \"Abdominal fat pad\"              \"Acinus\"                        \n  [5] \"Adipose tissue\"                 \"Adrenal gland\"                 \n  [7] \"Adventitia\"                     \"Airway\"                        \n  [9] \"Airway epithelium\"              \"Allocortex\"                    \n [11] \"Alveolus\"                       \"Amniotic fluid\"                \n [13] \"Amniotic membrane\"              \"Ampullary\"                     \n [15] \"Anogenital tract\"               \"Antecubital vein\"              \n [17] \"Anterior cruciate ligament\"     \"Anterior presomitic mesoderm\"  \n [19] \"Aorta\"                          \"Aortic valve\"                  \n [21] \"Artery\"                         \"Arthrosis\"                     \n [23] \"Articular Cartilage\"            \"Ascites\"                       \n [25] \"Atrium\"                         \"Auditory cortex\"               \n [27] \"Basilar membrane\"               \"Beige Fat\"                     \n [29] \"Bile duct\"                      \"Biliary tract\"                 \n [31] \"Bladder\"                        \"Blood\"                         \n [33] \"Blood vessel\"                   \"Bone\"                          \n [35] \"Bone marrow\"                    \"Brain\"                         \n [37] \"Breast\"                         \"Bronchial vessel\"              \n [39] \"Bronchiole\"                     \"Bronchoalveolar lavage\"        \n [41] \"Bronchoalveolar system\"         \"Bronchus\"                      \n [43] \"Brown adipose tissue\"           \"Calvaria\"                      \n [45] \"Capillary\"                      \"Cardiac atrium\"                \n [47] \"Cardiovascular system\"          \"Carotid artery\"                \n [49] \"Carotid plaque\"                 \"Cartilage\"                     \n [51] \"Caudal cortex\"                  \"Caudal forebrain\"              \n [53] \"Caudal ganglionic eminence\"     \"Cavernosum\"                    \n [55] \"Central amygdala\"               \"Central nervous system\"        \n [57] \"Central Nervous System\"         \"Cerebellum\"                    \n [59] \"Cerebral organoid\"              \"Cerebrospinal fluid\"           \n [61] \"Choriocapillaris\"               \"Chorionic villi\"               \n [63] \"Chorionic villus\"               \"Choroid\"                       \n [65] \"Choroid plexus\"                 \"Colon\"                         \n [67] \"Colon epithelium\"               \"Colorectum\"                    \n [69] \"Cornea\"                         \"Corneal endothelium\"           \n [71] \"Corneal epithelium\"             \"Coronary artery\"               \n [73] \"Corpus callosum\"                \"Corpus luteum\"                 \n [75] \"Cortex\"                         \"Cortical layer\"                \n [77] \"Cortical thymus\"                \"Decidua\"                       \n [79] \"Deciduous tooth\"                \"Dental pulp\"                   \n [81] \"Dermis\"                         \"Diencephalon\"                  \n [83] \"Distal airway\"                  \"Dorsal forebrain\"              \n [85] \"Dorsal root ganglion\"           \"Dorsolateral prefrontal cortex\"\n [87] \"Ductal tissue\"                  \"Duodenum\"                      \n [89] \"Ectocervix\"                     \"Ectoderm\"                      \n [91] \"Embryo\"                         \"Embryoid body\"                 \n [93] \"Embryonic brain\"                \"Embryonic heart\"               \n [95] \"Embryonic Kidney\"               \"Embryonic prefrontal cortex\"   \n [97] \"Embryonic stem cell\"            \"Endocardium\"                   \n [99] \"Endocrine\"                      \"Endoderm\"                      \n[101] \"Endometrium\"                    \"Endometrium stroma\"            \n[103] \"Entorhinal cortex\"              \"Epidermis\"                     \n[105] \"Epithelium\"                     \"Esophagus\"                     \n[107] \"Eye\"                            \"Fat pad\"                       \n[109] \"Fetal brain\"                    \"Fetal gonad\"                   \n[111] \"Fetal heart\"                    \"Fetal ileums\"                  \n[113] \"Fetal kidney\"                   \"Fetal Leydig\"                  \n[115] \"Fetal liver\"                    \"Fetal lung\"                    \n[117] \"Fetal pancreas\"                 \"Fetal thymus\"                  \n[119] \"Fetal umbilical cord\"           \"Fetus\"                         \n[121] \"Foreskin\"                       \"Frontal cortex\"                \n[123] \"Fundic gland\"                   \"Gall bladder\"                  \n[125] \"Gastric corpus\"                 \"Gastric epithelium\"            \n[127] \"Gastric gland\"                  \"Gastrointestinal tract\"        \n[129] \"Germ\"                           \"Gingiva\"                       \n[131] \"Gonad\"                          \"Gut\"                           \n[133] \"Hair follicle\"                  \"Heart\"                         \n[135] \"Heart muscle\"                   \"Hippocampus\"                   \n[137] \"Ileum\"                          \"Inferior colliculus\"           \n[139] \"Interfollicular epidermis\"      \"Intervertebral disc\"           \n[141] \"Intestinal crypt\"               \"Intestine\"                     \n[143] \"Intrahepatic cholangio\"         \"Jejunum\"                       \n[145] \"Kidney\"                         \"Lacrimal gland\"                \n[147] \"Large intestine\"                \"Laryngeal squamous epithelium\" \n[149] \"Lateral ganglionic eminence\"    \"Ligament\"                      \n[151] \"Limb bud\"                       \"Limbal epithelium\"             \n[153] \"Liver\"                          \"Lumbar vertebra\"               \n[155] \"Lung\"                           \"Lymph\"                         \n[157] \"Lymph node\"                     \"Lymphatic vessel\"              \n[159] \"Lymphoid tissue\"                \"Malignant pleural effusion\"    \n[161] \"Mammary epithelium\"             \"Mammary gland\"                 \n[163] \"Medial ganglionic eminence\"     \"Medullary thymus\"              \n[165] \"Meniscus\"                       \"Mesoblast\"                     \n[167] \"Mesoderm\"                       \"Microvascular endothelium\"     \n[169] \"Microvessel\"                    \"Midbrain\"                      \n[171] \"Middle temporal gyrus\"          \"Milk\"                          \n[173] \"Molar\"                          \"Muscle\"                        \n[175] \"Myenteric plexus\"               \"Myocardium\"                    \n[177] \"Myometrium\"                     \"Nasal concha\"                  \n[179] \"Nasal epithelium\"               \"Nasal mucosa\"                  \n[181] \"Nasal polyp\"                    \"Neocortex\"                     \n[183] \"Nerve\"                          \"Nose\"                          \n[185] \"Nucleus pulposus\"               \"Olfactory neuroepithelium\"     \n[187] \"Optic nerve\"                    \"Oral cavity\"                   \n[189] \"Oral mucosa\"                    \"Osteoarthritic cartilage\"      \n[191] \"Ovarian cortex\"                 \"Ovarian follicle\"              \n[193] \"Ovary\"                          \"Oviduct\"                       \n[195] \"Pancreas\"                       \"Pancreatic acinar tissue\"      \n[197] \"Pancreatic duct\"                \"Pancreatic islet\"              \n[199] \"Periodontal ligament\"           \"Periodontium\"                  \n[201] \"Periosteum\"                     \"Peripheral blood\"              \n[203] \"Peritoneal fluid\"               \"Peritoneum\"                    \n[205] \"Pituitary\"                      \"Placenta\"                      \n[207] \"Plasma\"                         \"Pluripotent stem cell\"         \n[209] \"Polyp\"                          \"Posterior presomitic mesoderm\" \n[211] \"Prefrontal cortex\"              \"Premolar\"                      \n[213] \"Presomitic mesoderm\"            \"Primitive streak\"              \n[215] \"Prostate\"                       \"Pulmonary arteriy\"             \n[217] \"Pyloric gland\"                  \"Rectum\"                        \n[219] \"Renal glomerulus\"               \"Respiratory tract\"             \n[221] \"Retina\"                         \"Retinal organoid\"              \n[223] \"Retinal pigment epithelium\"     \"Right ventricle\"               \n[225] \"Saliva\"                         \"Salivary gland\"                \n[227] \"Scalp\"                          \"Sclerocorneal tissue\"          \n[229] \"Seminal plasma\"                 \"Septum transversum\"            \n[231] \"Serum\"                          \"Sinonasal mucosa\"              \n[233] \"Sinus tissue\"                   \"Skeletal muscle\"               \n[235] \"Skin\"                           \"Small intestinal crypt\"        \n[237] \"Small intestine\"                \"Soft tissue\"                   \n[239] \"Sperm\"                          \"Spinal cord\"                   \n[241] \"Spleen\"                         \"Splenic red pulp\"              \n[243] \"Sputum\"                         \"Stomach\"                       \n[245] \"Subcutaneous adipose tissue\"    \"Submandibular gland\"           \n[247] \"Subpallium\"                     \"Subplate\"                      \n[249] \"Subventricular zone\"            \"Superior frontal gyrus\"        \n[251] \"Sympathetic ganglion\"           \"Synovial fluid\"                \n[253] \"Synovium\"                       \"Taste bud\"                     \n[255] \"Tendon\"                         \"Testis\"                        \n[257] \"Thalamus\"                       \"Thymus\"                        \n[259] \"Thyroid\"                        \"Tonsil\"                        \n[261] \"Tooth\"                          \"Trachea\"                       \n[263] \"Tracheal airway epithelium\"     \"Transformed artery\"            \n[265] \"Trophoblast\"                    \"Umbilical cord\"                \n[267] \"Umbilical cord blood\"           \"Umbilical vein\"                \n[269] \"Undefined\"                      \"Urine\"                         \n[271] \"Urothelium\"                     \"Uterine cervix\"                \n[273] \"Uterus\"                         \"Vagina\"                        \n[275] \"Vein\"                           \"Venous blood\"                  \n[277] \"Ventral thalamus\"               \"Ventricle\"                     \n[279] \"Ventricular and atrial\"         \"Ventricular zone\"              \n[281] \"Visceral adipose tissue\"        \"Vocal fold\"                    \n[283] \"Whartons jelly\"                 \"White adipose tissue\"          \n[285] \"White matter\"                   \"Yolk sac\"                      \n\ngrep(\"blood\", unique(markers$tissue_type), value = T)\n\n[1] \"Peripheral blood\"     \"Umbilical cord blood\" \"Venous blood\"        \n\nmarkers &lt;- markers[markers$tissue_type %in% c(\n    \"Blood\", \"Venous blood\",\n    \"Serum\", \"Plasma\",\n    \"Spleen\", \"Bone marrow\", \"Lymph node\"\n), ]\n\n# remove strange characters etc.\ncelltype_list &lt;- lapply(unique(markers$cell_name), function(x) {\n    x &lt;- paste(markers$Symbol[markers$cell_name == x], sep = \",\")\n    x &lt;- gsub(\"[[]|[]]| |-\", \",\", x)\n    x &lt;- unlist(strsplit(x, split = \",\"))\n    x &lt;- unique(x[!x %in% c(\"\", \"NA\", \"family\")])\n    x &lt;- casefold(x, upper = T)\n})\nnames(celltype_list) &lt;- unique(markers$cell_name)\n\ncelltype_list &lt;- celltype_list[unlist(lapply(celltype_list, length)) &lt; 100]\ncelltype_list &lt;- celltype_list[unlist(lapply(celltype_list, length)) &gt; 5]\n\n\n# run fgsea for each of the clusters in the list\nres &lt;- lapply(DGE_list, function(x) {\n    gene_rank &lt;- setNames(x$avg_log2FC, x$gene)\n    fgseaRes &lt;- fgsea(pathways = celltype_list, stats = gene_rank, nperm = 10000, scoreType = \"pos\")\n    return(fgseaRes)\n})\nnames(res) &lt;- names(DGE_list)\n\n# You can filter and resort the table based on ES, NES or pvalue\nres &lt;- lapply(res, function(x) {\n    x[x$pval &lt; 0.01, ]\n})\nres &lt;- lapply(res, function(x) {\n    x[x$size &gt; 5, ]\n})\nres &lt;- lapply(res, function(x) {\n    x[order(x$NES, decreasing = T), ]\n})\n\n# show top 3 for each cluster.\nlapply(res, head, 3)\n\n$`0`\n                  pathway       pval       padj        ES      NES nMoreExtreme\n                   &lt;char&gt;      &lt;num&gt;      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt;\n1: CD1C+_B dendritic cell 0.00009999 0.00303303 0.7916808 1.864872            0\n2:             Eosinophil 0.00039996 0.00606606 0.8174086 1.725048            3\n3:             Neutrophil 0.00009999 0.00303303 0.7762022 1.724466            0\n    size  leadingEdge\n   &lt;int&gt;       &lt;list&gt;\n1:    48 S100A12,....\n2:    12 S100A8, ....\n3:    21 S100A8, ....\n\n$`1`\n               pathway      pval         padj        ES      NES nMoreExtreme\n                &lt;char&gt;     &lt;num&gt;        &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt;\n1: Natural killer cell 9.999e-05 0.0006460892 0.9217755 2.512186            0\n2:    Cytotoxic T cell 9.999e-05 0.0006460892 0.9864518 2.166333            0\n3:           Leukocyte 9.999e-05 0.0006460892 0.8739252 2.160756            0\n    size  leadingEdge\n   &lt;int&gt;       &lt;list&gt;\n1:    38 KLRF1, K....\n2:     6 PRF1, GZ....\n3:    15 NCAM1, P....\n\n$`2`\n       pathway      pval       padj        ES      NES nMoreExtreme  size\n        &lt;char&gt;     &lt;num&gt;      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1:      T cell 9.999e-05 0.00094435 0.8336451 2.104489            0    32\n2: CD8+ T cell 9.999e-05 0.00094435 0.8972402 2.053858            0    12\n3: CD4+ T cell 9.999e-05 0.00094435 0.9068085 2.052465            0    11\n    leadingEdge\n         &lt;list&gt;\n1: CD8B, GZ....\n2: CD8B, CD....\n3: CD8A, CD....\n\n$`3`\n             pathway      pval        padj        ES      NES nMoreExtreme\n              &lt;char&gt;     &lt;num&gt;       &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt;\n1:            B cell 9.999e-05 0.001874813 0.8799566 1.964896            0\n2: Follicular B cell 9.999e-05 0.001874813 0.9447876 1.936300            0\n3:      Naive B cell 9.999e-05 0.001874813 0.9209477 1.931583            0\n    size  leadingEdge\n   &lt;int&gt;       &lt;list&gt;\n1:    29 TCL1A, I....\n2:    10 TCL1A, I....\n3:    13 TCL1A, I....\n\n$`4`\n                    pathway       pval        padj        ES      NES\n                     &lt;char&gt;      &lt;num&gt;       &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1: Regulatory T (Treg) cell 0.00009999 0.001659834 0.9237045 1.886693\n2:                   T cell 0.00009999 0.001659834 0.8265070 1.872116\n3:                Leukocyte 0.00019998 0.002074793 0.8988479 1.856992\n   nMoreExtreme  size  leadingEdge\n          &lt;num&gt; &lt;int&gt;       &lt;list&gt;\n1:            0     9 CCR4, RT....\n2:            0    30 IL2RA, C....\n3:            1    10 CCR4, IL....\n\n$`5`\n          pathway       pval       padj        ES      NES nMoreExtreme  size\n           &lt;char&gt;      &lt;num&gt;      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1: CD16+ monocyte 0.00269973 0.06074393 0.8832915 1.766738           26     6\n2:     Macrophage 0.00009999 0.00899910 0.7733105 1.757869            0    24\n3:       Monocyte 0.00179982 0.06074393 0.6690926 1.540722           17    28\n    leadingEdge\n         &lt;list&gt;\n1: TCF7L2, ....\n2: C1QA, C1....\n3: MS4A7, L....\n\n$`6`\n         pathway      pval       padj        ES      NES nMoreExtreme  size\n          &lt;char&gt;     &lt;num&gt;      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1: Memory B cell 9.999e-05 0.00189981 0.9293822 1.917626            0    15\n2:   Plasma cell 9.999e-05 0.00189981 0.9140237 1.872718            0    13\n3:        B cell 9.999e-05 0.00189981 0.8399004 1.843811            0    37\n    leadingEdge\n         &lt;list&gt;\n1: KLK1, EB....\n2: IGHA2, C....\n3: CD80, CD....\n\n$`7`\n                      pathway      pval       padj        ES      NES\n                       &lt;char&gt;     &lt;num&gt;      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1: Central memory CD8+ T cell 9.999e-05 0.00109989 0.9244203 2.114857\n2: Central memory CD4+ T cell 9.999e-05 0.00109989 0.9043366 2.050447\n3:           Naive CD8 T cell 9.999e-05 0.00109989 0.9158253 2.050195\n   nMoreExtreme  size  leadingEdge\n          &lt;num&gt; &lt;int&gt;       &lt;list&gt;\n1:            0    12 TSHZ2, L....\n2:            0    11 TSHZ2, L....\n3:            0    10 TSHZ2, L....\n\n$`8`\n                   pathway       pval       padj        ES      NES\n                    &lt;char&gt;      &lt;num&gt;      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:           Megakaryocyte 0.00009999 0.00869913 0.9502128 1.856966\n2: Hematopoietic stem cell 0.00019998 0.00869913 0.8551228 1.648128\n3:                Platelet 0.00179982 0.05219478 0.7622409 1.510292\n   nMoreExtreme  size  leadingEdge\n          &lt;num&gt; &lt;int&gt;       &lt;list&gt;\n1:            0    16 GP9, PF4....\n2:            1    13 ESAM, MM....\n3:           17    21 GP9, PF4....\n\n$`9`\n       pathway      pval       padj        ES      NES nMoreExtreme  size\n        &lt;char&gt;     &lt;num&gt;      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1:      B cell 9.999e-05 0.00283305 0.9222713 2.131979            0    11\n2: Plasma cell 9.999e-05 0.00283305 0.9327666 2.104972            0     9\n3: Plasmablast 9.999e-05 0.00283305 0.8650491 1.999701            0    11\n    leadingEdge\n         &lt;list&gt;\n1: CDC20, C....\n2: IGHA1, J....\n3: IGHA1, M....\n\n\nLet’s plot the results.\n\nnew.cluster.ids &lt;- unlist(lapply(res, function(x) {\n    as.data.frame(x)[1, 1]\n}))\nannot = new.cluster.ids[as.character(alldata@active.ident)]\nnames(annot) = colnames(alldata)\nalldata$cellmarker_gsea &lt;- annot\n\nwrap_plots(\n    DimPlot(alldata, label = T, group.by = \"ref_gsea\") + NoAxes(),\n    DimPlot(alldata, label = T, group.by = \"cellmarker_gsea\") + NoAxes(),\n    ncol = 2\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nDo you think that the methods overlap well? Where do you see the most inconsistencies?\n\n\nIn this case we do not have any ground truth, and we cannot say which method performs best. You should keep in mind, that any celltype classification method is just a prediction, and you still need to use your common sense and knowledge of the biological system to judge if the results make sense.\nFinally, lets save the data with predictions.\n\nsaveRDS(ctrl, \"data/covid/results/seurat_covid_qc_dr_int_cl_ct-ctrl13.rds\")"
  },
  {
    "objectID": "labs/seurat/seurat_06_celltyping.html#meta-session",
    "href": "labs/seurat/seurat_06_celltyping.html#meta-session",
    "title": " Celltype prediction",
    "section": "8 Session info",
    "text": "8 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] fgsea_1.28.0                Azimuth_0.5.0              \n [3] shinyBS_0.61.1              pbmcref.SeuratData_1.0.0   \n [5] SeuratData_0.2.2.9001       SingleR_2.4.0              \n [7] celldex_1.12.0              SummarizedExperiment_1.32.0\n [9] Biobase_2.62.0              GenomicRanges_1.54.1       \n[11] GenomeInfoDb_1.38.1         IRanges_2.36.0             \n[13] S4Vectors_0.40.2            BiocGenerics_0.48.1        \n[15] MatrixGenerics_1.14.0       matrixStats_1.4.1          \n[17] scPred_1.9.2                pheatmap_1.0.12            \n[19] ggplot2_3.5.1               patchwork_1.2.0            \n[21] dplyr_1.1.4                 Seurat_5.1.0               \n[23] SeuratObject_5.0.2          sp_2.1-4                   \n\nloaded via a namespace (and not attached):\n  [1] R.methodsS3_1.8.2                 progress_1.2.3                   \n  [3] nnet_7.3-19                       poweRlaw_0.80.0                  \n  [5] goftest_1.2-3                     DT_0.33                          \n  [7] Biostrings_2.70.1                 vctrs_0.6.5                      \n  [9] spatstat.random_3.2-3             digest_0.6.37                    \n [11] png_0.1-8                         ggrepel_0.9.6                    \n [13] deldir_2.0-4                      parallelly_1.38.0                \n [15] MASS_7.3-60.0.1                   Signac_1.14.0                    \n [17] reshape2_1.4.4                    httpuv_1.6.15                    \n [19] foreach_1.5.2                     withr_3.0.1                      \n [21] xfun_0.47                         survival_3.7-0                   \n [23] EnsDb.Hsapiens.v86_2.99.0         memoise_2.0.1                    \n [25] ggbeeswarm_0.7.2                  zoo_1.8-12                       \n [27] gtools_3.9.5                      pbapply_1.7-2                    \n [29] R.oo_1.26.0                       prettyunits_1.2.0                \n [31] KEGGREST_1.42.0                   promises_1.3.0                   \n [33] httr_1.4.7                        restfulr_0.0.15                  \n [35] rhdf5filters_1.14.1               globals_0.16.3                   \n [37] fitdistrplus_1.2-1                rhdf5_2.46.1                     \n [39] miniUI_0.1.1.1                    generics_0.1.3                   \n [41] curl_5.2.1                        zlibbioc_1.48.0                  \n [43] ScaledMatrix_1.10.0               polyclip_1.10-7                  \n [45] GenomeInfoDbData_1.2.11           ExperimentHub_2.10.0             \n [47] SparseArray_1.2.2                 interactiveDisplayBase_1.40.0    \n [49] xtable_1.8-4                      stringr_1.5.1                    \n [51] pracma_2.4.4                      evaluate_0.24.0                  \n [53] S4Arrays_1.2.0                    BiocFileCache_2.10.1             \n [55] hms_1.1.3                         irlba_2.3.5.1                    \n [57] colorspace_2.1-1                  filelock_1.0.3                   \n [59] hdf5r_1.3.11                      ROCR_1.0-11                      \n [61] harmony_1.2.1                     reticulate_1.39.0                \n [63] spatstat.data_3.1-2               magrittr_2.0.3                   \n [65] lmtest_0.9-40                     readr_2.1.5                      \n [67] later_1.3.2                       lattice_0.22-6                   \n [69] spatstat.geom_3.2-9               future.apply_1.11.2              \n [71] scuttle_1.12.0                    scattermore_1.2                  \n [73] XML_3.99-0.17                     cowplot_1.1.3                    \n [75] RcppAnnoy_0.0.22                  class_7.3-22                     \n [77] pillar_1.9.0                      nlme_3.1-165                     \n [79] iterators_1.0.14                  caTools_1.18.3                   \n [81] compiler_4.3.3                    beachmat_2.18.0                  \n [83] RSpectra_0.16-2                   stringi_1.8.4                    \n [85] gower_1.0.1                       tensor_1.5                       \n [87] lubridate_1.9.3                   GenomicAlignments_1.38.0         \n [89] plyr_1.8.9                        crayon_1.5.3                     \n [91] abind_1.4-5                       BiocIO_1.12.0                    \n [93] googledrive_2.1.1                 locfit_1.5-9.9                   \n [95] bit_4.0.5                         fastmatch_1.1-4                  \n [97] codetools_0.2-20                  recipes_1.1.0                    \n [99] BiocSingular_1.18.0               plotly_4.10.4                    \n[101] mime_0.12                         splines_4.3.3                    \n[103] Rcpp_1.0.13                       fastDummies_1.7.4                \n[105] dbplyr_2.5.0                      sparseMatrixStats_1.14.0         \n[107] cellranger_1.1.0                  knitr_1.48                       \n[109] blob_1.2.4                        utf8_1.2.4                       \n[111] BiocVersion_3.18.1                seqLogo_1.68.0                   \n[113] AnnotationFilter_1.26.0           fs_1.6.4                         \n[115] listenv_0.9.1                     DelayedMatrixStats_1.24.0        \n[117] tibble_3.2.1                      Matrix_1.6-5                     \n[119] statmod_1.5.0                     tzdb_0.4.0                       \n[121] pkgconfig_2.0.3                   tools_4.3.3                      \n[123] cachem_1.1.0                      RSQLite_2.3.7                    \n[125] viridisLite_0.4.2                 DBI_1.2.3                        \n[127] fastmap_1.2.0                     rmarkdown_2.28                   \n[129] scales_1.3.0                      grid_4.3.3                       \n[131] ica_1.0-3                         shinydashboard_0.7.2             \n[133] Rsamtools_2.18.0                  AnnotationHub_3.10.0             \n[135] BiocManager_1.30.25               dotCall64_1.1-1                  \n[137] RANN_2.6.2                        rpart_4.1.23                     \n[139] farver_2.1.2                      yaml_2.3.10                      \n[141] rtracklayer_1.62.0                cli_3.6.3                        \n[143] purrr_1.0.2                       leiden_0.4.3.1                   \n[145] lifecycle_1.0.4                   caret_6.0-94                     \n[147] uwot_0.1.16                       bluster_1.12.0                   \n[149] presto_1.0.0                      lava_1.8.0                       \n[151] BSgenome.Hsapiens.UCSC.hg38_1.4.5 BiocParallel_1.36.0              \n[153] annotate_1.80.0                   timechange_0.3.0                 \n[155] gtable_0.3.5                      rjson_0.2.21                     \n[157] ggridges_0.5.6                    progressr_0.14.0                 \n[159] limma_3.58.1                      parallel_4.3.3                   \n[161] pROC_1.18.5                       edgeR_4.0.16                     \n[163] jsonlite_1.8.8                    RcppHNSW_0.6.0                   \n[165] TFBSTools_1.40.0                  bitops_1.0-8                     \n[167] bit64_4.0.5                       Rtsne_0.17                       \n[169] BiocNeighbors_1.20.0              spatstat.utils_3.1-0             \n[171] CNEr_1.38.0                       metapod_1.10.0                   \n[173] dqrng_0.3.2                       shinyjs_2.1.0                    \n[175] SeuratDisk_0.0.0.9021             R.utils_2.12.3                   \n[177] timeDate_4032.109                 lazyeval_0.2.2                   \n[179] shiny_1.9.1                       htmltools_0.5.8.1                \n[181] GO.db_3.18.0                      sctransform_0.4.1                \n[183] rappdirs_0.3.3                    ensembldb_2.26.0                 \n[185] glue_1.7.0                        TFMPvalue_0.0.9                  \n[187] spam_2.10-0                       googlesheets4_1.1.1              \n[189] XVector_0.42.0                    RCurl_1.98-1.16                  \n[191] scran_1.30.0                      BSgenome_1.70.1                  \n[193] gridExtra_2.3                     JASPAR2020_0.99.10               \n[195] igraph_2.0.3                      R6_2.5.1                         \n[197] SingleCellExperiment_1.24.0       tidyr_1.3.1                      \n[199] labeling_0.4.3                    RcppRoll_0.3.1                   \n[201] GenomicFeatures_1.54.1            cluster_2.1.6                    \n[203] Rhdf5lib_1.24.0                   gargle_1.5.2                     \n[205] ipred_0.9-15                      DirichletMultinomial_1.44.0      \n[207] DelayedArray_0.28.0               tidyselect_1.2.1                 \n[209] vipor_0.4.7                       ProtGenerics_1.34.0              \n[211] xml2_1.3.6                        AnnotationDbi_1.64.1             \n[213] future_1.34.0                     ModelMetrics_1.2.2.2             \n[215] rsvd_1.0.5                        munsell_0.5.1                    \n[217] KernSmooth_2.23-24                data.table_1.15.4                \n[219] htmlwidgets_1.6.4                 RColorBrewer_1.1-3               \n[221] biomaRt_2.58.0                    rlang_1.1.4                      \n[223] spatstat.sparse_3.1-0             spatstat.explore_3.2-6           \n[225] fansi_1.0.6                       hardhat_1.4.0                    \n[227] beeswarm_0.4.0                    prodlim_2024.06.25"
  },
  {
    "objectID": "labs/seurat/seurat_07_trajectory.html",
    "href": "labs/seurat/seurat_07_trajectory.html",
    "title": " Trajectory inference using Slingshot",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified."
  },
  {
    "objectID": "labs/seurat/seurat_07_trajectory.html#loading-libraries",
    "href": "labs/seurat/seurat_07_trajectory.html#loading-libraries",
    "title": " Trajectory inference using Slingshot",
    "section": "1 Loading libraries",
    "text": "1 Loading libraries\n\nsuppressPackageStartupMessages({\n  library(Seurat)\n  library(plotly)\n  options(rgl.printRglwidget = TRUE)\n  library(Matrix)\n  library(sparseMatrixStats)\n  library(slingshot)\n  library(tradeSeq)\n  library(patchwork)\n})\n\n# Define some color palette\npal &lt;- c(scales::hue_pal()(8), RColorBrewer::brewer.pal(9, \"Set1\"), RColorBrewer::brewer.pal(8, \"Set2\"))\nset.seed(1)\npal &lt;- rep(sample(pal, length(pal)), 200)\n\nNice function to easily draw a graph:\n\n# Add graph to the base R graphics plot\ndraw_graph &lt;- function(layout, graph, lwd = 0.2, col = \"grey\") {\n  res &lt;- rep(x = 1:(length(graph@p) - 1), times = (graph@p[-1] - graph@p[-length(graph@p)]))\n  segments(\n    x0 = layout[graph@i + 1, 1], x1 = layout[res, 1],\n    y0 = layout[graph@i + 1, 2], y1 = layout[res, 2], lwd = lwd, col = col\n  )\n}"
  },
  {
    "objectID": "labs/seurat/seurat_07_trajectory.html#preparing-data",
    "href": "labs/seurat/seurat_07_trajectory.html#preparing-data",
    "title": " Trajectory inference using Slingshot",
    "section": "2 Preparing data",
    "text": "2 Preparing data\nIf you have been using the Seurat, Bioconductor or Scanpy toolkits with your own data, you need to reach to the point where you have:\n\nA dimensionality reduction on which to run the trajectory (for example: PCA, ICA, MNN, harmony, Diffusion Maps, UMAP)\nThe cell clustering information (for example: from Louvain, K-means)\nA KNN/SNN graph (this is useful to inspect and sanity-check your trajectories)\n\nWe will be using a subset of a bone marrow dataset (originally containing about 100K cells) for this exercise on trajectory inference.\nThe bone marrow is the source of adult immune cells, and contains virtually all differentiation stages of cell from the immune system which later circulate in the blood to all other organs.\n\n\n\n\n\nYou can download the data:\n\n# download pre-computed data if missing or long compute\nfetch_data &lt;- TRUE\n\npath_trajectory &lt;- \"./data/trajectory\"\nif (!dir.exists(path_trajectory)) dir.create(path_trajectory, recursive = T)\n\n# url for source and intermediate data\npath_data &lt;- \"https://nextcloud.dc.scilifelab.se/public.php/webdav\"\ncurl_upass &lt;- \"-u zbC5fr2LbEZ9rSE:scRNAseq2025\"\npath_file &lt;- \"data/trajectory/trajectory_seurat_filtered.rds\"\n\nif (!dir.exists(dirname(path_file))) dir.create(dirname(path_file), recursive = TRUE)\nif (!file.exists(path_file)) download.file(url = file.path(path_data, \"trajectory/trajectory_seurat_filtered.rds\"), destfile = path_file, method = \"curl\", extra = curl_upass)\n\nWe already have pre-computed and subsetted the dataset (with 6688 cells and 3585 genes) following the analysis steps in this course. We then saved the objects, so you can use common tools to open and start to work with them (either in R or Python).\nIn addition there was some manual filtering done to remove clusters that are disconnected and cells that are hard to cluster, which can be seen in this script"
  },
  {
    "objectID": "labs/seurat/seurat_07_trajectory.html#reading-data",
    "href": "labs/seurat/seurat_07_trajectory.html#reading-data",
    "title": " Trajectory inference using Slingshot",
    "section": "3 Reading data",
    "text": "3 Reading data\n\nobj &lt;- readRDS(\"data/trajectory/trajectory_seurat_filtered.rds\")\n\n# Calculate cluster centroids (for plotting the labels later)\nmm &lt;- sparse.model.matrix(~ 0 + factor(obj$clusters_use))\ncolnames(mm) &lt;- levels(factor(obj$clusters_use))\ncentroids2d &lt;- as.matrix(t(t(obj@reductions$umap@cell.embeddings) %*% mm) / Matrix::colSums(mm))\n\nLet’s visualize which clusters we have in our dataset:\n\nvars &lt;- c(\"batches\", \"dataset\", \"clusters_use\", \"Phase\")\npl &lt;- list()\n\nfor (i in vars) {\n  pl[[i]] &lt;- DimPlot(obj, group.by = i, label = T) + theme_void() + NoLegend()\n}\nwrap_plots(pl)\n\n\n\n\n\n\n\n\nYou can check, for example, the number of cells in each cluster:\n\ntable(obj$clusters)\n\n\n  1   2   5   6   7   8   9  11  12  13  14  15  16  17  18  19  20  21  22  23 \n128  71  90 160 147 120 160 130 132  78  90 150 140  76 141  90  98 149  90  10 \n 25  26  27  28  29  32  33  34  35  36  37  38  41  43  44  45  46  47  49  50 \n 56 154  98  76 125 150 150 146 150 148 135 128 145 134 110 149 140 113 132  85 \n 52  53  54  55  57  58  59  60  61 \n126 129  57 129 147 127 118 120 101"
  },
  {
    "objectID": "labs/seurat/seurat_07_trajectory.html#exploring-the-data",
    "href": "labs/seurat/seurat_07_trajectory.html#exploring-the-data",
    "title": " Trajectory inference using Slingshot",
    "section": "4 Exploring the data",
    "text": "4 Exploring the data\nIt is crucial that you have some understanding of the dataset being analyzed. What are the clusters you see in your data and most importantly How are the clusters related to each other?. Well, let’s explore the data a bit. With the help of this table, write down which cluster numbers in your dataset express these key markers.\n\n\n\nMarker\nCell Type\n\n\n\n\nCd34\nHSC progenitor\n\n\nMs4a1\nB cell lineage\n\n\nCd3e\nT cell lineage\n\n\nLtf\nGranulocyte lineage\n\n\nCst3\nMonocyte lineage\n\n\nMcpt8\nMast Cell lineage\n\n\nAlas2\nRBC lineage\n\n\nSiglech\nDendritic cell lineage\n\n\nC1qc\nMacrophage cell lineage\n\n\nPf4\nMegakaryocyte cell lineage\n\n\n\n\nvars &lt;- c(\"Cd34\", \"Ms4a1\", \"Cd3e\", \"Ltf\", \"Cst3\", \"Mcpt8\", \"Alas2\", \"Siglech\", \"C1qc\", \"Pf4\")\npl &lt;- list()\n\npl &lt;- list(DimPlot(obj, group.by = \"clusters_use\", label = T) + theme_void() + NoLegend())\nfor (i in vars) {\n  pl[[i]] &lt;- FeaturePlot(obj, features = i, order = T) + theme_void() + NoLegend()\n}\nwrap_plots(pl)\n\n\n\n\n\n\n\n\nAnother way to better explore your data is to look in higher dimensions, to really get a sense for what is right or wrong. As mentioned in the dimensionality reduction exercises, here we ran UMAP with 3 dimensions.\n\n\n\n\n\n\nImportant\n\n\n\nThe UMAP needs to be computed to results in exactly 3 dimensions\n\n\nSince the steps below are identical to both Seurat and Scran pipelines, we will extract the matrices from both, so it is clear what is being used where and to remove long lines of code used to get those matrices. We will use them all. Plot in 3D with Plotly:\n\ndf &lt;- data.frame(obj@reductions$umap3d@cell.embeddings, variable = factor(obj$clusters_use))\ncolnames(df)[1:3] &lt;- c(\"UMAP_1\", \"UMAP_2\", \"UMAP_3\")\np_State &lt;- plot_ly(df, x = ~UMAP_1, y = ~UMAP_2, z = ~UMAP_3, color = ~variable, colors = pal, size = .5)  %&gt;% add_markers()\np_State\n\n\n\n\n\n\n# to save interactive plot and open in a new tab\ntry(htmlwidgets::saveWidget(p_State, selfcontained = T, \"data/trajectory/umap_3d_clustering_plotly.html\"), silent = T)\nutils::browseURL(\"data/trajectory/umap_3d_clustering_plotly.html\")\n\nWe can now compute the lineages on these dataset.\n\n# Define lineage ends\nENDS &lt;- c(\"17\", \"27\", \"25\", \"16\", \"26\", \"53\", \"49\")\n\nset.seed(1)\nlineages &lt;- as.SlingshotDataSet(getLineages(\n  data           = obj@reductions$umap3d@cell.embeddings,\n  clusterLabels  = obj$clusters_use,\n  dist.method    = \"mnn\", # It can be: \"simple\", \"scaled.full\", \"scaled.diag\", \"slingshot\" or \"mnn\"\n  end.clus       = ENDS, # You can also define the ENDS!\n  start.clus     = \"34\"\n)) # define where to START the trajectories\n\n\n# IF NEEDED, ONE CAN ALSO MANULALLY EDIT THE LINEAGES, FOR EXAMPLE:\n# sel &lt;- sapply( lineages@lineages, function(x){rev(x)[1]} ) %in% ENDS\n# lineages@lineages &lt;- lineages@lineages[ sel ]\n# names(lineages@lineages) &lt;- paste0(\"Lineage\",1:length(lineages@lineages))\n# lineages\n\n\n# Change the reduction to our \"fixed\" UMAP2d (FOR VISUALISATION ONLY)\nlineages@reducedDim &lt;- obj@reductions$umap@cell.embeddings\n\n{\n  plot(obj@reductions$umap@cell.embeddings, col = pal[obj$clusters_use], cex = .5, pch = 16)\n  lines(lineages, lwd = 1, col = \"black\", cex = 2)\n  text(centroids2d, labels = rownames(centroids2d), cex = 0.8, font = 2, col = \"white\")\n}\n\n\n\n\n\n\n\n\nMuch better!"
  },
  {
    "objectID": "labs/seurat/seurat_07_trajectory.html#defining-principal-curves",
    "href": "labs/seurat/seurat_07_trajectory.html#defining-principal-curves",
    "title": " Trajectory inference using Slingshot",
    "section": "5 Defining Principal Curves",
    "text": "5 Defining Principal Curves\nOnce the clusters are connected, Slingshot allows you to transform them to a smooth trajectory using principal curves. This is an algorithm that iteratively changes an initial curve to better match the data points. It was developed for linear data. To apply it to single-cell data, slingshot adds two enhancements:\n\nIt will run principal curves for each ‘lineage’, which is a set of clusters that go from a defined start cluster to some end cluster\nLineages with a same set of clusters will be constrained so that their principal curves remain bundled around the overlapping clusters\n\nSince the function getCurves() takes some time to run, we can speed up the convergence of the curve fitting process by reducing the amount of cells to use in each lineage. Ideally you could all cells, but here we had set approx_points to 300 to speed up. Feel free to adjust that for your dataset.\n\n# Define curves\ncurves &lt;- as.SlingshotDataSet(getCurves(\n  data          = lineages,\n  thresh        = 1e-1,\n  stretch       = 1e-1,\n  allow.breaks  = F,\n  approx_points = 100\n))\n\ncurves\n\nclass: SlingshotDataSet \n\n Samples Dimensions\n    5828          2\n\nlineages: 7 \nLineage1: 34  18  36  33  55  59  44  60  58  29  8  43  47  49  \nLineage2: 34  18  11  15  46  9  1  2  5  13  28  17  \nLineage3: 34  18  11  15  35  7  32  6  54  25  \nLineage4: 34  18  11  15  35  7  32  6  27  \nLineage5: 34  18  36  21  12  20  16  \nLineage6: 34  18  36  33  55  38  53  \nLineage7: 34  18  36  26  \n\ncurves: 7 \nCurve1: Length: 6.7241  Samples: 2161.05\nCurve2: Length: 7.3487  Samples: 2097.02\nCurve3: Length: 3.5349  Samples: 1502.25\nCurve4: Length: 2.5623  Samples: 1387.66\nCurve5: Length: 2.9268  Samples: 979.78\nCurve6: Length: 2.8976  Samples: 1086.34\nCurve7: Length: 2.1323  Samples: 644.86\n\n# Plots\n{\n  plot(obj@reductions$umap@cell.embeddings, col = pal[obj$clusters_use], pch = 16)\n  lines(curves, lwd = 2, col = \"black\")\n  text(centroids2d, labels = rownames(centroids2d), cex = 1, font = 2)\n}\n\n\n\n\n\n\n\n\nWith those results in hands, we can now compute the differentiation pseudotime.\n\npseudotime &lt;- slingPseudotime(curves, na = FALSE)\ncellWeights &lt;- slingCurveWeights(curves)\n\nx &lt;- rowMeans(pseudotime)\nx &lt;- x / max(x)\no &lt;- order(x)\n\n{\n  plot(obj@reductions$umap@cell.embeddings[o, ],\n    main = paste0(\"pseudotime\"), pch = 16, cex = 0.4, axes = F, xlab = \"\", ylab = \"\",\n    col = colorRampPalette(c(\"grey70\", \"orange3\", \"firebrick\", \"purple4\"))(99)[x[o] * 98 + 1]\n  )\n  points(centroids2d, cex = 2.5, pch = 16, col = \"#FFFFFF99\")\n  text(centroids2d, labels = rownames(centroids2d), cex = 1, font = 2)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe pseudotime represents the distance of every cell to the starting cluster!"
  },
  {
    "objectID": "labs/seurat/seurat_07_trajectory.html#finding-differentially-expressed-genes",
    "href": "labs/seurat/seurat_07_trajectory.html#finding-differentially-expressed-genes",
    "title": " Trajectory inference using Slingshot",
    "section": "6 Finding differentially expressed genes",
    "text": "6 Finding differentially expressed genes\nThe main way to interpret a trajectory is to find genes that change along the trajectory. There are many ways to define differential expression along a trajectory:\n\nExpression changes along a particular path (i.e. change with pseudotime)\nExpression differences between branches\nExpression changes at branch points\nExpression changes somewhere along the trajectory\n…\n\ntradeSeq is a recently proposed algorithm to find trajectory differentially expressed genes. It works by smoothing the gene expression along the trajectory by fitting a smoother using generalized additive models (GAMs), and testing whether certain coefficients are statistically different between points in the trajectory.\n\nBiocParallel::register(BiocParallel::MulticoreParam())\n\nThe fitting of GAMs can take quite a while, so for demonstration purposes we first do a very stringent filtering of the genes.\n\n\n\n\n\n\nTip\n\n\n\nIn an ideal experiment, you would use all the genes, or at least those defined as being variable.\n\n\n\nsel_cells &lt;- split(colnames(obj@assays$RNA@data), obj$clusters_use)\nsel_cells &lt;- unlist(lapply(sel_cells, function(x) {\n  set.seed(1)\n  return(sample(x, 20))\n}))\n\ngv &lt;- as.data.frame(na.omit(scran::modelGeneVar(obj@assays$RNA@data[, sel_cells])))\ngv &lt;- gv[order(gv$bio, decreasing = T), ]\nsel_genes &lt;- sort(rownames(gv)[1:500])\n\nFitting the model:\n\n\n\n\n\n\nCaution\n\n\n\nThis is a slow compute intensive step, we will not run this now and instead use a pre-computed file in the step below.\n\n\n\npath_file &lt;- \"data/trajectory/seurat_scegam.rds\"\n\n# fetch_data is defined at the top of this document\nif (!fetch_data) {\n  sceGAM &lt;- fitGAM(\n    counts = drop0(obj@assays$RNA@data[sel_genes, sel_cells]),\n    pseudotime = pseudotime[sel_cells, ],\n    cellWeights = cellWeights[sel_cells, ],\n    nknots = 5, verbose = T, parallel = T, sce = TRUE,\n    BPPARAM = BiocParallel::MulticoreParam()\n  )\n  saveRDS(sceGAM, path_file)\n}\n\nDownload the precomputed file.\n\npath_file &lt;- \"data/trajectory/seurat_scegam.rds\"\n\n# fetch_data is defined at the top of this document\nif (fetch_data) {\n  if (!file.exists(path_file)) download.file(url = file.path(path_data, \"trajectory/seurat_scegam.rds\"), destfile = path_file, method = \"curl\", extra = curl_upass)\n}\n\n\n# read data\nsceGAM &lt;- readRDS(path_file)\n\n\nplotGeneCount(curves, clusters = obj$clusters_use, models = sceGAM)\n\n\n\n\n\n\n\nlineages\n\nclass: SlingshotDataSet \n\n Samples Dimensions\n    5828          2\n\nlineages: 7 \nLineage1: 34  18  36  33  55  59  44  60  58  29  8  43  47  49  \nLineage2: 34  18  11  15  46  9  1  2  5  13  28  17  \nLineage3: 34  18  11  15  35  7  32  6  54  25  \nLineage4: 34  18  11  15  35  7  32  6  27  \nLineage5: 34  18  36  21  12  20  16  \nLineage6: 34  18  36  33  55  38  53  \nLineage7: 34  18  36  26  \n\ncurves: 0 \n\n\n\nlc &lt;- sapply(lineages@lineages, function(x) {\n  rev(x)[1]\n})\nnames(lc) &lt;- gsub(\"Lineage\", \"L\", names(lc))\n\n{\n  plot(obj@reductions$umap@cell.embeddings, col = pal[obj$clusters_use], pch = 16)\n  lines(curves, lwd = 2, col = \"black\")\n  points(centroids2d[lc, ], col = \"black\", pch = 16, cex = 4)\n  text(centroids2d[lc, ], labels = names(lc), cex = 1, font = 2, col = \"white\")\n}\n\n\n\n\n\n\n\n\n\n6.1 Genes that change with pseudotime\nWe can first look at general trends of gene expression across pseudotime.\n\nset.seed(8)\nres &lt;- na.omit(associationTest(sceGAM, contrastType = \"consecutive\"))\nres &lt;- res[res$pvalue &lt; 1e-3, ]\nres &lt;- res[res$waldStat &gt; mean(res$waldStat), ]\nres &lt;- res[order(res$waldStat, decreasing = T), ]\nres[1:10, ]\n\n\n\n  \n\n\n\nWe can plot their expression:\n\npar(mfrow = c(4, 4), mar = c(.1, .1, 2, 1))\n{\n  plot(obj@reductions$umap@cell.embeddings, col = pal[obj$clusters_use], cex = .5, pch = 16, axes = F, xlab = \"\", ylab = \"\")\n  lines(curves, lwd = 2, col = \"black\")\n  points(centroids2d[lc, ], col = \"black\", pch = 15, cex = 3, xpd = T)\n  text(centroids2d[lc, ], labels = names(lc), cex = 1, font = 2, col = \"white\", xpd = T)\n}\n\nvars &lt;- rownames(res[1:15, ])\nvars &lt;- na.omit(vars[vars != \"NA\"])\n\nfor (i in vars) {\n  x &lt;- drop0(obj@assays$RNA@data)[i, ]\n  x &lt;- (x - min(x)) / (max(x) - min(x))\n  o &lt;- order(x)\n  plot(obj@reductions$umap@cell.embeddings[o, ],\n    main = paste0(i), pch = 16, cex = 0.5, axes = F, xlab = \"\", ylab = \"\",\n    col = colorRampPalette(c(\"lightgray\", \"grey60\", \"navy\"))(99)[x[o] * 98 + 1]\n  )\n}\n\n\n\n\n\n\n\n\n\n\n6.2 Genes that change between two pseudotime points\nWe can define custom pseudotime values of interest if we’re interested in genes that change between particular point in pseudotime. By default, we can look at differences between start and end:\n\nres &lt;- na.omit(startVsEndTest(sceGAM, pseudotimeValues = c(0, 1)))\nres &lt;- res[res$pvalue &lt; 1e-3, ]\nres &lt;- res[res$waldStat &gt; mean(res$waldStat), ]\nres &lt;- res[order(res$waldStat, decreasing = T), ]\nres[1:10, 1:6]\n\n\n\n  \n\n\n\nYou can see now that there are several more columns, one for each lineage. This table represents the differential expression within each lineage, to identify which genes go up or down. Let’s check lineage 1:\n\n# Get the top UP and Down regulated in lineage 1\nres_lin1 &lt;- sort(setNames(res$logFClineage1, rownames(res)))\nvars &lt;- names(c(rev(res_lin1)[1:7], res_lin1[1:8]))\nvars &lt;- na.omit(vars[vars != \"NA\"])\n\npar(mfrow = c(4, 4), mar = c(.1, .1, 2, 1))\n\n{\n  plot(obj@reductions$umap@cell.embeddings, col = pal[obj$clusters_use], cex = .5, pch = 16, axes = F, xlab = \"\", ylab = \"\")\n  lines(curves, lwd = 2, col = \"black\")\n  points(centroids2d[lc, ], col = \"black\", pch = 15, cex = 3, xpd = T)\n  text(centroids2d[lc, ], labels = names(lc), cex = 1, font = 2, col = \"white\", xpd = T)\n}\n\nfor (i in vars) {\n  x &lt;- drop0(obj@assays$RNA@data)[i, ]\n  x &lt;- (x - min(x)) / (max(x) - min(x))\n  o &lt;- order(x)\n  plot(obj@reductions$umap@cell.embeddings[o, ],\n    main = paste0(i), pch = 16, cex = 0.5, axes = F, xlab = \"\", ylab = \"\",\n    col = colorRampPalette(c(\"lightgray\", \"grey60\", \"navy\"))(99)[x[o] * 98 + 1]\n  )\n}\n\n\n\n\n\n\n\n\n\n\n6.3 Genes that are different between lineages\nMore interesting are genes that are different between two branches. We may have seen some of these genes already pop up in previous analyses of pseudotime. There are several ways to define “different between branches”, and each have their own functions:\n\nDifferent at the end points, using diffEndTest()\nDifferent at the branching point, using earlyDETest()\nDifferent somewhere in pseudotime the branching point, using patternTest()\n\nNote that the last function requires that the pseudotimes between two lineages are aligned.\n\nres &lt;- na.omit(diffEndTest(sceGAM))\nres &lt;- res[res$pvalue &lt; 1e-3, ]\nres &lt;- res[res$waldStat &gt; mean(res$waldStat), ]\nres &lt;- res[order(res$waldStat, decreasing = T), ]\nres[1:10, ]\n\n\n\n  \n\n\n\nYou can see now that there are even more columns, one for the pairwise comparison between each lineage. Let’s check lineage 1 vs lineage 2:\n\n# Get the top UP and Down regulated in lineage 1 vs 2\nres_lin1_2 &lt;- sort(setNames(res$logFC1_2, rownames(res)))\nvars &lt;- names(c(rev(res_lin1_2)[1:7], res_lin1_2[1:8]))\nvars &lt;- na.omit(vars[vars != \"NA\"])\n\npar(mfrow = c(4, 4), mar = c(.1, .1, 2, 1))\n{\n  plot(obj@reductions$umap@cell.embeddings, col = pal[obj$clusters_use], cex = .5, pch = 16, axes = F, xlab = \"\", ylab = \"\")\n  lines(curves, lwd = 2, col = \"black\")\n  points(centroids2d[lc, ], col = \"black\", pch = 15, cex = 3, xpd = T)\n  text(centroids2d[lc, ], labels = names(lc), cex = 1, font = 2, col = \"white\", xpd = T)\n}\n\nfor (i in vars) {\n  x &lt;- drop0(obj@assays$RNA@data)[i, ]\n  x &lt;- (x - min(x)) / (max(x) - min(x))\n  o &lt;- order(x)\n  plot(obj@reductions$umap@cell.embeddings[o, ],\n    main = paste0(i), pch = 16, cex = 0.5, axes = F, xlab = \"\", ylab = \"\",\n    col = colorRampPalette(c(\"lightgray\", \"grey60\", \"navy\"))(99)[x[o] * 98 + 1]\n  )\n}\n\n\n\n\n\n\n\n\nCheck out this vignette for a more in-depth overview of tradeSeq and many other differential expression tests."
  },
  {
    "objectID": "labs/seurat/seurat_07_trajectory.html#generating-batch-corrected-data-for-differential-gene-expression",
    "href": "labs/seurat/seurat_07_trajectory.html#generating-batch-corrected-data-for-differential-gene-expression",
    "title": " Trajectory inference using Slingshot",
    "section": "7 Generating batch-corrected data for differential gene expression",
    "text": "7 Generating batch-corrected data for differential gene expression\nBefore computing differential gene expression, sometimes it is a good idea to make sure our dataset is somewhat homogeneous (without very strong batch effects). In this dataset, we actually used data from 4 different technologies (Drop-seq, SmartSeq2 and 10X) and therefore massive differences in read counts can be observed:\nIf you want to know more about how to control for this issue, please have a look at batch_corrected_counts.Rmd"
  },
  {
    "objectID": "labs/seurat/seurat_07_trajectory.html#references",
    "href": "labs/seurat/seurat_07_trajectory.html#references",
    "title": " Trajectory inference using Slingshot",
    "section": "8 References",
    "text": "8 References\nCannoodt, Robrecht, Wouter Saelens, and Yvan Saeys. 2016. “Computational Methods for Trajectory Inference from Single-Cell Transcriptomics.” European Journal of Immunology 46 (11): 2496–2506. doi.\nSaelens, Wouter, Robrecht Cannoodt, Helena Todorov, and Yvan Saeys. 2019. “A Comparison of Single-Cell Trajectory Inference Methods.” Nature Biotechnology 37 (5): 547–54. doi."
  },
  {
    "objectID": "labs/seurat/seurat_07_trajectory.html#session-info",
    "href": "labs/seurat/seurat_07_trajectory.html#session-info",
    "title": " Trajectory inference using Slingshot",
    "section": "9 Session info",
    "text": "9 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] patchwork_1.2.0             tradeSeq_1.16.0            \n [3] slingshot_2.10.0            TrajectoryUtils_1.10.0     \n [5] SingleCellExperiment_1.24.0 SummarizedExperiment_1.32.0\n [7] Biobase_2.62.0              GenomicRanges_1.54.1       \n [9] GenomeInfoDb_1.38.1         IRanges_2.36.0             \n[11] S4Vectors_0.40.2            BiocGenerics_0.48.1        \n[13] princurve_2.1.6             sparseMatrixStats_1.14.0   \n[15] MatrixGenerics_1.14.0       matrixStats_1.4.1          \n[17] Matrix_1.6-5                plotly_4.10.4              \n[19] ggplot2_3.5.1               Seurat_5.1.0               \n[21] SeuratObject_5.0.2          sp_2.1-4                   \n\nloaded via a namespace (and not attached):\n  [1] RcppAnnoy_0.0.22          splines_4.3.3            \n  [3] later_1.3.2               bitops_1.0-8             \n  [5] tibble_3.2.1              polyclip_1.10-7          \n  [7] fastDummies_1.7.4         lifecycle_1.0.4          \n  [9] edgeR_4.0.16              globals_0.16.3           \n [11] lattice_0.22-6            MASS_7.3-60.0.1          \n [13] crosstalk_1.2.1           magrittr_2.0.3           \n [15] limma_3.58.1              rmarkdown_2.28           \n [17] yaml_2.3.10               metapod_1.10.0           \n [19] httpuv_1.6.15             sctransform_0.4.1        \n [21] spam_2.10-0               spatstat.sparse_3.1-0    \n [23] reticulate_1.39.0         cowplot_1.1.3            \n [25] pbapply_1.7-2             RColorBrewer_1.1-3       \n [27] abind_1.4-5               zlibbioc_1.48.0          \n [29] Rtsne_0.17                purrr_1.0.2              \n [31] RCurl_1.98-1.16           GenomeInfoDbData_1.2.11  \n [33] ggrepel_0.9.6             irlba_2.3.5.1            \n [35] listenv_0.9.1             spatstat.utils_3.1-0     \n [37] goftest_1.2-3             RSpectra_0.16-2          \n [39] dqrng_0.3.2               spatstat.random_3.2-3    \n [41] fitdistrplus_1.2-1        parallelly_1.38.0        \n [43] DelayedMatrixStats_1.24.0 leiden_0.4.3.1           \n [45] codetools_0.2-20          DelayedArray_0.28.0      \n [47] scuttle_1.12.0            tidyselect_1.2.1         \n [49] farver_2.1.2              ScaledMatrix_1.10.0      \n [51] viridis_0.6.5             spatstat.explore_3.2-6   \n [53] jsonlite_1.8.8            BiocNeighbors_1.20.0     \n [55] progressr_0.14.0          ggridges_0.5.6           \n [57] survival_3.7-0            tools_4.3.3              \n [59] ica_1.0-3                 Rcpp_1.0.13              \n [61] glue_1.7.0                gridExtra_2.3            \n [63] SparseArray_1.2.2         xfun_0.47                \n [65] mgcv_1.9-1                dplyr_1.1.4              \n [67] withr_3.0.1               fastmap_1.2.0            \n [69] bluster_1.12.0            fansi_1.0.6              \n [71] digest_0.6.37             rsvd_1.0.5               \n [73] R6_2.5.1                  mime_0.12                \n [75] colorspace_2.1-1          scattermore_1.2          \n [77] tensor_1.5                spatstat.data_3.1-2      \n [79] utf8_1.2.4                tidyr_1.3.1              \n [81] generics_0.1.3            data.table_1.15.4        \n [83] httr_1.4.7                htmlwidgets_1.6.4        \n [85] S4Arrays_1.2.0            uwot_0.1.16              \n [87] pkgconfig_2.0.3           gtable_0.3.5             \n [89] lmtest_0.9-40             XVector_0.42.0           \n [91] htmltools_0.5.8.1         dotCall64_1.1-1          \n [93] scales_1.3.0              png_0.1-8                \n [95] scran_1.30.0              knitr_1.48               \n [97] reshape2_1.4.4            nlme_3.1-165             \n [99] zoo_1.8-12                stringr_1.5.1            \n[101] KernSmooth_2.23-24        parallel_4.3.3           \n[103] miniUI_0.1.1.1            pillar_1.9.0             \n[105] grid_4.3.3                vctrs_0.6.5              \n[107] RANN_2.6.2                promises_1.3.0           \n[109] BiocSingular_1.18.0       beachmat_2.18.0          \n[111] xtable_1.8-4              cluster_2.1.6            \n[113] evaluate_0.24.0           cli_3.6.3                \n[115] locfit_1.5-9.9            compiler_4.3.3           \n[117] rlang_1.1.4               crayon_1.5.3             \n[119] future.apply_1.11.2       labeling_0.4.3           \n[121] plyr_1.8.9                stringi_1.8.4            \n[123] viridisLite_0.4.2         deldir_2.0-4             \n[125] BiocParallel_1.36.0       munsell_0.5.1            \n[127] lazyeval_0.2.2            spatstat.geom_3.2-9      \n[129] RcppHNSW_0.6.0            future_1.34.0            \n[131] statmod_1.5.0             shiny_1.9.1              \n[133] ROCR_1.0-11               igraph_2.0.3"
  },
  {
    "objectID": "labs/seurat/seurat_08_spatial.html",
    "href": "labs/seurat/seurat_08_spatial.html",
    "title": " Spatial Transcriptomics",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified.\nThis tutorial is adapted from the Seurat vignette.\nSpatial transcriptomic data with the Visium platform is in many ways similar to scRNAseq data. It contains UMI counts for 5-20 cells instead of single cells, but is still quite sparse in the same way as scRNAseq data is, but with the additional information about spatial location in the tissue.\nHere we will first run quality control in a similar manner to scRNAseq data, then QC filtering, dimensionality reduction, integration and clustering. Then we will use scRNAseq data from mouse cortex to run label transfer to predict celltypes in the Visium spots.\nWe will use two Visium spatial transcriptomics dataset of the mouse brain (Sagittal), which are publicly available from the 10x genomics website. Note, that these dataset have already been filtered for spots that does not overlap with the tissue."
  },
  {
    "objectID": "labs/seurat/seurat_08_spatial.html#meta-st_prep",
    "href": "labs/seurat/seurat_08_spatial.html#meta-st_prep",
    "title": " Spatial Transcriptomics",
    "section": "1 Preparation",
    "text": "1 Preparation\nLoad packages\n\n# remotes::install_github('satijalab/seurat-data', dependencies=FALSE)\n\nsuppressPackageStartupMessages({\n    library(Matrix)\n    library(dplyr)\n    library(SeuratData)\n    library(Seurat)\n    library(ggplot2)\n    library(patchwork)\n    library(dplyr)\n})\n\nLoad ST data\nThe package SeuratData has some seurat objects for different datasets. Among those are spatial transcriptomics data from mouse brain and kidney. Here we will download and process sections from the mouse brain.\n\n# download pre-computed data if missing or long compute\nfetch_data &lt;- TRUE\n\n# url for source and intermediate data\npath_data &lt;- \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\n\noutdir &lt;- \"data/spatial/\"\nif (!dir.exists(outdir)) dir.create(outdir, showWarnings = F)\n\n# to list available datasets in SeuratData you can run AvailableData()\n\n# first we dowload the dataset\nif (!(\"stxBrain.SeuratData\" %in% rownames(SeuratData::InstalledData()))) {\n    InstallData(\"stxBrain\")\n}\n\n# now we can list what datasets we have downloaded\nInstalledData()\n\n\n\n  \n\n\n# now we will load the seurat object for one section\nbrain1 &lt;- LoadData(\"stxBrain\", type = \"anterior1\")\nbrain2 &lt;- LoadData(\"stxBrain\", type = \"posterior1\")\n\nMerge into one seurat object\n\nbrain &lt;- merge(brain1, brain2)\nbrain\n\nAn object of class Seurat \n31053 features across 6049 samples within 1 assay \nActive assay: Spatial (31053 features, 0 variable features)\n 2 images present: anterior1, posterior1\n\n\nAs you can see, now we do not have the assay RNA, but instead an assay called Spatial."
  },
  {
    "objectID": "labs/seurat/seurat_08_spatial.html#meta-st_qc",
    "href": "labs/seurat/seurat_08_spatial.html#meta-st_qc",
    "title": " Spatial Transcriptomics",
    "section": "2 Quality control",
    "text": "2 Quality control\nSimilar to scRNA-seq we use statistics on number of counts, number of features and percent mitochondria for quality control.\nNow the counts and feature counts are calculated on the Spatial assay, so they are named nCount_Spatial and nFeature_Spatial.\n\nbrain &lt;- PercentageFeatureSet(brain, \"^mt-\", col.name = \"percent_mito\")\nbrain &lt;- PercentageFeatureSet(brain, \"^Hb.*-\", col.name = \"percent_hb\")\n\nVlnPlot(brain, features = c(\"nCount_Spatial\", \"nFeature_Spatial\", \"percent_mito\", \"percent_hb\"), pt.size = 0.1, ncol = 2) + NoLegend()\n\n\n\n\n\n\n\n\nWe can also plot the same data onto the tissue section.\n\nSpatialFeaturePlot(brain, features = c(\"nCount_Spatial\", \"nFeature_Spatial\", \"percent_mito\", \"percent_hb\"))\n\n\n\n\n\n\n\n\nAs you can see, the spots with low number of counts/features and high mitochondrial content are mainly towards the edges of the tissue. It is quite likely that these regions are damaged tissue. You may also see regions within a tissue with low quality if you have tears or folds in your section.\nBut remember, for some tissue types, the amount of genes expressed and proportion mitochondria may also be a biological features, so bear in mind what tissue you are working on and what these features mean.\n\n2.1 Filter spots\nSelect all spots with less than 25% mitocondrial reads, less than 20% hb-reads and 500 detected genes. You must judge for yourself based on your knowledge of the tissue what are appropriate filtering criteria for your dataset.\n\nbrain &lt;- brain[, brain$nFeature_Spatial &gt; 500 & brain$percent_mito &lt; 25 & brain$percent_hb &lt; 20]\n\nAnd replot onto tissue section:\n\nSpatialFeaturePlot(brain, features = c(\"nCount_Spatial\", \"nFeature_Spatial\", \"percent_mito\"))\n\n\n\n\n\n\n\n\n\n\n2.2 Top expressed genes\nAs for scRNA-seq data, we will look at what the top expressed genes are.\n\nC &lt;- GetAssayData(brain, assay = \"Spatial\", slot = \"counts\")\nC@x &lt;- C@x / rep.int(colSums(C), diff(C@p))\nmost_expressed &lt;- order(Matrix::rowSums(C), decreasing = T)[20:1]\nboxplot(as.matrix(t(C[most_expressed, ])),\n    cex = 0.1, las = 1, xlab = \"% total count per cell\",\n    col = (scales::hue_pal())(20)[20:1], horizontal = TRUE\n)\n\n\n\n\n\n\n\nrm(C)\ngc()\n\n            used   (Mb) gc trigger   (Mb)  max used   (Mb)\nNcells   3360409  179.5    5248368  280.3   5248368  280.3\nVcells 189921573 1449.0  375078628 2861.7 357748573 2729.5\n\n\nAs you can see, the mitochondrial genes are among the top expressed genes. Also the lncRNA gene Bc1 (brain cytoplasmic RNA 1). Also one hemoglobin gene.\n\n\n2.3 Filter genes\nWe will remove the Bc1 gene, hemoglobin genes (blood contamination) and the mitochondrial genes.\n\ndim(brain)\n\n[1] 31053  5789\n\n# Filter Bl1\nbrain &lt;- brain[!grepl(\"Bc1\", rownames(brain)), ]\n\n# Filter Mitocondrial\nbrain &lt;- brain[!grepl(\"^mt-\", rownames(brain)), ]\n\n# Filter Hemoglobin gene (optional if that is a problem on your data)\nbrain &lt;- brain[!grepl(\"^Hb.*-\", rownames(brain)), ]\n\ndim(brain)\n\n[1] 31031  5789"
  },
  {
    "objectID": "labs/seurat/seurat_08_spatial.html#meta-st_analysis",
    "href": "labs/seurat/seurat_08_spatial.html#meta-st_analysis",
    "title": " Spatial Transcriptomics",
    "section": "3 Analysis",
    "text": "3 Analysis\nWe will proceed with the data in a very similar manner to scRNA-seq data.\nFor ST data, the Seurat team recommends to use SCTransform() for normalization, so we will do that. SCTransform() will select variable genes and normalize in one step.\n\nbrain &lt;- SCTransform(brain, assay = \"Spatial\", method = \"poisson\", verbose = TRUE)\n\nNow we can plot gene expression of individual genes, the gene Hpca is a strong hippocampal marker and Ttr is a marker of the choroid plexus.\n\nSpatialFeaturePlot(brain, features = c(\"Hpca\", \"Ttr\"))\n\n\n\n\n\n\n\n\nIf you want to see the tissue better you can modify point size and transparency of the points.\n\nSpatialFeaturePlot(brain, features = \"Ttr\", pt.size.factor = 1, alpha = c(0.1, 1))\n\n\n\n\n\n\n\n\n\n3.1 Dimensionality reduction and clustering\nWe can then now run dimensionality reduction and clustering using the same workflow as we use for scRNA-seq analysis.\nBut make sure you run it on the SCT assay.\n\nbrain &lt;- RunPCA(brain, assay = \"SCT\", verbose = FALSE)\nbrain &lt;- FindNeighbors(brain, reduction = \"pca\", dims = 1:30)\nbrain &lt;- FindClusters(brain, verbose = FALSE)\nbrain &lt;- RunUMAP(brain, reduction = \"pca\", dims = 1:30)\n\nWe can then plot clusters onto umap or onto the tissue section.\n\nDimPlot(brain, reduction = \"umap\", group.by = c(\"ident\", \"orig.ident\"))\n\n\n\n\n\n\n\n\n\nSpatialDimPlot(brain)\n\n\n\n\n\n\n\n\nWe can also plot each cluster separately\n\nSpatialDimPlot(brain, cells.highlight = CellsByIdentities(brain), facet.highlight = TRUE, ncol = 5)\n\n\n\n\n\n\n\n\n\n\n3.2 Integration\nQuite often, there are strong batch effects between different ST sections, so it may be a good idea to integrate the data across sections.\nWe will do a similar integration as in the Data Integration lab, but this time we will use the SCT assay for integration. Therefore we need to run PrepSCTIntegration() which will compute the sctransform residuals for all genes in both the datasets.\n\n# create a list of the original data that we loaded to start with\nst.list &lt;- list(anterior1 = brain1, posterior1 = brain2)\n\n# run SCT on both datasets\nst.list &lt;- lapply(st.list, SCTransform, assay = \"Spatial\", method = \"poisson\")\n\n# need to set maxSize for PrepSCTIntegration to work\noptions(future.globals.maxSize = 2000 * 1024^2) # set allowed size to 2K MiB\n\nst.features &lt;- SelectIntegrationFeatures(st.list, nfeatures = 3000, verbose = FALSE)\nst.list &lt;- PrepSCTIntegration(object.list = st.list, anchor.features = st.features, verbose = FALSE)\n\nNow we can perform the actual integration.\n\nint.anchors &lt;- FindIntegrationAnchors(object.list = st.list, normalization.method = \"SCT\", verbose = FALSE, anchor.features = st.features)\nbrain.integrated &lt;- IntegrateData(anchorset = int.anchors, normalization.method = \"SCT\", verbose = FALSE)\n\nrm(int.anchors, st.list)\ngc()\n\n            used   (Mb) gc trigger   (Mb)   max used   (Mb)\nNcells   3530321  188.6    5248368  280.3    5248368  280.3\nVcells 546165692 4167.0 1148292444 8760.8 1147466814 8754.5\n\n\nThen we run dimensionality reduction and clustering as before.\n\nbrain.integrated &lt;- RunPCA(brain.integrated, verbose = FALSE)\nbrain.integrated &lt;- FindNeighbors(brain.integrated, dims = 1:30)\nbrain.integrated &lt;- FindClusters(brain.integrated, verbose = FALSE)\nbrain.integrated &lt;- RunUMAP(brain.integrated, dims = 1:30)\n\n\nDimPlot(brain.integrated, reduction = \"umap\", group.by = c(\"ident\", \"orig.ident\"))\n\n\n\n\n\n\n\n\n\nSpatialDimPlot(brain.integrated)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nDo you see any differences between the integrated and non-integrated clustering? Judge for yourself, which of the clusterings do you think looks best? As a reference, you can compare to brain regions in the Allen brain atlas.\n\n\n\n\n3.3 Spatially Variable Features\nThere are two main workflows to identify molecular features that correlate with spatial location within a tissue. The first is to perform differential expression based on spatially distinct clusters, the other is to find features that have spatial patterning without taking clusters or spatial annotation into account. First, we will do differential expression between clusters just as we did for the scRNAseq data before.\n\n# differential expression between cluster 1 and cluster 6\nde_markers &lt;- FindMarkers(brain.integrated, ident.1 = 5, ident.2 = 6)\n\n# plot top markers\nSpatialFeaturePlot(object = brain.integrated, features = rownames(de_markers)[1:3], alpha = c(0.1, 1), ncol = 3)\n\n\n\n\n\n\n\n\nSpatial transcriptomics allows researchers to investigate how gene expression trends varies in space, thus identifying spatial patterns of gene expression. For this purpose there are multiple methods, such as SpatailDE, SPARK, Trendsceek, HMRF and Splotch.\nIn FindSpatiallyVariables() the default method in Seurat (method = ‘markvariogram’), is inspired by the Trendsceek, which models spatial transcriptomics data as a mark point process and computes a ‘variogram’, which identifies genes whose expression level is dependent on their spatial location. More specifically, this process calculates gamma(r) values measuring the dependence between two spots a certain “r” distance apart. By default, we use an r-value of ‘5’ in these analyses, and only compute these values for variable genes (where variation is calculated independently of spatial location) to save time.\n\n\n\n\n\n\nCaution\n\n\n\nTakes a long time to run, so skip this step for now!\n\n\n\n# brain &lt;- FindSpatiallyVariableFeatures(brain, assay = \"SCT\", features = VariableFeatures(brain)[1:1000],\n#     selection.method = \"markvariogram\")\n\n# We would get top features from SpatiallyVariableFeatures\n# top.features &lt;- head(SpatiallyVariableFeatures(brain, selection.method = \"markvariogram\"), 6)"
  },
  {
    "objectID": "labs/seurat/seurat_08_spatial.html#meta-st_ss",
    "href": "labs/seurat/seurat_08_spatial.html#meta-st_ss",
    "title": " Spatial Transcriptomics",
    "section": "4 Single cell data",
    "text": "4 Single cell data\nWe can use a scRNA-seq dataset as a reference to predict the proportion of different celltypes in the Visium spots. Keep in mind that it is important to have a reference that contains all the celltypes you expect to find in your spots. Ideally it should be a scRNA-seq reference from the exact same tissue. We will use a reference scRNA-seq dataset of ~14,000 adult mouse cortical cell taxonomy from the Allen Institute, generated with the SMART-Seq2 protocol.\nFirst download the seurat data:\n\nif (!dir.exists(\"data/spatial/visium\")) dir.create(\"data/spatial/visium\", recursive = TRUE)\npath_file &lt;- \"data/spatial/visium/allen_cortex.rds\"\nif (!file.exists(path_file)) download.file(url = file.path(path_data, \"spatial/visium/allen_cortex.rds\"), destfile = path_file)\n\nFor speed, and for a more fair comparison of the celltypes, we will subsample all celltypes to a maximum of 200 cells per class (subclass).\n\nar &lt;- readRDS(\"data/spatial/visium/allen_cortex.rds\")\n\n# check number of cells per subclass\ntable(ar$subclass)\n\n\n     Astro         CR       Endo    L2/3 IT         L4      L5 IT      L5 PT \n       368          7         94        982       1401        880        544 \n     L6 CT      L6 IT        L6b      Lamp5 Macrophage      Meis2         NP \n       960       1872        358       1122         51         45        362 \n     Oligo       Peri      Pvalb   Serpinf1        SMC       Sncg        Sst \n        91         32       1337         27         55        125       1741 \n       Vip       VLMC \n      1728         67 \n\n# select 200 cells per subclass, fist set subclass ass active.ident\nIdents(ar) &lt;- ar$subclass\nar &lt;- subset(ar, cells = WhichCells(ar, downsample = 200))\n\n# check again number of cells per subclass\ntable(ar$subclass)\n\n\n     Astro         CR       Endo    L2/3 IT         L4      L5 IT      L5 PT \n       200          7         94        200        200        200        200 \n     L6 CT      L6 IT        L6b      Lamp5 Macrophage      Meis2         NP \n       200        200        200        200         51         45        200 \n     Oligo       Peri      Pvalb   Serpinf1        SMC       Sncg        Sst \n        91         32        200         27         55        125        200 \n       Vip       VLMC \n       200         67 \n\n\nThen run normalization and dimensionality reduction.\n\n# First run SCTransform and PCA\nar &lt;- SCTransform(ar, ncells = 3000, verbose = FALSE, method = \"poisson\") %&gt;%\n    RunPCA(verbose = FALSE) %&gt;%\n    RunUMAP(dims = 1:30)\n\n# the annotation is stored in the 'subclass' column of object metadata\nDimPlot(ar, label = TRUE)"
  },
  {
    "objectID": "labs/seurat/seurat_08_spatial.html#meta-st_sub",
    "href": "labs/seurat/seurat_08_spatial.html#meta-st_sub",
    "title": " Spatial Transcriptomics",
    "section": "5 Subset ST for cortex",
    "text": "5 Subset ST for cortex\nSince the scRNAseq dataset was generated from the mouse cortex, we will subset the visium dataset in order to select mainly the spots part of the cortex. Note that the integration can also be performed on the whole brain slice, but it would give rise to false positive cell type assignments and therefore it should be interpreted with more care.\n\n# subset for the anterior dataset\ncortex &lt;- subset(brain.integrated, subset = orig.ident == \"anterior1\")\n\n# there seems to be an error in the subsetting, so the posterior1 image is not removed, do it manually\ncortex@images$posterior1 &lt;- NULL\n\n# add coordinates to metadata\n# note that this only returns one slide by default\ncortex$imagerow &lt;- GetTissueCoordinates(cortex)$imagerow\ncortex$imagecol &lt;- GetTissueCoordinates(cortex)$imagecol\n\n# subset for a specific region\ncortex &lt;- subset(cortex, subset = imagerow &gt; 400 | imagecol &lt; 150, invert = TRUE)\ncortex &lt;- subset(cortex, subset = imagerow &gt; 275 & imagecol &gt; 370, invert = TRUE)\ncortex &lt;- subset(cortex, subset = imagerow &gt; 250 & imagecol &gt; 440, invert = TRUE)\n\n# also subset for Frontal cortex clusters\ncortex &lt;- subset(cortex, subset = seurat_clusters %in% c(1, 2, 3, 4, 5))\n\np1 &lt;- SpatialDimPlot(cortex, crop = TRUE)\np2 &lt;- SpatialDimPlot(cortex, crop = FALSE, pt.size.factor = 1, label.size = 3)\np1 + p2"
  },
  {
    "objectID": "labs/seurat/seurat_08_spatial.html#meta-st_deconv",
    "href": "labs/seurat/seurat_08_spatial.html#meta-st_deconv",
    "title": " Spatial Transcriptomics",
    "section": "6 Deconvolution",
    "text": "6 Deconvolution\nDeconvolution is a method to estimate the abundance (or proportion) of different celltypes in a bulkRNAseq dataset using a single cell reference. As the Visium data can be seen as a small bulk, we can both use methods for traditional bulkRNAseq as well as methods especially developed for Visium data. Some methods for deconvolution are DWLS, cell2location, Tangram, Stereoscope, RCTD, SCDC and many more.\nHere we will use SCDC for deconvolution of celltypes in the Visium spots. For more information on the tool please check their website: https://meichendong.github.io/SCDC/articles/SCDC.html. First, make sure the packages you need are installed.\n\ninst &lt;- installed.packages()\n\nif (!(\"xbioc\" %in% rownames(inst))) {\n    remotes::install_github(\"renozao/xbioc\", dependencies = FALSE)\n}\nif (!(\"SCDC\" %in% rownames(inst))) {\n    remotes::install_github(\"meichendong/SCDC\", dependencies = FALSE)\n}\n\nsuppressPackageStartupMessages(library(SCDC))\nsuppressPackageStartupMessages(library(Biobase))\n\n\n6.1 Select genes for deconvolution\nMost deconvolution methods does a prior gene selection and there are different options that are used: - Use variable genes in the SC data. - Use variable genes in both SC and ST data - DE genes between clusters in the SC data.\nIn this case we will use top DE genes per cluster, so first we have to run DGE detection on the scRNAseq data.\nFor SCDC we want to find unique markers per cluster, so we select top 20 DEGs per cluster. Ideally you should run with a larger set of genes, perhaps 100 genes per cluster to get better results. However, for the sake of speed, we are now selecting only top20 genes and it still takes about 10 minutes to run.\n\nar@active.assay &lt;- \"RNA\"\n\nmarkers_sc &lt;- FindAllMarkers(ar,\n    only.pos = TRUE,\n    logfc.threshold = 0.1,\n    test.use = \"wilcox\",\n    min.pct = 0.05,\n    min.diff.pct = 0.1,\n    max.cells.per.ident = 200,\n    return.thresh = 0.05,\n    assay = \"RNA\"\n)\n\n# Filter for genes that are also present in the ST data\nmarkers_sc &lt;- markers_sc[markers_sc$gene %in% rownames(cortex), ]\n\n\n# Select top 20 genes per cluster, select top by first p-value, then absolute diff in pct, then quota of pct.\nmarkers_sc$pct.diff &lt;- markers_sc$pct.1 - markers_sc$pct.2\nmarkers_sc$log.pct.diff &lt;- log2((markers_sc$pct.1 * 99 + 1) / (markers_sc$pct.2 * 99 + 1))\nmarkers_sc %&gt;%\n    group_by(cluster) %&gt;%\n    top_n(-100, p_val) %&gt;%\n    top_n(50, pct.diff) %&gt;%\n    top_n(20, log.pct.diff) -&gt; top20\nm_feats &lt;- unique(as.character(top20$gene))\n\n\n\n6.2 Create Expression Sets\nFor SCDC both the SC and the ST data need to be in the format of an Expression set with the count matrices as AssayData. We also subset the matrices for the genes we selected in the previous step.\n\neset_SC &lt;- ExpressionSet(\n    assayData = as.matrix(ar@assays$RNA@counts[m_feats, ]),\n    phenoData = AnnotatedDataFrame(ar@meta.data)\n)\neset_ST &lt;- ExpressionSet(assayData = as.matrix(cortex@assays$Spatial@counts[m_feats, ]), phenoData = AnnotatedDataFrame(cortex@meta.data))\n\n\n\n6.3 Deconvolve\nWe then run the deconvolution defining the celltype of interest as “subclass” column in the single cell data.\n\n\n\n\n\n\nCaution\n\n\n\nThis is a slow compute intensive step, we will not run this now and instead use a pre-computed file in the step below.\n\n\n\n# this code block is not executed\n\n# fetch_data is defined at the top of this document\nif (!fetch_data) {\n  deconvolution_crc &lt;- SCDC::SCDC_prop(\n    bulk.eset = eset_ST,\n    sc.eset = eset_SC,\n    ct.varname = \"subclass\",\n    ct.sub = as.character(unique(eset_SC$subclass))\n  )\n  saveRDS(deconvolution_crc, \"data/spatial/visium/seurat_scdc.rds\")\n}\n\nDownload the precomputed file.\n\n# fetch_data is defined at the top of this document\npath_file &lt;- \"data/spatial/visium/seurat_scdc.rds\"\nif (fetch_data) {\n  if (!file.exists(path_file)) download.file(url = file.path(path_data, \"spatial/visium/results/seurat_scdc.rds\"), destfile = path_file)\n}\n\n\ndeconvolution_crc &lt;- readRDS(path_file)\n\nNow we have a matrix with predicted proportions of each celltypes for each visium spot in prop.est.mvw.\n\nhead(deconvolution_crc$prop.est.mvw)\n\n                     Lamp5 Sncg Serpinf1          Vip          Sst      Pvalb\nAAACTCGTGATATAAG-1_1     0    0        0 0.000000e+00 0.0003020068 0.00000000\nAAACTGCTGGCTCCAA-1_1     0    0        0 0.000000e+00 0.1544641392 0.07943494\nAAAGGGATGTAGCAAG-1_1     0    0        0 0.000000e+00 0.2742639441 0.00000000\nAAATACCTATAAGCAT-1_1     0    0        0 0.000000e+00 0.0803576731 0.40436150\nAAATCGTGTACCACAA-1_1     0    0        0 0.000000e+00 0.0692640621 0.00000000\nAAATGATTCGATCAGC-1_1     0    0        0 1.705303e-06 0.0169468859 0.08888082\n                           Endo         Peri        L6 CT          L6b\nAAACTCGTGATATAAG-1_1 0.00000000 0.000000e+00 0.0000000000 1.512806e-01\nAAACTGCTGGCTCCAA-1_1 0.02562850 0.000000e+00 0.0280520546 1.959849e-05\nAAAGGGATGTAGCAAG-1_1 0.01131595 0.000000e+00 0.0000000000 0.000000e+00\nAAATACCTATAAGCAT-1_1 0.07365610 1.399958e-05 0.0036921008 0.000000e+00\nAAATCGTGTACCACAA-1_1 0.02785003 5.235782e-06 0.0002147064 2.458057e-01\nAAATGATTCGATCAGC-1_1 0.01403814 2.633453e-02 0.2657130174 0.000000e+00\n                            L6 IT CR    L2/3 IT        L5 PT NP        L4\nAAACTCGTGATATAAG-1_1 0.000000e+00  0 0.00000000 0.0000000000  0 0.0000000\nAAACTGCTGGCTCCAA-1_1 1.699877e-05  0 0.38974934 0.0000000000  0 0.0000000\nAAAGGGATGTAGCAAG-1_1 2.237113e-04  0 0.00000000 0.0000000000  0 0.1814651\nAAATACCTATAAGCAT-1_1 0.000000e+00  0 0.00000000 0.0000793099  0 0.0000000\nAAATCGTGTACCACAA-1_1 2.755082e-05  0 0.31058665 0.0000000000  0 0.0000000\nAAATGATTCGATCAGC-1_1 1.350970e-01  0 0.01172995 0.1013133001  0 0.1530583\n                           Oligo      L5 IT Meis2      Astro  Macrophage VLMC\nAAACTCGTGATATAAG-1_1 0.606350282 0.00000000     0 0.00000000 0.242067127    0\nAAACTGCTGGCTCCAA-1_1 0.070102264 0.00000000     0 0.20493666 0.047592071    0\nAAAGGGATGTAGCAAG-1_1 0.000000000 0.36553725     0 0.15879807 0.008395941    0\nAAATACCTATAAGCAT-1_1 0.090470397 0.00000000     0 0.32968096 0.017682500    0\nAAATCGTGTACCACAA-1_1 0.205850104 0.00000000     0 0.11515601 0.025239945    0\nAAATGATTCGATCAGC-1_1 0.002151596 0.09261913     0 0.08687805 0.005237623    0\n                              SMC\nAAACTCGTGATATAAG-1_1 0.000000e+00\nAAACTGCTGGCTCCAA-1_1 3.440261e-06\nAAAGGGATGTAGCAAG-1_1 0.000000e+00\nAAATACCTATAAGCAT-1_1 5.461144e-06\nAAATCGTGTACCACAA-1_1 0.000000e+00\nAAATGATTCGATCAGC-1_1 0.000000e+00\n\n\nNow we take the deconvolution output and add it to the Seurat object as a new assay.\n\ncortex@assays[[\"SCDC\"]] &lt;- CreateAssayObject(data = t(deconvolution_crc$prop.est.mvw))\n\n# Seems to be a bug in SeuratData package that the key is not set and any plotting function etc. will throw an error.\nif (length(cortex@assays$SCDC@key) == 0) {\n    cortex@assays$SCDC@key &lt;- \"scdc_\"\n}\n\n\nDefaultAssay(cortex) &lt;- \"SCDC\"\nSpatialFeaturePlot(cortex, features = c(\"L2/3 IT\", \"L4\"), pt.size.factor = 1.6, ncol = 2, crop = TRUE)\n\n\n\n\n\n\n\n\nBased on these prediction scores, we can also predict cell types whose location is spatially restricted. We use the same methods based on marked point processes to define spatially variable features, but use the cell type prediction scores as the “marks” rather than gene expression.\n\n# FindSpatiallyVariableFeatures() does not work with markvariogram or moransi\n# this chunk is disabled\n\ncortex &lt;- FindSpatiallyVariableFeatures(cortex,\n    assay = \"SCDC\", selection.method = \"markvariogram\",\n    features = rownames(cortex), r.metric = 5, slot = \"data\"\n)\ntop.clusters &lt;- head(SpatiallyVariableFeatures(cortex), 4)\nSpatialPlot(object = cortex, features = top.clusters, ncol = 2)\n\nWe can also visualize the scores per cluster in ST data.\n\n# this chunk is disabled\n\nVlnPlot(cortex, group.by = \"seurat_clusters\", features = top.clusters, pt.size = 0, ncol = 2)\n\nKeep in mind that the deconvolution results are just predictions, depending on how well your scRNAseq data covers the celltypes that are present in the ST data and on how parameters, gene selection etc. are tuned you may get different results.\n\n\n\n\n\n\nDiscuss\n\n\n\nSubset for another region that does not contain cortex cells and check what you get from the label transfer. Suggested region is the right end of the posterial section that you can select like this:\n\n# subset for the anterior dataset\nsubregion &lt;- subset(brain.integrated, subset = orig.ident == \"posterior1\")\n\n# there seems to be an error in the subsetting, so the posterior1 image is not removed, do it manually\nsubregion@images$anterior1 &lt;- NULL\n\n# add coordinates to metadata\n# note that this only returns one slide by default\nsubregion$imagerow &lt;- GetTissueCoordinates(subregion)$imagerow\nsubregion$imagecol &lt;- GetTissueCoordinates(subregion)$imagecol\n\n# subset for a specific region\nsubregion &lt;- subset(subregion, subset = imagecol &gt; 400, invert = FALSE)\n\np1 &lt;- SpatialDimPlot(subregion, crop = TRUE, label = TRUE)\np2 &lt;- SpatialDimPlot(subregion, crop = FALSE, label = TRUE, pt.size.factor = 1, label.size = 3)\np1 + p2"
  },
  {
    "objectID": "labs/seurat/seurat_08_spatial.html#meta-session",
    "href": "labs/seurat/seurat_08_spatial.html#meta-session",
    "title": " Spatial Transcriptomics",
    "section": "7 Session info",
    "text": "7 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: Etc/UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] Biobase_2.62.0            BiocGenerics_0.48.1      \n [3] SCDC_0.0.0.9000           patchwork_1.1.2          \n [5] ggplot2_3.4.2             SeuratObject_4.1.3       \n [7] Seurat_4.3.0              stxBrain.SeuratData_0.1.1\n [9] SeuratData_0.2.2          dplyr_1.1.2              \n[11] Matrix_1.5-4             \n\nloaded via a namespace (and not attached):\n  [1] RColorBrewer_1.1-3      rstudioapi_0.14         jsonlite_1.8.5         \n  [4] magrittr_2.0.3          spatstat.utils_3.0-3    farver_2.1.1           \n  [7] rmarkdown_2.22          zlibbioc_1.48.0         vctrs_0.6.2            \n [10] ROCR_1.0-11             memoise_2.0.1           spatstat.explore_3.2-1 \n [13] RCurl_1.98-1.12         htmltools_0.5.5         xbioc_0.1.19           \n [16] sctransform_0.3.5       parallelly_1.36.0       KernSmooth_2.23-20     \n [19] htmlwidgets_1.6.2       ica_1.0-3               plyr_1.8.8             \n [22] cachem_1.0.8            plotly_4.10.2           zoo_1.8-12             \n [25] igraph_1.4.3            mime_0.12               lifecycle_1.0.3        \n [28] pkgconfig_2.0.3         R6_2.5.1                fastmap_1.1.1          \n [31] GenomeInfoDbData_1.2.11 fitdistrplus_1.1-11     future_1.32.0          \n [34] shiny_1.7.4             digest_0.6.31           colorspace_2.1-0       \n [37] S4Vectors_0.40.2        AnnotationDbi_1.64.1    tensor_1.5             \n [40] irlba_2.3.5.1           RSQLite_2.3.1           labeling_0.4.2         \n [43] progressr_0.13.0        fansi_1.0.4             spatstat.sparse_3.0-1  \n [46] nnls_1.4                httr_1.4.6              polyclip_1.10-4        \n [49] abind_1.4-5             compiler_4.3.0          bit64_4.0.5            \n [52] withr_2.5.0             backports_1.4.1         DBI_1.1.3              \n [55] pkgmaker_0.32.8         MASS_7.3-58.4           rappdirs_0.3.3         \n [58] tools_4.3.0             lmtest_0.9-40           httpuv_1.6.11          \n [61] future.apply_1.11.0     goftest_1.2-3           glue_1.6.2             \n [64] nlme_3.1-162            promises_1.2.0.1        grid_4.3.0             \n [67] checkmate_2.2.0         Rtsne_0.16              cluster_2.1.4          \n [70] reshape2_1.4.4          generics_0.1.3          gtable_0.3.3           \n [73] spatstat.data_3.0-1     tidyr_1.3.0             data.table_1.14.8      \n [76] XVector_0.42.0          sp_1.6-1                utf8_1.2.3             \n [79] spatstat.geom_3.2-1     RcppAnnoy_0.0.20        ggrepel_0.9.3          \n [82] RANN_2.6.1              pillar_1.9.0            stringr_1.5.0          \n [85] later_1.3.1             splines_4.3.0           lattice_0.21-8         \n [88] bit_4.0.5               survival_3.5-5          deldir_1.0-9           \n [91] tidyselect_1.2.0        registry_0.5-1          Biostrings_2.70.2      \n [94] miniUI_0.1.1.1          pbapply_1.7-0           knitr_1.43             \n [97] gridExtra_2.3           IRanges_2.36.0          scattermore_1.2        \n[100] stats4_4.3.0            xfun_0.39               matrixStats_1.0.0      \n[103] pheatmap_1.0.12         stringi_1.7.12          lazyeval_0.2.2         \n[106] yaml_2.3.7              evaluate_0.21           codetools_0.2-19       \n[109] tibble_3.2.1            BiocManager_1.30.21     cli_3.6.1              \n[112] uwot_0.1.14             xtable_1.8-4            reticulate_1.30        \n[115] munsell_0.5.0           GenomeInfoDb_1.38.5     Rcpp_1.0.10            \n[118] globals_0.16.2          spatstat.random_3.1-5   L1pack_0.41-24         \n[121] png_0.1-8               parallel_4.3.0          ellipsis_0.3.2         \n[124] assertthat_0.2.1        blob_1.2.4              fastmatrix_0.5         \n[127] bitops_1.0-7            listenv_0.9.0           viridisLite_0.4.2      \n[130] scales_1.2.1            ggridges_0.5.4          leiden_0.4.3           \n[133] purrr_1.0.1             crayon_1.5.2            rlang_1.1.1            \n[136] KEGGREST_1.42.0         cowplot_1.1.1"
  },
  {
    "objectID": "labs/bioc/bioc_01_qc.html",
    "href": "labs/bioc/bioc_01_qc.html",
    "title": " Quality Control",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified."
  },
  {
    "objectID": "labs/bioc/bioc_01_qc.html#meta-qc_data",
    "href": "labs/bioc/bioc_01_qc.html#meta-qc_data",
    "title": " Quality Control",
    "section": "1 Get data",
    "text": "1 Get data\nIn this tutorial, we will run all tutorials with a set of 8 PBMC 10x datasets from 4 covid-19 patients and 4 healthy controls, the samples have been subsampled to 1500 cells per sample. We can start by defining our paths.\n\n# download pre-computed annotation\nfetch_annotation &lt;- TRUE\n\n# url for source and intermediate data\npath_data &lt;- \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\n\npath_covid &lt;- \"./data/covid\"\nif (!dir.exists(path_covid)) dir.create(path_covid, recursive = T)\n\npath_results &lt;- \"data/covid/results\"\nif (!dir.exists(path_results)) dir.create(path_results, recursive = T)\n\n\nfile_list &lt;- c(\n    \"normal_pbmc_13.h5\", \"normal_pbmc_14.h5\", \"normal_pbmc_19.h5\", \"normal_pbmc_5.h5\",\n    \"ncov_pbmc_15.h5\", \"ncov_pbmc_16.h5\", \"ncov_pbmc_17.h5\", \"ncov_pbmc_1.h5\"\n)\n\nfor (i in file_list) {\n    path_file &lt;- file.path(path_covid, i)\n    if (!file.exists(path_file)) {\n        download.file(url = file.path(file.path(path_data, \"covid\"), i), destfile = path_file)\n    }\n}\n\nWith data in place, now we can start loading libraries we will use in this tutorial.\n\nsuppressPackageStartupMessages({\n    library(scater)\n    library(scran)\n    library(patchwork) # combining figures\n    library(org.Hs.eg.db)\n    library(scDblFinder)\n})\n\nWe can first load the data individually by reading directly from HDF5 file format (.h5).\n\ncov.15 &lt;- Seurat::Read10X_h5(\n    filename = file.path(path_covid, \"ncov_pbmc_15.h5\"),\n    use.names = T\n)\ncov.1 &lt;- Seurat::Read10X_h5(\n    filename = file.path(path_covid, \"ncov_pbmc_1.h5\"),\n    use.names = T\n)\ncov.16 &lt;- Seurat::Read10X_h5(\n    filename = file.path(path_covid, \"ncov_pbmc_16.h5\"),\n    use.names = T\n)\ncov.17 &lt;- Seurat::Read10X_h5(\n    filename = file.path(path_covid, \"ncov_pbmc_17.h5\"),\n    use.names = T\n)\n\nctrl.5 &lt;- Seurat::Read10X_h5(\n    filename = file.path(path_covid, \"normal_pbmc_5.h5\"),\n    use.names = T\n)\nctrl.13 &lt;- Seurat::Read10X_h5(\n    filename = file.path(path_covid, \"normal_pbmc_13.h5\"),\n    use.names = T\n)\nctrl.14 &lt;- Seurat::Read10X_h5(\n    filename = file.path(path_covid, \"normal_pbmc_14.h5\"),\n    use.names = T\n)\nctrl.19 &lt;- Seurat::Read10X_h5(\n    filename = file.path(path_covid, \"normal_pbmc_19.h5\"),\n    use.names = T\n)"
  },
  {
    "objectID": "labs/bioc/bioc_01_qc.html#meta-qc_collate",
    "href": "labs/bioc/bioc_01_qc.html#meta-qc_collate",
    "title": " Quality Control",
    "section": "2 Collate",
    "text": "2 Collate\nWe can now merge them objects into a single object. Each analysis workflow (Seurat, Scater, Scanpy, etc) has its own way of storing data. We will add dataset labels as cell.ids just in case you have overlapping barcodes between the datasets. After that we add a column type in the metadata to define covid and ctrl samples.\n\nsce &lt;- SingleCellExperiment(assays = list(counts = cbind(cov.1, cov.15, cov.16, cov.17, ctrl.5, ctrl.13, ctrl.14,ctrl.19)))\ndim(sce)\n\n[1] 33538 12000\n\n# Adding metadata\nsce@colData$sample &lt;- unlist(sapply(c(\"cov.1\", \"cov.15\", \"cov.16\", \"cov.17\", \"ctrl.5\", \"ctrl.13\", \"ctrl.14\",\"ctrl.19\"), function(x) rep(x, ncol(get(x)))))\nsce@colData$type &lt;- ifelse(grepl(\"cov\", sce@colData$sample), \"Covid\", \"Control\")\n\nOnce you have created the merged Seurat object, the count matrices and individual count matrices and objects are not needed anymore. It is a good idea to remove them and run garbage collect to free up some memory.\n\n# remove all objects that will not be used.\nrm(cov.15, cov.1, cov.17, cov.16, ctrl.5, ctrl.13, ctrl.14, ctrl.19)\n# run garbage collect to free up memory\ngc()\n\n           used  (Mb) gc trigger   (Mb) limit (Mb)  max used  (Mb)\nNcells 11937546 637.6   17365280  927.5         NA  17108978 913.8\nVcells 47509097 362.5  143610096 1095.7      65536 118686521 905.6\n\n\nHere is how the count matrix and the metadata look like for every cell.\n\nhead(counts(sce)[, 1:10])\n\n6 x 10 sparse Matrix of class \"dgCMatrix\"\n                               \nMIR1302-2HG . . . . . . . . . .\nFAM138A     . . . . . . . . . .\nOR4F5       . . . . . . . . . .\nAL627309.1  . . . . . . . . . .\nAL627309.3  . . . . . . . . . .\nAL627309.2  . . . . . . . . . .\n\nhead(sce@colData, 10)\n\nDataFrame with 10 rows and 2 columns\n                        sample        type\n                   &lt;character&gt; &lt;character&gt;\nAGGTAGGTCGTTGTTT-1       cov.1       Covid\nTAGAGTCGTCCTCCAT-1       cov.1       Covid\nCCCTGATAGCGAACTG-1       cov.1       Covid\nTCATCATTCCACGTAA-1       cov.1       Covid\nATTTACCCAAGCCTGC-1       cov.1       Covid\nGTTGTCCTCTAGAACC-1       cov.1       Covid\nCCTCCAACAAGAGATT-1       cov.1       Covid\nAATAGAGGTGTGAGCA-1       cov.1       Covid\nGGTGGCTAGCGAATGC-1       cov.1       Covid\nTCGGGCACAGTGTGGA-1       cov.1       Covid"
  },
  {
    "objectID": "labs/bioc/bioc_01_qc.html#meta-qc_calqc",
    "href": "labs/bioc/bioc_01_qc.html#meta-qc_calqc",
    "title": " Quality Control",
    "section": "3 Calculate QC",
    "text": "3 Calculate QC\nHaving the data in a suitable format, we can start calculating some quality metrics. We can for example calculate the percentage of mitochondrial and ribosomal genes per cell and add to the metadata. The proportion of hemoglobin genes can give an indication of red blood cell contamination, but in some tissues it can also be the case that some celltypes have higher content of hemoglobin. This will be helpful to visualize them across different metadata parameters (i.e. datasetID and chemistry version). There are several ways of doing this. The QC metrics are finally added to the metadata table.\nCiting from Simple Single Cell workflows (Lun, McCarthy & Marioni, 2017): High proportions are indicative of poor-quality cells (Islam et al. 2014; Ilicic et al. 2016), possibly because of loss of cytoplasmic RNA from perforated cells. The reasoning is that mitochondria are larger than individual transcript molecules and less likely to escape through tears in the cell membrane.\n\n# Mitochondrial genes\nmito_genes &lt;- rownames(sce)[grep(\"^MT-\", rownames(sce))]\n# Ribosomal genes\nribo_genes &lt;- rownames(sce)[grep(\"^RP[SL]\", rownames(sce))]\n# Hemoglobin genes - includes all genes starting with HB except HBP.\nhb_genes &lt;- rownames(sce)[grep(\"^HB[^(P|E|S)]\", rownames(sce))]\n\nFirst, let Scran calculate some general qc-stats for genes and cells with the function perCellQCMetrics. It can also calculate proportion of counts for specific gene subsets, so first we need to define which genes are mitochondrial, ribosomal and hemoglobin.\n\nsce &lt;- addPerCellQC(sce, flatten = T, subsets = list(mt = mito_genes, hb = hb_genes, ribo = ribo_genes))\n\n# Way2: Doing it manually\nsce@colData$percent_mito &lt;- Matrix::colSums(counts(sce)[mito_genes, ]) / sce@colData$total * 100\n\nNow you can see that we have additional data in the metadata slot.\n\nhead(colData(sce))\n\nDataFrame with 6 rows and 15 columns\n                        sample        type       sum  detected subsets_mt_sum\n                   &lt;character&gt; &lt;character&gt; &lt;numeric&gt; &lt;integer&gt;      &lt;numeric&gt;\nAGGTAGGTCGTTGTTT-1       cov.1       Covid      1396       656             26\nTAGAGTCGTCCTCCAT-1       cov.1       Covid      1613       779            186\nCCCTGATAGCGAACTG-1       cov.1       Covid      9482      2036            761\nTCATCATTCCACGTAA-1       cov.1       Covid      4357       875           2960\nATTTACCCAAGCCTGC-1       cov.1       Covid     12466      3290            686\nGTTGTCCTCTAGAACC-1       cov.1       Covid      5541      1606            707\n                   subsets_mt_detected subsets_mt_percent subsets_hb_sum\n                             &lt;integer&gt;          &lt;numeric&gt;      &lt;numeric&gt;\nAGGTAGGTCGTTGTTT-1                   8            1.86246              1\nTAGAGTCGTCCTCCAT-1                  10           11.53131              0\nCCCTGATAGCGAACTG-1                  11            8.02573              3\nTCATCATTCCACGTAA-1                  13           67.93665              2\nATTTACCCAAGCCTGC-1                  12            5.50297              1\nGTTGTCCTCTAGAACC-1                  12           12.75943              3\n                   subsets_hb_detected subsets_hb_percent subsets_ribo_sum\n                             &lt;integer&gt;          &lt;numeric&gt;        &lt;numeric&gt;\nAGGTAGGTCGTTGTTT-1                   1         0.07163324               11\nTAGAGTCGTCCTCCAT-1                   0         0.00000000               96\nCCCTGATAGCGAACTG-1                   1         0.03163889             4157\nTCATCATTCCACGTAA-1                   2         0.04590314               99\nATTTACCCAAGCCTGC-1                   1         0.00802182             2281\nGTTGTCCTCTAGAACC-1                   2         0.05414185             1664\n                   subsets_ribo_detected subsets_ribo_percent     total\n                               &lt;integer&gt;            &lt;numeric&gt; &lt;numeric&gt;\nAGGTAGGTCGTTGTTT-1                     9             0.787966      1396\nTAGAGTCGTCCTCCAT-1                    45             5.951643      1613\nCCCTGATAGCGAACTG-1                    85            43.840962      9482\nTCATCATTCCACGTAA-1                    52             2.272206      4357\nATTTACCCAAGCCTGC-1                    82            18.297770     12466\nGTTGTCCTCTAGAACC-1                    80            30.030680      5541\n                   percent_mito\n                      &lt;numeric&gt;\nAGGTAGGTCGTTGTTT-1      1.86246\nTAGAGTCGTCCTCCAT-1     11.53131\nCCCTGATAGCGAACTG-1      8.02573\nTCATCATTCCACGTAA-1     67.93665\nATTTACCCAAGCCTGC-1      5.50297\nGTTGTCCTCTAGAACC-1     12.75943"
  },
  {
    "objectID": "labs/bioc/bioc_01_qc.html#meta-qc_plotqc",
    "href": "labs/bioc/bioc_01_qc.html#meta-qc_plotqc",
    "title": " Quality Control",
    "section": "4 Plot QC",
    "text": "4 Plot QC\nNow we can plot some of the QC variables as violin plots.\n\n# total is total UMIs per cell\n# detected is number of detected genes.\n# the different gene subset percentages are listed as subsets_mt_percent etc.\n\nwrap_plots(\n    plotColData(sce, y = \"detected\", x = \"sample\", colour_by = \"sample\"),\n    plotColData(sce, y = \"total\", x = \"sample\", colour_by = \"sample\"),\n    plotColData(sce, y = \"subsets_mt_percent\", x = \"sample\", colour_by = \"sample\"),\n    plotColData(sce, y = \"subsets_ribo_percent\", x = \"sample\", colour_by = \"sample\"),\n    plotColData(sce, y = \"subsets_hb_percent\", x = \"sample\", colour_by = \"sample\"),\n    ncol = 3\n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nAs you can see, there is quite some difference in quality for these samples, with for instance the covid_15 and covid_16 samples having cells with fewer detected genes and more mitochondrial content. As the ribosomal proteins are highly expressed they will make up a larger proportion of the transcriptional landscape when fewer of the lowly expressed genes are detected. We can also plot the different QC-measures as scatter plots.\n\nplotColData(sce, x = \"total\", y = \"detected\", colour_by = \"sample\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nPlot additional QC stats that we have calculated as scatter plots. How are the different measures correlated? Can you explain why?"
  },
  {
    "objectID": "labs/bioc/bioc_01_qc.html#meta-qc_filter",
    "href": "labs/bioc/bioc_01_qc.html#meta-qc_filter",
    "title": " Quality Control",
    "section": "5 Filtering",
    "text": "5 Filtering\n\n5.1 Detection-based filtering\nA standard approach is to filter cells with low number of reads as well as genes that are present in at least a given number of cells. Here we will only consider cells with at least 200 detected genes and genes need to be expressed in at least 3 cells. Please note that those values are highly dependent on the library preparation method used.\nIn Scran, we can use the function quickPerCellQC to filter out outliers from distributions of qc stats, such as detected genes, gene subsets etc. But in this case, we will take one setting at a time and run through the steps of filtering cells.\n\ndim(sce)\n\n[1] 33538 12000\n\nselected_c &lt;- colnames(sce)[sce$detected &gt; 200]\nselected_f &lt;- rownames(sce)[Matrix::rowSums(counts(sce)) &gt; 3]\n\nsce.filt &lt;- sce[selected_f, selected_c]\ndim(sce.filt)\n\n[1] 18877 10656\n\n\nExtremely high number of detected genes could indicate doublets. However, depending on the cell type composition in your sample, you may have cells with higher number of genes (and also higher counts) from one cell type. In this case, we will run doublet prediction further down, so we will skip this step now, but the code below is an example of how it can be run:\n\n# skip for now and run doublet detection instead...\n\n# high.det.v3 &lt;- sce.filt$nFeatures &gt; 4100\n# high.det.v2 &lt;- (sce.filt$nFeatures &gt; 2000) & (sce.filt$sample_id == \"v2.1k\")\n\n# remove these cells\n# sce.filt &lt;- sce.filt[ , (!high.det.v3) & (!high.det.v2)]\n\n# check number of cells\n# ncol(sce.filt)\n\nAdditionally, we can also see which genes contribute the most to such reads. We can for instance plot the percentage of counts per gene.\nIn Scater, you can also use the function plotHighestExprs() to plot the gene contribution, but the function is quite slow, so we will do it on our own instead..\n\n# Compute the relative expression of each gene per cell\n# Use sparse matrix operations, if your dataset is large, doing matrix devisions the regular way will take a very long time.\nC &lt;- counts(sce.filt)\nC@x &lt;- C@x / rep.int(colSums(C), diff(C@p)) * 100\nmost_expressed &lt;- order(Matrix::rowSums(C), decreasing = T)[20:1]\nboxplot(as.matrix(t(C[most_expressed, ])), cex = .1, las = 1, xlab = \"% total count per cell\", col = scales::hue_pal()(20)[20:1], horizontal = TRUE)\n\n\n\n\n\n\n\nrm(C)\n\n# also, there is the option of running the function \"plotHighestExprs\" in the scater package, however, this function takes very long to execute.\n\nAs you can see, MALAT1 constitutes up to 30% of the UMIs from a single cell and the other top genes are mitochondrial and ribosomal genes. It is quite common that nuclear lincRNAs have correlation with quality and mitochondrial reads, so high detection of MALAT1 may be a technical issue. Let us assemble some information about such genes, which are important for quality control and downstream filtering.\n\n\n5.2 Mito/Ribo filtering\nWe also have quite a lot of cells with high proportion of mitochondrial and low proportion of ribosomal reads. It would be wise to remove those cells, if we have enough cells left after filtering. Another option would be to either remove all mitochondrial reads from the dataset and hope that the remaining genes still have enough biological signal. A third option would be to just regress out the percent_mito variable during scaling. In this case we had as much as 99.7% mitochondrial reads in some of the cells, so it is quite unlikely that there is much cell type signature left in those. Looking at the plots, make reasonable decisions on where to draw the cutoff. In this case, the bulk of the cells are below 20% mitochondrial reads and that will be used as a cutoff. We will also remove cells with less than 5% ribosomal reads.\n\nselected_mito &lt;- sce.filt$subsets_mt_percent &lt; 20\nselected_ribo &lt;- sce.filt$subsets_ribo_percent &gt; 5\n\n# and subset the object to only keep those cells\nsce.filt &lt;- sce.filt[, selected_mito & selected_ribo]\ndim(sce.filt)\n\n[1] 18877  7431\n\n\nAs you can see, a large proportion of sample covid_15 is filtered out. Also, there is still quite a lot of variation in percent_mito, so it will have to be dealt with in the data analysis step. We can also notice that the percent_ribo are also highly variable, but that is expected since different cell types have different proportions of ribosomal content, according to their function.\n\n\n5.3 Plot filtered QC\nLets plot the same QC-stats once more.\n\nwrap_plots(\n    plotColData(sce, y = \"detected\", x = \"sample\", colour_by = \"sample\"),\n    plotColData(sce, y = \"total\", x = \"sample\", colour_by = \"sample\"),\n    plotColData(sce, y = \"subsets_mt_percent\", x = \"sample\", colour_by = \"sample\"),\n    plotColData(sce, y = \"subsets_ribo_percent\", x = \"sample\", colour_by = \"sample\"),\n    plotColData(sce, y = \"subsets_hb_percent\", x = \"sample\", colour_by = \"sample\"),\n    ncol = 3\n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n5.4 Filter genes\nAs the level of expression of mitochondrial and MALAT1 genes are judged as mainly technical, it can be wise to remove them from the dataset before any further analysis. In this case we will also remove the HB genes.\n\ndim(sce.filt)\n\n[1] 18877  7431\n\n# Filter MALAT1\nsce.filt &lt;- sce.filt[!grepl(\"MALAT1\", rownames(sce.filt)), ]\n\n# Filter Mitocondrial\nsce.filt &lt;- sce.filt[!grepl(\"^MT-\", rownames(sce.filt)), ]\n\n# Filter Ribossomal gene (optional if that is a problem on your data)\n# sce.filt &lt;- sce.filt[ ! grepl(\"^RP[SL]\", rownames(sce.filt)), ]\n\n# Filter Hemoglobin gene  (optional if that is a problem on your data)\nsce.filt &lt;- sce.filt[!grepl(\"^HB[^(P|E|S)]\", rownames(sce.filt)), ]\n\ndim(sce.filt)\n\n[1] 18854  7431"
  },
  {
    "objectID": "labs/bioc/bioc_01_qc.html#meta-qc_sex",
    "href": "labs/bioc/bioc_01_qc.html#meta-qc_sex",
    "title": " Quality Control",
    "section": "6 Sample sex",
    "text": "6 Sample sex\nWhen working with human or animal samples, you should ideally constrain your experiments to a single sex to avoid including sex bias in the conclusions. However this may not always be possible. By looking at reads from chromosomeY (males) and XIST (X-inactive specific transcript) expression (mainly female) it is quite easy to determine per sample which sex it is. It can also be a good way to detect if there has been any mislabelling in which case, the sample metadata sex does not agree with the computational predictions.\nTo get chromosome information for all genes, you should ideally parse the information from the gtf file that you used in the mapping pipeline as it has the exact same annotation version/gene naming. However, it may not always be available, as in this case where we have downloaded public data. R package biomaRt can be used to fetch annotation information. The code to run biomaRt is provided. As the biomart instances are quite often unresponsive, we will download and use a file that was created in advance.\n\n\n\n\n\n\nTip\n\n\n\n\n\nHere is the code to download annotation data from Ensembl using biomaRt. We will not run this now and instead use a pre-computed file in the step below.\n\n# fetch_annotation is defined at the top of this document\nif (!fetch_annotation) {\n  suppressMessages(library(biomaRt))\n\n  # initialize connection to mart, may take some time if the sites are unresponsive.\n  mart &lt;- useMart(\"ENSEMBL_MART_ENSEMBL\", dataset = \"hsapiens_gene_ensembl\")\n\n  # fetch chromosome info plus some other annotations\n  genes_table &lt;- try(biomaRt::getBM(attributes = c(\n    \"ensembl_gene_id\", \"external_gene_name\",\n    \"description\", \"gene_biotype\", \"chromosome_name\", \"start_position\"\n  ), mart = mart, useCache = F))\n\n  write.csv(genes_table, file = \"data/covid/results/genes_table.csv\")\n}\n\n\n\n\nDownload precomputed data.\n\n# fetch_annotation is defined at the top of this document\nif (fetch_annotation) {\n  genes_file &lt;- file.path(path_results, \"genes_table.csv\")\n  if (!file.exists(genes_file)) download.file(file.path(path_data, \"covid/results/genes_table.csv\"), destfile = genes_file)\n}\n\n\ngenes.table &lt;- read.csv(genes_file)\ngenes.table &lt;- genes.table[genes.table$external_gene_name %in% rownames(sce.filt), ]\n\nNow that we have the chromosome information, we can calculate the proportion of reads that comes from chromosome Y per cell.But first we have to remove all genes in the pseudoautosmal regions of chrY that are: * chromosome:GRCh38:Y:10001 - 2781479 is shared with X: 10001 - 2781479 (PAR1) * chromosome:GRCh38:Y:56887903 - 57217415 is shared with X: 155701383 - 156030895 (PAR2)\n\npar1 = c(10001, 2781479)\npar2 = c(56887903, 57217415)\np1.gene = genes.table$external_gene_name[genes.table$start_position &gt; par1[1] & genes.table$start_position &lt; par1[2] & genes.table$chromosome_name == \"Y\"]\np2.gene = genes.table$external_gene_name[genes.table$start_position &gt; par2[1] & genes.table$start_position &lt; par2[2] & genes.table$chromosome_name == \"Y\"]\n\nchrY.gene &lt;- genes.table$external_gene_name[genes.table$chromosome_name == \"Y\"]\nchrY.gene = setdiff(chrY.gene, c(p1.gene, p2.gene))\n\nsce.filt@colData$pct_chrY &lt;- Matrix::colSums(counts(sce.filt)[chrY.gene, ]) / colSums(counts(sce.filt))\n\nThen plot XIST expression vs chrY proportion. As you can see, the samples are clearly on either side, even if some cells do not have detection of either.\n\n# as plotColData cannot take an expression vs metadata, we need to add in XIST expression to colData\nsce.filt@colData$XIST &lt;- counts(sce.filt)[\"XIST\", ] / colSums(counts(sce.filt)) * 10000\nplotColData(sce.filt, \"XIST\", \"pct_chrY\")\n\n\n\n\n\n\n\n\nPlot as violins.\n\nwrap_plots(\n    plotColData(sce.filt, y = \"XIST\", x = \"sample\", colour_by = \"sample\"),\n    plotColData(sce.filt, y = \"pct_chrY\", x = \"sample\", colour_by = \"sample\"),\n    ncol = 2\n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nHere, we can see clearly that we have three males and five females, can you see which samples they are? Do you think this will cause any problems for downstream analysis? Discuss with your group: what would be the best way to deal with this type of sex bias?"
  },
  {
    "objectID": "labs/bioc/bioc_01_qc.html#meta-qc_cellcycle",
    "href": "labs/bioc/bioc_01_qc.html#meta-qc_cellcycle",
    "title": " Quality Control",
    "section": "7 Cell cycle state",
    "text": "7 Cell cycle state\nWe here perform cell cycle scoring. To score a gene list, the algorithm calculates the difference of mean expression of the given list and the mean expression of reference genes. To build the reference, the function randomly chooses a bunch of genes matching the distribution of the expression of the given list. Cell cycle scoring adds three slots in the metadata, a score for S phase, a score for G2M phase and the predicted cell cycle phase.\n\nhs.pairs &lt;- readRDS(system.file(\"exdata\", \"human_cycle_markers.rds\", package = \"scran\"))\nanno &lt;- select(org.Hs.eg.db, keys = rownames(sce.filt), keytype = \"SYMBOL\", column = \"ENSEMBL\")\nensembl &lt;- anno$ENSEMBL[match(rownames(sce.filt), anno$SYMBOL)]\n\n# Use only genes related to biological process cell cycle to speed up\n# https://www.ebi.ac.uk/QuickGO/term/GO:0007049 = cell cycle (BP,Biological Process)\nGOs &lt;- na.omit(select(org.Hs.eg.db, keys = na.omit(ensembl), keytype = \"ENSEMBL\", column = \"GO\"))\nGOs &lt;- GOs[GOs$GO == \"GO:0007049\", \"ENSEMBL\"]\nhs.pairs &lt;- lapply(hs.pairs, function(x) {\n    x[rowSums(apply(x, 2, function(i) i %in% GOs)) &gt;= 1, ]\n})\nstr(hs.pairs)\n\nList of 3\n $ G1 :'data.frame':    6633 obs. of  2 variables:\n  ..$ first : chr [1:6633] \"ENSG00000100519\" \"ENSG00000100519\" \"ENSG00000100519\" \"ENSG00000100519\" ...\n  ..$ second: chr [1:6633] \"ENSG00000065135\" \"ENSG00000080345\" \"ENSG00000101266\" \"ENSG00000135679\" ...\n $ S  :'data.frame':    8308 obs. of  2 variables:\n  ..$ first : chr [1:8308] \"ENSG00000255302\" \"ENSG00000119969\" \"ENSG00000179051\" \"ENSG00000127586\" ...\n  ..$ second: chr [1:8308] \"ENSG00000100519\" \"ENSG00000100519\" \"ENSG00000100519\" \"ENSG00000136856\" ...\n $ G2M:'data.frame':    6235 obs. of  2 variables:\n  ..$ first : chr [1:6235] \"ENSG00000100519\" \"ENSG00000136856\" \"ENSG00000136856\" \"ENSG00000136856\" ...\n  ..$ second: chr [1:6235] \"ENSG00000146457\" \"ENSG00000138028\" \"ENSG00000227268\" \"ENSG00000101265\" ...\n\ncc.ensembl &lt;- ensembl[ensembl %in% GOs] # This is the fastest (less genes), but less accurate too\n# cc.ensembl &lt;- ensembl[ ensembl %in% unique(unlist(hs.pairs))]\n\nassignments &lt;- cyclone(sce.filt[ensembl %in% cc.ensembl, ], hs.pairs, gene.names = ensembl[ensembl %in% cc.ensembl])\nsce.filt$G1.score &lt;- assignments$scores$G1\nsce.filt$G2M.score &lt;- assignments$scores$G2M\nsce.filt$S.score &lt;- assignments$scores$S\nsce.filt$phase &lt;- assignments$phases\n\ntable(sce.filt$phase)\n\n\n  G1  G2M    S \n4334  827 1750 \n\n\nWe can now create a violin plot for the cell cycle scores as well.\n\nwrap_plots(\n    plotColData(sce.filt, y = \"G2M.score\", x = \"G1.score\", colour_by = \"phase\"),\n    plotColData(sce.filt, y = \"G2M.score\", x = \"sample\", colour_by = \"sample\"),\n    plotColData(sce.filt, y = \"G1.score\", x = \"sample\", colour_by = \"sample\"),\n    plotColData(sce.filt, y = \"S.score\", x = \"sample\", colour_by = \"sample\"),\n    ncol = 4\n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nCyclone predicts most cells as G1, but also quite a lot of cells with high S-Phase scores. Compare to results with Seurat and Scanpy and see how different predictors will give clearly different results.\nCyclone does an automatic prediction of cell cycle phase with a default cutoff of the scores at 0.5 As you can see this does not fit this data very well, so be cautious with using these predictions. Instead we suggest that you look at the scores."
  },
  {
    "objectID": "labs/bioc/bioc_01_qc.html#meta-qc_doublet",
    "href": "labs/bioc/bioc_01_qc.html#meta-qc_doublet",
    "title": " Quality Control",
    "section": "8 Predict doublets",
    "text": "8 Predict doublets\nDoublets/Multiples of cells in the same well/droplet is a common issue in scRNAseq protocols. Especially in droplet-based methods with overloading of cells. In a typical 10x experiment the proportion of doublets is linearly dependent on the amount of loaded cells. As indicated from the Chromium user guide, doublet rates are about as follows:\n\nMost doublet detectors simulates doublets by merging cell counts and predicts doublets as cells that have similar embeddings as the simulated doublets. Most such packages need an assumption about the number/proportion of expected doublets in the dataset. The data you are using is subsampled, but the original datasets contained about 5 000 cells per sample, hence we can assume that they loaded about 9 000 cells and should have a doublet rate at about 4%.\n\n\n\n\n\n\nCaution\n\n\n\nIdeally doublet prediction should be run on each sample separately, especially if your samples have different proportions of cell types. In this case, the data is subsampled so we have very few cells per sample and all samples are sorted PBMCs, so it is okay to run them together.\n\n\nThere is a method to predict if a cluster consists of mainly doublets findDoubletClusters(), but we can also predict individual cells based on simulations using the function computeDoubletDensity() which we will do here. Doublet detection will be performed using PCA, so we need to first normalize the data and run variable gene detection, as well as UMAP for visualization. These steps will be explored in more detail in coming exercises.\n\nsce.filt &lt;- logNormCounts(sce.filt)\ndec &lt;- modelGeneVar(sce.filt, block = sce.filt$sample)\nhvgs &lt;- getTopHVGs(dec, n = 2000)\n\nsce.filt &lt;- runPCA(sce.filt, subset_row = hvgs)\n\nsce.filt &lt;- runUMAP(sce.filt, pca = 10)\n\n\n# run computeDoubletDensity with 10 principal components.\nsce.filt &lt;- scDblFinder(sce.filt, dims = 10)\n\n\nwrap_plots(\n    plotUMAP(sce.filt, colour_by = \"scDblFinder.score\"),\n    plotUMAP(sce.filt, colour_by = \"scDblFinder.class\"),\n    plotUMAP(sce.filt, colour_by = \"sample\"),\n    ncol = 3\n)\n\n\n\n\n\n\n\n\nWe should expect that two cells have more detected genes than a single cell, lets check if our predicted doublets also have more detected genes in general.\n\nplotColData(sce.filt, y = \"detected\", x = \"scDblFinder.class\")\n\n\n\n\n\n\n\n\nNow, lets remove all predicted doublets from our data.\n\nsce.filt &lt;- sce.filt[, sce.filt$scDblFinder.class == \"singlet\"]\ndim(sce.filt)\n\n[1] 18854  6741\n\n\nTo summarize, lets check how many cells we have removed per sample, we started with 1500 cells per sample. Looking back at the intitial QC plots does it make sense that some samples have much fewer cells now?\n\ntable(sce$sample)\n\n\n  cov.1  cov.15  cov.16  cov.17 ctrl.13 ctrl.14 ctrl.19  ctrl.5 \n   1500    1500    1500    1500    1500    1500    1500    1500 \n\ntable(sce.filt$sample)\n\n\n  cov.1  cov.15  cov.16  cov.17 ctrl.13 ctrl.14 ctrl.19  ctrl.5 \n    807     516     351    1030    1101     914    1069     953"
  },
  {
    "objectID": "labs/bioc/bioc_01_qc.html#meta-qc_save",
    "href": "labs/bioc/bioc_01_qc.html#meta-qc_save",
    "title": " Quality Control",
    "section": "9 Save data",
    "text": "9 Save data\nFinally, lets save the QC-filtered data for further analysis. Create output directory data/covid/results and save data to that folder. This will be used in downstream labs.\n\nsaveRDS(sce.filt, file.path(path_results, \"bioc_covid_qc.rds\"))"
  },
  {
    "objectID": "labs/bioc/bioc_01_qc.html#meta-session",
    "href": "labs/bioc/bioc_01_qc.html#meta-session",
    "title": " Quality Control",
    "section": "10 Session info",
    "text": "10 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] scDblFinder_1.16.0          org.Hs.eg.db_3.18.0        \n [3] AnnotationDbi_1.64.1        patchwork_1.2.0            \n [5] scran_1.30.0                scater_1.30.1              \n [7] ggplot2_3.5.1               scuttle_1.12.0             \n [9] SingleCellExperiment_1.24.0 SummarizedExperiment_1.32.0\n[11] Biobase_2.62.0              GenomicRanges_1.54.1       \n[13] GenomeInfoDb_1.38.1         IRanges_2.36.0             \n[15] S4Vectors_0.40.2            BiocGenerics_0.48.1        \n[17] MatrixGenerics_1.14.0       matrixStats_1.4.1          \n\nloaded via a namespace (and not attached):\n  [1] RcppAnnoy_0.0.22          splines_4.3.3            \n  [3] later_1.3.2               BiocIO_1.12.0            \n  [5] bitops_1.0-8              tibble_3.2.1             \n  [7] polyclip_1.10-7           XML_3.99-0.17            \n  [9] fastDummies_1.7.4         lifecycle_1.0.4          \n [11] edgeR_4.0.16              hdf5r_1.3.11             \n [13] globals_0.16.3            lattice_0.22-6           \n [15] MASS_7.3-60.0.1           magrittr_2.0.3           \n [17] limma_3.58.1              plotly_4.10.4            \n [19] rmarkdown_2.28            yaml_2.3.10              \n [21] metapod_1.10.0            httpuv_1.6.15            \n [23] Seurat_5.1.0              sctransform_0.4.1        \n [25] spam_2.10-0               spatstat.sparse_3.1-0    \n [27] sp_2.1-4                  reticulate_1.39.0        \n [29] cowplot_1.1.3             pbapply_1.7-2            \n [31] DBI_1.2.3                 RColorBrewer_1.1-3       \n [33] abind_1.4-5               zlibbioc_1.48.0          \n [35] Rtsne_0.17                purrr_1.0.2              \n [37] RCurl_1.98-1.16           GenomeInfoDbData_1.2.11  \n [39] ggrepel_0.9.6             irlba_2.3.5.1            \n [41] spatstat.utils_3.1-0      listenv_0.9.1            \n [43] goftest_1.2-3             RSpectra_0.16-2          \n [45] spatstat.random_3.2-3     dqrng_0.3.2              \n [47] fitdistrplus_1.2-1        parallelly_1.38.0        \n [49] DelayedMatrixStats_1.24.0 leiden_0.4.3.1           \n [51] codetools_0.2-20          DelayedArray_0.28.0      \n [53] tidyselect_1.2.1          farver_2.1.2             \n [55] ScaledMatrix_1.10.0       viridis_0.6.5            \n [57] spatstat.explore_3.2-6    GenomicAlignments_1.38.0 \n [59] jsonlite_1.8.8            BiocNeighbors_1.20.0     \n [61] progressr_0.14.0          ggridges_0.5.6           \n [63] survival_3.7-0            tools_4.3.3              \n [65] ica_1.0-3                 Rcpp_1.0.13              \n [67] glue_1.7.0                gridExtra_2.3            \n [69] SparseArray_1.2.2         xfun_0.47                \n [71] dplyr_1.1.4               withr_3.0.1              \n [73] fastmap_1.2.0             bluster_1.12.0           \n [75] fansi_1.0.6               digest_0.6.37            \n [77] rsvd_1.0.5                R6_2.5.1                 \n [79] mime_0.12                 colorspace_2.1-1         \n [81] scattermore_1.2           tensor_1.5               \n [83] spatstat.data_3.1-2       RSQLite_2.3.7            \n [85] utf8_1.2.4                tidyr_1.3.1              \n [87] generics_0.1.3            data.table_1.15.4        \n [89] rtracklayer_1.62.0        httr_1.4.7               \n [91] htmlwidgets_1.6.4         S4Arrays_1.2.0           \n [93] uwot_0.1.16               pkgconfig_2.0.3          \n [95] gtable_0.3.5              blob_1.2.4               \n [97] lmtest_0.9-40             XVector_0.42.0           \n [99] htmltools_0.5.8.1         dotCall64_1.1-1          \n[101] SeuratObject_5.0.2        scales_1.3.0             \n[103] png_0.1-8                 knitr_1.48               \n[105] reshape2_1.4.4            rjson_0.2.21             \n[107] nlme_3.1-165              cachem_1.1.0             \n[109] zoo_1.8-12                stringr_1.5.1            \n[111] KernSmooth_2.23-24        parallel_4.3.3           \n[113] miniUI_0.1.1.1            vipor_0.4.7              \n[115] restfulr_0.0.15           pillar_1.9.0             \n[117] grid_4.3.3                vctrs_0.6.5              \n[119] RANN_2.6.2                promises_1.3.0           \n[121] BiocSingular_1.18.0       beachmat_2.18.0          \n[123] xtable_1.8-4              cluster_2.1.6            \n[125] beeswarm_0.4.0            evaluate_0.24.0          \n[127] cli_3.6.3                 locfit_1.5-9.9           \n[129] compiler_4.3.3            Rsamtools_2.18.0         \n[131] rlang_1.1.4               crayon_1.5.3             \n[133] future.apply_1.11.2       labeling_0.4.3           \n[135] plyr_1.8.9                ggbeeswarm_0.7.2         \n[137] stringi_1.8.4             deldir_2.0-4             \n[139] viridisLite_0.4.2         BiocParallel_1.36.0      \n[141] munsell_0.5.1             Biostrings_2.70.1        \n[143] lazyeval_0.2.2            spatstat.geom_3.2-9      \n[145] Matrix_1.6-5              RcppHNSW_0.6.0           \n[147] sparseMatrixStats_1.14.0  bit64_4.0.5              \n[149] future_1.34.0             KEGGREST_1.42.0          \n[151] statmod_1.5.0             shiny_1.9.1              \n[153] ROCR_1.0-11               igraph_2.0.3             \n[155] memoise_2.0.1             bit_4.0.5                \n[157] xgboost_2.1.1.1"
  },
  {
    "objectID": "labs/bioc/bioc_02_dimred.html",
    "href": "labs/bioc/bioc_02_dimred.html",
    "title": " Dimensionality Reduction",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified."
  },
  {
    "objectID": "labs/bioc/bioc_02_dimred.html#meta-dimred_prep",
    "href": "labs/bioc/bioc_02_dimred.html#meta-dimred_prep",
    "title": " Dimensionality Reduction",
    "section": "1 Data preparation",
    "text": "1 Data preparation\nFirst, let’s load all necessary libraries and the QC-filtered dataset from the previous step.\n\nsuppressPackageStartupMessages({\n    library(scater)\n    library(scran)\n    library(patchwork)\n    library(ggplot2)\n    library(umap)\n})\n\n\n# download pre-computed data if missing or long compute\nfetch_data &lt;- TRUE\n\n# url for source and intermediate data\npath_data &lt;- \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\npath_file &lt;- \"data/covid/results/bioc_covid_qc.rds\"\nif (!dir.exists(dirname(path_file))) dir.create(dirname(path_file), recursive = TRUE)\nif (fetch_data && !file.exists(path_file)) download.file(url = file.path(path_data, \"covid/results/bioc_covid_qc.rds\"), destfile = path_file)\nsce &lt;- readRDS(path_file)"
  },
  {
    "objectID": "labs/bioc/bioc_02_dimred.html#meta-dimred_fs",
    "href": "labs/bioc/bioc_02_dimred.html#meta-dimred_fs",
    "title": " Dimensionality Reduction",
    "section": "2 Feature selection",
    "text": "2 Feature selection\nWe first need to define which features/genes are important in our dataset to distinguish cell types. For this purpose, we need to find genes that are highly variable across cells, which in turn will also provide a good separation of the cell clusters.\nWith modelGeneVar we can specify a blocking parameter, so the trend fitting and variance decomposition is done separately for each batch.\n\nsce &lt;- computeSumFactors(sce, sizes = c(20, 40, 60, 80))\nsce &lt;- logNormCounts(sce)\nvar.out &lt;- modelGeneVar(sce, block = sce$sample)\nhvgs &lt;- getTopHVGs(var.out, n = 2000)\n\nWe can plot the total variance and the biological variance vs mean expressioni for one of the samples.\n\npar(mfrow = c(1, 2))\n# plot mean over TOTAL variance\n# Visualizing the fit:\nfit.var &lt;- metadata(var.out$per.block[[1]])\n{\n    plot(fit.var$mean, fit.var$var,\n        xlab = \"Mean of log-expression\",\n        ylab = \"Variance of log-expression\",\n        main = \"Total variance\"\n    )\n    curve(fit.var$trend(x), col = \"dodgerblue\", add = TRUE, lwd = 2)\n\n    # Select 1000 top variable genes\n    hvg.out &lt;- getTopHVGs(var.out, n = 1000)\n\n    # highligt those cells in the plot\n    cutoff &lt;- rownames(var.out) %in% hvg.out\n    points(fit.var$mean[cutoff], fit.var$var[cutoff], col = \"red\", pch = 16, cex = .6)\n}\n\n{\n    # plot mean over BIOLOGICAL variance for same sample\n    plot(var.out$per.block[[1]]$mean, var.out$per.block[[1]]$bio, pch = 16, cex = 0.4, \n         xlab = \"Mean log-expression\", \n         ylab = \"Variance of log-expression\",\n         main = \"Biological variance\")\n    lines(c(min(var.out$per.block[[1]]$mean), max(var.out$per.block[[1]]$mean)), c(0, 0), col = \"dodgerblue\", lwd = 2)\n    points(var.out$per.block[[1]]$mean[cutoff], var.out$per.block[[1]]$bio[cutoff], col = \"red\", pch = 16, cex = .6)\n}\n\n\n\n\n\n\n\n\nWe can plot the same for all the samples, also showing the technical variance.\n\ncutoff &lt;- rownames(var.out) %in% hvgs\n\npar(mfrow = c(1, 3))\n    plot(var.out$mean, var.out$total, pch = 16, cex = 0.4, \n         xlab = \"Mean log-expression\", \n         ylab = \"Variance of log-expression\",\n         main = \"Total variance\")\n    points(var.out$mean[cutoff], var.out$total[cutoff], col = \"red\", pch = 16, cex = .6)\n\n    plot(var.out$mean, var.out$bio, pch = 16, cex = 0.4, \n         xlab = \"Mean log-expression\", \n         ylab = \"Variance of log-expression\",\n         main = \"Biological variance\")\n    points(var.out$mean[cutoff], var.out$bio[cutoff], col = \"red\", pch = 16, cex = .6)\n    \n    plot(var.out$mean, var.out$tech, pch = 16, cex = 0.4, \n         xlab = \"Mean log-expression\", \n         ylab = \"Variance of log-expression\",\n         main = \"Technical variance\")\n    points(var.out$mean[cutoff], var.out$tech[cutoff], col = \"red\", pch = 16, cex = .6)"
  },
  {
    "objectID": "labs/bioc/bioc_02_dimred.html#meta-dimred_zs",
    "href": "labs/bioc/bioc_02_dimred.html#meta-dimred_zs",
    "title": " Dimensionality Reduction",
    "section": "3 Z-score transformation",
    "text": "3 Z-score transformation\nNow that the genes have been selected, we now proceed with PCA. Since each gene has a different expression level, it means that genes with higher expression values will naturally have higher variation that will be captured by PCA. This means that we need to somehow give each gene a similar weight when performing PCA (see below). The common practice is to center and scale each gene before performing PCA. This exact scaling called Z-score normalization is very useful for PCA, clustering and plotting heatmaps. Additionally, we can use regression to remove any unwanted sources of variation from the dataset, such as cell cycle, sequencing depth, percent mitochondria etc. This is achieved by doing a generalized linear regression using these parameters as co-variates in the model. Then the residuals of the model are taken as the regressed data. Although perhaps not in the best way, batch effect regression can also be done here. By default, variables are scaled in the PCA step and is not done separately. But it could be achieved by running the commands below:\nHowever, unlike the Seurat, this step is implemented inside the PCA function below. Here we will show you how to add the scaledData back to the object.\n\n# sce@assays$data@listData$scaled.data &lt;- apply(exprs(sce)[rownames(hvg.out),,drop=FALSE],2,function(x) scale(x,T,T))\n# rownames(sce@assays$data@listData$scaled.data) &lt;- rownames(hvg.out)"
  },
  {
    "objectID": "labs/bioc/bioc_02_dimred.html#meta-dimred_pca",
    "href": "labs/bioc/bioc_02_dimred.html#meta-dimred_pca",
    "title": " Dimensionality Reduction",
    "section": "4 PCA",
    "text": "4 PCA\nPerforming PCA has many useful applications and interpretations, which much depends on the data used. In the case of single-cell data, we want to segregate samples based on gene expression patterns in the data.\nWe use the logcounts and then set scale_features to TRUE in order to scale each gene.\n\n# runPCA and specify the variable genes to use for dim reduction with subset_row\nsce &lt;- runPCA(sce, exprs_values = \"logcounts\", ncomponents = 50, subset_row = hvgs, scale = TRUE)\n\nWe then plot the first principal components.\n\nwrap_plots(\n    plotReducedDim(sce, dimred = \"PCA\", colour_by = \"sample\", ncomponents = 1:2, point_size = 0.6),\n    plotReducedDim(sce, dimred = \"PCA\", colour_by = \"sample\", ncomponents = 3:4, point_size = 0.6),\n    plotReducedDim(sce, dimred = \"PCA\", colour_by = \"sample\", ncomponents = 5:6, point_size = 0.6),\n    ncol = 3\n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nTo identify which genes (Seurat) or metadata parameters (Scater/Scran) contribute the most to each PC, one can retrieve the loading matrix information. Unfortunately, this is not implemented in Scater/Scran, so you will need to compute PCA using logcounts.\nHere, we can check how the different metadata variables contributes to each PC. This can be important to look at to understand different biases you may have in your data.\n\nplotExplanatoryPCs(sce,nvars_to_plot = 15)\n\n\n\n\n\n\n\n\nClearly, the sample id contributes to many of the PCs and PC7 but you can also see that many PCs are effected by different QC parameters.\nWe can also plot the amount of variance explained by each PC.\n\nplot(attr(reducedDim(sce, \"PCA\"), \"percentVar\")[1:50] * 100, type = \"l\", ylab = \"% variance\", xlab = \"Principal component #\")\npoints(attr(reducedDim(sce, \"PCA\"), \"percentVar\")[1:50] * 100, pch = 21, bg = \"grey\", cex = .5)\n\n\n\n\n\n\n\n\nBased on this plot, we can see that the top 8 PCs retain a lot of information, while other PCs contain progressively less. However, it is still advisable to use more PCs since they might contain information about rare cell types (such as platelets and DCs in this dataset)"
  },
  {
    "objectID": "labs/bioc/bioc_02_dimred.html#meta-dimred_tsne",
    "href": "labs/bioc/bioc_02_dimred.html#meta-dimred_tsne",
    "title": " Dimensionality Reduction",
    "section": "5 tSNE",
    "text": "5 tSNE\nWe will now run BH-tSNE.\n\nset.seed(42)\nsce &lt;- runTSNE(sce, dimred = \"PCA\", n_dimred = 30, perplexity = 30, name = \"tSNE_on_PCA\")\n\nWe plot the tSNE scatterplot colored by dataset. We can clearly see the effect of batches present in the dataset.\n\nplotReducedDim(sce, dimred = \"tSNE_on_PCA\", colour_by = \"sample\")"
  },
  {
    "objectID": "labs/bioc/bioc_02_dimred.html#meta-dimred_umap",
    "href": "labs/bioc/bioc_02_dimred.html#meta-dimred_umap",
    "title": " Dimensionality Reduction",
    "section": "6 UMAP",
    "text": "6 UMAP\nWe can now run UMAP for cell embeddings.\n\nsce &lt;- runUMAP(sce, dimred = \"PCA\", n_dimred = 30, ncomponents = 2, name = \"UMAP_on_PCA\")\n# see ?umap and ?runUMAP for more info\n\nUMAP is plotted colored per dataset. Although less distinct as in the tSNE, we still see quite an effect of the different batches in the data.\n\nsce &lt;- runUMAP(sce, dimred = \"PCA\", n_dimred = 30, ncomponents = 10, name = \"UMAP10_on_PCA\")\n# see ?umap and ?runUMAP for more info\n\nWe can now plot PCA, UMAP and tSNE side by side for comparison. Have a look at the UMAP and tSNE. What similarities/differences do you see? Can you explain the differences based on what you learned during the lecture? Also, we can conclude from the dimensionality reductions that our dataset contains a batch effect that needs to be corrected before proceeding to clustering and differential gene expression analysis.\n\nwrap_plots(\n    plotReducedDim(sce, dimred = \"UMAP_on_PCA\", colour_by = \"sample\") +\n        ggplot2::ggtitle(label = \"UMAP_on_PCA\"),\n    plotReducedDim(sce, dimred = \"UMAP10_on_PCA\", colour_by = \"sample\", ncomponents = 1:2) +\n        ggplot2::ggtitle(label = \"UMAP10_on_PCA\"),\n    plotReducedDim(sce, dimred = \"UMAP10_on_PCA\", colour_by = \"sample\", ncomponents = 3:4) +\n        ggplot2::ggtitle(label = \"UMAP10_on_PCA\"),\n    ncol = 3\n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nWe have now done Variable gene selection, PCA and UMAP with the settings we selected for you. Test a few different ways of selecting variable genes, number of PCs for UMAP and check how it influences your embedding."
  },
  {
    "objectID": "labs/bioc/bioc_02_dimred.html#meta-dimred_zsg",
    "href": "labs/bioc/bioc_02_dimred.html#meta-dimred_zsg",
    "title": " Dimensionality Reduction",
    "section": "7 Z-scores & DR graphs",
    "text": "7 Z-scores & DR graphs\nAlthough running a second dimensionality reduction (i.e tSNE or UMAP) on PCA would be a standard approach (because it allows higher computation efficiency), the options are actually limitless. Below we will show a couple of other common options such as running directly on the scaled data (z-scores) (which was used for PCA) or on a graph built from scaled data. We will only work with UMAPs, but the same applies for tSNE.\n\n7.1 UMAP from z-scores\nTo run tSNE or UMAP on the scaled data, one first needs to select the number of variables to use. This is because including dimensions that do contribute to the separation of your cell types will in the end mask those differences. Another reason for it is because running with all genes/features also will take longer or might be computationally unfeasible. Therefore we will use the scaled data of the highly variable genes.\n\nsce &lt;- runUMAP(sce, exprs_values = \"logcounts\", name = \"UMAP_on_ScaleData\")\n\n\n\n7.2 UMAP from graph\nTo run tSNE or UMAP on the a graph, we first need to build a graph from the data. In fact, both tSNE and UMAP first build a graph from the data using a specified distance matrix and then optimize the embedding. Since a graph is just a matrix containing distances from cell to cell and as such, you can run either UMAP or tSNE using any other distance metric desired. Euclidean and Correlation are usually the most commonly used.\n\n# Build Graph\nnn &lt;- RANN::nn2(reducedDim(sce, \"PCA\"), k = 30)\nnames(nn) &lt;- c(\"idx\", \"dist\")\ng &lt;- buildKNNGraph(sce, k = 30, use.dimred = \"PCA\")\nreducedDim(sce, \"KNN\") &lt;- igraph::as_adjacency_matrix(g)\n\n# Run UMAP and rename it for comparisson\n# temp &lt;- umap::umap.defaults\ntry(reducedDim(sce, \"UMAP_on_Graph\") &lt;- NULL)\nreducedDim(sce, \"UMAP_on_Graph\") &lt;- uwot::umap(X = NULL, n_components = 2, nn_method = nn)\n\nWe can now plot the UMAP comparing both on PCA vs ScaledSata vs Graph.\n\nwrap_plots(\n    plotReducedDim(sce, dimred = \"UMAP_on_PCA\", colour_by = \"sample\") +\n        ggplot2::ggtitle(label = \"UMAP_on_PCA\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_ScaleData\", colour_by = \"sample\") +\n        ggplot2::ggtitle(label = \"UMAP_on_ScaleData\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_Graph\", colour_by = \"sample\") +\n        ggplot2::ggtitle(label = \"UMAP_on_Graph\"),\n    ncol = 3\n) + plot_layout(guides = \"collect\")"
  },
  {
    "objectID": "labs/bioc/bioc_02_dimred.html#meta-dimred_plotgenes",
    "href": "labs/bioc/bioc_02_dimred.html#meta-dimred_plotgenes",
    "title": " Dimensionality Reduction",
    "section": "8 Genes of interest",
    "text": "8 Genes of interest\nLet’s plot some marker genes for different cell types onto the embedding.\n\n\n\nMarkers\nCell Type\n\n\n\n\nCD3E\nT cells\n\n\nCD3E CD4\nCD4+ T cells\n\n\nCD3E CD8A\nCD8+ T cells\n\n\nGNLY, NKG7\nNK cells\n\n\nMS4A1\nB cells\n\n\nCD14, LYZ, CST3, MS4A7\nCD14+ Monocytes\n\n\nFCGR3A, LYZ, CST3, MS4A7\nFCGR3A+ Monocytes\n\n\nFCER1A, CST3\nDCs\n\n\n\n\nplotlist &lt;- list()\nfor (i in c(\"CD3E\", \"CD4\", \"CD8A\", \"NKG7\", \"GNLY\", \"MS4A1\", \"CD14\", \"LYZ\", \"MS4A7\", \"FCGR3A\", \"CST3\", \"FCER1A\")) {\n    plotlist[[i]] &lt;- plotReducedDim(sce, dimred = \"UMAP_on_PCA\", colour_by = i, by_exprs_values = \"logcounts\") +\n        scale_fill_gradientn(colours = colorRampPalette(c(\"grey90\", \"orange3\", \"firebrick\", \"firebrick\", \"red\", \"red\"))(10)) +\n        ggtitle(label = i) + theme(plot.title = element_text(size = 20))\n}\nwrap_plots(plotlist, ncol = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nSelect some of your dimensionality reductions and plot some of the QC stats that were calculated in the previous lab. Can you see if some of the separation in your data is driven by quality of the cells?\n\n\n\nplotlist &lt;- list()\nfor (i in c(\"detected\", \"total\", \"subsets_mt_percent\",\"subsets_ribo_percent\",\"subsets_hb_percent\")) { \n    plotlist[[i]] &lt;- plotReducedDim(sce, dimred = \"UMAP_on_PCA\", colour_by = i, by_exprs_values = \"logcounts\") +\n        scale_fill_gradientn(colours = colorRampPalette(c(\"grey90\", \"orange3\", \"firebrick\", \"firebrick\", \"red\", \"red\"))(10)) +\n        ggtitle(label = i) + theme(plot.title = element_text(size = 20))\n}\nwrap_plots(plotlist, ncol = 3)"
  },
  {
    "objectID": "labs/bioc/bioc_02_dimred.html#meta-dimred_save",
    "href": "labs/bioc/bioc_02_dimred.html#meta-dimred_save",
    "title": " Dimensionality Reduction",
    "section": "9 Save data",
    "text": "9 Save data\nWe can finally save the object for use in future steps.\n\nsaveRDS(sce, \"data/covid/results/bioc_covid_qc_dr.rds\")"
  },
  {
    "objectID": "labs/bioc/bioc_02_dimred.html#meta-session",
    "href": "labs/bioc/bioc_02_dimred.html#meta-session",
    "title": " Dimensionality Reduction",
    "section": "10 Session info",
    "text": "10 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] umap_0.2.10.0               patchwork_1.2.0            \n [3] scran_1.30.0                scater_1.30.1              \n [5] ggplot2_3.5.1               scuttle_1.12.0             \n [7] SingleCellExperiment_1.24.0 SummarizedExperiment_1.32.0\n [9] Biobase_2.62.0              GenomicRanges_1.54.1       \n[11] GenomeInfoDb_1.38.1         IRanges_2.36.0             \n[13] S4Vectors_0.40.2            BiocGenerics_0.48.1        \n[15] MatrixGenerics_1.14.0       matrixStats_1.4.1          \n\nloaded via a namespace (and not attached):\n [1] bitops_1.0-8              gridExtra_2.3            \n [3] rlang_1.1.4               magrittr_2.0.3           \n [5] RcppAnnoy_0.0.22          compiler_4.3.3           \n [7] DelayedMatrixStats_1.24.0 png_0.1-8                \n [9] vctrs_0.6.5               pkgconfig_2.0.3          \n[11] crayon_1.5.3              fastmap_1.2.0            \n[13] XVector_0.42.0            labeling_0.4.3           \n[15] utf8_1.2.4                rmarkdown_2.28           \n[17] ggbeeswarm_0.7.2          xfun_0.47                \n[19] bluster_1.12.0            zlibbioc_1.48.0          \n[21] beachmat_2.18.0           jsonlite_1.8.8           \n[23] DelayedArray_0.28.0       BiocParallel_1.36.0      \n[25] irlba_2.3.5.1             parallel_4.3.3           \n[27] cluster_2.1.6             R6_2.5.1                 \n[29] limma_3.58.1              reticulate_1.39.0        \n[31] Rcpp_1.0.13               knitr_1.48               \n[33] Matrix_1.6-5              igraph_2.0.3             \n[35] tidyselect_1.2.1          abind_1.4-5              \n[37] yaml_2.3.10               viridis_0.6.5            \n[39] codetools_0.2-20          lattice_0.22-6           \n[41] tibble_3.2.1              withr_3.0.1              \n[43] askpass_1.2.0             evaluate_0.24.0          \n[45] Rtsne_0.17                pillar_1.9.0             \n[47] generics_0.1.3            RCurl_1.98-1.16          \n[49] sparseMatrixStats_1.14.0  munsell_0.5.1            \n[51] scales_1.3.0              glue_1.7.0               \n[53] metapod_1.10.0            tools_4.3.3              \n[55] BiocNeighbors_1.20.0      ScaledMatrix_1.10.0      \n[57] RSpectra_0.16-2           locfit_1.5-9.9           \n[59] RANN_2.6.2                cowplot_1.1.3            \n[61] grid_4.3.3                edgeR_4.0.16             \n[63] colorspace_2.1-1          GenomeInfoDbData_1.2.11  \n[65] beeswarm_0.4.0            BiocSingular_1.18.0      \n[67] vipor_0.4.7               cli_3.6.3                \n[69] rsvd_1.0.5                fansi_1.0.6              \n[71] S4Arrays_1.2.0            viridisLite_0.4.2        \n[73] dplyr_1.1.4               uwot_0.1.16              \n[75] gtable_0.3.5              digest_0.6.37            \n[77] SparseArray_1.2.2         ggrepel_0.9.6            \n[79] dqrng_0.3.2               htmlwidgets_1.6.4        \n[81] farver_2.1.2              htmltools_0.5.8.1        \n[83] lifecycle_1.0.4           statmod_1.5.0            \n[85] openssl_2.2.1"
  },
  {
    "objectID": "labs/bioc/bioc_03_integration.html",
    "href": "labs/bioc/bioc_03_integration.html",
    "title": " Data Integration",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified.\nIn this tutorial we will look at different ways of integrating multiple single cell RNA-seq datasets. We will explore a few different methods to correct for batch effects across datasets. Seurat uses the data integration method presented in Comprehensive Integration of Single Cell Data, while Scran and Scanpy use a mutual Nearest neighbour method (MNN). Below you can find a list of some methods for single data integration:"
  },
  {
    "objectID": "labs/bioc/bioc_03_integration.html#meta-int_prep",
    "href": "labs/bioc/bioc_03_integration.html#meta-int_prep",
    "title": " Data Integration",
    "section": "1 Data preparation",
    "text": "1 Data preparation\nLet’s first load necessary libraries and the data saved in the previous lab.\n\nsuppressPackageStartupMessages({\n    library(scater)\n    library(scran)\n    library(patchwork)\n    library(ggplot2)\n    library(batchelor)\n    library(harmony)\n    library(basilisk)\n})\n\n# path to conda env for python environment with scanorama.\ncondapath =  \"/Users/asabjor/miniconda3/envs/scanpy_2024_nopip\"\n\n\n# download pre-computed data if missing or long compute\nfetch_data &lt;- TRUE\n\n# url for source and intermediate data\npath_data &lt;- \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\npath_file &lt;- \"data/covid/results/bioc_covid_qc_dr.rds\"\nif (!dir.exists(dirname(path_file))) dir.create(dirname(path_file), recursive = TRUE)\nif (fetch_data && !file.exists(path_file)) download.file(url = file.path(path_data, \"covid/results/bioc_covid_qc_dr.rds\"), destfile = path_file)\nsce &lt;- readRDS(path_file)\nprint(reducedDims(sce))\n\nList of length 8\nnames(8): PCA UMAP tSNE_on_PCA ... UMAP_on_ScaleData KNN UMAP_on_Graph\n\n\nWe split the combined object into a list, with each dataset as an element. We perform standard preprocessing (log-normalization), and identify variable features individually for each dataset based on a variance stabilizing transformation (vst).\nIf you recall from the dimensionality reduction exercise, we can run variable genes detection with a blocking parameter to avoid including batch effect genes. Here we will explore the genesets we get with and without the blocking parameter and also the variable genes per dataset.\n\nvar.out &lt;- modelGeneVar(sce, block = sce$sample)\nhvgs &lt;- getTopHVGs(var.out, n = 2000)\n\nvar.out.nobatch &lt;- modelGeneVar(sce)\nhvgs.nobatch &lt;- getTopHVGs(var.out.nobatch, n = 2000)\n\n# the var out with block has a data frame of data frames in column 7. \n# one per dataset.\nhvgs_per_dataset &lt;- lapply(var.out[[7]], getTopHVGs, n=2000)\n                           \nhvgs_per_dataset$all = hvgs\nhvgs_per_dataset$all.nobatch = hvgs.nobatch\n\n\n\n\ntemp &lt;- unique(unlist(hvgs_per_dataset))\noverlap &lt;- sapply(hvgs_per_dataset, function(x) {\n    temp %in% x\n})\n\n\npheatmap::pheatmap(t(overlap * 1), cluster_rows = F, color = c(\"grey90\", \"grey20\")) ## MNN\n\n\n\n\n\n\n\n\nAs you can see, there are a lot of genes that are variable in just one dataset. There are also some genes in the gene set that was selected using all the data without blocking samples, that are not variable in any of the individual datasets. These are most likely genes driven by batch effects.\nThe best way to select features for integration is to combine the information on variable genes across the dataset. This is what we have in the all section where the information on variable features in the different datasets is combined.\nFor all downstream integration we will use this set of genes so that it is comparable across the methods. We already used that set of genes in the dimensionality reduction exercise to run scaling and pca.\nWe also store the variable gene information in the object for use furhter down the line.\n\nmetadata(sce)$hvgs = hvgs"
  },
  {
    "objectID": "labs/bioc/bioc_03_integration.html#meta-dimred_harmony",
    "href": "labs/bioc/bioc_03_integration.html#meta-dimred_harmony",
    "title": " Data Integration",
    "section": "3 Harmony",
    "text": "3 Harmony\nAn alternative method for integration is Harmony, for more details on the method, please se their paper Nat. Methods. This method runs the integration on a dimensionality reduction, in most applications the PCA. So first, we prefer to have scaling and PCA with the same set of genes that were used for the CCA integration, which we ran earlier.\n\nlibrary(harmony)\n\nreducedDimNames(sce)\n\n [1] \"PCA\"               \"UMAP\"              \"tSNE_on_PCA\"      \n [4] \"UMAP_on_PCA\"       \"UMAP10_on_PCA\"     \"UMAP_on_ScaleData\"\n [7] \"KNN\"               \"UMAP_on_Graph\"     \"MNN\"              \n[10] \"tSNE_on_MNN\"       \"UMAP_on_MNN\"      \n\nsce &lt;- RunHarmony(\n    sce,\n    group.by.vars = \"sample\",\n    reduction.save = \"harmony\",\n    reduction = \"PCA\",\n    dims.use = 1:50\n)\n\n# Here we use all PCs computed from Harmony for UMAP calculation\nsce &lt;- runUMAP(sce, dimred = \"harmony\", n_dimred = 50, ncomponents = 2, name = \"UMAP_on_Harmony\")\n\nplotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"sample\", point_size = 0.6) + ggplot2::ggtitle(label = \"UMAP_on_Harmony\")"
  },
  {
    "objectID": "labs/bioc/bioc_03_integration.html#meta-dimred_scanorama",
    "href": "labs/bioc/bioc_03_integration.html#meta-dimred_scanorama",
    "title": " Data Integration",
    "section": "4 Scanorama",
    "text": "4 Scanorama\nAnother integration method is Scanorama (see Nat. Biotech.). This method is implemented in python, but we can run it through the Reticulate package.\nWe will run it with the same set of variable genes, but first we have to create a list of all the objects per sample.\n\nscelist &lt;- lapply(unique(sce$sample), function(x) {\n    x &lt;- t(as.matrix(assay(sce, \"logcounts\")[hvgs,sce$sample == x]))\n})\ngenelist =  rep(list(hvgs),length(scelist))\n\nlapply(scelist, dim)\n\n[[1]]\n[1]  807 2000\n\n[[2]]\n[1]  516 2000\n\n[[3]]\n[1]  351 2000\n\n[[4]]\n[1] 1030 2000\n\n[[5]]\n[1]  953 2000\n\n[[6]]\n[1] 1101 2000\n\n[[7]]\n[1]  914 2000\n\n[[8]]\n[1] 1069 2000\n\n\nScanorama is implemented in python, but through reticulate we can load python packages and run python functions. In this case we also use the basilisk package for a more clean activation of python environment.\nAt the top of this script, we set the variable condapath to point to the conda environment where scanorama is included.\n\n# run scanorama via basilisk with scelist and genelist as input.\nintegrated.data = basiliskRun(env=condapath, fun=function(datas, genes) {\n  scanorama &lt;- reticulate::import(\"scanorama\")\n  output &lt;- scanorama$integrate(datasets_full = datas,\n                                         genes_list = genes )\n  return(output)\n}, datas = scelist, genes = genelist, testload=\"scanorama\")\n\nFound 2000 genes among all datasets\n[[0.         0.57945736 0.43304843 0.30873786 0.48373557 0.34820322\n  0.29739777 0.10780669]\n [0.         0.         0.78875969 0.30426357 0.38195173 0.21317829\n  0.19186047 0.13953488]\n [0.         0.         0.         0.20512821 0.42735043 0.45584046\n  0.28774929 0.25356125]\n [0.         0.         0.         0.         0.21406086 0.05048544\n  0.12621359 0.17669903]\n [0.         0.         0.         0.         0.         0.85624344\n  0.63483736 0.19363891]\n [0.         0.         0.         0.         0.         0.\n  0.78110808 0.47427502]\n [0.         0.         0.         0.         0.         0.\n  0.         0.68662301]\n [0.         0.         0.         0.         0.         0.\n  0.         0.        ]]\nProcessing datasets (4, 5)\nProcessing datasets (1, 2)\nProcessing datasets (5, 6)\nProcessing datasets (6, 7)\nProcessing datasets (4, 6)\nProcessing datasets (0, 1)\nProcessing datasets (0, 4)\nProcessing datasets (5, 7)\nProcessing datasets (2, 5)\nProcessing datasets (0, 2)\nProcessing datasets (2, 4)\nProcessing datasets (1, 4)\nProcessing datasets (0, 5)\nProcessing datasets (0, 3)\nProcessing datasets (1, 3)\nProcessing datasets (0, 6)\nProcessing datasets (2, 6)\nProcessing datasets (2, 7)\nProcessing datasets (3, 4)\nProcessing datasets (1, 5)\nProcessing datasets (2, 3)\nProcessing datasets (4, 7)\nProcessing datasets (1, 6)\nProcessing datasets (3, 7)\nProcessing datasets (1, 7)\nProcessing datasets (3, 6)\nProcessing datasets (0, 7)\n\nintdimred &lt;- do.call(rbind, integrated.data[[1]])\ncolnames(intdimred) &lt;- paste0(\"PC_\", 1:100)\nrownames(intdimred) &lt;- colnames(logcounts(sce))\n\n# Add standard deviations in order to draw Elbow Plots \nstdevs &lt;- apply(intdimred, MARGIN = 2, FUN = sd)\nattr(intdimred, \"varExplained\") &lt;- stdevs\n\nreducedDim(sce, \"Scanorama\") &lt;- intdimred\n\n# Here we use all PCs computed from Scanorama for UMAP calculation\nsce &lt;- runUMAP(sce, dimred = \"Scanorama\", n_dimred = 50, ncomponents = 2, name = \"UMAP_on_Scanorama\")\n\nplotReducedDim(sce, dimred = \"UMAP_on_Scanorama\", colour_by = \"sample\", point_size = 0.6) + ggplot2::ggtitle(label = \"UMAP_on_Scanorama\")"
  },
  {
    "objectID": "labs/bioc/bioc_03_integration.html#overview-all-methods",
    "href": "labs/bioc/bioc_03_integration.html#overview-all-methods",
    "title": " Data Integration",
    "section": "5 Overview all methods",
    "text": "5 Overview all methods\nNow we will plot UMAPS with all three integration methods side by side.\n\np1 &lt;- plotReducedDim(sce, dimred = \"UMAP_on_PCA\", colour_by = \"sample\", point_size = 0.6) + ggplot2::ggtitle(label = \"UMAP_on_PCA\")\np2 &lt;- plotReducedDim(sce, dimred = \"UMAP_on_MNN\", colour_by = \"sample\", point_size = 0.6) + ggplot2::ggtitle(label = \"UMAP_on_MNN\")\np3 &lt;- plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"sample\", point_size = 0.6) + ggplot2::ggtitle(label = \"UMAP_on_Harmony\")\np4 &lt;- plotReducedDim(sce, dimred = \"UMAP_on_Scanorama\", colour_by = \"sample\", point_size = 0.6) + ggplot2::ggtitle(label = \"UMAP_on_Scanorama\")\n\nwrap_plots(p1, p2, p3, p4, nrow = 2) +\n    plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nLook at the different integration results, which one do you think looks the best? How would you motivate selecting one method over the other? How do you think you could best evaluate if the integration worked well?\n\n\nLet’s save the integrated data for further analysis.\n\nsaveRDS(sce, \"data/covid/results/bioc_covid_qc_dr_int.rds\")"
  },
  {
    "objectID": "labs/bioc/bioc_03_integration.html#meta-session",
    "href": "labs/bioc/bioc_03_integration.html#meta-session",
    "title": " Data Integration",
    "section": "6 Session info",
    "text": "6 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] basilisk_1.14.1             harmony_1.2.1              \n [3] Rcpp_1.0.13                 batchelor_1.18.0           \n [5] patchwork_1.2.0             scran_1.30.0               \n [7] scater_1.30.1               ggplot2_3.5.1              \n [9] scuttle_1.12.0              SingleCellExperiment_1.24.0\n[11] SummarizedExperiment_1.32.0 Biobase_2.62.0             \n[13] GenomicRanges_1.54.1        GenomeInfoDb_1.38.1        \n[15] IRanges_2.36.0              S4Vectors_0.40.2           \n[17] BiocGenerics_0.48.1         MatrixGenerics_1.14.0      \n[19] matrixStats_1.4.1          \n\nloaded via a namespace (and not attached):\n [1] bitops_1.0-8              gridExtra_2.3            \n [3] rlang_1.1.4               magrittr_2.0.3           \n [5] RcppAnnoy_0.0.22          compiler_4.3.3           \n [7] dir.expiry_1.10.0         DelayedMatrixStats_1.24.0\n [9] png_0.1-8                 vctrs_0.6.5              \n[11] pkgconfig_2.0.3           crayon_1.5.3             \n[13] fastmap_1.2.0             XVector_0.42.0           \n[15] labeling_0.4.3            utf8_1.2.4               \n[17] rmarkdown_2.28            ggbeeswarm_0.7.2         \n[19] xfun_0.47                 bluster_1.12.0           \n[21] zlibbioc_1.48.0           beachmat_2.18.0          \n[23] jsonlite_1.8.8            DelayedArray_0.28.0      \n[25] BiocParallel_1.36.0       irlba_2.3.5.1            \n[27] parallel_4.3.3            cluster_2.1.6            \n[29] R6_2.5.1                  RColorBrewer_1.1-3       \n[31] limma_3.58.1              reticulate_1.39.0        \n[33] knitr_1.48                Matrix_1.6-5             \n[35] igraph_2.0.3              tidyselect_1.2.1         \n[37] abind_1.4-5               yaml_2.3.10              \n[39] viridis_0.6.5             codetools_0.2-20         \n[41] lattice_0.22-6            tibble_3.2.1             \n[43] basilisk.utils_1.14.1     withr_3.0.1              \n[45] evaluate_0.24.0           Rtsne_0.17               \n[47] pillar_1.9.0              filelock_1.0.3           \n[49] generics_0.1.3            RCurl_1.98-1.16          \n[51] sparseMatrixStats_1.14.0  munsell_0.5.1            \n[53] scales_1.3.0              RhpcBLASctl_0.23-42      \n[55] glue_1.7.0                metapod_1.10.0           \n[57] pheatmap_1.0.12           tools_4.3.3              \n[59] BiocNeighbors_1.20.0      ScaledMatrix_1.10.0      \n[61] locfit_1.5-9.9            cowplot_1.1.3            \n[63] grid_4.3.3                edgeR_4.0.16             \n[65] colorspace_2.1-1          GenomeInfoDbData_1.2.11  \n[67] beeswarm_0.4.0            BiocSingular_1.18.0      \n[69] vipor_0.4.7               cli_3.6.3                \n[71] rsvd_1.0.5                fansi_1.0.6              \n[73] S4Arrays_1.2.0            viridisLite_0.4.2        \n[75] dplyr_1.1.4               uwot_0.1.16              \n[77] ResidualMatrix_1.12.0     gtable_0.3.5             \n[79] digest_0.6.37             SparseArray_1.2.2        \n[81] ggrepel_0.9.6             dqrng_0.3.2              \n[83] farver_2.1.2              htmlwidgets_1.6.4        \n[85] htmltools_0.5.8.1         lifecycle_1.0.4          \n[87] statmod_1.5.0"
  },
  {
    "objectID": "labs/bioc/bioc_04_clustering.html",
    "href": "labs/bioc/bioc_04_clustering.html",
    "title": " Clustering",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified.\nIn this tutorial, we will continue the analysis of the integrated dataset. We will use the integrated PCA or CCA to perform the clustering. First, we will construct a \\(k\\)-nearest neighbor graph in order to perform a clustering on the graph. We will also show how to perform hierarchical clustering and k-means clustering on the selected space.\nLet’s first load all necessary libraries and also the integrated dataset from the previous step.\nsuppressPackageStartupMessages({\n    library(scater)\n    library(scran)\n    library(patchwork)\n    library(ggplot2)\n    library(pheatmap)\n    library(igraph)\n    library(clustree)\n    library(bluster)\n})\n# download pre-computed data if missing or long compute\nfetch_data &lt;- TRUE\n\n# url for source and intermediate data\npath_data &lt;- \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\npath_file &lt;- \"data/covid/results/bioc_covid_qc_dr_int.rds\"\nif (!dir.exists(dirname(path_file))) dir.create(dirname(path_file), recursive = TRUE)\nif (fetch_data && !file.exists(path_file)) download.file(url = file.path(path_data, \"covid/results/bioc_covid_qc_dr_int.rds\"), destfile = path_file)\nsce &lt;- readRDS(path_file)\nprint(reducedDims(sce))\n\nList of length 15\nnames(15): PCA UMAP tSNE_on_PCA ... UMAP_on_Harmony Scanorama UMAP_on_Scanorama"
  },
  {
    "objectID": "labs/bioc/bioc_04_clustering.html#meta-clust_graphclust",
    "href": "labs/bioc/bioc_04_clustering.html#meta-clust_graphclust",
    "title": " Clustering",
    "section": "1 Graph clustering",
    "text": "1 Graph clustering\nThe procedure of clustering on a Graph can be generalized as 3 main steps:\n- Build a kNN graph from the data.\n- Prune spurious connections from kNN graph (optional step). This is a SNN graph.\n- Find groups of cells that maximizes the connections within the group compared other groups.\n\n1.1 Building kNN / SNN graph\nThe first step into graph clustering is to construct a k-nn graph, in case you don’t have one. For this, we will use the PCA space. Thus, as done for dimensionality reduction, we will use ony the top N PCA dimensions for this purpose (the same used for computing UMAP / tSNE).\n\n# These 2 lines are for demonstration purposes only\ng &lt;- buildKNNGraph(sce, k = 30, use.dimred = \"harmony\")\nreducedDim(sce, \"KNN\") &lt;- igraph::as_adjacency_matrix(g)\n\n# These 2 lines are the most recommended, it first run the KNN graph construction and then creates the SNN graph.\ng &lt;- buildSNNGraph(sce, k = 30, use.dimred = \"harmony\")\nreducedDim(sce, \"SNN\") &lt;- as_adjacency_matrix(g, attr = \"weight\")\n\nWe can take a look at the kNN and SNN graphs. The kNN graph is a matrix where every connection between cells is represented as \\(1\\)s. This is called a unweighted graph (default in Seurat). In the SNN graph on the other hand, some cell connections have more importance than others, and the graph scales from \\(0\\) to a maximum distance (in this case \\(1\\)). Usually, the smaller the distance, the closer two points are, and stronger is their connection. This is called a weighted graph. Both weighted and unweighted graphs are suitable for clustering, but clustering on unweighted graphs is faster for large datasets (&gt; 100k cells).\n\n# plot the KNN graph\npheatmap(reducedDim(sce, \"KNN\")[1:200, 1:200],\n    col = c(\"white\", \"black\"), border_color = \"grey90\",\n    legend = F, cluster_rows = F, cluster_cols = F, fontsize = 2\n)\n\n\n\n\n\n\n\n# or the SNN graph\npheatmap(reducedDim(sce, \"SNN\")[1:200, 1:200],\n    col = colorRampPalette(c(\"white\", \"yellow\", \"red\", \"black\"))(20),\n    border_color = \"grey90\",\n    legend = T, cluster_rows = F, cluster_cols = F, fontsize = 2\n)\n\n\n\n\n\n\n\n\nAs you can see, the way Scran computes the SNN graph is different to Seurat. It gives edges to all cells that shares a neighbor, but weights the edges by how similar the neighbors are. Hence, the SNN graph has more edges than the KNN graph.\nThere are 3 different options how to define the SNN these are:\n\nrank- scoring based on shared close neighbors, i.e. ranking the neighbors of two cells and comparing the ranks.\nnumber - number of shared neighbors\njaccard - calculate Jaccard similarity, same as in Seurat.\n\n\n\n1.2 Clustering on a graph\nOnce the graph is built, we can now perform graph clustering. The clustering is done respective to a resolution which can be interpreted as how coarse you want your cluster to be. Higher resolution means higher number of clusters.\nFor clustering we can use the function clusterCells() which actually runs the steps of building the KNN and SNN graph for us, and also does the graph partition. All the clustering builds on the bluster package and we specify the different options using the NNGraphParam() class.\nSome parameters to consider are:\n\nshared, can be TRUE/FALSE - construct SNN graph (TRUE) or cluster on the KNN graph (FALSE)\ntype - for SNN graph method, can be rank, number or jaccard\nk - number of neighbors in the KNN construction. Can be any function implemented in ighraph\ncluster.fun - which community detection method.\ncluster.args - paramters to the different clustering functions\n\nSo to find out what the different options are for the different methods you would have to check the documentation in the igraph package, e.g. ?igraph::cluster_leiden.\nHere we will use the integration with Harmony to build the graph, and the umap built on Harmony for visualization.\nOBS! There is no method to select fewer than the total 50 components in the embedding for creating the graph, so here we create a new reducedDim instance with only 20 components.\n\nreducedDims(sce)$harmony2 = reducedDims(sce)$harmony[,1:20]\n\nsce$louvain_k30 &lt;- clusterCells(sce, use.dimred = \"harmony2\", BLUSPARAM=SNNGraphParam(k=30, cluster.fun=\"louvain\",  cluster.args = list(resolution=0.5)))\nsce$louvain_k20 &lt;- clusterCells(sce, use.dimred = \"harmony2\", BLUSPARAM=SNNGraphParam(k=20, cluster.fun=\"louvain\",  cluster.args = list(resolution=0.5)))\nsce$louvain_k10 &lt;- clusterCells(sce, use.dimred = \"harmony2\", BLUSPARAM=SNNGraphParam(k=10, cluster.fun=\"louvain\",  cluster.args = list(resolution=0.5)))\n\nsce$leiden_k30 &lt;- clusterCells(sce, use.dimred = \"harmony2\", BLUSPARAM=SNNGraphParam(k=30, cluster.fun=\"leiden\",  cluster.args = list(resolution_parameter=0.3)))\nsce$leiden_k20 &lt;- clusterCells(sce, use.dimred = \"harmony2\", BLUSPARAM=SNNGraphParam(k=20, cluster.fun=\"leiden\",  cluster.args = list(resolution_parameter=0.3)))\nsce$leiden_k10 &lt;- clusterCells(sce, use.dimred = \"harmony2\", BLUSPARAM=SNNGraphParam(k=10, cluster.fun=\"leiden\",  cluster.args = list(resolution_parameter=0.3)))\n\n\n\nwrap_plots(\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"louvain_k30\") +\n        ggplot2::ggtitle(label = \"louvain_k30\"),  \n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"louvain_k20\") +\n        ggplot2::ggtitle(label = \"louvain_k20\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"louvain_k10\") +\n        ggplot2::ggtitle(label = \"louvain_k10\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"leiden_k30\") +\n        ggplot2::ggtitle(label = \"leiden_k30\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"leiden_k20\") +\n        ggplot2::ggtitle(label = \"leiden_k20\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"leiden_k10\") +\n        ggplot2::ggtitle(label = \"leiden_k10\"),\n    ncol = 3\n)\n\n\n\n\n\n\n\n\nWe can now use the clustree package to visualize how cells are distributed between clusters depending on resolution.\n\nsuppressPackageStartupMessages(library(clustree))\nclustree(sce, prefix = \"louvain_k\")"
  },
  {
    "objectID": "labs/bioc/bioc_04_clustering.html#meta-clust_kmean",
    "href": "labs/bioc/bioc_04_clustering.html#meta-clust_kmean",
    "title": " Clustering",
    "section": "2 K-means clustering",
    "text": "2 K-means clustering\nK-means is a generic clustering algorithm that has been used in many application areas. In R, it can be applied via the kmeans() function. Typically, it is applied to a reduced dimension representation of the expression data (most often PCA, because of the interpretability of the low-dimensional distances). We need to define the number of clusters in advance. Since the results depend on the initialization of the cluster centers, it is typically recommended to run K-means with multiple starting configurations (via the nstart argument).\n\nsce$kmeans_5 &lt;- clusterCells(sce, use.dimred = \"harmony2\", BLUSPARAM=KmeansParam(centers=5))\nsce$kmeans_10 &lt;- clusterCells(sce, use.dimred = \"harmony2\", BLUSPARAM=KmeansParam(centers=10))  \nsce$kmeans_15 &lt;- clusterCells(sce, use.dimred = \"harmony2\", BLUSPARAM=KmeansParam(centers=15))\nsce$kmeans_20 &lt;- clusterCells(sce, use.dimred = \"harmony2\", BLUSPARAM=KmeansParam(centers=20))\n\n\nwrap_plots(\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"kmeans_5\") +\n        ggplot2::ggtitle(label = \"KMeans5\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"kmeans_10\") +\n        ggplot2::ggtitle(label = \"KMeans10\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"kmeans_15\") +\n        ggplot2::ggtitle(label = \"KMeans15\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"kmeans_15\") +\n        ggplot2::ggtitle(label = \"KMeans20\"),\n    ncol = 2\n)\n\n\n\n\n\n\n\n\n\nclustree(sce, prefix = \"kmeans_\")"
  },
  {
    "objectID": "labs/bioc/bioc_04_clustering.html#meta-clust_hier",
    "href": "labs/bioc/bioc_04_clustering.html#meta-clust_hier",
    "title": " Clustering",
    "section": "3 Hierarchical clustering",
    "text": "3 Hierarchical clustering\nThere is the optioni to run hierarchical clustering in the clusterCells function using HclustParam, but there are limited options to specify distances such as correlations that we show below, so we will run the clustering with our own implementation.\n\n3.1 Defining distance between cells\nThe base R stats package already contains a function dist that calculates distances between all pairs of samples. Since we want to compute distances between samples, rather than among genes, we need to transpose the data before applying it to the dist function. This can be done by simply adding the transpose function t() to the data. The distance methods available in dist are: ‘euclidean’, ‘maximum’, ‘manhattan’, ‘canberra’, ‘binary’ or ‘minkowski’.\n\nd &lt;- dist(reducedDim(sce, \"harmony2\"), method = \"euclidean\")\n\nAs you might have realized, correlation is not a method implemented in the dist() function. However, we can create our own distances and transform them to a distance object. We can first compute sample correlations using the cor function.\nAs you already know, correlation range from -1 to 1, where 1 indicates that two samples are closest, -1 indicates that two samples are the furthest and 0 is somewhat in between. This, however, creates a problem in defining distances because a distance of 0 indicates that two samples are closest, 1 indicates that two samples are the furthest and distance of -1 is not meaningful. We thus need to transform the correlations to a positive scale (a.k.a. adjacency):\n\\[adj = \\frac{1- cor}{2}\\]\nOnce we transformed the correlations to a 0-1 scale, we can simply convert it to a distance object using as.dist() function. The transformation does not need to have a maximum of 1, but it is more intuitive to have it at 1, rather than at any other number.\n\n# Compute sample correlations\nsample_cor &lt;- cor(Matrix::t(reducedDim(sce, \"harmony2\")))\n\n# Transform the scale from correlations\nsample_cor &lt;- (1 - sample_cor) / 2\n\n# Convert it to a distance object\nd2 &lt;- as.dist(sample_cor)\n\n\n\n3.2 Clustering cells\nAfter having calculated the distances between samples, we can now proceed with the hierarchical clustering per-se. We will use the function hclust() for this purpose, in which we can simply run it with the distance objects created above. The methods available are: ‘ward.D’, ‘ward.D2’, ‘single’, ‘complete’, ‘average’, ‘mcquitty’, ‘median’ or ‘centroid’. It is possible to plot the dendrogram for all cells, but this is very time consuming and we will omit for this tutorial.\n\n# euclidean\nh_euclidean &lt;- hclust(d, method = \"ward.D2\")\n\n# correlation\nh_correlation &lt;- hclust(d2, method = \"ward.D2\")\n\nOnce your dendrogram is created, the next step is to define which samples belong to a particular cluster. After identifying the dendrogram, we can now literally cut the tree at a fixed threshold (with cutree) at different levels to define the clusters. We can either define the number of clusters or decide on a height. We can simply try different clustering levels.\n\n# euclidean distance\nsce$hc_euclidean_5 &lt;- factor(cutree(h_euclidean, k = 5))\nsce$hc_euclidean_10 &lt;- factor(cutree(h_euclidean, k = 10))\nsce$hc_euclidean_15 &lt;- factor(cutree(h_euclidean, k = 15))\n\n# correlation distance\nsce$hc_corelation_5 &lt;- factor(cutree(h_correlation, k = 5))\nsce$hc_corelation_10 &lt;- factor(cutree(h_correlation, k = 10))\nsce$hc_corelation_15 &lt;- factor(cutree(h_correlation, k = 15))\n\nwrap_plots(\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"hc_euclidean_5\") +\n        ggplot2::ggtitle(label = \"HC_euclidean_5\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"hc_euclidean_10\") +\n        ggplot2::ggtitle(label = \"HC_euclidean_10\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"hc_euclidean_15\") +\n        ggplot2::ggtitle(label = \"HC_euclidean_15\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"hc_corelation_5\") +\n        ggplot2::ggtitle(label = \"HC_correlation_5\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"hc_corelation_10\") +\n        ggplot2::ggtitle(label = \"HC_correlation_10\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"hc_corelation_15\") +\n        ggplot2::ggtitle(label = \"HC_correlation_15\"),\n    ncol = 3\n)\n\n\n\n\n\n\n\n\nFinally, lets save the clustered data for further analysis.\n\nsaveRDS(sce, \"data/covid/results/bioc_covid_qc_dr_int_cl.rds\")"
  },
  {
    "objectID": "labs/bioc/bioc_04_clustering.html#meta-clust_distribution",
    "href": "labs/bioc/bioc_04_clustering.html#meta-clust_distribution",
    "title": " Clustering",
    "section": "4 Distribution of clusters",
    "text": "4 Distribution of clusters\nNow, we can select one of our clustering methods and compare the proportion of samples across the clusters.\n\np1 &lt;- ggplot(as.data.frame(colData(sce)), aes(x = leiden_k20, fill = sample)) +\n    geom_bar(position = \"fill\")\np2 &lt;- ggplot(as.data.frame(colData(sce)), aes(x = leiden_k20, fill = type)) +\n    geom_bar(position = \"fill\")\n\np1 + p2\n\n\n\n\n\n\n\n\nIn this case we have quite good representation of each sample in each cluster. But there are clearly some biases with more cells from one sample in some clusters and also more covid cells in some of the clusters.\nWe can also plot it in the other direction, the proportion of each cluster per sample.\n\nggplot(as.data.frame(colData(sce)), aes(x = sample, fill = leiden_k20)) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nBy now you should know how to plot different features onto your data. Take the QC metrics that were calculated in the first exercise, that should be stored in your data object, and plot it as violin plots per cluster using the clustering method of your choice. For example, plot number of UMIS, detected genes, percent mitochondrial reads. Then, check carefully if there is any bias in how your data is separated by quality metrics. Could it be explained biologically, or could there be a technical bias there?\n\n\n\nwrap_plots(\n    plotColData(sce, y = \"detected\", x = \"leiden_k20\", colour_by = \"leiden_k20\"),\n    plotColData(sce, y = \"total\", x = \"leiden_k20\", colour_by = \"leiden_k20\"),\n    plotColData(sce, y = \"subsets_mt_percent\", x = \"leiden_k20\", colour_by = \"leiden_k20\"),\n    plotColData(sce, y = \"subsets_ribo_percent\", x = \"leiden_k20\", colour_by = \"leiden_k20\"),\n    plotColData(sce, y = \"subsets_hb_percent\", x = \"leiden_k20\", colour_by = \"leiden_k20\"),\n    ncol = 3\n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nSome clusters that are clearly defined by higher number of genes and counts. These are either doublets or a larger celltype. And some clusters with low values on these metrics that are either low quality cells or a smaller celltype. You will have to explore these clusters in more detail to judge what you believe them to be."
  },
  {
    "objectID": "labs/bioc/bioc_04_clustering.html#meta-session",
    "href": "labs/bioc/bioc_04_clustering.html#meta-session",
    "title": " Clustering",
    "section": "6 Session info",
    "text": "6 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] harmony_1.2.1               Rcpp_1.0.13                \n [3] bluster_1.12.0              clustree_0.5.1             \n [5] ggraph_2.2.1                igraph_2.0.3               \n [7] pheatmap_1.0.12             patchwork_1.2.0            \n [9] scran_1.30.0                scater_1.30.1              \n[11] ggplot2_3.5.1               scuttle_1.12.0             \n[13] SingleCellExperiment_1.24.0 SummarizedExperiment_1.32.0\n[15] Biobase_2.62.0              GenomicRanges_1.54.1       \n[17] GenomeInfoDb_1.38.1         IRanges_2.36.0             \n[19] S4Vectors_0.40.2            BiocGenerics_0.48.1        \n[21] MatrixGenerics_1.14.0       matrixStats_1.4.1          \n\nloaded via a namespace (and not attached):\n [1] bitops_1.0-8              gridExtra_2.3            \n [3] rlang_1.1.4               magrittr_2.0.3           \n [5] compiler_4.3.3            DelayedMatrixStats_1.24.0\n [7] vctrs_0.6.5               pkgconfig_2.0.3          \n [9] crayon_1.5.3              fastmap_1.2.0            \n[11] backports_1.5.0           XVector_0.42.0           \n[13] labeling_0.4.3            utf8_1.2.4               \n[15] rmarkdown_2.28            ggbeeswarm_0.7.2         \n[17] purrr_1.0.2               xfun_0.47                \n[19] cachem_1.1.0              zlibbioc_1.48.0          \n[21] beachmat_2.18.0           jsonlite_1.8.8           \n[23] DelayedArray_0.28.0       BiocParallel_1.36.0      \n[25] tweenr_2.0.3              irlba_2.3.5.1            \n[27] parallel_4.3.3            cluster_2.1.6            \n[29] R6_2.5.1                  RColorBrewer_1.1-3       \n[31] limma_3.58.1              knitr_1.48               \n[33] FNN_1.1.4                 Matrix_1.6-5             \n[35] tidyselect_1.2.1          abind_1.4-5              \n[37] yaml_2.3.10               viridis_0.6.5            \n[39] codetools_0.2-20          lattice_0.22-6           \n[41] tibble_3.2.1              withr_3.0.1              \n[43] evaluate_0.24.0           polyclip_1.10-7          \n[45] pillar_1.9.0              checkmate_2.3.2          \n[47] generics_0.1.3            RCurl_1.98-1.16          \n[49] sparseMatrixStats_1.14.0  munsell_0.5.1            \n[51] scales_1.3.0              RhpcBLASctl_0.23-42      \n[53] glue_1.7.0                metapod_1.10.0           \n[55] tools_4.3.3               BiocNeighbors_1.20.0     \n[57] ScaledMatrix_1.10.0       locfit_1.5-9.9           \n[59] graphlayouts_1.1.1        cowplot_1.1.3            \n[61] tidygraph_1.3.0           grid_4.3.3               \n[63] tidyr_1.3.1               edgeR_4.0.16             \n[65] colorspace_2.1-1          GenomeInfoDbData_1.2.11  \n[67] beeswarm_0.4.0            BiocSingular_1.18.0      \n[69] ggforce_0.4.2             vipor_0.4.7              \n[71] cli_3.6.3                 rsvd_1.0.5               \n[73] fansi_1.0.6               S4Arrays_1.2.0           \n[75] viridisLite_0.4.2         dplyr_1.1.4              \n[77] uwot_0.1.16               gtable_0.3.5             \n[79] digest_0.6.37             SparseArray_1.2.2        \n[81] ggrepel_0.9.6             dqrng_0.3.2              \n[83] htmlwidgets_1.6.4         farver_2.1.2             \n[85] memoise_2.0.1             htmltools_0.5.8.1        \n[87] lifecycle_1.0.4           statmod_1.5.0            \n[89] MASS_7.3-60.0.1"
  },
  {
    "objectID": "labs/bioc/bioc_05_dge.html",
    "href": "labs/bioc/bioc_05_dge.html",
    "title": " Differential gene expression",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified.\nIn this tutorial we will cover differential gene expression, which comprises an extensive range of topics and methods. In single cell, differential expresison can have multiple functionalities such as identifying marker genes for cell populations, as well as identifying differentially regulated genes across conditions (healthy vs control). We will also cover controlling batch effect in your test.\nWe can first load the data from the clustering session. Moreover, we can already decide which clustering resolution to use. First let’s define using the louvain clustering to identifying differentially expressed genes.\nsuppressPackageStartupMessages({\n    library(scater)\n    library(scran)\n    # library(venn)\n    library(patchwork)\n    library(ggplot2)\n    library(pheatmap)\n    library(igraph)\n    library(dplyr)\n})\n# download pre-computed data if missing or long compute\nfetch_data &lt;- TRUE\n\n# url for source and intermediate data\npath_data &lt;- \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\npath_file &lt;- \"data/covid/results/bioc_covid_qc_dr_int_cl.rds\"\nif (!dir.exists(dirname(path_file))) dir.create(dirname(path_file), recursive = TRUE)\nif (fetch_data && !file.exists(path_file)) download.file(url = file.path(path_data, \"covid/results/bioc_covid_qc_dr_int_cl.rds\"), destfile = path_file)\nsce &lt;- readRDS(path_file)\nprint(reducedDims(sce))\n\nList of length 17\nnames(17): PCA UMAP tSNE_on_PCA UMAP_on_PCA ... UMAP_on_Scanorama SNN harmony2"
  },
  {
    "objectID": "labs/bioc/bioc_05_dge.html#meta-dge_cmg",
    "href": "labs/bioc/bioc_05_dge.html#meta-dge_cmg",
    "title": " Differential gene expression",
    "section": "2 Cell marker genes",
    "text": "2 Cell marker genes\nLet us first compute a ranking for the highly differential genes in each cluster. There are many different tests and parameters to be chosen that can be used to refine your results. When looking for marker genes, we want genes that are positively expressed in a cell type and possibly not expressed in others.\nIn the scran function findMarkers t-test, wilcoxon test and binomial test implemented.\n\n# Compute differentiall expression\nmarkers_genes &lt;- scran::findMarkers(\n    x = sce,\n    groups = as.character(sce$leiden_k20),\n    test.type = \"wilcox\",\n    lfc = .5,\n    pval.type = \"all\",\n    direction = \"up\"\n)\n\n# List of dataFrames with the results for each cluster\nmarkers_genes\n\nList of length 13\nnames(13): 1 10 11 12 13 2 3 4 5 6 7 8 9\n\n# Visualizing the expression of one\nhead(markers_genes[[\"1\"]])\n\nDataFrame with 6 rows and 15 columns\n          p.value       FDR summary.AUC    AUC.10    AUC.11    AUC.12    AUC.13\n        &lt;numeric&gt; &lt;numeric&gt;   &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\nS100A8          1         1    0.282842  0.959148  0.942246  0.977685  0.980941\nIFI6            1         1    0.262378  0.499180  0.364114  0.443247  0.467511\nRETN            1         1    0.304136  0.411009  0.383941  0.415450  0.410955\nALOX5AP         1         1    0.303393  0.404740  0.453944  0.287193  0.335212\nS100A12         1         1    0.245282  0.870458  0.865589  0.876760  0.871185\nISG15           1         1    0.289798  0.433757  0.299824  0.380235  0.353287\n            AUC.2     AUC.3     AUC.4     AUC.5     AUC.6     AUC.7     AUC.8\n        &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\nS100A8   0.982921  0.282842  0.985213  0.983036  0.985045  0.854209  0.984766\nIFI6     0.507736  0.262378  0.504875  0.457423  0.462969  0.549265  0.403180\nRETN     0.416034  0.262345  0.415249  0.415516  0.416258  0.400304  0.416099\nALOX5AP  0.387884  0.259607  0.362574  0.336117  0.303393  0.441705  0.250192\nS100A12  0.875237  0.245282  0.878158  0.875534  0.877325  0.839671  0.874472\nISG15    0.398547  0.273513  0.429643  0.389656  0.370755  0.480924  0.339772\n            AUC.9\n        &lt;numeric&gt;\nS100A8   0.414321\nIFI6     0.316388\nRETN     0.304136\nALOX5AP  0.312316\nS100A12  0.381497\nISG15    0.289798\n\n\nWe can now select the top 25 overexpressed genes for plotting.\n\n# Colect the top 25 genes for each cluster and put the into a single table\ntop25 &lt;- lapply(names(markers_genes), function(x) {\n    temp &lt;- markers_genes[[x]][1:25, 1:2]\n    temp$gene &lt;- rownames(markers_genes[[x]])[1:25]\n    temp$cluster &lt;- x\n    return(temp)\n})\ntop25 &lt;- as_tibble(do.call(rbind, top25))\ntop25$p.value[top25$p.value == 0] &lt;- 1e-300\ntop25\n\n\n\n  \n\n\n\nWe can plot them as barplots per cluster.\n\npar(mfrow = c(1, 5), mar = c(4, 6, 3, 1))\nfor (i in unique(top25$cluster)) {\n    barplot(sort(setNames(-log10(top25$p.value), top25$gene)[top25$cluster == i], F),\n        horiz = T, las = 1, main = paste0(i, \" vs. rest\"), border = \"white\", yaxs = \"i\", xlab = \"-log10FC\"\n    )\n    abline(v = c(0, -log10(0.05)), lty = c(1, 2))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can visualize them as a heatmap. Here we are selecting the top 5.\n\ntop25 %&gt;%\n    group_by(cluster) %&gt;%\n    slice_min(p.value, n = 5, with_ties = FALSE) -&gt; top5\n\n\nscater::plotHeatmap(sce[, order(sce$leiden_k20)],\n    features = unique(top5$gene),\n    center = T, zlim = c(-3, 3),\n    colour_columns_by = \"leiden_k20\",\n    show_colnames = F, cluster_cols = F,\n    fontsize_row = 6,\n    color = colorRampPalette(c(\"purple\", \"black\", \"yellow\"))(90)\n)\n\n\n\n\n\n\n\n\nWe can also plot a violin plot for each gene.\n\nscater::plotExpression(sce, features = unique(top5$gene), x = \"leiden_k20\", ncol = 5, colour_by = \"leiden_k20\", scales = \"free\")\n\n\n\n\n\n\n\n\nAnother way is by representing the overall group expression and detection rates in a dot-plot.\n\nplotDots(sce, features = unique(top5$gene), group=\"leiden_k20\")"
  },
  {
    "objectID": "labs/bioc/bioc_05_dge.html#meta-dge_cond",
    "href": "labs/bioc/bioc_05_dge.html#meta-dge_cond",
    "title": " Differential gene expression",
    "section": "3 DGE across conditions",
    "text": "3 DGE across conditions\nThe second way of computing differential expression is to answer which genes are differentially expressed within a cluster. For example, in our case we have libraries comming from patients and controls and we would like to know which genes are influenced the most in a particular cell type. For this end, we will first subset our data for the desired cell cluster, then change the cell identities to the variable of comparison (which now in our case is the type, e.g. Covid/Ctrl).\n\n# Filter cells from that cluster\ncell_selection &lt;- sce[, sce$leiden_k20 == 2]\n\n# Compute differentiall expression\nDGE_cell_selection &lt;- findMarkers(\n    x = cell_selection,\n    groups = cell_selection@colData$type,\n    lfc = .25,\n    pval.type = \"all\",\n    direction = \"any\"\n)\ntop5_cell_selection &lt;- lapply(names(DGE_cell_selection), function(x) {\n    temp &lt;- DGE_cell_selection[[x]][1:5, 1:2]\n    temp$gene &lt;- rownames(DGE_cell_selection[[x]])[1:5]\n    temp$cluster &lt;- x\n    return(temp)\n})\ntop5_cell_selection &lt;- as_tibble(do.call(rbind, top5_cell_selection))\ntop5_cell_selection\n\n\n\n  \n\n\n\nWe can now plot the expression across the type.\n\nscater::plotExpression(cell_selection, features = unique(top5_cell_selection$gene), x = \"type\", ncol = 5, colour_by = \"type\")\n\n\n\n\n\n\n\n\nOr we can plot them as dotplots to see the expression in each individual sample.\n\nplotDots(cell_selection, unique(top5_cell_selection$gene), group=\"sample\")\n\n\n\n\n\n\n\n\nClearly many of the top Covid genes are only high in the covid_17 sample, and not a general feature of covid patients. In this case using a method that can control for pseudreplication or pseudobulk methods would be appropriate. We have a more thorough discussion on this issue with sample batch effects in differential expression across conditions in the Seurat tutorial."
  },
  {
    "objectID": "labs/bioc/bioc_05_dge.html#meta-dge_gsa",
    "href": "labs/bioc/bioc_05_dge.html#meta-dge_gsa",
    "title": " Differential gene expression",
    "section": "4 Gene Set Analysis (GSA)",
    "text": "4 Gene Set Analysis (GSA)\n\n4.1 Hypergeometric enrichment test\nHaving a defined list of differentially expressed genes, you can now look for their combined function using hypergeometric test.\n\n# Load additional packages\nlibrary(enrichR)\n\n# Check available databases to perform enrichment (then choose one)\nenrichR::listEnrichrDbs()\n\n\n\n  \n\n\n# Perform enrichment\ntop_DGE &lt;- DGE_cell_selection$Covid[(DGE_cell_selection$Covid$p.value &lt; 0.01) & (abs(DGE_cell_selection$Covid[, grep(\"logFC.C\", colnames(DGE_cell_selection$Covid))]) &gt; 0.25), ]\n\nenrich_results &lt;- enrichr(\n    genes     = rownames(top_DGE),\n    databases = \"GO_Biological_Process_2017b\"\n)[[1]]\n\nUploading data to Enrichr... Done.\n  Querying GO_Biological_Process_2017b... Done.\nParsing results... Done.\n\n\nSome databases of interest:\nGO_Biological_Process_2017bKEGG_2019_HumanKEGG_2019_MouseWikiPathways_2019_HumanWikiPathways_2019_Mouse\nYou visualize your results using a simple barplot, for example:\n\n{\n    par(mfrow = c(1, 1), mar = c(3, 25, 2, 1))\n    barplot(\n        height = -log10(enrich_results$P.value)[10:1],\n        names.arg = enrich_results$Term[10:1],\n        horiz = TRUE,\n        las = 1,\n        border = FALSE,\n        cex.names = .6\n    )\n    abline(v = c(-log10(0.05)), lty = 2)\n    abline(v = 0, lty = 1)\n}"
  },
  {
    "objectID": "labs/bioc/bioc_05_dge.html#meta-dge_gsea",
    "href": "labs/bioc/bioc_05_dge.html#meta-dge_gsea",
    "title": " Differential gene expression",
    "section": "5 Gene Set Enrichment Analysis (GSEA)",
    "text": "5 Gene Set Enrichment Analysis (GSEA)\nBesides the enrichment using hypergeometric test, we can also perform gene set enrichment analysis (GSEA), which scores ranked genes list (usually based on fold changes) and computes permutation test to check if a particular gene set is more present in the Up-regulated genes, among the DOWN_regulated genes or not differentially regulated.\n\n# Create a gene rank based on the gene expression fold change\ngene_rank &lt;- setNames(DGE_cell_selection$Covid[, grep(\"logFC.C\", colnames(DGE_cell_selection$Covid))], casefold(rownames(DGE_cell_selection$Covid), upper = T))\n\nOnce our list of genes are sorted, we can proceed with the enrichment itself. We can use the package to get gene set from the Molecular Signature Database (MSigDB) and select KEGG pathways as an example.\n\nlibrary(msigdbr)\n\n# Download gene sets\nmsigdbgmt &lt;- msigdbr::msigdbr(\"Homo sapiens\")\nmsigdbgmt &lt;- as.data.frame(msigdbgmt)\n\n# List available gene sets\nunique(msigdbgmt$gs_subcat)\n\n [1] \"MIR:MIR_Legacy\"  \"TFT:TFT_Legacy\"  \"CGP\"             \"TFT:GTRD\"       \n [5] \"\"                \"VAX\"             \"CP:BIOCARTA\"     \"CGN\"            \n [9] \"GO:BP\"           \"GO:CC\"           \"IMMUNESIGDB\"     \"GO:MF\"          \n[13] \"HPO\"             \"CP:KEGG\"         \"MIR:MIRDB\"       \"CM\"             \n[17] \"CP\"              \"CP:PID\"          \"CP:REACTOME\"     \"CP:WIKIPATHWAYS\"\n\n# Subset which gene set you want to use.\nmsigdbgmt_subset &lt;- msigdbgmt[msigdbgmt$gs_subcat == \"CP:WIKIPATHWAYS\", ]\ngmt &lt;- lapply(unique(msigdbgmt_subset$gs_name), function(x) {\n    msigdbgmt_subset[msigdbgmt_subset$gs_name == x, \"gene_symbol\"]\n})\nnames(gmt) &lt;- unique(paste0(msigdbgmt_subset$gs_name, \"_\", msigdbgmt_subset$gs_exact_source))\n\nNext, we will run GSEA. This will result in a table containing information for several pathways. We can then sort and filter those pathways to visualize only the top ones. You can select/filter them by either p-value or normalized enrichment score (NES).\n\nlibrary(fgsea)\n\n# Perform enrichemnt analysis\nfgseaRes &lt;- fgsea(pathways = gmt, stats = gene_rank, minSize = 15, maxSize = 500, nperm = 10000)\nfgseaRes &lt;- fgseaRes[order(fgseaRes$NES, decreasing = T), ]\n\n# Filter the results table to show only the top 10 UP or DOWN regulated processes (optional)\ntop10_UP &lt;- fgseaRes$pathway[1:10]\n\n# Nice summary table (shown as a plot)\nplotGseaTable(gmt[top10_UP], gene_rank, fgseaRes, gseaParam = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nWhich KEGG pathways are upregulated in this cluster? Which KEGG pathways are dowregulated in this cluster? Change the pathway source to another gene set (e.g. CP:WIKIPATHWAYS or CP:REACTOME or CP:BIOCARTA or GO:BP) and check the if you get similar results?\n\n\nFinally, let’s save the integrated data for further analysis.\n\nsaveRDS(sce, \"data/covid/results/bioc_covid_qc_dr_int_cl_dge.rds\")"
  },
  {
    "objectID": "labs/bioc/bioc_05_dge.html#meta-session",
    "href": "labs/bioc/bioc_05_dge.html#meta-session",
    "title": " Differential gene expression",
    "section": "6 Session info",
    "text": "6 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] fgsea_1.28.0                msigdbr_7.5.1              \n [3] enrichR_3.2                 dplyr_1.1.4                \n [5] igraph_2.0.3                pheatmap_1.0.12            \n [7] patchwork_1.2.0             scran_1.30.0               \n [9] scater_1.30.1               ggplot2_3.5.1              \n[11] scuttle_1.12.0              SingleCellExperiment_1.24.0\n[13] SummarizedExperiment_1.32.0 Biobase_2.62.0             \n[15] GenomicRanges_1.54.1        GenomeInfoDb_1.38.1        \n[17] IRanges_2.36.0              S4Vectors_0.40.2           \n[19] BiocGenerics_0.48.1         MatrixGenerics_1.14.0      \n[21] matrixStats_1.4.1          \n\nloaded via a namespace (and not attached):\n [1] bitops_1.0-8              gridExtra_2.3            \n [3] rlang_1.1.4               magrittr_2.0.3           \n [5] compiler_4.3.3            DelayedMatrixStats_1.24.0\n [7] vctrs_0.6.5               pkgconfig_2.0.3          \n [9] crayon_1.5.3              fastmap_1.2.0            \n[11] XVector_0.42.0            labeling_0.4.3           \n[13] utf8_1.2.4                rmarkdown_2.28           \n[15] ggbeeswarm_0.7.2          xfun_0.47                \n[17] bluster_1.12.0            WriteXLS_6.7.0           \n[19] zlibbioc_1.48.0           beachmat_2.18.0          \n[21] jsonlite_1.8.8            DelayedArray_0.28.0      \n[23] BiocParallel_1.36.0       irlba_2.3.5.1            \n[25] parallel_4.3.3            cluster_2.1.6            \n[27] R6_2.5.1                  RColorBrewer_1.1-3       \n[29] limma_3.58.1              Rcpp_1.0.13              \n[31] knitr_1.48                Matrix_1.6-5             \n[33] tidyselect_1.2.1          abind_1.4-5              \n[35] yaml_2.3.10               viridis_0.6.5            \n[37] codetools_0.2-20          curl_5.2.1               \n[39] lattice_0.22-6            tibble_3.2.1             \n[41] withr_3.0.1               evaluate_0.24.0          \n[43] pillar_1.9.0              generics_0.1.3           \n[45] RCurl_1.98-1.16           sparseMatrixStats_1.14.0 \n[47] munsell_0.5.1             scales_1.3.0             \n[49] glue_1.7.0                metapod_1.10.0           \n[51] tools_4.3.3               data.table_1.15.4        \n[53] BiocNeighbors_1.20.0      ScaledMatrix_1.10.0      \n[55] locfit_1.5-9.9            babelgene_22.9           \n[57] fastmatch_1.1-4           cowplot_1.1.3            \n[59] grid_4.3.3                edgeR_4.0.16             \n[61] colorspace_2.1-1          GenomeInfoDbData_1.2.11  \n[63] beeswarm_0.4.0            BiocSingular_1.18.0      \n[65] vipor_0.4.7               cli_3.6.3                \n[67] rsvd_1.0.5                fansi_1.0.6              \n[69] S4Arrays_1.2.0            viridisLite_0.4.2        \n[71] gtable_0.3.5              digest_0.6.37            \n[73] SparseArray_1.2.2         ggrepel_0.9.6            \n[75] dqrng_0.3.2               rjson_0.2.21             \n[77] htmlwidgets_1.6.4         farver_2.1.2             \n[79] htmltools_0.5.8.1         lifecycle_1.0.4          \n[81] httr_1.4.7                statmod_1.5.0"
  },
  {
    "objectID": "labs/bioc/bioc_06_celltyping.html",
    "href": "labs/bioc/bioc_06_celltyping.html",
    "title": " Celltype prediction",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified.\nCelltype prediction can either be performed on indiviudal cells where each cell gets a predicted celltype label, or on the level of clusters. All methods are based on similarity to other datasets, single cell or sorted bulk RNAseq, or uses known marker genes for each cell type.\nIdeally celltype predictions should be run on each sample separately and not using the integrated data. In this case we will select one sample from the Covid data, ctrl_13 and predict celltype by cell on that sample.\nSome methods will predict a celltype to each cell based on what it is most similar to, even if that celltype is not included in the reference. Other methods include an uncertainty so that cells with low similarity scores will be unclassified.\nThere are multiple different methods to predict celltypes, here we will just cover a few of those.\nWe will use a reference PBMC dataset from the scPred package which is provided as a Seurat object with counts. And we will test classification based on the scPred and scMap methods. Finally we will use gene set enrichment predict celltype based on the DEGs of each cluster."
  },
  {
    "objectID": "labs/bioc/bioc_06_celltyping.html#meta-ct_read",
    "href": "labs/bioc/bioc_06_celltyping.html#meta-ct_read",
    "title": " Celltype prediction",
    "section": "1 Read data",
    "text": "1 Read data\nFirst, lets load required libraries\n\nsuppressPackageStartupMessages({\n    library(scater)\n    library(scran)\n    library(dplyr)\n    library(patchwork)\n    library(ggplot2)\n    library(pheatmap)\n    library(scPred)\n    library(scmap)\n    library(SingleR)\n})\n\nLet’s read in the saved Covid-19 data object from the clustering step.\n\n# download pre-computed data if missing or long compute\nfetch_data &lt;- TRUE\n\n# url for source and intermediate data\npath_data &lt;- \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\npath_file &lt;- \"data/covid/results/bioc_covid_qc_dr_int_cl.rds\"\nif (!dir.exists(dirname(path_file))) dir.create(dirname(path_file), recursive = TRUE)\nif (fetch_data && !file.exists(path_file)) download.file(url = file.path(path_data, \"covid/results/bioc_covid_qc_dr_int_cl.rds\"), destfile = path_file)\nalldata &lt;- readRDS(path_file)\n\nLet’s read in the saved Covid-19 data object from the clustering step.\n\nctrl.sce &lt;- alldata[, alldata$sample == \"ctrl.13\"]\n\n# remove all old dimensionality reductions as they will mess up the analysis further down\nreducedDims(ctrl.sce) &lt;- NULL"
  },
  {
    "objectID": "labs/bioc/bioc_06_celltyping.html#meta-ct_ref",
    "href": "labs/bioc/bioc_06_celltyping.html#meta-ct_ref",
    "title": " Celltype prediction",
    "section": "2 Reference data",
    "text": "2 Reference data\nLoad the reference dataset with annotated labels that is provided by the scPred package, it is a subsampled set of cells from human PBMCs.\n\nreference &lt;- scPred::pbmc_1\nreference\n\nAn object of class Seurat \n32838 features across 3500 samples within 1 assay \nActive assay: RNA (32838 features, 0 variable features)\n 2 layers present: counts, data\n\n\nConvert to a SCE object.\n\nref.sce &lt;- Seurat::as.SingleCellExperiment(reference)\n\nRerun analysis pipeline. Run normalization, feature selection and dimensionality reduction\n\n# Normalize\nref.sce &lt;- computeSumFactors(ref.sce)\nref.sce &lt;- logNormCounts(ref.sce)\n\n# Variable genes\nvar.out &lt;- modelGeneVar(ref.sce, method = \"loess\")\nhvg.ref &lt;- getTopHVGs(var.out, n = 1000)\n\n# Dim reduction\nref.sce &lt;- runPCA(ref.sce,\n    exprs_values = \"logcounts\", scale = T,\n    ncomponents = 30, subset_row = hvg.ref\n)\nref.sce &lt;- runUMAP(ref.sce, dimred = \"PCA\")\n\n\nplotReducedDim(ref.sce, dimred = \"UMAP\", colour_by = \"cell_type\")\n\n\n\n\n\n\n\n\nRun all steps of the analysis for the ctrl sample as well. Use the clustering from the integration lab with resolution 0.5.\n\n# Normalize\nctrl.sce &lt;- computeSumFactors(ctrl.sce)\nctrl.sce &lt;- logNormCounts(ctrl.sce)\n\n# Variable genes\nvar.out &lt;- modelGeneVar(ctrl.sce, method = \"loess\")\nhvg.ctrl &lt;- getTopHVGs(var.out, n = 1000)\n\n# Dim reduction\nctrl.sce &lt;- runPCA(ctrl.sce, exprs_values = \"logcounts\", scale = T, ncomponents = 30, subset_row = hvg.ctrl)\nctrl.sce &lt;- runUMAP(ctrl.sce, dimred = \"PCA\")\n\n\nplotReducedDim(ctrl.sce, dimred = \"UMAP\", colour_by = \"leiden_k20\")"
  },
  {
    "objectID": "labs/bioc/bioc_06_celltyping.html#scmap",
    "href": "labs/bioc/bioc_06_celltyping.html#scmap",
    "title": " Celltype prediction",
    "section": "3 scMap",
    "text": "3 scMap\nThe scMap package is one method for projecting cells from a scRNA-seq experiment on to the cell-types or individual cells identified in a different experiment. It can be run on different levels, either projecting by cluster or by single cell, here we will try out both.\nFor scmap cell type labels must be stored in the cell_type1 column of the colData slots, and gene ids that are consistent across both datasets must be stored in the feature_symbol column of the rowData slots.\n\n3.1 scMap cluster\n\n# add in slot cell_type1\nref.sce$cell_type1 &lt;- ref.sce$cell_type\n# create a rowData slot with feature_symbol\nrd &lt;- data.frame(feature_symbol = rownames(ref.sce))\nrownames(rd) &lt;- rownames(ref.sce)\nrowData(ref.sce) &lt;- rd\n\n# same for the ctrl dataset\n# create a rowData slot with feature_symbol\nrd &lt;- data.frame(feature_symbol = rownames(ctrl.sce))\nrownames(rd) &lt;- rownames(ctrl.sce)\nrowData(ctrl.sce) &lt;- rd\n\nThen we can select variable features in both datasets.\n\n# select features\ncounts(ctrl.sce) &lt;- as.matrix(counts(ctrl.sce))\nlogcounts(ctrl.sce) &lt;- as.matrix(logcounts(ctrl.sce))\nctrl.sce &lt;- selectFeatures(ctrl.sce, suppress_plot = TRUE)\n\ncounts(ref.sce) &lt;- as.matrix(counts(ref.sce))\nlogcounts(ref.sce) &lt;- as.matrix(logcounts(ref.sce))\nref.sce &lt;- selectFeatures(ref.sce, suppress_plot = TRUE)\n\nThen we need to index the reference dataset by cluster, default is the clusters in cell_type1.\n\nref.sce &lt;- indexCluster(ref.sce)\n\nNow we project the Covid-19 dataset onto that index.\n\nproject_cluster &lt;- scmapCluster(\n    projection = ctrl.sce,\n    index_list = list(\n        ref = metadata(ref.sce)$scmap_cluster_index\n    )\n)\n\n# projected labels\ntable(project_cluster$scmap_cluster_labs)\n\n\n     B cell  CD4 T cell  CD8 T cell         cDC       cMono      ncMono \n         72         109         112          29         201         142 \n    NK cell         pDC Plasma cell  unassigned \n        280           2           1         153 \n\n\nThen add the predictions to metadata and plot UMAP.\n\n# add in predictions\nctrl.sce$scmap_cluster &lt;- project_cluster$scmap_cluster_labs\n\nplotReducedDim(ctrl.sce, dimred = \"UMAP\", colour_by = \"scmap_cluster\")"
  },
  {
    "objectID": "labs/bioc/bioc_06_celltyping.html#scmap-cell",
    "href": "labs/bioc/bioc_06_celltyping.html#scmap-cell",
    "title": " Celltype prediction",
    "section": "4 scMap cell",
    "text": "4 scMap cell\nWe can instead index the refernce data based on each single cell and project our data onto the closest neighbor in that dataset.\n\nref.sce &lt;- indexCell(ref.sce)\n\nAgain we need to index the reference dataset.\n\nproject_cell &lt;- scmapCell(\n    projection = ctrl.sce,\n    index_list = list(\n        ref = metadata(ref.sce)$scmap_cell_index\n    )\n)\n\nWe now get a table with index for the 5 nearest neigbors in the reference dataset for each cell in our dataset. We will select the celltype of the closest neighbor and assign it to the data.\n\ncell_type_pred &lt;- colData(ref.sce)$cell_type1[project_cell$ref[[1]][1, ]]\ntable(cell_type_pred)\n\ncell_type_pred\n     B cell  CD4 T cell  CD8 T cell         cDC       cMono      ncMono \n         96         162         264          30         220         148 \n    NK cell Plasma cell \n        180           1 \n\n\nThen add the predictions to metadata and plot umap.\n\n# add in predictions\nctrl.sce$scmap_cell &lt;- cell_type_pred\n\nplotReducedDim(ctrl.sce, dimred = \"UMAP\", colour_by = \"scmap_cell\")\n\n\n\n\n\n\n\n\nPlot both:\n\nwrap_plots(\n    plotReducedDim(ctrl.sce, dimred = \"UMAP\", colour_by = \"scmap_cluster\"),\n    plotReducedDim(ctrl.sce, dimred = \"UMAP\", colour_by = \"scmap_cell\"),\n    ncol = 2\n)"
  },
  {
    "objectID": "labs/bioc/bioc_06_celltyping.html#meta-ct_scpred",
    "href": "labs/bioc/bioc_06_celltyping.html#meta-ct_scpred",
    "title": " Celltype prediction",
    "section": "5 scPred",
    "text": "5 scPred\nscPred will train a classifier based on all principal components. First, getFeatureSpace() will create a scPred object stored in the @misc slot where it extracts the PCs that best separates the different celltypes. Then trainModel() will do the actual training for each celltype.\nscPred works with Seurat objects, so we will convert both objects to seurat objects. You may see a lot of warnings about renaming things, but as long as you do not see an Error, you should be fine.\n\nsuppressPackageStartupMessages(library(Seurat))\n\nreference &lt;- Seurat::as.Seurat(ref.sce)\nctrl &lt;- Seurat::as.Seurat(ctrl.sce)\n\nThe loadings matrix is lost when converted to Seurat object, and scPred needs that information. So we need to rerun PCA with Seurat and the same hvgs.\n\nVariableFeatures(reference) &lt;- hvg.ref\nreference &lt;- reference %&gt;%\n    ScaleData(verbose = F) %&gt;%\n    RunPCA(verbose = F)\n\nVariableFeatures(ctrl) &lt;- hvg.ctrl\nctrl &lt;- ctrl %&gt;%\n    ScaleData(verbose = F) %&gt;%\n    RunPCA(verbose = F)\n\n\nreference &lt;- getFeatureSpace(reference, \"cell_type\")\n\n●  Extracting feature space for each cell type...\nDONE!\n\nreference &lt;- trainModel(reference)\n\n●  Training models for each cell type...\nDONE!\n\n\nscPred will train a classifier based on all principal components. First, getFeatureSpace() will create a scPred object stored in the @misc slot where it extracts the PCs that best separates the different celltypes. Then trainModel() will do the actual training for each celltype.\n\nget_scpred(reference)\n\n'scPred' object\n✔  Prediction variable = cell_type \n✔  Discriminant features per cell type\n✔  Training model(s)\nSummary\n\n|Cell type   |    n| Features|Method    |   ROC|  Sens|  Spec|\n|:-----------|----:|--------:|:---------|-----:|-----:|-----:|\n|B cell      |  280|       50|svmRadial | 1.000| 1.000| 1.000|\n|CD4 T cell  | 1620|       50|svmRadial | 0.994| 0.972| 0.963|\n|CD8 T cell  |  945|       50|svmRadial | 0.973| 0.859| 0.971|\n|cDC         |   26|       50|svmRadial | 0.994| 0.727| 0.999|\n|cMono       |  212|       50|svmRadial | 1.000| 0.957| 0.997|\n|ncMono      |   79|       50|svmRadial | 1.000| 0.962| 0.999|\n|NK cell     |  312|       50|svmRadial | 0.998| 0.926| 0.995|\n|pDC         |   20|       50|svmRadial | 1.000| 0.950| 1.000|\n|Plasma cell |    6|       50|svmRadial | 1.000| 1.000| 1.000|\n\n\nYou can optimize parameters for each dataset by chaining parameters and testing different types of models, see more at: https://powellgenomicslab.github.io/scPred/articles/introduction.html. But for now, we will continue with this model. Now, let’s predict celltypes on our data, where scPred will align the two datasets with Harmony and then perform classification.\n\nctrl &lt;- scPredict(ctrl, reference)\n\n●  Matching reference with new dataset...\n     ─ 1000 features present in reference loadings\n     ─ 938 features shared between reference and new dataset\n     ─ 93.8% of features in the reference are present in new dataset\n●  Aligning new data to reference...\n●  Classifying cells...\nDONE!\n\n\n\nDimPlot(ctrl, group.by = \"scpred_prediction\", label = T, repel = T) + NoAxes()\n\n\n\n\n\n\n\n\nNow plot how many cells of each celltypes can be found in each cluster.\n\nggplot(ctrl@meta.data, aes(x = louvain_SNNk15, fill = scpred_prediction)) +\n    geom_bar() +\n    theme_classic()\n\n\n\n\n\n\n\n\nAdd the predictions into the SCE object\n\nctrl.sce@colData$scpred_prediction &lt;- ctrl$scpred_prediction"
  },
  {
    "objectID": "labs/bioc/bioc_06_celltyping.html#meta-ct_compare",
    "href": "labs/bioc/bioc_06_celltyping.html#meta-ct_compare",
    "title": " Celltype prediction",
    "section": "6 Compare results",
    "text": "6 Compare results\nNow we will compare the output of the two methods using the convenient function in scPred crossTab() that prints the overlap between two metadata slots.\n\ntable(ctrl.sce$scmap_cell, ctrl.sce$singler.hpca)\n\n             \n              B_cell Macrophage Monocyte NK_cell Platelets T_cells\n  B cell          94          0        0       0         0       1\n  CD4 T cell       0          0        0       3         0     158\n  CD8 T cell       0          0        0     139         0     118\n  cDC              2          0       27       0         0       0\n  cMono            2          0      202       0         1       0\n  ncMono           0          2      145       0         0       0\n  NK cell          0          0        0     165         0       9\n  Plasma cell      1          0        0       0         0       0\n\n\nOr plot onto umap:\n\nwrap_plots(\n    plotReducedDim(ctrl.sce, dimred = \"UMAP\", colour_by = \"scmap_cluster\"),\n    plotReducedDim(ctrl.sce, dimred = \"UMAP\", colour_by = \"scmap_cell\"),\n    plotReducedDim(ctrl.sce, dimred = \"UMAP\", colour_by = \"singler.immune\"),\n    plotReducedDim(ctrl.sce, dimred = \"UMAP\", colour_by = \"singler.hpca\"),\n    plotReducedDim(ctrl.sce, dimred = \"UMAP\", colour_by = \"singler.ref\"),\n    ncol = 3\n)\n\n\n\n\n\n\n\n\nAs you can see, the methods using the same reference all have similar results. While for instance singleR with different references give quite different predictions. This really shows that a relevant reference is the key in having reliable celltype predictions rather than the method used."
  },
  {
    "objectID": "labs/bioc/bioc_06_celltyping.html#meta-ct_gsea",
    "href": "labs/bioc/bioc_06_celltyping.html#meta-ct_gsea",
    "title": " Celltype prediction",
    "section": "7 GSEA with celltype markers",
    "text": "7 GSEA with celltype markers\nAnother option, where celltype can be classified on cluster level is to use gene set enrichment among the DEGs with known markers for different celltypes. Similar to how we did functional enrichment for the DEGs in the differential expression exercise. There are some resources for celltype gene sets that can be used. Such as CellMarker, PanglaoDB or celltype gene sets at MSigDB. We can also look at overlap between DEGs in a reference dataset and the dataset you are analyzing.\n\n7.1 DEG overlap\nFirst, lets extract top DEGs for our Covid-19 dataset and the reference dataset. When we run differential expression for our dataset, we want to report as many genes as possible, hence we set the cutoffs quite lenient.\n\n# run differential expression in our dataset, using clustering at resolution 0.3\nDGE_list &lt;- scran::findMarkers(\n    x = alldata,\n    groups = as.character(alldata$leiden_k20),\n    pval.type = \"all\",\n    min.prop = 0\n)\n\n\n# Compute differential gene expression in reference dataset (that has cell annotation)\nref_DGE &lt;- scran::findMarkers(\n    x = ref.sce,\n    groups = as.character(ref.sce$cell_type),\n    pval.type = \"all\",\n    direction = \"up\"\n)\n\n# Identify the top cell marker genes in reference dataset\n# select top 50 with hihgest foldchange among top 100 signifcant genes.\nref_list &lt;- lapply(ref_DGE, function(x) {\n    x$logFC &lt;- rowSums(as.matrix(x[, grep(\"logFC\", colnames(x))]))\n    x %&gt;%\n        as.data.frame() %&gt;%\n        filter(p.value &lt; 0.01) %&gt;%\n        top_n(-100, p.value) %&gt;%\n        top_n(50, logFC) %&gt;%\n        rownames()\n})\n\nunlist(lapply(ref_list, length))\n\n     B cell  CD4 T cell  CD8 T cell         cDC       cMono      ncMono \n         50          50          19          17          50          50 \n    NK cell         pDC Plasma cell \n         50          50          24 \n\n\nNow we can run GSEA for the DEGs from our dataset and check for enrichment of top DEGs in the reference dataset.\n\nsuppressPackageStartupMessages(library(fgsea))\n\n# run fgsea for each of the clusters in the list\nres &lt;- lapply(DGE_list, function(x) {\n    x$logFC &lt;- rowSums(as.matrix(x[, grep(\"logFC\", colnames(x))]))\n    gene_rank &lt;- setNames(x$logFC, rownames(x))\n    fgseaRes &lt;- fgsea(pathways = ref_list, stats = gene_rank, nperm = 10000)\n    return(fgseaRes)\n})\nnames(res) &lt;- names(DGE_list)\n\n# You can filter and resort the table based on ES, NES or pvalue\nres &lt;- lapply(res, function(x) {\n    x[x$pval &lt; 0.1, ]\n})\nres &lt;- lapply(res, function(x) {\n    x[x$size &gt; 2, ]\n})\nres &lt;- lapply(res, function(x) {\n    x[order(x$NES, decreasing = T), ]\n})\nres\n\n$`1`\n       pathway         pval         padj         ES       NES nMoreExtreme\n        &lt;char&gt;        &lt;num&gt;        &lt;num&gt;      &lt;num&gt;     &lt;num&gt;        &lt;num&gt;\n1:       cMono 0.0010298661 0.0015822785  0.9475486  2.261276            0\n2:      ncMono 0.0010548523 0.0015822785  0.7915538  1.899211            0\n3:  CD8 T cell 0.0037413773 0.0048103422 -0.8640088 -1.451804           31\n4: Plasma cell 0.0003449862 0.0007762190 -0.8824599 -1.530887            2\n5:      B cell 0.0001107297 0.0003321891 -0.8360423 -1.550833            0\n6:     NK cell 0.0001104484 0.0003321891 -0.8525261 -1.586638            0\n7:  CD4 T cell 0.0001102657 0.0003321891 -0.9218630 -1.718905            0\n    size  leadingEdge\n   &lt;int&gt;       &lt;list&gt;\n1:    47 S100A8, ....\n2:    49 AIF1, S1....\n3:    18 CCL5, IL....\n4:    24 RPL36AL,....\n5:    47 RPS5, RP....\n6:    49 B2M, NKG....\n7:    50 RPL3, RP....\n\n$`10`\n       pathway         pval         padj         ES       NES nMoreExtreme\n        &lt;char&gt;        &lt;num&gt;        &lt;num&gt;      &lt;num&gt;     &lt;num&gt;        &lt;num&gt;\n1: Plasma cell 0.0148148148 0.0222222222  0.6576329  1.721790            1\n2:  CD8 T cell 0.0945711072 0.1215914236 -0.7452621 -1.212938          924\n3:      B cell 0.0027056819 0.0048702275 -0.7622529 -1.295898           26\n4:     NK cell 0.0002003807 0.0004508566 -0.7899193 -1.345112            1\n5:       cMono 0.0001002104 0.0003006313 -0.8298881 -1.410884            0\n6:      ncMono 0.0001001904 0.0003006313 -0.8641771 -1.471562            0\n7:  CD4 T cell 0.0001001904 0.0003006313 -0.8690484 -1.480696            0\n    size  leadingEdge\n   &lt;int&gt;       &lt;list&gt;\n1:    24 JCHAIN, ....\n2:    18 IL32, CC....\n3:    47 CD52, CX....\n4:    49 B2M, HCS....\n5:    47 JUND, S1....\n6:    49 S100A4, ....\n7:    50 RPL38, R....\n\n$`11`\n       pathway         pval         padj         ES       NES nMoreExtreme\n        &lt;char&gt;        &lt;num&gt;        &lt;num&gt;      &lt;num&gt;     &lt;num&gt;        &lt;num&gt;\n1:      ncMono 0.0001330849 0.0009098261  0.9785302  1.897494            0\n2:         cDC 0.0067663257 0.0101494886  0.8955752  1.521429           42\n3:       cMono 0.0907621869 0.1021074603  0.6882224  1.326830          673\n4: Plasma cell 0.0144570901 0.0185876873 -0.7608749 -1.560885           46\n5:  CD8 T cell 0.0005641749 0.0010155148 -0.9246833 -1.789615            1\n6:      B cell 0.0003881988 0.0009098261 -0.7699594 -1.804747            0\n7:     NK cell 0.0004019293 0.0009098261 -0.8180535 -1.922275            0\n8:  CD4 T cell 0.0004043672 0.0009098261 -0.9361761 -2.210839            0\n    size  leadingEdge\n   &lt;int&gt;       &lt;list&gt;\n1:    49 LST1, AI....\n2:    17 HLA-DPA1....\n3:    47 TYROBP, ....\n4:    24 ISG20, P....\n5:    18 CCL5, IL....\n6:    47 CXCR4, R....\n7:    49 NKG7, GN....\n8:    50 RPL3, RP....\n\n$`12`\n       pathway         pval         padj         ES       NES nMoreExtreme\n        &lt;char&gt;        &lt;num&gt;        &lt;num&gt;      &lt;num&gt;     &lt;num&gt;        &lt;num&gt;\n1:  CD4 T cell 0.0001014919 0.0009134274  0.9667146  1.659533            0\n2: Plasma cell 0.0040016849 0.0117647059  0.8463036  1.397589           37\n3:      B cell 0.0113994911 0.0170992366  0.7606321  1.301894          111\n4:  CD8 T cell 0.0595648427 0.0765833692  0.7948827  1.285249          552\n5:      ncMono 0.0065359477 0.0117647059 -0.6928308 -1.925996            0\n6:         cDC 0.0013089005 0.0058900524 -0.9317140 -2.097910            0\n7:       cMono 0.0056497175 0.0117647059 -0.7907891 -2.192421            0\n    size  leadingEdge\n   &lt;int&gt;       &lt;list&gt;\n1:    50 IL7R, RP....\n2:    24 RPL36AL,....\n3:    47 RPS5, RP....\n4:    18 IL32, CD....\n5:    49 FCER1G, ....\n6:    17 HLA-DRA,....\n7:    47 S100A9, ....\n\n$`13`\n       pathway         pval         padj         ES       NES nMoreExtreme\n        &lt;char&gt;        &lt;num&gt;        &lt;num&gt;      &lt;num&gt;     &lt;num&gt;        &lt;num&gt;\n1:      B cell 0.0001072386 0.0003531489  0.9492350  1.736001            0\n2:  CD4 T cell 0.0001065303 0.0003531489  0.9017719  1.658073            0\n3:         cDC 0.0001177163 0.0003531489  0.9396430  1.564647            0\n4: Plasma cell 0.0234930094 0.0261662198  0.8008948  1.380153          204\n5:         pDC 0.0261662198 0.0261662198  0.7359128  1.345869          243\n6:      ncMono 0.0015748031 0.0020247469 -0.7153196 -1.854475            0\n7:  CD8 T cell 0.0006891799 0.0015506547 -0.9344223 -2.013821            0\n8:       cMono 0.0014771049 0.0020247469 -0.7806780 -2.016802            0\n9:     NK cell 0.0015748031 0.0020247469 -0.8788032 -2.278309            0\n    size  leadingEdge\n   &lt;int&gt;       &lt;list&gt;\n1:    47 MS4A1, C....\n2:    50 RPS6, RP....\n3:    17 HLA-DRA,....\n4:    24 ISG20, P....\n5:    47 TCF4, IR....\n6:    49 S100A4, ....\n7:    18 CCL5, IL....\n8:    47 S100A9, ....\n9:    49 HCST, NK....\n\n$`2`\n      pathway         pval         padj         ES       NES nMoreExtreme  size\n       &lt;char&gt;        &lt;num&gt;        &lt;num&gt;      &lt;num&gt;     &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1:     B cell 0.0008149959 0.0019417476  0.9744244  2.237231            0    47\n2:        cDC 0.0010787487 0.0019417476  0.9451680  1.835723            1    17\n3: CD4 T cell 0.0209030100 0.0268752986  0.6547482  1.519947           24    50\n4: CD8 T cell 0.0046557216 0.0069835825 -0.8803482 -1.454118           37    18\n5:      cMono 0.0009116809 0.0019417476 -0.8246370 -1.511900            7    47\n6:     ncMono 0.0001134945 0.0005107252 -0.8978324 -1.653999            0    49\n7:    NK cell 0.0001134945 0.0005107252 -0.9079530 -1.672644            0    49\n    leadingEdge\n         &lt;list&gt;\n1: MS4A1, C....\n2: HLA-DRA,....\n3: RPL13, R....\n4: CCL5, IL....\n5: S100A6, ....\n6: S100A4, ....\n7: HCST, NK....\n\n$`3`\n      pathway         pval         padj         ES       NES nMoreExtreme  size\n       &lt;char&gt;        &lt;num&gt;        &lt;num&gt;      &lt;num&gt;     &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1:      cMono 0.0001004117 0.0004518526  0.9381483  1.498749            0    47\n2:     ncMono 0.0001003210 0.0004518526  0.9029378  1.444777            0    49\n3:        cDC 0.0242889765 0.0437201578  0.8453342  1.293574          233    17\n4:        pDC 0.0976001607 0.1254859208  0.7224114  1.154096          971    47\n5:     B cell 0.0232558140 0.0437201578 -0.5880374 -1.756967            0    47\n6:    NK cell 0.0294117647 0.0441176471 -0.6368801 -1.914328            0    49\n7: CD8 T cell 0.0029069767 0.0087209302 -0.9651183 -2.306945            0    18\n    leadingEdge\n         &lt;list&gt;\n1: S100A9, ....\n2: AIF1, S1....\n3: HLA-DRA,....\n4: CTSB, PL....\n5: CXCR4, M....\n6: GNLY, NK....\n7: IL32, CC....\n\n$`4`\n       pathway         pval         padj         ES       NES nMoreExtreme\n        &lt;char&gt;        &lt;num&gt;        &lt;num&gt;      &lt;num&gt;     &lt;num&gt;        &lt;num&gt;\n1:  CD4 T cell 0.0012345679 0.0018518519  0.9814702  2.427369            0\n2: Plasma cell 0.0659276546 0.0847641274 -0.7613848 -1.291510          564\n3:     NK cell 0.0007634420 0.0017177446 -0.8072728 -1.471093            6\n4:         cDC 0.0010765550 0.0018518519 -0.9051946 -1.479618            8\n5:         pDC 0.0005488474 0.0016465423 -0.8181195 -1.484140            4\n6:       cMono 0.0001097695 0.0004939627 -0.9109960 -1.652626            0\n7:      ncMono 0.0001090631 0.0004939627 -0.9320652 -1.698503            0\n    size  leadingEdge\n   &lt;int&gt;       &lt;list&gt;\n1:    50 IL7R, LD....\n2:    24 UBE2J1, ....\n3:    49 NKG7, GN....\n4:    17 HLA-DRA,....\n5:    47 PLEK, NP....\n6:    47 S100A9, ....\n7:    49 FCER1G, ....\n\n$`5`\n      pathway         pval         padj         ES       NES nMoreExtreme  size\n       &lt;char&gt;        &lt;num&gt;        &lt;num&gt;      &lt;num&gt;     &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1:    NK cell 0.0011402509 0.0025655644  0.9457207  2.188187            0    49\n2: CD8 T cell 0.0007656968 0.0022970904  0.9791938  1.951587            0    18\n3:     B cell 0.0466264399 0.0699396599 -0.7074609 -1.273585          424    47\n4:        cDC 0.0167224080 0.0301003344 -0.8357465 -1.373437          144    17\n5:     ncMono 0.0001095890 0.0004936917 -0.9096682 -1.643606            0    49\n6:      cMono 0.0001097093 0.0004936917 -0.9166590 -1.650187            0    47\n    leadingEdge\n         &lt;list&gt;\n1: NKG7, GN....\n2: CCL5, GZ....\n3: RPS11, C....\n4: HLA-DRA,....\n5: FTH1, FC....\n6: S100A9, ....\n\n$`6`\n      pathway         pval         padj         ES       NES nMoreExtreme  size\n       &lt;char&gt;        &lt;num&gt;        &lt;num&gt;      &lt;num&gt;     &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1:    NK cell 0.0008658009 0.0015584416  0.9824343  2.182156            0    49\n2: CD8 T cell 0.0099183197 0.0127521254  0.8797244  1.718261           16    18\n3:        cDC 0.0037484885 0.0056227328 -0.8850041 -1.443850           30    17\n4:     ncMono 0.0001130327 0.0002545249 -0.8306379 -1.500661            0    49\n5:     B cell 0.0001130838 0.0002545249 -0.8683857 -1.564638            0    47\n6:      cMono 0.0001130838 0.0002545249 -0.8816325 -1.588506            0    47\n7: CD4 T cell 0.0001131222 0.0002545249 -0.9015195 -1.630264            0    50\n    leadingEdge\n         &lt;list&gt;\n1: NKG7, GN....\n2: CCL5, GZ....\n3: HLA-DRA,....\n4: COTL1, F....\n5: RPL18A, ....\n6: S100A9, ....\n7: RPS12, R....\n\n$`7`\n       pathway         pval        padj         ES       NES nMoreExtreme  size\n        &lt;char&gt;        &lt;num&gt;       &lt;num&gt;      &lt;num&gt;     &lt;num&gt;        &lt;num&gt; &lt;int&gt;\n1: Plasma cell 0.0483046911 0.072457037 -0.7454380 -1.302320          415    24\n2:     NK cell 0.0105061003 0.018910981 -0.7216614 -1.362524           92    49\n3:         cDC 0.0056133786 0.012630102 -0.8595163 -1.445975           47    17\n4:       cMono 0.0006795017 0.002038505 -0.7813280 -1.470055            5    47\n5:      B cell 0.0006795017 0.002038505 -0.7830898 -1.473370            5    47\n6:  CD4 T cell 0.0001127269 0.001014542 -0.9004861 -1.703752            0    50\n    leadingEdge\n         &lt;list&gt;\n1: PPIB, RP....\n2: ITGB2, N....\n3: HLA-DRB1....\n4: JUND, S1....\n5: RPS23, R....\n6: RPL34, R....\n\n$`8`\n       pathway         pval         padj         ES       NES nMoreExtreme\n        &lt;char&gt;        &lt;num&gt;        &lt;num&gt;      &lt;num&gt;     &lt;num&gt;        &lt;num&gt;\n1:     NK cell 0.0001069748 0.0004813864  0.9521049  1.741057            0\n2:  CD4 T cell 0.0001066894 0.0004813864  0.8671427  1.589332            0\n3:  CD8 T cell 0.0011807770 0.0032234957  0.9191795  1.535125            9\n4: Plasma cell 0.0035562694 0.0045723463  0.8596421  1.482464           30\n5:      ncMono 0.0030581040 0.0045723463 -0.6383894 -1.665998            1\n6:         cDC 0.0031665611 0.0045723463 -0.8278322 -1.751560            4\n7:       cMono 0.0014326648 0.0032234957 -0.8438412 -2.190169            0\n    size  leadingEdge\n   &lt;int&gt;       &lt;list&gt;\n1:    49 NKG7, GN....\n2:    50 RPL3, RP....\n3:    18 CCL5, GZ....\n4:    24 PPIB, FK....\n5:    49 COTL1, A....\n6:    17 HLA-DRA,....\n7:    47 S100A9, ....\n\n$`9`\n       pathway         pval         padj         ES       NES nMoreExtreme\n        &lt;char&gt;        &lt;num&gt;        &lt;num&gt;      &lt;num&gt;     &lt;num&gt;        &lt;num&gt;\n1:       cMono 0.0001404494 0.0005285412  0.9521871  1.878727            0\n2:      ncMono 0.0001397233 0.0005285412  0.9102892  1.805097            0\n3:         cDC 0.0360740981 0.0405833604  0.8402793  1.440331          221\n4: Plasma cell 0.0135658915 0.0174418605 -0.7790175 -1.564322           48\n5:  CD8 T cell 0.0002622607 0.0005285412 -0.9233197 -1.759120            0\n6:     NK cell 0.0003514938 0.0005285412 -0.8037175 -1.848796            0\n7:      B cell 0.0003469813 0.0005285412 -0.8921524 -2.035759            0\n8:  CD4 T cell 0.0003523608 0.0005285412 -0.9271993 -2.142847            0\n    size  leadingEdge\n   &lt;int&gt;       &lt;list&gt;\n1:    47 S100A9, ....\n2:    49 AIF1, S1....\n3:    17 HLA-DRA,....\n4:    24 ISG20, P....\n5:    18 IL32, CC....\n6:    49 NKG7, GN....\n7:    47 CXCR4, R....\n8:    50 RPL3, RP....\n\n\nSelecting top significant overlap per cluster, we can now rename the clusters according to the predicted labels. OBS! Be aware that if you have some clusters that have non-significant p-values for all the gene sets, the cluster label will not be very reliable. Also, the gene sets you are using may not cover all the celltypes you have in your dataset and hence predictions may just be the most similar celltype. Also, some of the clusters have very similar p-values to multiple celltypes, for instance the ncMono and cMono celltypes are equally good for some clusters.\n\nnew.cluster.ids &lt;- unlist(lapply(res, function(x) {\n    as.data.frame(x)[1, 1]\n}))\n\nalldata$ref_gsea &lt;- new.cluster.ids[as.character(alldata$leiden_k20)]\n\nwrap_plots(\n    plotReducedDim(alldata, dimred = \"UMAP\", colour_by = \"leiden_k20\"),\n    plotReducedDim(alldata, dimred = \"UMAP\", colour_by = \"ref_gsea\"),\n    ncol = 2\n)\n\n\n\n\n\n\n\n\nCompare the results with the other celltype prediction methods in the ctrl_13 sample.\n\nctrl.sce$ref_gsea &lt;- alldata$ref_gsea[alldata$sample == \"ctrl.13\"]\n\nwrap_plots(\n    plotReducedDim(ctrl.sce, dimred = \"UMAP\", colour_by = \"ref_gsea\"),\n    plotReducedDim(ctrl.sce, dimred = \"UMAP\", colour_by = \"scmap_cell\"),\n    plotReducedDim(ctrl.sce, dimred = \"UMAP\", colour_by = \"singler.hpca\"),\n    ncol = 3\n)\n\n\n\n\n\n\n\n\n\n\n7.2 With annotated gene sets\nWe have downloaded the celltype gene lists from http://bio-bigdata.hrbmu.edu.cn/CellMarker/CellMarker_download.html and converted the excel file to a csv for you. Read in the gene lists and do some filtering.\n\npath_file &lt;- file.path(\"data/human_cell_markers.txt\")\nif (!file.exists(path_file)) download.file(file.path(path_data, \"human_cell_markers.txt\"), destfile = path_file)\n\n\nmarkers &lt;- read.delim(\"data/human_cell_markers.txt\")\nmarkers &lt;- markers[markers$speciesType == \"Human\", ]\nmarkers &lt;- markers[markers$cancerType == \"Normal\", ]\n\n# Filter by tissue (to reduce computational time and have tissue-specific classification)\n# sort(unique(markers$tissueType))\n# grep(\"blood\",unique(markers$tissueType),value = T)\n# markers &lt;- markers [ markers$tissueType %in% c(\"Blood\",\"Venous blood\",\n#                                                \"Serum\",\"Plasma\",\n#                                                \"Spleen\",\"Bone marrow\",\"Lymph node\"), ]\n\n\n# remove strange characters etc.\ncelltype_list &lt;- lapply(unique(markers$cellName), function(x) {\n    x &lt;- paste(markers$geneSymbol[markers$cellName == x], sep = \",\")\n    x &lt;- gsub(\"[[]|[]]| |-\", \",\", x)\n    x &lt;- unlist(strsplit(x, split = \",\"))\n    x &lt;- unique(x[!x %in% c(\"\", \"NA\", \"family\")])\n    x &lt;- casefold(x, upper = T)\n})\nnames(celltype_list) &lt;- unique(markers$cellName)\n# celltype_list &lt;- lapply(celltype_list , function(x) {x[1:min(length(x),50)]} )\ncelltype_list &lt;- celltype_list[unlist(lapply(celltype_list, length)) &lt; 100]\ncelltype_list &lt;- celltype_list[unlist(lapply(celltype_list, length)) &gt; 5]\n\n\n# run fgsea for each of the clusters in the list\nres &lt;- lapply(DGE_list, function(x) {\n    x$logFC &lt;- rowSums(as.matrix(x[, grep(\"logFC\", colnames(x))]))\n    gene_rank &lt;- setNames(x$logFC, rownames(x))\n    fgseaRes &lt;- fgsea(pathways = celltype_list, stats = gene_rank, nperm = 10000)\n    return(fgseaRes)\n})\nnames(res) &lt;- names(DGE_list)\n\n# You can filter and resort the table based on ES, NES or pvalue\nres &lt;- lapply(res, function(x) {\n    x[x$pval &lt; 0.01, ]\n})\nres &lt;- lapply(res, function(x) {\n    x[x$size &gt; 5, ]\n})\nres &lt;- lapply(res, function(x) {\n    x[order(x$NES, decreasing = T), ]\n})\n\n# show top 3 for each cluster.\nlapply(res, head, 3)\n\n$`1`\n                           pathway        pval       padj        ES      NES\n                            &lt;char&gt;       &lt;num&gt;      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:                      Neutrophil 0.001517451 0.05705615 0.8930496 2.257529\n2:          CD1C+_B dendritic cell 0.001090513 0.05705615 0.9386464 2.235849\n3: Monocyte derived dendritic cell 0.001367054 0.05705615 0.9177964 1.885585\n   nMoreExtreme  size  leadingEdge\n          &lt;num&gt; &lt;int&gt;       &lt;list&gt;\n1:            0    82 S100A8, ....\n2:            0    54 S100A8, ....\n3:            1    18 S100A8, ....\n\n$`10`\n                        pathway       pval       padj         ES       NES\n                         &lt;char&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;     &lt;num&gt;\n1:          Natural killer cell 0.00850085 0.05707714 -0.6955236 -1.202945\n2: Megakaryocyte erythroid cell 0.00840084 0.05707714 -0.6957935 -1.203071\n3:        CD4+ cytotoxic T cell 0.00360036 0.03981575 -0.7042291 -1.218833\n   nMoreExtreme  size  leadingEdge\n          &lt;num&gt; &lt;int&gt;       &lt;list&gt;\n1:           84    84 PTPRC, C....\n2:           83    83 PTPRC, C....\n3:           35    86 RAP1B, Z....\n\n$`11`\n            pathway         pval       padj        ES      NES nMoreExtreme\n             &lt;char&gt;        &lt;num&gt;      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt;\n1: Mesenchymal cell 0.0005168626 0.05886446 0.8164307 1.631806            3\n2:     Stromal cell 0.0012524353 0.05886446 0.8477874 1.602625            8\n3:       Neutrophil 0.0011059228 0.05886446 0.7669280 1.578995            8\n    size  leadingEdge\n   &lt;int&gt;       &lt;list&gt;\n1:    61 COTL1, S....\n2:    38 PECAM1, ....\n3:    82 LST1, FC....\n\n$`12`\n             pathway         pval        padj        ES      NES nMoreExtreme\n              &lt;char&gt;        &lt;num&gt;       &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt;\n1: Naive CD4+ T cell 0.0001031353 0.009694719 0.8809675 1.490760            0\n2:       CD4+ T cell 0.0003153911 0.014823381 0.8931280 1.484227            2\n3: Naive CD8+ T cell 0.0001001904 0.009694719 0.8320149 1.456091            0\n    size  leadingEdge\n   &lt;int&gt;       &lt;list&gt;\n1:    34 IL7R, RP....\n2:    25 IL7R, LT....\n3:    91 LDHB, NP....\n\n$`13`\n                    pathway        pval       padj         ES       NES\n                     &lt;char&gt;       &lt;num&gt;      &lt;num&gt;      &lt;num&gt;     &lt;num&gt;\n1:        Follicular B cell 0.001935557 0.02722468  0.8788086  1.500461\n2: Morula cell (Blastomere) 0.006952965 0.05027529  0.7111727  1.345584\n3:               Myeloblast 0.009193481 0.06401387 -0.9435484 -1.614268\n   nMoreExtreme  size  leadingEdge\n          &lt;num&gt; &lt;int&gt;       &lt;list&gt;\n1:           16    22 MS4A1, C....\n2:           67    88 RPL39, R....\n3:           21     6 CSF3R, L....\n\n$`2`\n                        pathway        pval       padj         ES       NES\n                         &lt;char&gt;       &lt;num&gt;      &lt;num&gt;      &lt;num&gt;     &lt;num&gt;\n1:                  Acinar cell 0.008064516 0.06317204 -0.7542708 -1.406864\n2:                  FOXN4+ cell 0.005870625 0.05973237 -0.7383047 -1.412975\n3: Hematopoietic precursor cell 0.007874016 0.06317204 -0.9638801 -1.413231\n   nMoreExtreme  size  leadingEdge\n          &lt;num&gt; &lt;int&gt;       &lt;list&gt;\n1:           70    57 LYZ, IL3....\n2:           52    75 LYZ, ENO....\n3:           58     6  PTPRC, CD14\n\n$`3`\n                           pathway         pval        padj        ES      NES\n                            &lt;char&gt;        &lt;num&gt;       &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:                      Neutrophil 0.0001000200 0.006311478 0.9045921 1.466125\n2: Monocyte derived dendritic cell 0.0002077059 0.009762177 0.9394712 1.443466\n3:                      Glial cell 0.0005325948 0.014303975 0.9605309 1.438322\n   nMoreExtreme  size  leadingEdge\n          &lt;num&gt; &lt;int&gt;       &lt;list&gt;\n1:            0    82 S100A9, ....\n2:            1    18 S100A9, ....\n3:            4    12 CD14, VI....\n\n$`4`\n             pathway         pval       padj        ES      NES nMoreExtreme\n              &lt;char&gt;        &lt;num&gt;      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;        &lt;num&gt;\n1: Naive CD8+ T cell 0.0025773196 0.04037621 0.8307127 2.294368            0\n2: Naive CD4+ T cell 0.0009354537 0.02198316 0.9078407 2.109190            0\n3:       CD4+ T cell 0.0007645260 0.02164153 0.9222369 2.034293            0\n    size  leadingEdge\n   &lt;int&gt;       &lt;list&gt;\n1:    91 LDHB, PI....\n2:    34 IL7R, TC....\n3:    25 LTB, IL7....\n\n$`5`\n                 pathway         pval       padj        ES      NES\n                  &lt;char&gt;        &lt;num&gt;      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1: CD4+ cytotoxic T cell 0.0018050542 0.03305785 0.8915720 2.162451\n2:   Natural killer cell 0.0017513135 0.03305785 0.7741180 1.878243\n3:           CD8+ T cell 0.0007911392 0.03305785 0.9352951 1.860485\n   nMoreExtreme  size  leadingEdge\n          &lt;num&gt; &lt;int&gt;       &lt;list&gt;\n1:            0    86 NKG7, CC....\n2:            0    84 NKG7, GN....\n3:            0    19 NKG7, CD....\n\n$`6`\n                             pathway        pval       padj        ES      NES\n                              &lt;char&gt;       &lt;num&gt;      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:             CD4+ cytotoxic T cell 0.001158749 0.03500753 0.9516604 2.265915\n2: Effector CD8+ memory T (Tem) cell 0.001111111 0.03500753 0.8866221 2.084440\n3:               Natural killer cell 0.001149425 0.03500753 0.8111768 1.926784\n   nMoreExtreme  size  leadingEdge\n          &lt;num&gt; &lt;int&gt;       &lt;list&gt;\n1:            0    86 NKG7, GN....\n2:            0    79 GNLY, GZ....\n3:            0    84 NKG7, GN....\n\n$`7`\n            pathway        pval       padj         ES       NES nMoreExtreme\n             &lt;char&gt;       &lt;num&gt;      &lt;num&gt;      &lt;num&gt;     &lt;num&gt;        &lt;num&gt;\n1:    Megakaryocyte 0.001439885 0.09023278  0.8669567  1.844022            1\n2:         Platelet 0.005833333 0.18277778  0.7101754  1.690124            6\n3: Adventitial cell 0.008125224 0.19090811 -0.9370995 -1.420829           67\n    size  leadingEdge\n   &lt;int&gt;       &lt;list&gt;\n1:    26 PPBP, PF....\n2:    45 GP9, ITG....\n3:     7  PTPRC, CD44\n\n$`8`\n                             pathway         pval        padj        ES\n                              &lt;char&gt;        &lt;num&gt;       &lt;num&gt;     &lt;num&gt;\n1:             CD4+ cytotoxic T cell 0.0001029760 0.006481865 0.9079269\n2: Effector CD8+ memory T (Tem) cell 0.0001034340 0.006481865 0.8889122\n3:               Natural killer cell 0.0001030291 0.006481865 0.8291171\n        NES nMoreExtreme  size  leadingEdge\n      &lt;num&gt;        &lt;num&gt; &lt;int&gt;       &lt;list&gt;\n1: 1.724108            0    86 NKG7, GN....\n2: 1.681468            0    79 GNLY, GZ....\n3: 1.573421            0    84 NKG7, GN....\n\n$`9`\n                  pathway         pval       padj        ES      NES\n                   &lt;char&gt;        &lt;num&gt;      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:             Neutrophil 0.0001294666 0.01294409 0.9122455 1.918616\n2: CD1C+_B dendritic cell 0.0001377031 0.01294409 0.9192254 1.844511\n3:           Stromal cell 0.0008788633 0.04130658 0.8697282 1.660327\n   nMoreExtreme  size  leadingEdge\n          &lt;num&gt; &lt;int&gt;       &lt;list&gt;\n1:            0    82 S100A9, ....\n2:            0    54 S100A9, ....\n3:            5    38 VIM, TIM....\n\n\n#CT_GSEA8:\n\nnew.cluster.ids &lt;- unlist(lapply(res, function(x) {\n    as.data.frame(x)[1, 1]\n}))\nalldata$cellmarker_gsea &lt;- new.cluster.ids[as.character(alldata$leiden_k20)]\n\nwrap_plots(\n    plotReducedDim(alldata, dimred = \"UMAP\", colour_by = \"cellmarker_gsea\"),\n    plotReducedDim(alldata, dimred = \"UMAP\", colour_by = \"ref_gsea\"),\n    ncol = 2\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nDo you think that the methods overlap well? Where do you see the most inconsistencies?\n\n\nIn this case we do not have any ground truth, and we cannot say which method performs best. You should keep in mind, that any celltype classification method is just a prediction, and you still need to use your common sense and knowledge of the biological system to judge if the results make sense.\nFinally, lets save the data with predictions.\n\nsaveRDS(ctrl.sce, \"data/covid/results/bioc_covid_qc_dr_int_cl_ct-ctrl13.rds\")"
  },
  {
    "objectID": "labs/bioc/bioc_06_celltyping.html#meta-session",
    "href": "labs/bioc/bioc_06_celltyping.html#meta-session",
    "title": " Celltype prediction",
    "section": "8 Session info",
    "text": "8 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] fgsea_1.28.0                celldex_1.12.0             \n [3] Seurat_5.1.0                SeuratObject_5.0.2         \n [5] sp_2.1-4                    SingleR_2.4.0              \n [7] scmap_1.24.0                scPred_1.9.2               \n [9] pheatmap_1.0.12             patchwork_1.2.0            \n[11] dplyr_1.1.4                 scran_1.30.0               \n[13] scater_1.30.1               ggplot2_3.5.1              \n[15] scuttle_1.12.0              SingleCellExperiment_1.24.0\n[17] SummarizedExperiment_1.32.0 Biobase_2.62.0             \n[19] GenomicRanges_1.54.1        GenomeInfoDb_1.38.1        \n[21] IRanges_2.36.0              S4Vectors_0.40.2           \n[23] BiocGenerics_0.48.1         MatrixGenerics_1.14.0      \n[25] matrixStats_1.4.1          \n\nloaded via a namespace (and not attached):\n  [1] spatstat.sparse_3.1-0         bitops_1.0-8                 \n  [3] lubridate_1.9.3               httr_1.4.7                   \n  [5] RColorBrewer_1.1-3            tools_4.3.3                  \n  [7] sctransform_0.4.1             utf8_1.2.4                   \n  [9] R6_2.5.1                      lazyeval_0.2.2               \n [11] uwot_0.1.16                   withr_3.0.1                  \n [13] gridExtra_2.3                 progressr_0.14.0             \n [15] cli_3.6.3                     spatstat.explore_3.2-6       \n [17] fastDummies_1.7.4             labeling_0.4.3               \n [19] spatstat.data_3.1-2           randomForest_4.7-1.2         \n [21] proxy_0.4-27                  ggridges_0.5.6               \n [23] pbapply_1.7-2                 harmony_1.2.1                \n [25] parallelly_1.38.0             limma_3.58.1                 \n [27] RSQLite_2.3.7                 FNN_1.1.4                    \n [29] generics_0.1.3                ica_1.0-3                    \n [31] spatstat.random_3.2-3         Matrix_1.6-5                 \n [33] ggbeeswarm_0.7.2              fansi_1.0.6                  \n [35] abind_1.4-5                   lifecycle_1.0.4              \n [37] yaml_2.3.10                   edgeR_4.0.16                 \n [39] BiocFileCache_2.10.1          recipes_1.1.0                \n [41] SparseArray_1.2.2             Rtsne_0.17                   \n [43] blob_1.2.4                    grid_4.3.3                   \n [45] promises_1.3.0                dqrng_0.3.2                  \n [47] ExperimentHub_2.10.0          crayon_1.5.3                 \n [49] miniUI_0.1.1.1                lattice_0.22-6               \n [51] beachmat_2.18.0               cowplot_1.1.3                \n [53] KEGGREST_1.42.0               pillar_1.9.0                 \n [55] knitr_1.48                    metapod_1.10.0               \n [57] future.apply_1.11.2           codetools_0.2-20             \n [59] fastmatch_1.1-4               leiden_0.4.3.1               \n [61] googleVis_0.7.3               glue_1.7.0                   \n [63] data.table_1.15.4             vctrs_0.6.5                  \n [65] png_0.1-8                     spam_2.10-0                  \n [67] gtable_0.3.5                  cachem_1.1.0                 \n [69] gower_1.0.1                   xfun_0.47                    \n [71] S4Arrays_1.2.0                mime_0.12                    \n [73] prodlim_2024.06.25            survival_3.7-0               \n [75] timeDate_4032.109             iterators_1.0.14             \n [77] hardhat_1.4.0                 lava_1.8.0                   \n [79] statmod_1.5.0                 bluster_1.12.0               \n [81] interactiveDisplayBase_1.40.0 fitdistrplus_1.2-1           \n [83] ROCR_1.0-11                   ipred_0.9-15                 \n [85] nlme_3.1-165                  bit64_4.0.5                  \n [87] filelock_1.0.3                RcppAnnoy_0.0.22             \n [89] irlba_2.3.5.1                 vipor_0.4.7                  \n [91] KernSmooth_2.23-24            rpart_4.1.23                 \n [93] DBI_1.2.3                     colorspace_2.1-1             \n [95] nnet_7.3-19                   tidyselect_1.2.1             \n [97] curl_5.2.1                    bit_4.0.5                    \n [99] compiler_4.3.3                BiocNeighbors_1.20.0         \n[101] DelayedArray_0.28.0           plotly_4.10.4                \n[103] scales_1.3.0                  lmtest_0.9-40                \n[105] rappdirs_0.3.3                stringr_1.5.1                \n[107] digest_0.6.37                 goftest_1.2-3                \n[109] spatstat.utils_3.1-0          rmarkdown_2.28               \n[111] XVector_0.42.0                htmltools_0.5.8.1            \n[113] pkgconfig_2.0.3               sparseMatrixStats_1.14.0     \n[115] dbplyr_2.5.0                  fastmap_1.2.0                \n[117] rlang_1.1.4                   htmlwidgets_1.6.4            \n[119] shiny_1.9.1                   DelayedMatrixStats_1.24.0    \n[121] farver_2.1.2                  zoo_1.8-12                   \n[123] jsonlite_1.8.8                BiocParallel_1.36.0          \n[125] ModelMetrics_1.2.2.2          BiocSingular_1.18.0          \n[127] RCurl_1.98-1.16               magrittr_2.0.3               \n[129] GenomeInfoDbData_1.2.11       dotCall64_1.1-1              \n[131] munsell_0.5.1                 Rcpp_1.0.13                  \n[133] viridis_0.6.5                 reticulate_1.39.0            \n[135] stringi_1.8.4                 pROC_1.18.5                  \n[137] zlibbioc_1.48.0               MASS_7.3-60.0.1              \n[139] AnnotationHub_3.10.0          plyr_1.8.9                   \n[141] parallel_4.3.3                listenv_0.9.1                \n[143] ggrepel_0.9.6                 deldir_2.0-4                 \n[145] Biostrings_2.70.1             splines_4.3.3                \n[147] tensor_1.5                    locfit_1.5-9.9               \n[149] igraph_2.0.3                  spatstat.geom_3.2-9          \n[151] RcppHNSW_0.6.0                reshape2_1.4.4               \n[153] ScaledMatrix_1.10.0           BiocVersion_3.18.1           \n[155] evaluate_0.24.0               BiocManager_1.30.25          \n[157] foreach_1.5.2                 httpuv_1.6.15                \n[159] RANN_2.6.2                    tidyr_1.3.1                  \n[161] purrr_1.0.2                   polyclip_1.10-7              \n[163] future_1.34.0                 scattermore_1.2              \n[165] rsvd_1.0.5                    xtable_1.8-4                 \n[167] e1071_1.7-14                  RSpectra_0.16-2              \n[169] later_1.3.2                   viridisLite_0.4.2            \n[171] class_7.3-22                  tibble_3.2.1                 \n[173] memoise_2.0.1                 AnnotationDbi_1.64.1         \n[175] beeswarm_0.4.0                cluster_2.1.6                \n[177] timechange_0.3.0              globals_0.16.3               \n[179] caret_6.0-94"
  },
  {
    "objectID": "labs/bioc/bioc_08_spatial.html",
    "href": "labs/bioc/bioc_08_spatial.html",
    "title": " Spatial Transcriptomics",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified.\nSpatial transcriptomic data with the Visium platform is in many ways similar to scRNAseq data. It contains UMI counts for 5-20 cells instead of single cells, but is still quite sparse in the same way as scRNAseq data is, but with the additional information about spatial location in the tissue.\nHere we will first run quality control in a similar manner to scRNAseq data, then QC filtering, dimensionality reduction, integration and clustering. Then we will use scRNAseq data from mouse cortex to run label transfer to predict celltypes in the Visium spots.\nWe will use two Visium spatial transcriptomics dataset of the mouse brain (Sagittal), which are publicly available from the 10x genomics website. Note, that these dataset have already been filtered for spots that does not overlap with the tissue."
  },
  {
    "objectID": "labs/bioc/bioc_08_spatial.html#meta-st_prep",
    "href": "labs/bioc/bioc_08_spatial.html#meta-st_prep",
    "title": " Spatial Transcriptomics",
    "section": "1 Preparation",
    "text": "1 Preparation\nLoad packages\n\n# BiocManager::install('DropletUtils',update = F)\n# BiocManager::install(\"Spaniel\",update = F)\n# remotes::install_github(\"RachelQueen1/Spaniel\", ref = \"Development\" ,upgrade = F,dependencies = F)\n# remotes::install_github(\"renozao/xbioc\")\n# remotes::install_github(\"meichendong/SCDC\")\n\nsuppressPackageStartupMessages({\n    library(Spaniel)\n    # library(biomaRt)\n    library(SingleCellExperiment)\n    library(Matrix)\n    library(dplyr)\n    library(scran)\n    library(SingleR)\n    library(scater)\n    library(ggplot2)\n    library(patchwork)\n})\n\nLoad ST data\n\n# url for source and intermediate data\npath_data &lt;- \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\n\n\nif (!dir.exists(\"data/spatial/visium/Anterior\")) dir.create(\"data/spatial/visium/Anterior\", recursive = T)\nif (!dir.exists(\"data/spatial/visium/Posterior\")) dir.create(\"data/spatial/visium/Posterior\", recursive = T)\n\nfile_list &lt;- c(\n    \"spatial/visium/Anterior/V1_Mouse_Brain_Sagittal_Anterior_filtered_feature_bc_matrix.tar.gz\",\n    \"spatial/visium/Anterior/V1_Mouse_Brain_Sagittal_Anterior_spatial.tar.gz\",\n    \"spatial/visium/Posterior/V1_Mouse_Brain_Sagittal_Posterior_filtered_feature_bc_matrix.tar.gz\",\n    \"spatial/visium/Posterior/V1_Mouse_Brain_Sagittal_Posterior_spatial.tar.gz\"\n)\n\nfor (i in file_list) {\n    if (!file.exists(file.path(\"data\", i))) {\n        cat(paste0(\"Downloading \", file.path(path_data, i), \" to \", file.path(\"data\", i), \"\\n\"))\n        download.file(url = file.path(path_data, i), destfile = file.path(\"data\", i))\n    }\n    cat(paste0(\"Uncompressing \", file.path(\"data\", i), \"\\n\"))\n    system(paste0(\"tar -xvzf \", file.path(\"data\", i), \" -C \", dirname(file.path(\"data\", i))))\n}\n\nUncompressing data/spatial/visium/Anterior/V1_Mouse_Brain_Sagittal_Anterior_filtered_feature_bc_matrix.tar.gz\nUncompressing data/spatial/visium/Anterior/V1_Mouse_Brain_Sagittal_Anterior_spatial.tar.gz\nUncompressing data/spatial/visium/Posterior/V1_Mouse_Brain_Sagittal_Posterior_filtered_feature_bc_matrix.tar.gz\nUncompressing data/spatial/visium/Posterior/V1_Mouse_Brain_Sagittal_Posterior_spatial.tar.gz\n\n\nMerge the objects into one SCE object.\n\nsce.a &lt;- Spaniel::createVisiumSCE(tenXDir = \"data/spatial/visium/Anterior\", resolution = \"Low\")\nsce.p &lt;- Spaniel::createVisiumSCE(tenXDir = \"data/spatial/visium/Posterior\", resolution = \"Low\")\nsce &lt;- cbind(sce.a, sce.p)\n\nsce$Sample &lt;- basename(sub(\"/filtered_feature_bc_matrix\", \"\", sce$Sample))\n\nlll &lt;- list(sce.a, sce.p)\nlll &lt;- lapply(lll, function(x) x@metadata)\nnames(lll) &lt;- c(\"Anterior\", \"Posterior\")\nsce@metadata &lt;- lll\n\nWe can further convert the gene ensembl IDs to gene names using biomaRt.\n\nmart &lt;- biomaRt::useMart(biomart = \"ENSEMBL_MART_ENSEMBL\", dataset = \"mmusculus_gene_ensembl\")\nannot &lt;- biomaRt::getBM(attributes = c(\"ensembl_gene_id\", \"external_gene_name\", \"gene_biotype\"), mart = mart, useCache = F)\nsaveRDS(annot, \"data/spatial/visium/annot.rds\")\n\nWe will use a file that was created in advance.\n\npath_file &lt;- \"data/spatial/visium/annot.rds\"\nif (!file.exists(path_file)) download.file(url = file.path(path_data, \"spatial/visium/annot.rds\"), destfile = path_file)\nannot &lt;- readRDS(path_file)\n\n\ngene_names &lt;- as.character(annot[match(rownames(sce), annot[, \"ensembl_gene_id\"]), \"external_gene_name\"])\ngene_names[is.na(gene_names)] &lt;- \"\"\n\nsce &lt;- sce[gene_names != \"\", ]\nrownames(sce) &lt;- gene_names[gene_names != \"\"]\ndim(sce)\n\n[1] 32053  6050"
  },
  {
    "objectID": "labs/bioc/bioc_08_spatial.html#meta-st_qc",
    "href": "labs/bioc/bioc_08_spatial.html#meta-st_qc",
    "title": " Spatial Transcriptomics",
    "section": "2 Quality control",
    "text": "2 Quality control\nSimilar to scRNA-seq we use statistics on number of counts, number of features and percent mitochondria for quality control.\nNow the counts and feature counts are calculated on the Spatial assay, so they are named nCount_Spatial and nFeature_Spatial.\n\n# Mitochondrial genes\nmito_genes &lt;- rownames(sce)[grep(\"^mt-\", rownames(sce))]\n\n# Ribosomal genes\nribo_genes &lt;- rownames(sce)[grep(\"^Rp[sl]\", rownames(sce))]\n\n# Hemoglobin genes - includes all genes starting with HB except HBP.\nhb_genes &lt;- rownames(sce)[grep(\"^Hb[^(p)]\", rownames(sce))]\n\nsce &lt;- addPerCellQC(sce, flatten = T, subsets = list(mt = mito_genes, hb = hb_genes, ribo = ribo_genes))\n\nhead(colData(sce))\n\nDataFrame with 6 rows and 24 columns\n       Sample            Barcode   Section    Spot_Y    Spot_X   Image_Y\n  &lt;character&gt;        &lt;character&gt; &lt;integer&gt; &lt;integer&gt; &lt;integer&gt; &lt;integer&gt;\n1    Anterior AAACAAGTATCTCCCA-1         1        50       102      7474\n2    Anterior AAACACCAATAACTGC-1         1        59        19      8552\n3    Anterior AAACAGAGCGACTCCT-1         1        14        94      3163\n4    Anterior AAACAGCTTTCAGAAG-1         1        43         9      6636\n5    Anterior AAACAGGGTCTATATT-1         1        47        13      7115\n6    Anterior AAACATGGTGAGAGGA-1         1        62         0      8912\n    Image_X   pixel_x   pixel_y       sum  detected     total       sum\n  &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n1      8500   438.898   214.079     13991      4462     13991     13960\n2      2788   143.959   158.417     39797      8126     39797     39742\n3      7950   410.499   436.678     29951      6526     29951     29905\n4      2100   108.434   257.349     42333      8190     42333     42262\n5      2375   122.633   232.616     35700      8090     35700     35660\n6      1480    76.420   139.828     22148      6518     22148     22096\n   detected subsets_mt_sum subsets_mt_detected subsets_mt_percent\n  &lt;integer&gt;      &lt;numeric&gt;           &lt;integer&gt;          &lt;numeric&gt;\n1      4458           1521                  12           10.89542\n2      8116           3977                  12           10.00705\n3      6520           4265                  12           14.26183\n4      8181           2870                  12            6.79097\n5      8083           1831                  13            5.13460\n6      6509           2390                  12           10.81644\n  subsets_hb_sum subsets_hb_detected subsets_hb_percent subsets_ribo_sum\n       &lt;numeric&gt;           &lt;integer&gt;          &lt;numeric&gt;        &lt;numeric&gt;\n1             60                   4           0.429799              826\n2            831                   6           2.090987             2199\n3            111                   5           0.371175             1663\n4            117                   5           0.276844             3129\n5             73                   5           0.204711             2653\n6            134                   5           0.606445             1478\n  subsets_ribo_detected subsets_ribo_percent     total\n              &lt;integer&gt;            &lt;numeric&gt; &lt;numeric&gt;\n1                    85              5.91691     13960\n2                    89              5.53319     39742\n3                    88              5.56094     29905\n4                    88              7.40381     42262\n5                    90              7.43971     35660\n6                    84              6.68899     22096\n\nwrap_plots(plotColData(sce, y = \"detected\", x = \"Sample\", colour_by = \"Sample\"),\n    plotColData(sce, y = \"total\", x = \"Sample\", colour_by = \"Sample\"),\n    plotColData(sce, y = \"subsets_mt_percent\", x = \"Sample\", colour_by = \"Sample\"),\n    plotColData(sce, y = \"subsets_ribo_percent\", x = \"Sample\", colour_by = \"Sample\"),\n    plotColData(sce, y = \"subsets_hb_percent\", x = \"Sample\", colour_by = \"Sample\"),\n    ncol = 3\n)\n\n\n\n\n\n\n\n\nWe can also plot the same data onto the tissue section.\n\nsamples &lt;- c(\"Anterior\", \"Posterior\")\nto_plot &lt;- c(\"detected\", \"total\", \"subsets_mt_percent\", \"subsets_ribo_percent\", \"subsets_hb_percent\")\n\nplist &lt;- list()\nn &lt;- 1\nfor (j in to_plot) {\n    for (i in samples) {\n        temp &lt;- sce[, sce$Sample == i]\n        temp@metadata &lt;- temp@metadata[[i]]\n        plist[[n]] &lt;- spanielPlot(\n            object = temp,\n            plotType = \"Cluster\",\n            clusterRes = j, customTitle = j,\n            techType = \"Visium\",\n            ptSizeMax = 1, ptSizeMin = .1\n        )\n        n &lt;- n + 1\n    }\n}\n\nwrap_plots(plist, ncol = 2)\n\n\n\n\n\n\n\n\nAs you can see, the spots with low number of counts/features and high mitochondrial content are mainly towards the edges of the tissue. It is quite likely that these regions are damaged tissue. You may also see regions within a tissue with low quality if you have tears or folds in your section.\nBut remember, for some tissue types, the amount of genes expressed and proportion mitochondria may also be a biological features, so bear in mind what tissue you are working on and what these features mean.\n\n2.1 Filter spots\nSelect all spots with less than 25% mitocondrial reads, less than 20% hb-reads and 500 detected genes. You must judge for yourself based on your knowledge of the tissue what are appropriate filtering criteria for your dataset.\n\nsce &lt;- sce[, sce$detected &gt; 500 &\n    sce$subsets_mt_percent &lt; 25 &\n    sce$subsets_hb_percent &lt; 20]\ndim(sce)\n\n[1] 32053  5804\n\n\nAnd replot onto tissue section:\n\nsamples &lt;- c(\"Anterior\", \"Posterior\")\nto_plot &lt;- c(\"detected\", \"total\", \"subsets_mt_percent\", \"subsets_mt_percent\", \"subsets_hb_percent\")\n\nplist &lt;- list()\nn &lt;- 1\nfor (j in to_plot) {\n    for (i in samples) {\n        temp &lt;- sce[, sce$Sample == i]\n        temp@metadata &lt;- temp@metadata[[i]]\n        plist[[n]] &lt;- spanielPlot(\n            object = temp,\n            plotType = \"Cluster\",\n            clusterRes = j, customTitle = j,\n            techType = \"Visium\",\n            ptSizeMax = 1, ptSizeMin = .1\n        )\n        n &lt;- n + 1\n    }\n}\n\nwrap_plots(plist, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n2.2 Top expressed genes\nAs for scRNA-seq data, we will look at what the top expressed genes are.\n\nC &lt;- counts(sce)\nC@x &lt;- C@x / rep.int(colSums(C), diff(C@p))\nmost_expressed &lt;- order(Matrix::rowSums(C), decreasing = T)[20:1]\nboxplot(as.matrix(t(C[most_expressed, ])), cex = .1, las = 1, xlab = \"% total count per cell\", col = scales::hue_pal()(20)[20:1], horizontal = TRUE)\n\n\n\n\n\n\n\nrm(C)\n\nAs you can see, the mitochondrial genes are among the top expressed genes. Also the lncRNA gene Bc1 (brain cytoplasmic RNA 1). Also one hemoglobin gene.\n\n\n2.3 Filter genes\nWe will remove the Bc1 gene, hemoglobin genes (blood contamination) and the mitochondrial genes.\n\ndim(sce)\n\n[1] 32053  5804\n\n# Filter Bl1\nsce &lt;- sce[!grepl(\"Bc1\", rownames(sce)), ]\n\n# Filter Mitocondrial\nsce &lt;- sce[!grepl(\"^mt-\", rownames(sce)), ]\n\n# Filter Hemoglobin gene (optional if that is a problem on your data)\nsce &lt;- sce[!grepl(\"^Hb.*-\", rownames(sce)), ]\n\ndim(sce)\n\n[1] 32031  5804"
  },
  {
    "objectID": "labs/bioc/bioc_08_spatial.html#meta-st_analysis",
    "href": "labs/bioc/bioc_08_spatial.html#meta-st_analysis",
    "title": " Spatial Transcriptomics",
    "section": "3 Analysis",
    "text": "3 Analysis\nWe will proceed with the data in a very similar manner to scRNA-seq data.\n\nsce &lt;- computeSumFactors(sce, sizes = c(20, 40, 60, 80))\nsce &lt;- logNormCounts(sce)\n\nNow we can plot gene expression of individual genes, the gene Hpca is a strong hippocampal marker and Ttr is a marker of the choroid plexus.\n\nsamples &lt;- c(\"Anterior\", \"Posterior\")\nto_plot &lt;- c(\"Hpca\", \"Ttr\")\n\nplist &lt;- list()\nn &lt;- 1\nfor (j in to_plot) {\n    for (i in samples) {\n        temp &lt;- sce[, sce$Sample == i]\n        temp@metadata &lt;- temp@metadata[[i]]\n        plist[[n]] &lt;- spanielPlot(\n            object = temp,\n            plotType = \"Gene\",\n            gene = j,\n            customTitle = j,\n            techType = \"Visium\",\n            ptSizeMax = 1, ptSizeMin = .1\n        )\n        n &lt;- n + 1\n    }\n}\n\nwrap_plots(plist, ncol = 2)\n\n\n\n\n\n\n\n\n\n3.1 Dimensionality reduction and clustering\nWe can then now run dimensionality reduction and clustering using the same workflow as we use for scRNA-seq analysis.\nBut make sure you run it on the SCT assay.\n\nvar.out &lt;- modelGeneVar(sce, method = \"loess\")\nhvgs &lt;- getTopHVGs(var.out, n = 2000)\nsce &lt;- runPCA(sce,\n    exprs_values = \"logcounts\",\n    subset_row = hvgs,\n    ncomponents = 50,\n    ntop = 100,\n    scale = T\n)\ng &lt;- buildSNNGraph(sce, k = 5, use.dimred = \"PCA\")\nsce$louvain_SNNk5 &lt;- factor(igraph::cluster_louvain(g)$membership)\nsce &lt;- runUMAP(sce,\n    dimred = \"PCA\", n_dimred = 50, ncomponents = 2, min_dist = 0.1, spread = .3,\n    metric = \"correlation\", name = \"UMAP_on_PCA\"\n)\n\nWe can then plot clusters onto umap or onto the tissue section.\n\nsamples &lt;- c(\"Anterior\", \"Posterior\")\nto_plot &lt;- c(\"louvain_SNNk5\")\n\nplist &lt;- list()\nn &lt;- 1\nfor (j in to_plot) {\n    for (i in samples) {\n        temp &lt;- sce[, sce$Sample == i]\n        temp@metadata &lt;- temp@metadata[[i]]\n        plist[[n]] &lt;- spanielPlot(\n            object = temp,\n            plotType = \"Cluster\", clusterRes = j,\n            customTitle = j,\n            techType = \"Visium\",\n            ptSizeMax = 1, ptSizeMin = .1\n        )\n        n &lt;- n + 1\n    }\n}\n\nplist[[3]] &lt;- plotReducedDim(sce, dimred = \"UMAP_on_PCA\", colour_by = \"louvain_SNNk5\")\nplist[[4]] &lt;- plotReducedDim(sce, dimred = \"UMAP_on_PCA\", colour_by = \"Sample\")\n\nwrap_plots(plist, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n3.2 Integration\nQuite often, there are strong batch effects between different ST sections, so it may be a good idea to integrate the data across sections.\nWe will do a similar integration as in the Data Integration lab.\n\nmnn_out &lt;- batchelor::fastMNN(sce, subset.row = hvgs, batch = factor(sce$Sample), k = 20, d = 50)\n\nreducedDim(sce, \"MNN\") &lt;- reducedDim(mnn_out, \"corrected\")\nrm(mnn_out)\ngc()\n\n            used   (Mb) gc trigger   (Mb)  max used   (Mb)\nNcells  10071452  537.9   14514348  775.2  14514348  775.2\nVcells 191850107 1463.8  373707512 2851.2 373706567 2851.2\n\n\nThen we run dimensionality reduction and clustering as before.\n\ng &lt;- buildSNNGraph(sce, k = 5, use.dimred = \"MNN\")\nsce$louvain_SNNk5 &lt;- factor(igraph::cluster_louvain(g)$membership)\nsce &lt;- runUMAP(sce,\n    dimred = \"MNN\", n_dimred = 50, ncomponents = 2, min_dist = 0.1, spread = .3,\n    metric = \"correlation\", name = \"UMAP_on_MNN\"\n)\n\n\nsamples &lt;- c(\"Anterior\", \"Posterior\")\nto_plot &lt;- c(\"louvain_SNNk5\")\n\nplist &lt;- list()\nn &lt;- 1\nfor (j in to_plot) {\n    for (i in samples) {\n        temp &lt;- sce[, sce$Sample == i]\n        temp@metadata &lt;- temp@metadata[[i]]\n        plist[[n]] &lt;- spanielPlot(\n            object = temp,\n            plotType = \"Cluster\", clusterRes = j,\n            customTitle = j,\n            techType = \"Visium\",\n            ptSizeMax = 1, ptSizeMin = .1\n        )\n        n &lt;- n + 1\n    }\n}\n\nplist[[3]] &lt;- plotReducedDim(sce, dimred = \"UMAP_on_MNN\", colour_by = \"louvain_SNNk5\")\nplist[[4]] &lt;- plotReducedDim(sce, dimred = \"UMAP_on_MNN\", colour_by = \"Sample\")\n\nwrap_plots(plist, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nDo you see any differences between the integrated and non-integrated clustering? Judge for yourself, which of the clusterings do you think looks best? As a reference, you can compare to brain regions in the Allen brain atlas.\n\n\n\n\n3.3 Spatially Variable Features\nThere are two main workflows to identify molecular features that correlate with spatial location within a tissue. The first is to perform differential expression based on spatially distinct clusters, the other is to find features that have spatial patterning without taking clusters or spatial annotation into account. First, we will do differential expression between clusters just as we did for the scRNAseq data before.\n\n# differential expression between cluster 4 and cluster 6\ncell_selection &lt;- sce[, sce$louvain_SNNk5 %in% c(4, 6)]\ncell_selection$louvain_SNNk5 &lt;- factor(cell_selection$louvain_SNNk5)\n\nmarkers_genes &lt;- scran::findMarkers(\n    x = cell_selection,\n    groups = cell_selection$louvain_SNNk5,\n    lfc = .25,\n    pval.type = \"all\",\n    direction = \"up\"\n)\n\n# List of dataFrames with the results for each cluster\ntop5_cell_selection &lt;- lapply(names(markers_genes), function(x) {\n    temp &lt;- markers_genes[[x]][1:5, 1:2]\n    temp$gene &lt;- rownames(markers_genes[[x]])[1:5]\n    temp$cluster &lt;- x\n    return(temp)\n})\ntop5_cell_selection &lt;- as_tibble(do.call(rbind, top5_cell_selection))\ntop5_cell_selection\n\n\n\n  \n\n\n# plot top markers\nsamples &lt;- c(\"Anterior\", \"Posterior\")\nto_plot &lt;- top5_cell_selection$gene[1:5]\n\nplist &lt;- list()\nn &lt;- 1\nfor (j in to_plot) {\n    for (i in samples) {\n        temp &lt;- sce[, sce$Sample == i]\n        temp@metadata &lt;- temp@metadata[[i]]\n        plist[[n]] &lt;- spanielPlot(\n            object = temp,\n            plotType = \"Gene\",\n            gene = j,\n            customTitle = j,\n            techType = \"Visium\",\n            ptSizeMax = 1, ptSizeMin = .1\n        )\n        n &lt;- n + 1\n    }\n}\nwrap_plots(plist, ncol = 2)"
  },
  {
    "objectID": "labs/bioc/bioc_08_spatial.html#meta-st_ss",
    "href": "labs/bioc/bioc_08_spatial.html#meta-st_ss",
    "title": " Spatial Transcriptomics",
    "section": "4 Single cell data",
    "text": "4 Single cell data\nWe can use a scRNA-seq dataset as a reference to predict the proportion of different celltypes in the Visium spots. Keep in mind that it is important to have a reference that contains all the celltypes you expect to find in your spots. Ideally it should be a scRNA-seq reference from the exact same tissue. We will use a reference scRNA-seq dataset of ~14,000 adult mouse cortical cell taxonomy from the Allen Institute, generated with the SMART-Seq2 protocol.\nFirst dowload the seurat data:\n\npath_file &lt;- \"data/spatial/visium/allen_cortex.rds\"\nif (!file.exists(path_file)) download.file(url = file.path(path_data, \"spatial/visium/allen_cortex.rds\"), destfile = path_file)\n\nFor speed, and for a more fair comparison of the celltypes, we will subsample all celltypes to a maximum of 200 cells per class (subclass).\n\nar &lt;- readRDS(path_file)\nar_sce &lt;- Seurat::as.SingleCellExperiment(ar)\nrm(ar)\ngc()\n\n            used   (Mb) gc trigger   (Mb)  max used   (Mb)\nNcells  10176118  543.5   18536477  990.0  18536477  990.0\nVcells 577054357 4402.6  832326279 6350.2 577457206 4405.7\n\n# check number of cells per subclass\nar_sce$subclass &lt;- sub(\"/\", \"_\", sub(\" \", \"_\", ar_sce$subclass))\ntable(ar_sce$subclass)\n\n\n     Astro         CR       Endo    L2_3_IT         L4      L5_IT      L5_PT \n       368          7         94        982       1401        880        544 \n     L6_CT      L6_IT        L6b      Lamp5 Macrophage      Meis2         NP \n       960       1872        358       1122         51         45        362 \n     Oligo       Peri      Pvalb   Serpinf1        SMC       Sncg        Sst \n        91         32       1337         27         55        125       1741 \n       Vip       VLMC \n      1728         67 \n\n# select 20 cells per subclass, fist set subclass as active.ident\nsubset_cells &lt;- lapply(unique(ar_sce$subclass), function(x) {\n    if (sum(ar_sce$subclass == x) &gt; 20) {\n        temp &lt;- sample(colnames(ar_sce)[ar_sce$subclass == x], size = 20)\n    } else {\n        temp &lt;- colnames(ar_sce)[ar_sce$subclass == x]\n    }\n})\nar_sce &lt;- ar_sce[, unlist(subset_cells)]\n\n# check again number of cells per subclass\ntable(ar_sce$subclass)\n\n\n     Astro         CR       Endo    L2_3_IT         L4      L5_IT      L5_PT \n        20          7         20         20         20         20         20 \n     L6_CT      L6_IT        L6b      Lamp5 Macrophage      Meis2         NP \n        20         20         20         20         20         20         20 \n     Oligo       Peri      Pvalb   Serpinf1        SMC       Sncg        Sst \n        20         20         20         20         20         20         20 \n       Vip       VLMC \n        20         20 \n\n\nThen run normalization and dimensionality reduction.\n\nar_sce &lt;- computeSumFactors(ar_sce, sizes = c(20, 40, 60, 80))\nar_sce &lt;- logNormCounts(ar_sce)\nallen.var.out &lt;- modelGeneVar(ar_sce, method = \"loess\")\nallen.hvgs &lt;- getTopHVGs(allen.var.out, n = 2000)"
  },
  {
    "objectID": "labs/bioc/bioc_08_spatial.html#meta-st_sub",
    "href": "labs/bioc/bioc_08_spatial.html#meta-st_sub",
    "title": " Spatial Transcriptomics",
    "section": "5 Subset ST for cortex",
    "text": "5 Subset ST for cortex\nSince the scRNAseq dataset was generated from the mouse cortex, we will subset the visium dataset in order to select mainly the spots part of the cortex. Note that the integration can also be performed on the whole brain slice, but it would give rise to false positive cell type assignments and therefore it should be interpreted with more care.\n\n5.1 Integrate with scRNAseq\nHere, will use SingleR for prediciting which cell types are present in the dataset. We can first select the anterior part as an example (to speed up predictions).\n\nsce.anterior &lt;- sce[, sce$Sample == \"Anterior\"]\nsce.anterior@metadata &lt;- sce.anterior@metadata[[\"Anterior\"]]\n\nNext, we select the highly variable genes that are present in both datasets.\n\n# Find common highly variable genes\ncommon_hvgs &lt;- intersect(allen.hvgs, hvgs)\n\n# Predict cell classes\npred.grun &lt;- SingleR(\n    test = sce.anterior[common_hvgs, ],\n    ref = ar_sce[common_hvgs, ],\n    labels = ar_sce$subclass\n)\n\n# Transfer the classes to the SCE object\nsce.anterior$cell_prediction &lt;- pred.grun$labels\nsce.anterior@colData &lt;- cbind(\n    sce.anterior@colData,\n    as.data.frame.matrix(table(list(1:ncol(sce.anterior), sce.anterior$cell_prediction)))\n)\n\nThen we can plot the predicted cell populations back to tissue.\n\n# Plot cell predictions\nspanielPlot(\n    object = sce.anterior,\n    plotType = \"Cluster\",\n    clusterRes = \"cell_prediction\",\n    customTitle = \"cell_prediction\",\n    techType = \"Visium\",\n    ptSizeMax = 1, ptSizeMin = .1\n)\n\n\n\n\n\n\n\n\n\nplist &lt;- list()\nn &lt;- 1\nfor (i in c(\"L2_3_IT\", \"L4\", \"L5_IT\", \"L6_IT\")) {\n    plist[[n]] &lt;- spanielPlot(\n        object = sce.anterior,\n        plotType = \"Cluster\",\n        clusterRes = i,\n        customTitle = i,\n        techType = \"Visium\", ptSize = .3,\n        ptSizeMax = 1, ptSizeMin = .1\n    )\n    n &lt;- n + 1\n}\nwrap_plots(plist, ncol = 2)\n\n\n\n\n\n\n\n\nKeep in mind, that the scores are “just” prediction scores, and do not correspond to proportion of cells that are of a certain celltype or similar. It mainly tell you that gene expression in a certain spot is hihgly similar/dissimilar to gene expression of a celltype. If we look at the scores, we see that some spots got really clear predictions by celltype, while others did not have high scores for any of the celltypes.\nWe can also plot the gene expression and add filters together, too:\n\nspanielPlot(\n    object = sce.anterior,\n    plotType = \"Gene\",\n    gene = \"Wfs1\",\n    showFilter = sce.anterior$L4,\n    customTitle = \"\",\n    techType = \"Visium\",\n    ptSize = 0, ptSizeMin = -.3, ptSizeMax = 1\n)"
  },
  {
    "objectID": "labs/bioc/bioc_08_spatial.html#meta-session",
    "href": "labs/bioc/bioc_08_spatial.html#meta-session",
    "title": " Spatial Transcriptomics",
    "section": "6 Session info",
    "text": "6 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: Etc/UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] patchwork_1.1.2             scater_1.30.1              \n [3] ggplot2_3.4.2               SingleR_2.4.1              \n [5] scran_1.30.0                scuttle_1.12.0             \n [7] dplyr_1.1.2                 Matrix_1.5-4               \n [9] SingleCellExperiment_1.24.0 SummarizedExperiment_1.32.0\n[11] Biobase_2.62.0              GenomicRanges_1.54.1       \n[13] GenomeInfoDb_1.38.5         IRanges_2.36.0             \n[15] S4Vectors_0.40.2            BiocGenerics_0.48.1        \n[17] MatrixGenerics_1.14.0       matrixStats_1.0.0          \n[19] Spaniel_1.16.0             \n\nloaded via a namespace (and not attached):\n  [1] RcppAnnoy_0.0.20          batchelor_1.18.1         \n  [3] splines_4.3.0             later_1.3.1              \n  [5] bitops_1.0-7              R.oo_1.25.0              \n  [7] tibble_3.2.1              polyclip_1.10-4          \n  [9] lifecycle_1.0.3           edgeR_4.0.7              \n [11] globals_0.16.2            lattice_0.21-8           \n [13] MASS_7.3-58.4             magrittr_2.0.3           \n [15] limma_3.58.1              plotly_4.10.2            \n [17] rmarkdown_2.22            yaml_2.3.7               \n [19] metapod_1.10.1            httpuv_1.6.11            \n [21] Seurat_4.3.0              sctransform_0.3.5        \n [23] sp_1.6-1                  spatstat.sparse_3.0-1    \n [25] reticulate_1.30           cowplot_1.1.1            \n [27] pbapply_1.7-0             RColorBrewer_1.1-3       \n [29] ResidualMatrix_1.12.0     abind_1.4-5              \n [31] zlibbioc_1.48.0           Rtsne_0.16               \n [33] R.utils_2.12.2            purrr_1.0.1              \n [35] RCurl_1.98-1.12           GenomeInfoDbData_1.2.11  \n [37] ggrepel_0.9.3             irlba_2.3.5.1            \n [39] listenv_0.9.0             spatstat.utils_3.0-3     \n [41] goftest_1.2-3             spatstat.random_3.1-5    \n [43] dqrng_0.3.0               fitdistrplus_1.1-11      \n [45] parallelly_1.36.0         DelayedMatrixStats_1.24.0\n [47] DropletUtils_1.22.0       leiden_0.4.3             \n [49] codetools_0.2-19          DelayedArray_0.28.0      \n [51] tidyselect_1.2.0          farver_2.1.1             \n [53] viridis_0.6.3             ScaledMatrix_1.10.0      \n [55] spatstat.explore_3.2-1    jsonlite_1.8.5           \n [57] BiocNeighbors_1.20.2      ellipsis_0.3.2           \n [59] progressr_0.13.0          ggridges_0.5.4           \n [61] survival_3.5-5            tools_4.3.0              \n [63] ica_1.0-3                 Rcpp_1.0.10              \n [65] glue_1.6.2                gridExtra_2.3            \n [67] SparseArray_1.2.3         xfun_0.39                \n [69] HDF5Array_1.30.0          withr_2.5.0              \n [71] fastmap_1.1.1             rhdf5filters_1.14.1      \n [73] bluster_1.12.0            fansi_1.0.4              \n [75] digest_0.6.31             rsvd_1.0.5               \n [77] R6_2.5.1                  mime_0.12                \n [79] colorspace_2.1-0          scattermore_1.2          \n [81] tensor_1.5                spatstat.data_3.0-1      \n [83] R.methodsS3_1.8.2         utf8_1.2.3               \n [85] tidyr_1.3.0               generics_0.1.3           \n [87] data.table_1.14.8         httr_1.4.6               \n [89] htmlwidgets_1.6.2         S4Arrays_1.2.0           \n [91] uwot_0.1.14               pkgconfig_2.0.3          \n [93] gtable_0.3.3              lmtest_0.9-40            \n [95] XVector_0.42.0            htmltools_0.5.5          \n [97] SeuratObject_4.1.3        scales_1.2.1             \n [99] png_0.1-8                 knitr_1.43               \n[101] rstudioapi_0.14           reshape2_1.4.4           \n[103] nlme_3.1-162              rhdf5_2.46.1             \n[105] zoo_1.8-12                stringr_1.5.0            \n[107] KernSmooth_2.23-20        parallel_4.3.0           \n[109] miniUI_0.1.1.1            vipor_0.4.5              \n[111] pillar_1.9.0              grid_4.3.0               \n[113] vctrs_0.6.2               RANN_2.6.1               \n[115] promises_1.2.0.1          BiocSingular_1.18.0      \n[117] beachmat_2.18.0           xtable_1.8-4             \n[119] cluster_2.1.4             beeswarm_0.4.0           \n[121] evaluate_0.21             cli_3.6.1                \n[123] locfit_1.5-9.8            compiler_4.3.0           \n[125] rlang_1.1.1               crayon_1.5.2             \n[127] future.apply_1.11.0       labeling_0.4.2           \n[129] plyr_1.8.8                ggbeeswarm_0.7.2         \n[131] stringi_1.7.12            viridisLite_0.4.2        \n[133] deldir_1.0-9              BiocParallel_1.36.0      \n[135] munsell_0.5.0             lazyeval_0.2.2           \n[137] spatstat.geom_3.2-1       sparseMatrixStats_1.14.0 \n[139] future_1.32.0             Rhdf5lib_1.24.1          \n[141] statmod_1.5.0             shiny_1.7.4              \n[143] ROCR_1.0-11               igraph_1.4.3"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html",
    "href": "labs/scanpy/scanpy_01_qc.html",
    "title": " Quality Control",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run Python commands unless it starts with %%bash, in which case, those chunks run shell commands."
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#meta-qc_data",
    "href": "labs/scanpy/scanpy_01_qc.html#meta-qc_data",
    "title": " Quality Control",
    "section": "1 Get data",
    "text": "1 Get data\nIn this tutorial, we will run all tutorials with a set of 8 PBMC 10x datasets from 4 covid-19 patients and 4 healthy controls, the samples have been subsampled to 1500 cells per sample. We can start by defining our paths.\n\nimport os\n\n# download pre-computed annotation\nfetch_annotation = False\n\npath_data = \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\n\npath_covid = \"./data/covid\"\nif not os.path.exists(path_covid):\n    os.makedirs(path_covid, exist_ok=True)\n\npath_results = \"data/covid/results\"\nif not os.path.exists(path_results):\n    os.makedirs(path_results, exist_ok=True)\n\n\nimport urllib.request\n\nfile_list = [\n    \"normal_pbmc_13.h5\", \"normal_pbmc_14.h5\", \"normal_pbmc_19.h5\", \"normal_pbmc_5.h5\",\n    \"ncov_pbmc_15.h5\", \"ncov_pbmc_16.h5\", \"ncov_pbmc_17.h5\", \"ncov_pbmc_1.h5\"\n]\n\nfor i in file_list:\n    path_file = os.path.join(path_covid, i)\n    if not os.path.exists(path_file):\n        file_url = os.path.join(path_data, \"covid\", i)\n        urllib.request.urlretrieve(file_url, path_file)\n\nWith data in place, now we can start loading libraries we will use in this tutorial.\n\nimport numpy as np\nimport pandas as pd\nimport scanpy as sc\nimport warnings\n\nwarnings.simplefilter(action='ignore', category=Warning)\n\n# verbosity: errors (0), warnings (1), info (2), hints (3)\nsc.settings.verbosity = 3\nsc.settings.set_figure_params(dpi=80)\n\nWe can first load the data individually by reading directly from HDF5 file format (.h5).\nIn Scanpy we read them into an Anndata object with the the function read_10x_h5\n\ndata_cov1 = sc.read_10x_h5(os.path.join(path_covid,'ncov_pbmc_1.h5'))\ndata_cov1.var_names_make_unique()\ndata_cov15 = sc.read_10x_h5(os.path.join(path_covid,'ncov_pbmc_15.h5'))\ndata_cov15.var_names_make_unique()\ndata_cov16 = sc.read_10x_h5(os.path.join(path_covid,'ncov_pbmc_16.h5'))\ndata_cov16.var_names_make_unique()\ndata_cov17 = sc.read_10x_h5(os.path.join(path_covid,'ncov_pbmc_17.h5'))\ndata_cov17.var_names_make_unique()\ndata_ctrl5 = sc.read_10x_h5(os.path.join(path_covid,'normal_pbmc_5.h5'))\ndata_ctrl5.var_names_make_unique()\ndata_ctrl13 = sc.read_10x_h5(os.path.join(path_covid,'normal_pbmc_13.h5'))\ndata_ctrl13.var_names_make_unique()\ndata_ctrl14 = sc.read_10x_h5(os.path.join(path_covid,'normal_pbmc_14.h5'))\ndata_ctrl14.var_names_make_unique()\ndata_ctrl19 = sc.read_10x_h5(os.path.join(path_covid,'normal_pbmc_19.h5'))\ndata_ctrl19.var_names_make_unique()\n\nreading ./data/covid/ncov_pbmc_1.h5\n (0:00:00)\nreading ./data/covid/ncov_pbmc_15.h5\n (0:00:00)\nreading ./data/covid/ncov_pbmc_16.h5\n (0:00:00)\nreading ./data/covid/ncov_pbmc_17.h5\n (0:00:00)\nreading ./data/covid/normal_pbmc_5.h5\n (0:00:00)\nreading ./data/covid/normal_pbmc_13.h5\n (0:00:00)\nreading ./data/covid/normal_pbmc_14.h5\n (0:00:00)\nreading ./data/covid/normal_pbmc_19.h5\n (0:00:00)"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#meta-qc_collate",
    "href": "labs/scanpy/scanpy_01_qc.html#meta-qc_collate",
    "title": " Quality Control",
    "section": "2 Collate",
    "text": "2 Collate\n\n\n# add some metadata\ndata_cov1.obs['type']=\"Covid\"\ndata_cov1.obs['sample']=\"covid_1\"\ndata_cov15.obs['type']=\"Covid\"\ndata_cov15.obs['sample']=\"covid_15\"\ndata_cov16.obs['type']=\"Covid\"\ndata_cov16.obs['sample']=\"covid_16\"\ndata_cov17.obs['type']=\"Covid\"\ndata_cov17.obs['sample']=\"covid_17\"\ndata_ctrl5.obs['type']=\"Ctrl\"\ndata_ctrl5.obs['sample']=\"ctrl_5\"\ndata_ctrl13.obs['type']=\"Ctrl\"\ndata_ctrl13.obs['sample']=\"ctrl_13\"\ndata_ctrl14.obs['type']=\"Ctrl\"\ndata_ctrl14.obs['sample']=\"ctrl_14\"\ndata_ctrl19.obs['type']=\"Ctrl\"\ndata_ctrl19.obs['sample']=\"ctrl_19\"\n\n# merge into one object.\nadata = data_cov1.concatenate(data_cov15, data_cov16, data_cov17, data_ctrl5, data_ctrl13, data_ctrl14, data_ctrl19)\n\n# and delete individual datasets to save space\ndel(data_cov1, data_cov15, data_cov16, data_cov17)\ndel(data_ctrl5, data_ctrl13, data_ctrl14, data_ctrl19)\n\nYou can print a summary of the datasets in the Scanpy object, or a summary of the whole object.\n\nprint(adata.obs['sample'].value_counts())\nadata\n\nsample\ncovid_1     1500\ncovid_15    1500\ncovid_16    1500\ncovid_17    1500\nctrl_5      1500\nctrl_13     1500\nctrl_14     1500\nctrl_19     1500\nName: count, dtype: int64\n\n\nAnnData object with n_obs × n_vars = 12000 × 33538\n    obs: 'type', 'sample', 'batch'\n    var: 'gene_ids', 'feature_types', 'genome'"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#meta-qc_calqc",
    "href": "labs/scanpy/scanpy_01_qc.html#meta-qc_calqc",
    "title": " Quality Control",
    "section": "3 Calculate QC",
    "text": "3 Calculate QC\nHaving the data in a suitable format, we can start calculating some quality metrics. We can for example calculate the percentage of mitochondrial and ribosomal genes per cell and add to the metadata. The proportion of hemoglobin genes can give an indication of red blood cell contamination. This will be helpful to visualize them across different metadata parameters (i.e. datasetID and chemistry version). There are several ways of doing this. The QC metrics are finally added to the metadata table.\nCiting from Simple Single Cell workflows (Lun, McCarthy & Marioni, 2017): High proportions are indicative of poor-quality cells (Islam et al. 2014; Ilicic et al. 2016), possibly because of loss of cytoplasmic RNA from perforated cells. The reasoning is that mitochondria are larger than individual transcript molecules and less likely to escape through tears in the cell membrane.\nFirst, let Scanpy calculate some general qc-stats for genes and cells with the function sc.pp.calculate_qc_metrics, similar to calculateQCmetrics() in Scater. It can also calculate proportion of counts for specific gene populations, so first we need to define which genes are mitochondrial, ribosomal and hemoglobin.\n\n# mitochondrial genes\nadata.var['mt'] = adata.var_names.str.startswith('MT-') \n# ribosomal genes\nadata.var['ribo'] = adata.var_names.str.startswith((\"RPS\",\"RPL\"))\n# hemoglobin genes.\nadata.var['hb'] = adata.var_names.str.contains((\"^HB[^(P|E|S)]\"))\n\nadata.var\n\n\n\n\n\n\n\n\ngene_ids\nfeature_types\ngenome\nmt\nribo\nhb\n\n\n\n\nMIR1302-2HG\nENSG00000243485\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\nFAM138A\nENSG00000237613\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\nOR4F5\nENSG00000186092\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\nAL627309.1\nENSG00000238009\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\nAL627309.3\nENSG00000239945\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\nAC233755.2\nENSG00000277856\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\nAC233755.1\nENSG00000275063\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\nAC240274.1\nENSG00000271254\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\nAC213203.1\nENSG00000277475\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\nFAM231C\nENSG00000268674\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\n\n\n33538 rows × 6 columns\n\n\n\n\nsc.pp.calculate_qc_metrics(adata, qc_vars=['mt','ribo','hb'], percent_top=None, log1p=False, inplace=True)\n\nNow you can see that we have additional data in the metadata slot.\nAnother opition to using the calculate_qc_metrics function is to calculate the values on your own and add to a metadata slot. An example for mito genes can be found below:\n\nmito_genes = adata.var_names.str.startswith('MT-')\n# for each cell compute fraction of counts in mito genes vs. all genes\n# the `.A1` is only necessary as X is sparse (to transform to a dense array after summing)\nadata.obs['percent_mt2'] = np.sum(\n    adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1\n# add the total counts per cell as observations-annotation to adata\nadata.obs['n_counts'] = adata.X.sum(axis=1).A1\n\nadata\n\nAnnData object with n_obs × n_vars = 12000 × 33538\n    obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts'\n    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts'"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#meta-qc_plotqc",
    "href": "labs/scanpy/scanpy_01_qc.html#meta-qc_plotqc",
    "title": " Quality Control",
    "section": "4 Plot QC",
    "text": "4 Plot QC\nNow we can plot some of the QC variables as violin plots.\n\nsc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt', 'pct_counts_ribo', 'pct_counts_hb'], jitter=0.4, groupby = 'sample', rotation= 45)\n\n\n\n\n\n\n\n\nAs you can see, there is quite some difference in quality for these samples, with for instance the covid_15 and covid_16 samples having cells with fewer detected genes and more mitochondrial content. As the ribosomal proteins are highly expressed they will make up a larger proportion of the transcriptional landscape when fewer of the lowly expressed genes are detected. We can also plot the different QC-measures as scatter plots.\n\nsc.pl.scatter(adata, x='total_counts', y='pct_counts_mt', color=\"sample\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nPlot additional QC stats that we have calculated as scatter plots. How are the different measures correlated? Can you explain why?"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#meta-qc_filter",
    "href": "labs/scanpy/scanpy_01_qc.html#meta-qc_filter",
    "title": " Quality Control",
    "section": "5 Filtering",
    "text": "5 Filtering\n\n5.1 Detection-based filtering\nA standard approach is to filter cells with low number of reads as well as genes that are present in at least a given number of cells. Here we will only consider cells with at least 200 detected genes and genes need to be expressed in at least 3 cells. Please note that those values are highly dependent on the library preparation method used.\n\nsc.pp.filter_cells(adata, min_genes=200)\nsc.pp.filter_genes(adata, min_cells=3)\n\nprint(adata.n_obs, adata.n_vars)\n\nfiltered out 1336 cells that have less than 200 genes expressed\nfiltered out 14047 genes that are detected in less than 3 cells\n10664 19491\n\n\nExtremely high number of detected genes could indicate doublets. However, depending on the cell type composition in your sample, you may have cells with higher number of genes (and also higher counts) from one cell type. In this case, we will run doublet prediction further down, so we will skip this step now, but the code below is an example of how it can be run:\n\n# skip for now as we are doing doublet prediction\n#keep_v2 = (adata.obs['n_genes_by_counts'] &lt; 2000) & (adata.obs['n_genes_by_counts'] &gt; 500) & (adata.obs['lib_prep'] == 'v2')\n#print(sum(keep_v2))\n\n# filter for gene detection for v3\n#keep_v3 = (adata.obs['n_genes_by_counts'] &lt; 4100) & (adata.obs['n_genes_by_counts'] &gt; 1000) & (adata.obs['lib_prep'] != 'v2')\n#print(sum(keep_v3))\n\n# keep both sets of cells\n#keep = (keep_v2) | (keep_v3)\n#print(sum(keep))\n#adata = adata[keep, :]\n\n#print(\"Remaining cells %d\"%adata.n_obs)\n\nAdditionally, we can also see which genes contribute the most to such reads. We can for instance plot the percentage of counts per gene.\n\nsc.pl.highest_expr_genes(adata, n_top=20)\n\nnormalizing counts per cell\n    finished (0:00:00)\n\n\n\n\n\n\n\n\n\nAs you can see, MALAT1 constitutes up to 30% of the UMIs from a single cell and the other top genes are mitochondrial and ribosomal genes. It is quite common that nuclear lincRNAs have correlation with quality and mitochondrial reads, so high detection of MALAT1 may be a technical issue. Let us assemble some information about such genes, which are important for quality control and downstream filtering.\n\n\n5.2 Mito/Ribo filtering\nWe also have quite a lot of cells with high proportion of mitochondrial and low proportion of ribosomal reads. It would be wise to remove those cells, if we have enough cells left after filtering. Another option would be to either remove all mitochondrial reads from the dataset and hope that the remaining genes still have enough biological signal. A third option would be to just regress out the percent_mito variable during scaling. In this case we had as much as 99.7% mitochondrial reads in some of the cells, so it is quite unlikely that there is much cell type signature left in those. Looking at the plots, make reasonable decisions on where to draw the cutoff. In this case, the bulk of the cells are below 20% mitochondrial reads and that will be used as a cutoff. We will also remove cells with less than 5% ribosomal reads.\n\n# filter for percent mito\nadata = adata[adata.obs['pct_counts_mt'] &lt; 20, :]\n\n# filter for percent ribo &gt; 0.05\nadata = adata[adata.obs['pct_counts_ribo'] &gt; 5, :]\n\nprint(\"Remaining cells %d\"%adata.n_obs)\n\nRemaining cells 7431\n\n\nAs you can see, a large proportion of sample covid_15 is filtered out. Also, there is still quite a lot of variation in percent_mito, so it will have to be dealt with in the data analysis step. We can also notice that the percent_ribo are also highly variable, but that is expected since different cell types have different proportions of ribosomal content, according to their function.\n\n\n5.3 Plot filtered QC\nLets plot the same QC-stats once more.\n\nsc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt','pct_counts_ribo', 'pct_counts_hb'], jitter=0.4, groupby = 'sample', rotation = 45)\n\n\n\n\n\n\n\n\n\n\n5.4 Filter genes\nAs the level of expression of mitochondrial and MALAT1 genes are judged as mainly technical, it can be wise to remove them from the dataset before any further analysis. In this case we will also remove the HB genes.\n\nmalat1 = adata.var_names.str.startswith('MALAT1')\n# we need to redefine the mito_genes since they were first \n# calculated on the full object before removing low expressed genes.\nmito_genes = adata.var_names.str.startswith('MT-')\nhb_genes = adata.var_names.str.contains('^HB[^(P|E|S)]')\n\nremove = np.add(mito_genes, malat1)\nremove = np.add(remove, hb_genes)\nkeep = np.invert(remove)\n\nadata = adata[:,keep]\n\nprint(adata.n_obs, adata.n_vars)\n\n7431 19468"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#meta-qc_sex",
    "href": "labs/scanpy/scanpy_01_qc.html#meta-qc_sex",
    "title": " Quality Control",
    "section": "6 Sample sex",
    "text": "6 Sample sex\nWhen working with human or animal samples, you should ideally constrain your experiments to a single sex to avoid including sex bias in the conclusions. However this may not always be possible. By looking at reads from chromosomeY (males) and XIST (X-inactive specific transcript) expression (mainly female) it is quite easy to determine per sample which sex it is. It can also be a good way to detect if there has been any mislabelling in which case, the sample metadata sex does not agree with the computational predictions.\nTo get choromosome information for all genes, you should ideally parse the information from the gtf file that you used in the mapping pipeline as it has the exact same annotation version/gene naming. However, it may not always be available, as in this case where we have downloaded public data. Hence, we will use biomart to fetch chromosome information.\n\n# requires pybiomart\nif not fetch_annotation:\n    annot = sc.queries.biomart_annotations(\"hsapiens\", [\"ensembl_gene_id\", \"external_gene_name\", \"start_position\", \"end_position\", \"chromosome_name\"], ).set_index(\"external_gene_name\")\n    # adata.var[annot.columns] = annot\n\nNow that we have the chromosome information, we can calculate the proportion of reads that comes from chromosome Y per cell.\n\nchrY_genes = adata.var_names.intersection(annot.index[annot.chromosome_name == \"Y\"])\nchrY_genes\n\nadata.obs['percent_chrY'] = np.sum(\n    adata[:, chrY_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1 * 100\n\nThen plot XIST expression vs chrY proportion. As you can see, the samples are clearly on either side, even if some cells do not have detection of either.\n\n# color inputs must be from either .obs or .var, so add in XIST expression to obs.\nadata.obs[\"XIST-counts\"] = adata.X[:,adata.var_names.str.match('XIST')].toarray()\n\nsc.pl.scatter(adata, x='XIST-counts', y='percent_chrY', color=\"sample\")\n\n\n\n\n\n\n\n\nPlot as violins.\n\nsc.pl.violin(adata, [\"XIST-counts\", \"percent_chrY\"], jitter=0.4, groupby = 'sample', rotation= 45)\n\n\n\n\n\n\n\n\nHere, we can see clearly that we have three males and five females, can you see which samples they are? Do you think this will cause any problems for downstream analysis? Discuss with your group: what would be the best way to deal with this type of sex bias?"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#meta-qc_cellcycle",
    "href": "labs/scanpy/scanpy_01_qc.html#meta-qc_cellcycle",
    "title": " Quality Control",
    "section": "7 Cell cycle state",
    "text": "7 Cell cycle state\nWe here perform cell cycle scoring. To score a gene list, the algorithm calculates the difference of mean expression of the given list and the mean expression of reference genes. To build the reference, the function randomly chooses a bunch of genes matching the distribution of the expression of the given list. Cell cycle scoring adds three slots in the metadata, a score for S phase, a score for G2M phase and the predicted cell cycle phase.\nFirst read the file with cell cycle genes, from Regev lab and split into S and G2M phase genes. We first download the file.\n\npath_file = os.path.join(path_results, 'regev_lab_cell_cycle_genes.txt')\nif not os.path.exists(path_file):\n    urllib.request.urlretrieve(os.path.join(path_data, 'regev_lab_cell_cycle_genes.txt'), path_file)\n\n\ncell_cycle_genes = [x.strip() for x in open('./data/covid/results/regev_lab_cell_cycle_genes.txt')]\nprint(len(cell_cycle_genes))\n\n# Split into 2 lists\ns_genes = cell_cycle_genes[:43]\ng2m_genes = cell_cycle_genes[43:]\n\ncell_cycle_genes = [x for x in cell_cycle_genes if x in adata.var_names]\nprint(len(cell_cycle_genes))\n\n97\n94\n\n\nBefore running cell cycle we have to normalize the data. In the scanpy object, the data slot will be overwritten with the normalized data. So first, save the raw data into the slot raw. Then run normalization, log transformation and scale the data.\n\n# save normalized counts in raw slot.\nadata.raw = adata\n\n# normalize to depth 10 000\nsc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n\n# logaritmize\nsc.pp.log1p(adata)\n\n# scale\nsc.pp.scale(adata)\n\nnormalizing by total count per cell\n    finished (0:00:00): normalized adata.X and added    'n_counts', counts per cell before normalization (adata.obs)\n... as `zero_center=True`, sparse input is densified and may lead to large memory consumption\n\n\nWe here perform cell cycle scoring. The function is actually a wrapper to sc.tl.score_gene_list, which is launched twice, to score separately S and G2M phases. Both sc.tl.score_gene_list and sc.tl.score_cell_cycle_genes are a port from Seurat and are supposed to work in a very similar way. To score a gene list, the algorithm calculates the difference of mean expression of the given list and the mean expression of reference genes. To build the reference, the function randomly chooses a bunch of genes matching the distribution of the expression of the given list. Cell cycle scoring adds three slots in data, a score for S phase, a score for G2M phase and the predicted cell cycle phase.\n\nsc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes)\n\ncalculating cell cycle phase\ncomputing score 'S_score'\nWARNING: genes are not in var_names and ignored: ['MLF1IP']\n    finished: added\n    'S_score', score of gene set (adata.obs).\n    774 total control genes are used. (0:00:00)\ncomputing score 'G2M_score'\nWARNING: genes are not in var_names and ignored: ['FAM64A', 'HN1']\n    finished: added\n    'G2M_score', score of gene set (adata.obs).\n    772 total control genes are used. (0:00:00)\n--&gt;     'phase', cell cycle phase (adata.obs)\n\n\nWe can now create a violin plot for the cell cycle scores as well.\n\nsc.pl.violin(adata, ['S_score', 'G2M_score'], jitter=0.4, groupby = 'sample', rotation=45)\n\n\n\n\n\n\n\n\nIn this case it looks like we only have a few cycling cells in these datasets.\nScanpy does an automatic prediction of cell cycle phase with a default cutoff of the scores at zero. As you can see this does not fit this data very well, so be cautios with using these predictions. Instead we suggest that you look at the scores.\n\nsc.pl.scatter(adata, x='S_score', y='G2M_score', color=\"phase\")"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#meta-qc_doublet",
    "href": "labs/scanpy/scanpy_01_qc.html#meta-qc_doublet",
    "title": " Quality Control",
    "section": "8 Predict doublets",
    "text": "8 Predict doublets\nDoublets/Multiples of cells in the same well/droplet is a common issue in scRNAseq protocols. Especially in droplet-based methods with overloading of cells. In a typical 10x experiment the proportion of doublets is linearly dependent on the amount of loaded cells. As indicated from the Chromium user guide, doublet rates are about as follows:\n\nMost doublet detectors simulates doublets by merging cell counts and predicts doublets as cells that have similar embeddings as the simulated doublets. Most such packages need an assumption about the number/proportion of expected doublets in the dataset. The data you are using is subsampled, but the original datasets contained about 5 000 cells per sample, hence we can assume that they loaded about 9 000 cells and should have a doublet rate at about 4%.\nFor doublet detection, we will use the package Scrublet, so first we need to get the raw counts from adata.raw.X and run scrublet with that matrix. Then we add in the doublet prediction info into our anndata object.\nDoublet prediction should be run for each dataset separately, so first we need to split the adata object into 6 separate objects, one per sample and then run scrublet on each of them.\n\nimport scrublet as scr\n\n# split per batch into new objects.\nbatches = adata.obs['sample'].cat.categories.tolist()\nalldata = {}\nfor batch in batches:\n    tmp = adata[adata.obs['sample'] == batch,]\n    print(batch, \":\", tmp.shape[0], \" cells\")\n    scrub = scr.Scrublet(tmp.raw.X)\n    out = scrub.scrub_doublets(verbose=False, n_prin_comps = 20)\n    alldata[batch] = pd.DataFrame({'doublet_score':out[0],'predicted_doublets':out[1]},index = tmp.obs.index)\n    print(alldata[batch].predicted_doublets.sum(), \" predicted_doublets\")\n\ncovid_1 : 900  cells\n24  predicted_doublets\ncovid_15 : 599  cells\n8  predicted_doublets\ncovid_16 : 373  cells\n3  predicted_doublets\ncovid_17 : 1101  cells\n17  predicted_doublets\nctrl_5 : 1052  cells\n35  predicted_doublets\nctrl_13 : 1173  cells\n52  predicted_doublets\nctrl_14 : 1063  cells\n33  predicted_doublets\nctrl_19 : 1170  cells\n37  predicted_doublets\n\n\n\n# add predictions to the adata object.\nscrub_pred = pd.concat(alldata.values())\nadata.obs['doublet_scores'] = scrub_pred['doublet_score'] \nadata.obs['predicted_doublets'] = scrub_pred['predicted_doublets'] \n\nsum(adata.obs['predicted_doublets'])\n\n209\n\n\nWe should expect that two cells have more detected genes than a single cell, lets check if our predicted doublets also have more detected genes in general.\n\n# add in column with singlet/doublet instead of True/Fals\n%matplotlib inline\n\nadata.obs['doublet_info'] = adata.obs[\"predicted_doublets\"].astype(str)\nsc.pl.violin(adata, 'n_genes_by_counts', jitter=0.4, groupby = 'doublet_info', rotation=45)\n\n\n\n\n\n\n\n\nNow, lets run PCA and UMAP and plot doublet scores onto UMAP to check the doublet predictions.\n\nsc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\nadata = adata[:, adata.var.highly_variable]\nsc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt'])\nsc.pp.scale(adata, max_value=10)\nsc.tl.pca(adata, svd_solver='arpack')\nsc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)\nsc.tl.umap(adata)\nsc.pl.umap(adata, color=['doublet_scores','doublet_info','sample'])\n\nextracting highly variable genes\n    finished (0:00:02)\n--&gt; added\n    'highly_variable', boolean vector (adata.var)\n    'means', float vector (adata.var)\n    'dispersions', float vector (adata.var)\n    'dispersions_norm', float vector (adata.var)\nregressing out ['total_counts', 'pct_counts_mt']\n    finished (0:00:37)\ncomputing PCA\n    on highly variable genes\n    with n_comps=50\n    finished (0:00:07)\ncomputing neighbors\n    using 'X_pca' with n_pcs = 40\n    finished: added to `.uns['neighbors']`\n    `.obsp['distances']`, distances for each pair of neighbors\n    `.obsp['connectivities']`, weighted adjacency matrix (0:00:13)\ncomputing UMAP\n    finished: added\n    'X_umap', UMAP coordinates (adata.obsm) (0:00:09)\n\n\n\n\n\n\n\n\n\nNow, lets remove all predicted doublets from our data.\n\n# also revert back to the raw counts as the main matrix in adata\nadata = adata.raw.to_adata() \n\nadata = adata[adata.obs['doublet_info'] == 'False',:]\nprint(adata.shape)\n\n(7222, 19468)"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#meta-qc_save",
    "href": "labs/scanpy/scanpy_01_qc.html#meta-qc_save",
    "title": " Quality Control",
    "section": "9 Save data",
    "text": "9 Save data\nFinally, lets save the QC-filtered data for further analysis. Create output directory data/covid/results and save data to that folder. This will be used in downstream labs.\n\nadata.write_h5ad('data/covid/results/scanpy_covid_qc.h5ad')"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#meta-session",
    "href": "labs/scanpy/scanpy_01_qc.html#meta-session",
    "title": " Quality Control",
    "section": "10 Session info",
    "text": "10 Session info\n\n\nClick here\n\n\nsc.logging.print_versions()\n\n-----\nanndata     0.10.5.post1\nscanpy      1.9.6\n-----\nPIL                 10.0.0\nannoy               NA\nanyio               NA\nasttokens           NA\nattr                23.1.0\nbabel               2.12.1\nbackcall            0.2.0\ncertifi             2023.11.17\ncffi                1.15.1\ncharset_normalizer  3.1.0\ncolorama            0.4.6\ncomm                0.1.3\ncycler              0.12.1\ncython_runtime      NA\ndateutil            2.8.2\ndebugpy             1.6.7\ndecorator           5.1.1\ndefusedxml          0.7.1\nexceptiongroup      1.2.0\nexecuting           1.2.0\nfastjsonschema      NA\nfuture              0.18.3\ngmpy2               2.1.2\nh5py                3.9.0\nidna                3.4\nigraph              0.10.8\nipykernel           6.23.1\nipython_genutils    0.2.0\njedi                0.18.2\njinja2              3.1.2\njoblib              1.3.2\njson5               NA\njsonpointer         2.0\njsonschema          4.17.3\njupyter_events      0.6.3\njupyter_server      2.6.0\njupyterlab_server   2.22.1\nkiwisolver          1.4.5\nlazy_loader         NA\nleidenalg           0.10.1\nllvmlite            0.41.1\nlouvain             0.8.1\nmarkupsafe          2.1.2\nmatplotlib          3.8.0\nmatplotlib_inline   0.1.6\nmpl_toolkits        NA\nmpmath              1.3.0\nnatsort             8.4.0\nnbformat            5.8.0\nnumba               0.58.1\nnumpy               1.26.3\nopt_einsum          v3.3.0\noverrides           NA\npackaging           23.1\npandas              2.2.0\nparso               0.8.3\npatsy               0.5.6\npexpect             4.8.0\npickleshare         0.7.5\npkg_resources       NA\nplatformdirs        3.5.1\nprometheus_client   NA\nprompt_toolkit      3.0.38\npsutil              5.9.5\nptyprocess          0.7.0\npure_eval           0.2.2\npvectorc            NA\npybiomart           0.2.0\npycparser           2.21\npydev_ipython       NA\npydevconsole        NA\npydevd              2.9.5\npydevd_file_utils   NA\npydevd_plugins      NA\npydevd_tracing      NA\npygments            2.15.1\npynndescent         0.5.11\npyparsing           3.1.1\npyrsistent          NA\npythonjsonlogger    NA\npytz                2023.3\nrequests            2.31.0\nrequests_cache      0.4.13\nrfc3339_validator   0.1.4\nrfc3986_validator   0.1.1\nscipy               1.12.0\nscrublet            NA\nseaborn             0.13.2\nsend2trash          NA\nsession_info        1.0.0\nsix                 1.16.0\nskimage             0.22.0\nsklearn             1.4.0\nsniffio             1.3.0\nsocks               1.7.1\nsparse              0.15.1\nstack_data          0.6.2\nstatsmodels         0.14.1\nsympy               1.12\ntexttable           1.7.0\nthreadpoolctl       3.2.0\ntorch               2.0.0\ntornado             6.3.2\ntqdm                4.65.0\ntraitlets           5.9.0\ntyping_extensions   NA\numap                0.5.5\nurllib3             2.0.2\nwcwidth             0.2.6\nwebsocket           1.5.2\nyaml                6.0\nzmq                 25.0.2\nzoneinfo            NA\nzstandard           0.19.0\n-----\nIPython             8.13.2\njupyter_client      8.2.0\njupyter_core        5.3.0\njupyterlab          4.0.1\nnotebook            6.5.4\n-----\nPython 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) [GCC 11.3.0]\nLinux-5.15.0-92-generic-x86_64-with-glibc2.35\n-----\nSession information updated at 2024-02-06 00:32"
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html",
    "href": "labs/scanpy/scanpy_02_dimred.html",
    "title": " Dimensionality Reduction",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run Python commands unless it starts with %%bash, in which case, those chunks run shell commands."
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#meta-dimred_prep",
    "href": "labs/scanpy/scanpy_02_dimred.html#meta-dimred_prep",
    "title": " Dimensionality Reduction",
    "section": "1 Data preparation",
    "text": "1 Data preparation\nFirst, let’s load all necessary libraries and the QC-filtered dataset from the previous step.\n\nimport numpy as np\nimport pandas as pd\nimport scanpy as sc\nimport matplotlib.pyplot as plt\nimport warnings\nimport os\nimport urllib.request\n\nwarnings.simplefilter(action=\"ignore\", category=Warning)\n\n# verbosity: errors (0), warnings (1), info (2), hints (3)\nsc.settings.verbosity = 3\n# sc.logging.print_versions()\n\nsc.settings.set_figure_params(dpi=80)\n\n\n# download pre-computed data if missing or long compute\nfetch_data = True\n\n# url for source and intermediate data\npath_data = \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\n\npath_results = \"data/covid/results\"\nif not os.path.exists(path_results):\n    os.makedirs(path_results, exist_ok=True)\n\npath_file = \"data/covid/results/scanpy_covid_qc.h5ad\"\n# if fetch_data is false and path_file doesn't exist\n\nif fetch_data and not os.path.exists(path_file):\n    urllib.request.urlretrieve(os.path.join(\n        path_data, 'covid/results/scanpy_covid_qc.h5ad'), path_file)\n\nadata = sc.read_h5ad(path_file)\nadata\n\nAnnData object with n_obs × n_vars = 7222 × 19468\n    obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info'\n    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells'\n    uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'umap'\n    obsm: 'X_pca', 'X_umap'\n    obsp: 'connectivities', 'distances'\n\n\nBefore variable gene selection we need to normalize and log transform the data. Then store the full matrix in the raw slot before doing variable gene selection.\n\n# normalize to depth 10 000\nsc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n\n# log transform\nsc.pp.log1p(adata)\n\n# store normalized counts in the raw slot, \n# we will subset adata.X for variable genes, but want to keep all genes matrix as well.\nadata.raw = adata\n\nadata\n\nnormalizing by total count per cell\n    finished (0:00:00): normalized adata.X and added    'n_counts', counts per cell before normalization (adata.obs)\nWARNING: adata.X seems to be already log-transformed.\n\n\nAnnData object with n_obs × n_vars = 7222 × 19468\n    obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info'\n    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells'\n    uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'umap'\n    obsm: 'X_pca', 'X_umap'\n    obsp: 'connectivities', 'distances'"
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#meta-dimred_fs",
    "href": "labs/scanpy/scanpy_02_dimred.html#meta-dimred_fs",
    "title": " Dimensionality Reduction",
    "section": "2 Feature selection",
    "text": "2 Feature selection\nWe first need to define which features/genes are important in our dataset to distinguish cell types. For this purpose, we need to find genes that are highly variable across cells, which in turn will also provide a good separation of the cell clusters.\n\n# compute variable genes\nsc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\nprint(\"Highly variable genes: %d\"%sum(adata.var.highly_variable))\n\n#plot variable genes\nsc.pl.highly_variable_genes(adata)\n\n# subset for variable genes in the dataset\nadata = adata[:, adata.var['highly_variable']]\n\nextracting highly variable genes\n    finished (0:00:01)\n--&gt; added\n    'highly_variable', boolean vector (adata.var)\n    'means', float vector (adata.var)\n    'dispersions', float vector (adata.var)\n    'dispersions_norm', float vector (adata.var)\nHighly variable genes: 2626"
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#meta-dimred_zs",
    "href": "labs/scanpy/scanpy_02_dimred.html#meta-dimred_zs",
    "title": " Dimensionality Reduction",
    "section": "3 Z-score transformation",
    "text": "3 Z-score transformation\nNow that the genes have been selected, we now proceed with PCA. Since each gene has a different expression level, it means that genes with higher expression values will naturally have higher variation that will be captured by PCA. This means that we need to somehow give each gene a similar weight when performing PCA (see below). The common practice is to center and scale each gene before performing PCA. This exact scaling called Z-score normalization is very useful for PCA, clustering and plotting heatmaps. Additionally, we can use regression to remove any unwanted sources of variation from the dataset, such as cell cycle, sequencing depth, percent mitochondria etc. This is achieved by doing a generalized linear regression using these parameters as co-variates in the model. Then the residuals of the model are taken as the regressed data. Although perhaps not in the best way, batch effect regression can also be done here. By default, variables are scaled in the PCA step and is not done separately. But it could be achieved by running the commands below:\n\n#run this line if you get the \"AttributeError: swapaxes not found\" \n# adata = adata.copy()\n\n# regress out unwanted variables\nsc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt'])\n\n# scale data, clip values exceeding standard deviation 10.\nsc.pp.scale(adata, max_value=10)\n\nregressing out ['total_counts', 'pct_counts_mt']\n    sparse input is densified and may lead to high memory use\n    finished (0:00:56)"
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#meta-dimred_pca",
    "href": "labs/scanpy/scanpy_02_dimred.html#meta-dimred_pca",
    "title": " Dimensionality Reduction",
    "section": "4 PCA",
    "text": "4 PCA\nPerforming PCA has many useful applications and interpretations, which much depends on the data used. In the case of single-cell data, we want to segregate samples based on gene expression patterns in the data.\nTo run PCA, you can use the function pca().\n\nsc.tl.pca(adata, svd_solver='arpack')\n\ncomputing PCA\n    on highly variable genes\n    with n_comps=50\n    finished (0:00:07)\n\n\nWe then plot the first principal components.\n\n# plot more PCS\nsc.pl.pca(adata, color='sample', components = ['1,2','3,4','5,6','7,8'], ncols=2)\n\n\n\n\n\n\n\n\nTo identify genes that contribute most to each PC, one can retrieve the loading matrix information.\n\n#Plot loadings\nsc.pl.pca_loadings(adata, components=[1,2,3,4,5,6,7,8])\n\n# OBS! only plots the positive axes genes from each PC!!\n\n\n\n\n\n\n\n\nThe function to plot loading genes only plots genes on the positive axes. Instead plot as a heatmaps, with genes on both positive and negative side, one per pc, and plot their expression amongst cells ordered by their position along the pc.\n\n# adata.obsm[\"X_pca\"] is the embeddings\n# adata.uns[\"pca\"] is pc variance\n# adata.varm['PCs'] is the loadings\n\ngenes = adata.var['gene_ids']\n\nfor pc in [1,2,3,4]:\n    g = adata.varm['PCs'][:,pc-1]\n    o = np.argsort(g)\n    sel = np.concatenate((o[:10],o[-10:])).tolist()\n    emb = adata.obsm['X_pca'][:,pc-1]\n    # order by position on that pc\n    tempdata = adata[np.argsort(emb),]\n    sc.pl.heatmap(tempdata, var_names = genes[sel].index.tolist(), groupby='predicted_doublets', swap_axes = True, use_raw=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also plot the amount of variance explained by each PC.\n\nsc.pl.pca_variance_ratio(adata, log=True, n_pcs = 50)\n\n\n\n\n\n\n\n\nBased on this plot, we can see that the top 8 PCs retain a lot of information, while other PCs contain progressively less. However, it is still advisable to use more PCs since they might contain information about rare cell types (such as platelets and DCs in this dataset)"
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#meta-dimred_tsne",
    "href": "labs/scanpy/scanpy_02_dimred.html#meta-dimred_tsne",
    "title": " Dimensionality Reduction",
    "section": "5 tSNE",
    "text": "5 tSNE\nWe will now run BH-tSNE.\n\nsc.tl.tsne(adata, n_pcs = 30)\n\ncomputing tSNE\n    using 'X_pca' with n_pcs = 30\n    using sklearn.manifold.TSNE\n    finished: added\n    'X_tsne', tSNE coordinates (adata.obsm) (0:00:23)\n\n\nWe plot the tSNE scatterplot colored by dataset. We can clearly see the effect of batches present in the dataset.\n\nsc.pl.tsne(adata, color='sample')"
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#meta-dimred_umap",
    "href": "labs/scanpy/scanpy_02_dimred.html#meta-dimred_umap",
    "title": " Dimensionality Reduction",
    "section": "6 UMAP",
    "text": "6 UMAP\nThe UMAP implementation in SCANPY uses a neighborhood graph as the distance matrix, so we need to first calculate the graph.\n\nsc.pp.neighbors(adata, n_pcs = 30, n_neighbors = 20)\n\ncomputing neighbors\n    using 'X_pca' with n_pcs = 30\n    finished: added to `.uns['neighbors']`\n    `.obsp['distances']`, distances for each pair of neighbors\n    `.obsp['connectivities']`, weighted adjacency matrix (0:00:14)\n\n\nWe can now run UMAP for cell embeddings.\n\nsc.tl.umap(adata)\nsc.pl.umap(adata, color='sample')\n\ncomputing UMAP\n    finished: added\n    'X_umap', UMAP coordinates (adata.obsm) (0:00:13)\n\n\n\n\n\n\n\n\n\nUMAP is plotted colored per dataset. Although less distinct as in the tSNE, we still see quite an effect of the different batches in the data.\n\n# run with 10 components, save to a new object so that the umap with 2D is not overwritten.\numap10 = sc.tl.umap(adata, n_components=10, copy=True)\nfig, axs = plt.subplots(1, 3, figsize=(10, 4), constrained_layout=True)\n\nsc.pl.umap(adata, color='sample',  title=\"UMAP\",\n           show=False, ax=axs[0], legend_loc=None)\nsc.pl.umap(umap10, color='sample', title=\"UMAP10\", show=False,\n           ax=axs[1], components=['1,2'], legend_loc=None)\nsc.pl.umap(umap10, color='sample', title=\"UMAP10\",\n           show=False, ax=axs[2], components=['3,4'], legend_loc=None)\n\n# we can also plot the umap with neighbor edges\nsc.pl.umap(adata, color='sample', title=\"UMAP\", edges=True)\n\ncomputing UMAP\n    finished: added\n    'X_umap', UMAP coordinates (adata.obsm) (0:00:16)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can now plot PCA, UMAP and tSNE side by side for comparison. Have a look at the UMAP and tSNE. What similarities/differences do you see? Can you explain the differences based on what you learned during the lecture? Also, we can conclude from the dimensionality reductions that our dataset contains a batch effect that needs to be corrected before proceeding to clustering and differential gene expression analysis.\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 8), constrained_layout=True)\nsc.pl.pca(adata, color='sample', components=['1,2'], ax=axs[0, 0], show=False)\nsc.pl.tsne(adata, color='sample', components=['1,2'], ax=axs[0, 1], show=False)\nsc.pl.umap(adata, color='sample', components=['1,2'], ax=axs[1, 0], show=False)\n\n&lt;Axes: title={'center': 'sample'}, xlabel='UMAP1', ylabel='UMAP2'&gt;\n\n\n\n\n\n\n\n\n\nFinally, we can compare the PCA, tSNE and UMAP.\n\n\n\n\n\n\nDiscuss\n\n\n\nWe have now done Variable gene selection, PCA and UMAP with the settings we selected for you. Test a few different ways of selecting variable genes, number of PCs for UMAP and check how it influences your embedding."
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#meta-dimred_plotgenes",
    "href": "labs/scanpy/scanpy_02_dimred.html#meta-dimred_plotgenes",
    "title": " Dimensionality Reduction",
    "section": "7 Genes of interest",
    "text": "7 Genes of interest\nLet’s plot some marker genes for different cell types onto the embedding.\n\n\n\nMarkers\nCell Type\n\n\n\n\nCD3E\nT cells\n\n\nCD3E CD4\nCD4+ T cells\n\n\nCD3E CD8A\nCD8+ T cells\n\n\nGNLY, NKG7\nNK cells\n\n\nMS4A1\nB cells\n\n\nCD14, LYZ, CST3, MS4A7\nCD14+ Monocytes\n\n\nFCGR3A, LYZ, CST3, MS4A7\nFCGR3A+ Monocytes\n\n\nFCER1A, CST3\nDCs\n\n\n\n\nsc.pl.umap(adata, color=[\"CD3E\", \"CD4\", \"CD8A\", \"GNLY\",\"NKG7\", \"MS4A1\",\"CD14\",\"LYZ\",\"CST3\",\"MS4A7\",\"FCGR3A\"])\n\n\n\n\n\n\n\n\nThe default is to plot gene expression in the normalized and log-transformed data. You can also plot it on the scaled and corrected data by using use_raw=False. However, not all of these genes are included in the variable gene set so we first need to filter them.\n\ngenes  = [\"CD3E\", \"CD4\", \"CD8A\", \"GNLY\",\"NKG7\", \"MS4A1\",\"CD14\",\"LYZ\",\"CST3\",\"MS4A7\",\"FCGR3A\"]\nvar_genes = adata.var.highly_variable\nvar_genes.index[var_genes]\nvarg = [x for x in genes if x in var_genes.index[var_genes]]\nsc.pl.umap(adata, color=varg, use_raw=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nSelect some of your dimensionality reductions and plot some of the QC stats that were calculated in the previous lab. Can you see if some of the separation in your data is driven by quality of the cells?"
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#meta-dimred_save",
    "href": "labs/scanpy/scanpy_02_dimred.html#meta-dimred_save",
    "title": " Dimensionality Reduction",
    "section": "8 Save data",
    "text": "8 Save data\nWe can finally save the object for use in future steps.\n\nadata.write_h5ad('data/covid/results/scanpy_covid_qc_dr.h5ad')\n\nJust a reminder, you need to keep in mind what you have in the X matrix. After these operations you have an X matrix with only variable genes, that are normalized, logtransformed and scaled.\nWe stored the expression of all genes in raw.X after doing lognormalization so that matrix is a sparse matrix with logtransformed values.\n\nprint(adata.X.shape)\nprint(adata.raw.X.shape)\n\nprint(adata.X[:3,:3])\nprint(adata.raw.X[:10,:10])\n\n(7222, 2626)\n(7222, 19468)\n[[-0.16998859 -0.06050171 -0.08070081]\n [-0.19315341 -0.09975121 -0.31379319]\n [-0.2051203  -0.11680799 -0.43194618]]\n  (1, 4)    0.7825693876867097\n  (8, 7)    1.1311041336746985"
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#meta-session",
    "href": "labs/scanpy/scanpy_02_dimred.html#meta-session",
    "title": " Dimensionality Reduction",
    "section": "9 Session info",
    "text": "9 Session info\n\n\nClick here\n\n\nsc.logging.print_versions()\n\n-----\nanndata     0.10.5.post1\nscanpy      1.9.6\n-----\nPIL                 10.0.0\nanyio               NA\nasttokens           NA\nattr                23.1.0\nbabel               2.12.1\nbackcall            0.2.0\ncertifi             2023.11.17\ncffi                1.15.1\ncharset_normalizer  3.1.0\ncolorama            0.4.6\ncomm                0.1.3\ncycler              0.12.1\ncython_runtime      NA\ndateutil            2.8.2\ndebugpy             1.6.7\ndecorator           5.1.1\ndefusedxml          0.7.1\nexceptiongroup      1.2.0\nexecuting           1.2.0\nfastjsonschema      NA\ngmpy2               2.1.2\nh5py                3.9.0\nidna                3.4\nigraph              0.10.8\nipykernel           6.23.1\nipython_genutils    0.2.0\njedi                0.18.2\njinja2              3.1.2\njoblib              1.3.2\njson5               NA\njsonpointer         2.0\njsonschema          4.17.3\njupyter_events      0.6.3\njupyter_server      2.6.0\njupyterlab_server   2.22.1\nkiwisolver          1.4.5\nleidenalg           0.10.1\nllvmlite            0.41.1\nlouvain             0.8.1\nmarkupsafe          2.1.2\nmatplotlib          3.8.0\nmatplotlib_inline   0.1.6\nmpl_toolkits        NA\nmpmath              1.3.0\nnatsort             8.4.0\nnbformat            5.8.0\nnetworkx            3.2.1\nnumba               0.58.1\nnumpy               1.26.3\nopt_einsum          v3.3.0\noverrides           NA\npackaging           23.1\npandas              2.2.0\nparso               0.8.3\npatsy               0.5.6\npexpect             4.8.0\npickleshare         0.7.5\npkg_resources       NA\nplatformdirs        3.5.1\nprometheus_client   NA\nprompt_toolkit      3.0.38\npsutil              5.9.5\nptyprocess          0.7.0\npure_eval           0.2.2\npvectorc            NA\npycparser           2.21\npydev_ipython       NA\npydevconsole        NA\npydevd              2.9.5\npydevd_file_utils   NA\npydevd_plugins      NA\npydevd_tracing      NA\npygments            2.15.1\npynndescent         0.5.11\npyparsing           3.1.1\npyrsistent          NA\npythonjsonlogger    NA\npytz                2023.3\nrequests            2.31.0\nrfc3339_validator   0.1.4\nrfc3986_validator   0.1.1\nscipy               1.12.0\nsend2trash          NA\nsession_info        1.0.0\nsix                 1.16.0\nsklearn             1.4.0\nsniffio             1.3.0\nsocks               1.7.1\nsparse              0.15.1\nstack_data          0.6.2\nstatsmodels         0.14.1\nsympy               1.12\ntexttable           1.7.0\nthreadpoolctl       3.2.0\ntorch               2.0.0\ntornado             6.3.2\ntqdm                4.65.0\ntraitlets           5.9.0\ntyping_extensions   NA\numap                0.5.5\nurllib3             2.0.2\nwcwidth             0.2.6\nwebsocket           1.5.2\nyaml                6.0\nzmq                 25.0.2\nzoneinfo            NA\nzstandard           0.19.0\n-----\nIPython             8.13.2\njupyter_client      8.2.0\njupyter_core        5.3.0\njupyterlab          4.0.1\nnotebook            6.5.4\n-----\nPython 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) [GCC 11.3.0]\nLinux-5.15.0-92-generic-x86_64-with-glibc2.35\n-----\nSession information updated at 2024-02-06 00:35"
  },
  {
    "objectID": "labs/scanpy/scanpy_03_integration.html",
    "href": "labs/scanpy/scanpy_03_integration.html",
    "title": " Data Integration",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run Python commands unless it starts with %%bash, in which case, those chunks run shell commands.\nIn this tutorial we will look at different ways of integrating multiple single cell RNA-seq datasets. We will explore a few different methods to correct for batch effects across datasets. Seurat uses the data integration method presented in Comprehensive Integration of Single Cell Data, while Scran and Scanpy use a mutual Nearest neighbour method (MNN). Below you can find a list of some methods for single data integration:"
  },
  {
    "objectID": "labs/scanpy/scanpy_03_integration.html#meta-int_prep",
    "href": "labs/scanpy/scanpy_03_integration.html#meta-int_prep",
    "title": " Data Integration",
    "section": "1 Data preparation",
    "text": "1 Data preparation\nLet’s first load necessary libraries and the data saved in the previous lab.\n\nimport numpy as np\nimport pandas as pd\nimport scanpy as sc\nimport matplotlib.pyplot as plt\nimport warnings\nimport os\nimport urllib.request\n\nwarnings.simplefilter(action='ignore', category=Warning)\n\n# verbosity: errors (0), warnings (1), info (2), hints (3)\nsc.settings.verbosity = 3             \n\nsc.settings.set_figure_params(dpi=80)\n%matplotlib inline\n\nCreate individual adata objects per batch.\n\n# download pre-computed data if missing or long compute\nfetch_data = True\n\n# url for source and intermediate data\npath_data = \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\n\npath_results = \"data/covid/results\"\nif not os.path.exists(path_results):\n    os.makedirs(path_results, exist_ok=True)\n\npath_file = \"data/covid/results/scanpy_covid_qc_dr.h5ad\"\nif fetch_data and not os.path.exists(path_file):\n    urllib.request.urlretrieve(os.path.join(\n        path_data, 'covid/results/scanpy_covid_qc_dr.h5ad'), path_file)\n\nadata = sc.read_h5ad(path_file)\nadata\n\nAnnData object with n_obs × n_vars = 7222 × 2626\n    obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info'\n    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'\n    uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap'\n    obsm: 'X_pca', 'X_tsne', 'X_umap'\n    varm: 'PCs'\n    obsp: 'connectivities', 'distances'\n\n\n\nprint(adata.X.shape)\n\n(7222, 2626)\n\n\nAs the stored AnnData object contains scaled data based on variable genes, we need to make a new object with the logtransformed normalized counts. The new variable gene selection should not be performed on the scaled data matrix.\n\nadata2 = adata.raw.to_adata() \n\n# in some versions of Anndata there is an issue with information on the logtransformation in the slot log1p.base so we set it to None to not get errors.\nadata2.uns['log1p']['base']=None\n\n# check that the matrix looks like normalized counts\nprint(adata2.X[1:10,1:10])\n\n  (0, 3)    0.7825693876867097\n  (7, 6)    1.1311041336746985"
  },
  {
    "objectID": "labs/scanpy/scanpy_03_integration.html#detect-variable-genes",
    "href": "labs/scanpy/scanpy_03_integration.html#detect-variable-genes",
    "title": " Data Integration",
    "section": "2 Detect variable genes",
    "text": "2 Detect variable genes\nVariable genes can be detected across the full dataset, but then we run the risk of getting many batch-specific genes that will drive a lot of the variation. Or we can select variable genes from each batch separately to get only celltype variation. In the dimensionality reduction exercise, we already selected variable genes, so they are already stored in adata.var.highly_variable.\n\nvar_genes_all = adata.var.highly_variable\n\nprint(\"Highly variable genes: %d\"%sum(var_genes_all))\n\nHighly variable genes: 2626\n\n\nDetect variable genes in each dataset separately using the batch_key parameter.\n\nsc.pp.highly_variable_genes(adata2, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key = 'sample')\n\nprint(\"Highly variable genes intersection: %d\"%sum(adata2.var.highly_variable_intersection))\n\nprint(\"Number of batches where gene is variable:\")\nprint(adata2.var.highly_variable_nbatches.value_counts())\n\nvar_genes_batch = adata2.var.highly_variable_nbatches &gt; 0\n\nextracting highly variable genes\n    finished (0:00:02)\n--&gt; added\n    'highly_variable', boolean vector (adata.var)\n    'means', float vector (adata.var)\n    'dispersions', float vector (adata.var)\n    'dispersions_norm', float vector (adata.var)\nHighly variable genes intersection: 122\nNumber of batches where gene is variable:\n0    7876\n1    4163\n2    3161\n3    2025\n4    1115\n5     559\n6     277\n7     170\n8     122\nName: highly_variable_nbatches, dtype: int64\n\n\nCompare overlap of variable genes with batches or with all data.\n\nprint(\"Any batch var genes: %d\"%sum(var_genes_batch))\nprint(\"All data var genes: %d\"%sum(var_genes_all))\nprint(\"Overlap: %d\"%sum(var_genes_batch & var_genes_all))\nprint(\"Variable genes in all batches: %d\"%sum(adata2.var.highly_variable_nbatches == 6))\nprint(\"Overlap batch instersection and all: %d\"%sum(var_genes_all & adata2.var.highly_variable_intersection))\n\nAny batch var genes: 11592\nAll data var genes: 2626\nOverlap: 2625\nVariable genes in all batches: 277\nOverlap batch instersection and all: 122\n\n\nSelect all genes that are variable in at least 2 datasets and use for remaining analysis.\n\nvar_select = adata2.var.highly_variable_nbatches &gt; 2\nvar_genes = var_select.index[var_select]\nlen(var_genes)\n\n4268"
  },
  {
    "objectID": "labs/scanpy/scanpy_03_integration.html#bbknn",
    "href": "labs/scanpy/scanpy_03_integration.html#bbknn",
    "title": " Data Integration",
    "section": "3 BBKNN",
    "text": "3 BBKNN\nFirst, we will run BBKNN that is implemented in scanpy.\n\nimport bbknn\nbbknn.bbknn(adata2,batch_key='sample')\n\n# Before calculating a new umap and tsne, we want to store the old one. \nadata2.obsm['X_umap_uncorr'] = adata2.obsm['X_umap']\nadata2.obsm['X_tsne_uncorr'] = adata2.obsm['X_tsne']\n\n\n# then run umap on the integrated space\nsc.tl.umap(adata2)\nsc.tl.tsne(adata2)\n\ncomputing batch balanced neighbors\n    finished: added to `.uns['neighbors']`\n    `.obsp['distances']`, distances for each pair of neighbors\n    `.obsp['connectivities']`, weighted adjacency matrix (0:00:02)\ncomputing UMAP\n    finished: added\n    'X_umap', UMAP coordinates (adata.obsm)\n    'umap', UMAP parameters (adata.uns) (0:00:12)\ncomputing tSNE\n    using 'X_pca' with n_pcs = 50\n    using sklearn.manifold.TSNE\n    finished: added\n    'X_tsne', tSNE coordinates (adata.obsm)\n    'tsne', tSNE parameters (adata.uns) (0:00:14)\n\n\nWe can now plot the unintegrated and the integrated space reduced dimensions.\n\nfig, axs = plt.subplots(2, 2, figsize=(10,8),constrained_layout=True)\nsc.pl.tsne(adata2, color=\"sample\", title=\"BBKNN Corrected tsne\", ax=axs[0,0], show=False)\nsc.pl.tsne(adata, color=\"sample\", title=\"Uncorrected tsne\", ax=axs[0,1], show=False)\nsc.pl.umap(adata2, color=\"sample\", title=\"BBKNN Corrected umap\", ax=axs[1,0], show=False)\nsc.pl.umap(adata, color=\"sample\", title=\"Uncorrected umap\", ax=axs[1,1], show=False)\n\n\n\n\n\n\n\n\nLet’s save the integrated data for further analysis.\n\n# Before calculating a new umap and tsne, we want to store the old one. \nadata2.obsm['X_umap_bbknn'] = adata2.obsm['X_umap']\nadata2.obsm['X_tsne_bbknn'] = adata2.obsm['X_tsne']\n\nsave_file = './data/covid/results/scanpy_covid_qc_dr_bbknn.h5ad'\nadata2.write_h5ad(save_file)"
  },
  {
    "objectID": "labs/scanpy/scanpy_03_integration.html#combat",
    "href": "labs/scanpy/scanpy_03_integration.html#combat",
    "title": " Data Integration",
    "section": "5 Combat",
    "text": "5 Combat\nBatch correction can also be performed with combat. Note that ComBat batch correction requires a dense matrix format as input (which is already the case in this example).\n\n# create a new object with lognormalized counts\nadata_combat = sc.AnnData(X=adata.raw.X, var=adata.raw.var, obs = adata.obs)\n\n# first store the raw data \nadata_combat.raw = adata_combat\n\n# run combat\nsc.pp.combat(adata_combat, key='sample')\n\nStandardizing Data across genes.\n\nFound 8 batches\n\nFound 0 numerical variables:\n    \n\nFound 39 genes with zero variance.\nFitting L/S model and finding priors\n\nFinding parametric adjustments\n\nAdjusting data\n\n\n\nThen we run the regular steps of dimensionality reduction on the combat corrected data. Variable gene selection, pca and umap with combat data.\n\nsc.pp.highly_variable_genes(adata_combat)\nprint(\"Highly variable genes: %d\"%sum(adata_combat.var.highly_variable))\nsc.pl.highly_variable_genes(adata_combat)\n\nsc.pp.pca(adata_combat, n_comps=30, use_highly_variable=True, svd_solver='arpack')\n\nsc.pp.neighbors(adata_combat)\n\nsc.tl.umap(adata_combat)\nsc.tl.tsne(adata_combat)\n\nextracting highly variable genes\n    finished (0:00:01)\n--&gt; added\n    'highly_variable', boolean vector (adata.var)\n    'means', float vector (adata.var)\n    'dispersions', float vector (adata.var)\n    'dispersions_norm', float vector (adata.var)\nHighly variable genes: 3923\n\n\n\n\n\n\n\n\n\ncomputing PCA\n    with n_comps=30\n    finished (0:00:01)\ncomputing neighbors\n    using 'X_pca' with n_pcs = 30\n    finished: added to `.uns['neighbors']`\n    `.obsp['distances']`, distances for each pair of neighbors\n    `.obsp['connectivities']`, weighted adjacency matrix (0:00:00)\ncomputing UMAP\n    finished: added\n    'X_umap', UMAP coordinates (adata.obsm)\n    'umap', UMAP parameters (adata.uns) (0:00:10)\ncomputing tSNE\n    using 'X_pca' with n_pcs = 30\n    using sklearn.manifold.TSNE\n    finished: added\n    'X_tsne', tSNE coordinates (adata.obsm)\n    'tsne', tSNE parameters (adata.uns) (0:00:14)\n\n\n\n# compare var_genes\nvar_genes_combat = adata_combat.var.highly_variable\nprint(\"With all data %d\"%sum(var_genes_all))\nprint(\"With combat %d\"%sum(var_genes_combat))\nprint(\"Overlap %d\"%sum(var_genes_all & var_genes_combat))\n\nprint(\"With 2 batches %d\"%sum(var_select))\nprint(\"Overlap %d\"%sum(var_genes_combat & var_select))\n\nWith all data 2626\nWith combat 3923\nOverlap 1984\nWith 2 batches 4268\nOverlap 2729\n\n\nWe can now plot the unintegrated and the integrated space reduced dimensions.\n\nfig, axs = plt.subplots(2, 2, figsize=(10,8),constrained_layout=True)\nsc.pl.tsne(adata2, color=\"sample\", title=\"Harmony tsne\", ax=axs[0,0], show=False)\nsc.pl.tsne(adata_combat, color=\"sample\", title=\"Combat tsne\", ax=axs[0,1], show=False)\nsc.pl.umap(adata2, color=\"sample\", title=\"Harmony umap\", ax=axs[1,0], show=False)\nsc.pl.umap(adata_combat, color=\"sample\", title=\"Combat umap\", ax=axs[1,1], show=False)\n\n\n\n\n\n\n\n\nLet’s save the integrated data for further analysis.\n\n#save to file\nsave_file = './data/covid/results/scanpy_covid_qc_dr_combat.h5ad'\nadata_combat.write_h5ad(save_file)"
  },
  {
    "objectID": "labs/scanpy/scanpy_03_integration.html#meta-int_scanorama",
    "href": "labs/scanpy/scanpy_03_integration.html#meta-int_scanorama",
    "title": " Data Integration",
    "section": "5 Scanorama",
    "text": "5 Scanorama\nTry out Scanorama for data integration as well. First we need to create individual AnnData objects from each of the datasets.\n\n# split per batch into new objects.\nbatches = adata.obs['sample'].cat.categories.tolist()\nalldata = {}\nfor batch in batches:\n    alldata[batch] = adata2[adata2.obs['sample'] == batch,]\n\nalldata   \n\n{'covid_1': View of AnnData object with n_obs × n_vars = 876 × 19468\n     obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n     uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap'\n     obsm: 'X_pca', 'X_tsne', 'X_umap'\n     obsp: 'connectivities', 'distances',\n 'covid_15': View of AnnData object with n_obs × n_vars = 591 × 19468\n     obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n     uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap'\n     obsm: 'X_pca', 'X_tsne', 'X_umap'\n     obsp: 'connectivities', 'distances',\n 'covid_16': View of AnnData object with n_obs × n_vars = 370 × 19468\n     obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n     uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap'\n     obsm: 'X_pca', 'X_tsne', 'X_umap'\n     obsp: 'connectivities', 'distances',\n 'covid_17': View of AnnData object with n_obs × n_vars = 1084 × 19468\n     obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n     uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap'\n     obsm: 'X_pca', 'X_tsne', 'X_umap'\n     obsp: 'connectivities', 'distances',\n 'ctrl_5': View of AnnData object with n_obs × n_vars = 1017 × 19468\n     obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n     uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap'\n     obsm: 'X_pca', 'X_tsne', 'X_umap'\n     obsp: 'connectivities', 'distances',\n 'ctrl_13': View of AnnData object with n_obs × n_vars = 1121 × 19468\n     obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n     uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap'\n     obsm: 'X_pca', 'X_tsne', 'X_umap'\n     obsp: 'connectivities', 'distances',\n 'ctrl_14': View of AnnData object with n_obs × n_vars = 1030 × 19468\n     obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n     uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap'\n     obsm: 'X_pca', 'X_tsne', 'X_umap'\n     obsp: 'connectivities', 'distances',\n 'ctrl_19': View of AnnData object with n_obs × n_vars = 1133 × 19468\n     obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n     uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap'\n     obsm: 'X_pca', 'X_tsne', 'X_umap'\n     obsp: 'connectivities', 'distances'}\n\n\n\nimport scanorama\n\n#subset the individual dataset to the variable genes we defined at the beginning\nalldata2 = dict()\nfor ds in alldata.keys():\n    print(ds)\n    alldata2[ds] = alldata[ds][:,var_genes]\n\n#convert to list of AnnData objects\nadatas = list(alldata2.values())\n\n# run scanorama.integrate\nscanorama.integrate_scanpy(adatas, dimred = 50)\n\ncovid_1\ncovid_15\ncovid_16\ncovid_17\nctrl_5\nctrl_13\nctrl_14\nctrl_19\nFound 4268 genes among all datasets\n[[0.         0.50761421 0.52972973 0.26845018 0.59488692 0.48401826\n  0.36757991 0.09973522]\n [0.         0.         0.81891892 0.33840948 0.43362832 0.23181049\n  0.29949239 0.17597293]\n [0.         0.         0.         0.22702703 0.49459459 0.52972973\n  0.42702703 0.3       ]\n [0.         0.         0.         0.         0.27138643 0.09132841\n  0.1300738  0.17387467]\n [0.         0.         0.         0.         0.         0.8446411\n  0.73647984 0.25419241]\n [0.         0.         0.         0.         0.         0.\n  0.82815534 0.44836717]\n [0.         0.         0.         0.         0.         0.\n  0.         0.78022948]\n [0.         0.         0.         0.         0.         0.\n  0.         0.        ]]\nProcessing datasets (4, 5)\nProcessing datasets (5, 6)\nProcessing datasets (1, 2)\nProcessing datasets (6, 7)\nProcessing datasets (4, 6)\nProcessing datasets (0, 4)\nProcessing datasets (2, 5)\nProcessing datasets (0, 2)\nProcessing datasets (0, 1)\nProcessing datasets (2, 4)\nProcessing datasets (0, 5)\nProcessing datasets (5, 7)\nProcessing datasets (1, 4)\nProcessing datasets (2, 6)\nProcessing datasets (0, 6)\nProcessing datasets (1, 3)\nProcessing datasets (2, 7)\nProcessing datasets (1, 6)\nProcessing datasets (3, 4)\nProcessing datasets (0, 3)\nProcessing datasets (4, 7)\nProcessing datasets (1, 5)\nProcessing datasets (2, 3)\nProcessing datasets (1, 7)\nProcessing datasets (3, 7)\nProcessing datasets (3, 6)\n\n\n\n#scanorama adds the corrected matrix to adata.obsm in each of the datasets in adatas.\nadatas[0].obsm['X_scanorama'].shape\n\n(876, 50)\n\n\n\n# Get all the integrated matrices.\nscanorama_int = [ad.obsm['X_scanorama'] for ad in adatas]\n\n# make into one matrix.\nall_s = np.concatenate(scanorama_int)\nprint(all_s.shape)\n\n# add to the AnnData object, create a new object first\nadata_sc = adata.copy()\nadata_sc.obsm[\"Scanorama\"] = all_s\n\n(7222, 50)\n\n\n\n# tsne and umap\nsc.pp.neighbors(adata_sc, n_pcs =30, use_rep = \"Scanorama\")\nsc.tl.umap(adata_sc)\nsc.tl.tsne(adata_sc, n_pcs = 30, use_rep = \"Scanorama\")\n\ncomputing neighbors\n    finished: added to `.uns['neighbors']`\n    `.obsp['distances']`, distances for each pair of neighbors\n    `.obsp['connectivities']`, weighted adjacency matrix (0:00:01)\ncomputing UMAP\n    finished: added\n    'X_umap', UMAP coordinates (adata.obsm) (0:00:10)\ncomputing tSNE\n    using sklearn.manifold.TSNE\n    finished: added\n    'X_tsne', tSNE coordinates (adata.obsm) (0:00:22)\n\n\nWe can now plot the unintegrated and the integrated space reduced dimensions.\n\nfig, axs = plt.subplots(2, 2, figsize=(10,8),constrained_layout=True)\nsc.pl.tsne(adata2, color=\"sample\", title=\"BBKNN tsne\", ax=axs[0,0], show=False)\nsc.pl.tsne(adata_sc, color=\"sample\", title=\"Scanorama tsne\", ax=axs[0,1], show=False)\nsc.pl.umap(adata2, color=\"sample\", title=\"BBKNN umap\", ax=axs[1,0], show=False)\nsc.pl.umap(adata_sc, color=\"sample\", title=\"Scanorama umap\", ax=axs[1,1], show=False)\n\n&lt;Axes: title={'center': 'Scanorama umap'}, xlabel='UMAP1', ylabel='UMAP2'&gt;\n\n\n\n\n\n\n\n\n\nLet’s save the integrated data for further analysis.\n\n#save to file\nsave_file = './data/covid/results/scanpy_covid_qc_dr_scanorama.h5ad'\nadata_sc.write_h5ad(save_file)"
  },
  {
    "objectID": "labs/scanpy/scanpy_03_integration.html#overview-all-methods",
    "href": "labs/scanpy/scanpy_03_integration.html#overview-all-methods",
    "title": " Data Integration",
    "section": "7 Overview all methods",
    "text": "7 Overview all methods\nNow we will plot UMAPS with all three integration methods side by side.\n\nfig, axs = plt.subplots(2, 3, figsize=(10,8),constrained_layout=True)\nsc.pl.umap(adata, color=\"sample\", title=\"Uncorrected\", ax=axs[0,0], show=False)\nsc.pl.embedding(adata2, 'X_umap_bbknn', color=\"sample\", title=\"BBKNN\", ax=axs[0,1], show=False)\nsc.pl.umap(adata_combat, color=\"sample\", title=\"Combat\", ax=axs[0,2], show=False)\nsc.pl.embedding(adata2, 'X_umap_harmony', color=\"sample\", title=\"Harmony\", ax=axs[1,0], show=False)\nsc.pl.umap(adata2, color=\"sample\", title=\"Scanorama\", ax=axs[1,1], show=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nLook at the different integration results, which one do you think looks the best? How would you motivate selecting one method over the other? How do you think you could best evaluate if the integration worked well?"
  },
  {
    "objectID": "labs/scanpy/scanpy_03_integration.html#extra-task",
    "href": "labs/scanpy/scanpy_03_integration.html#extra-task",
    "title": " Data Integration",
    "section": "8 Extra task",
    "text": "8 Extra task\nHave a look at the documentation for BBKNN\nTry changing some of the parameteres in BBKNN, such as distance metric, number of PCs and number of neighbors. How does the results change with different parameters? Can you explain why?"
  },
  {
    "objectID": "labs/scanpy/scanpy_03_integration.html#meta-session",
    "href": "labs/scanpy/scanpy_03_integration.html#meta-session",
    "title": " Data Integration",
    "section": "8 Session info",
    "text": "8 Session info\n\n\nClick here\n\n\nsc.logging.print_versions()\n\n-----\nanndata     0.10.5.post1\nscanpy      1.9.6\n-----\nPIL                 10.0.0\nannoy               NA\nanyio               NA\nasttokens           NA\nattr                23.1.0\nbabel               2.12.1\nbackcall            0.2.0\nbbknn               1.6.0\ncertifi             2023.11.17\ncffi                1.15.1\ncharset_normalizer  3.1.0\ncolorama            0.4.6\ncomm                0.1.3\ncycler              0.12.1\ncython_runtime      NA\ndateutil            2.8.2\ndebugpy             1.6.7\ndecorator           5.1.1\ndefusedxml          0.7.1\nexceptiongroup      1.2.0\nexecuting           1.2.0\nfastjsonschema      NA\nfbpca               NA\ngmpy2               2.1.2\nh5py                3.9.0\nidna                3.4\nigraph              0.10.8\nintervaltree        NA\nipykernel           6.23.1\nipython_genutils    0.2.0\njedi                0.18.2\njinja2              3.1.2\njoblib              1.3.2\njson5               NA\njsonpointer         2.0\njsonschema          4.17.3\njupyter_events      0.6.3\njupyter_server      2.6.0\njupyterlab_server   2.22.1\nkiwisolver          1.4.5\nleidenalg           0.10.1\nllvmlite            0.41.1\nlouvain             0.8.1\nmarkupsafe          2.1.2\nmatplotlib          3.8.0\nmatplotlib_inline   0.1.6\nmpl_toolkits        NA\nmpmath              1.3.0\nnatsort             8.4.0\nnbformat            5.8.0\nnumba               0.58.1\nnumpy               1.26.3\nopt_einsum          v3.3.0\noverrides           NA\npackaging           23.1\npandas              2.2.0\nparso               0.8.3\npatsy               0.5.6\npexpect             4.8.0\npickleshare         0.7.5\npkg_resources       NA\nplatformdirs        3.5.1\nprometheus_client   NA\nprompt_toolkit      3.0.38\npsutil              5.9.5\nptyprocess          0.7.0\npure_eval           0.2.2\npvectorc            NA\npycparser           2.21\npydev_ipython       NA\npydevconsole        NA\npydevd              2.9.5\npydevd_file_utils   NA\npydevd_plugins      NA\npydevd_tracing      NA\npygments            2.15.1\npynndescent         0.5.11\npyparsing           3.1.1\npyrsistent          NA\npythonjsonlogger    NA\npytz                2023.3\nrequests            2.31.0\nrfc3339_validator   0.1.4\nrfc3986_validator   0.1.1\nscanorama           1.7.4\nscipy               1.12.0\nsend2trash          NA\nsession_info        1.0.0\nsix                 1.16.0\nsklearn             1.4.0\nsniffio             1.3.0\nsocks               1.7.1\nsortedcontainers    2.4.0\nsparse              0.15.1\nstack_data          0.6.2\nsympy               1.12\ntexttable           1.7.0\nthreadpoolctl       3.2.0\ntorch               2.0.0\ntornado             6.3.2\ntqdm                4.65.0\ntraitlets           5.9.0\ntyping_extensions   NA\numap                0.5.5\nurllib3             2.0.2\nwcwidth             0.2.6\nwebsocket           1.5.2\nyaml                6.0\nzmq                 25.0.2\nzoneinfo            NA\nzstandard           0.19.0\n-----\nIPython             8.13.2\njupyter_client      8.2.0\njupyter_core        5.3.0\njupyterlab          4.0.1\nnotebook            6.5.4\n-----\nPython 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) [GCC 11.3.0]\nLinux-5.15.0-92-generic-x86_64-with-glibc2.35\n-----\nSession information updated at 2024-02-06 00:38"
  },
  {
    "objectID": "labs/scanpy/scanpy_04_clustering.html",
    "href": "labs/scanpy/scanpy_04_clustering.html",
    "title": " Clustering",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run Python commands unless it starts with %%bash, in which case, those chunks run shell commands.\nIn this tutorial we will continue the analysis of the integrated dataset. We will use the scanpy enbedding to perform the clustering using graph community detection algorithms.\nLet’s first load all necessary libraries and also the integrated dataset from the previous step.\nimport numpy as np\nimport pandas as pd\nimport scanpy as sc\nimport matplotlib.pyplot as plt\nimport warnings\nimport os\nimport urllib.request\n\nwarnings.simplefilter(action=\"ignore\", category=Warning)\n\n# verbosity: errors (0), warnings (1), info (2), hints (3)\nsc.settings.verbosity = 3\nsc.settings.set_figure_params(dpi=80)\n# download pre-computed data if missing or long compute\nfetch_data = True\n\n# url for source and intermediate data\npath_data = \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\n\npath_results = \"data/covid/results\"\nif not os.path.exists(path_results):\n    os.makedirs(path_results, exist_ok=True)\n\npath_file = \"data/covid/results/scanpy_covid_qc_dr_scanorama.h5ad\"\nif fetch_data and not os.path.exists(path_file):\n    urllib.request.urlretrieve(os.path.join(\n        path_data, 'covid/results/scanpy_covid_qc_dr_scanorama.h5ad'), path_file)\n\nadata = sc.read_h5ad(path_file)\nadata\n\nAnnData object with n_obs × n_vars = 7222 × 19468\n    obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info', 'leiden'\n    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n    uns: 'doublet_info_colors', 'hvg', 'leiden', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap'\n    obsm: 'Scanorama', 'X_pca', 'X_pca_harmony', 'X_tsne', 'X_tsne_bbknn', 'X_tsne_harmony', 'X_tsne_scanorama', 'X_tsne_uncorr', 'X_umap', 'X_umap_bbknn', 'X_umap_harmony', 'X_umap_scanorama', 'X_umap_uncorr'\n    obsp: 'connectivities', 'distances'"
  },
  {
    "objectID": "labs/scanpy/scanpy_04_clustering.html#meta-clust_graphclust",
    "href": "labs/scanpy/scanpy_04_clustering.html#meta-clust_graphclust",
    "title": " Clustering",
    "section": "1 Graph clustering",
    "text": "1 Graph clustering\nThe procedure of clustering on a Graph can be generalized as 3 main steps:\n- Build a kNN graph from the data.\n- Prune spurious connections from kNN graph (optional step). This is a SNN graph.\n- Find groups of cells that maximizes the connections within the group compared other groups.\nIf you recall from the integration, we already constructed a knn graph before running UMAP. Hence we do not need to do it again, and can run the community detection right away.\nThe modularity optimization algoritm in Scanpy are Leiden and Louvain. Lets test both and see how they compare.\n\n1.1 Leiden\n\nsc.tl.leiden(adata, key_added = \"leiden_1.0\") # default resolution in 1.0\nsc.tl.leiden(adata, resolution = 0.6, key_added = \"leiden_0.6\")\nsc.tl.leiden(adata, resolution = 0.4, key_added = \"leiden_0.4\")\nsc.tl.leiden(adata, resolution = 1.4, key_added = \"leiden_1.4\")\n\nrunning Leiden clustering\n    finished: found 20 clusters and added\n    'leiden_1.0', the cluster labels (adata.obs, categorical) (0:00:02)\nrunning Leiden clustering\n    finished: found 16 clusters and added\n    'leiden_0.6', the cluster labels (adata.obs, categorical) (0:00:01)\nrunning Leiden clustering\n    finished: found 13 clusters and added\n    'leiden_0.4', the cluster labels (adata.obs, categorical) (0:00:01)\nrunning Leiden clustering\n    finished: found 23 clusters and added\n    'leiden_1.4', the cluster labels (adata.obs, categorical) (0:00:02)\n\n\nPlot the clusters, as you can see, with increased resolution, we get higher granularity in the clustering.\n\nsc.pl.umap(adata, color=['leiden_0.4', 'leiden_0.6', 'leiden_1.0','leiden_1.4'])\n\n\n\n\n\n\n\n\nOnce we have done clustering, the relationships between clusters can be calculated as correlation in PCA space and we also visualize some of the marker genes that we used in the Dim Reduction lab onto the clusters.\n\nsc.tl.dendrogram(adata, groupby = \"leiden_0.6\")\nsc.pl.dendrogram(adata, groupby = \"leiden_0.6\")\n\ngenes  = [\"CD3E\", \"CD4\", \"CD8A\", \"GNLY\",\"NKG7\", \"MS4A1\",\"FCGR3A\",\"CD14\",\"LYZ\",\"CST3\",\"MS4A7\",\"FCGR1A\"]\nsc.pl.dotplot(adata, genes, groupby='leiden_0.6', dendrogram=True)\n\n    using 'X_pca' with n_pcs = 50\nStoring dendrogram info using `.uns['dendrogram_leiden_0.6']`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.2 Louvain\n\nsc.tl.louvain(adata, key_added = \"louvain_1.0\") # default resolution in 1.0\nsc.tl.louvain(adata, resolution = 0.6, key_added = \"louvain_0.6\")\nsc.tl.louvain(adata, resolution = 0.4, key_added = \"louvain_0.4\")\nsc.tl.louvain(adata, resolution = 1.4, key_added = \"louvain_1.4\")\n\nsc.pl.umap(adata, color=['louvain_0.4', 'louvain_0.6', 'louvain_1.0','louvain_1.4'])\n\nsc.tl.dendrogram(adata, groupby = \"louvain_0.6\")\nsc.pl.dendrogram(adata, groupby = \"louvain_0.6\")\n\ngenes  = [\"CD3E\", \"CD4\", \"CD8A\", \"GNLY\",\"NKG7\", \"MS4A1\",\"FCGR3A\",\"CD14\",\"LYZ\",\"CST3\",\"MS4A7\",\"FCGR1A\"]\n\nsc.pl.dotplot(adata, genes, groupby='louvain_0.6', dendrogram=True)\n\nrunning Louvain clustering\n    using the \"louvain\" package of Traag (2017)\n    finished: found 15 clusters and added\n    'louvain_1.0', the cluster labels (adata.obs, categorical) (0:00:00)\nrunning Louvain clustering\n    using the \"louvain\" package of Traag (2017)\n    finished: found 11 clusters and added\n    'louvain_0.6', the cluster labels (adata.obs, categorical) (0:00:00)\nrunning Louvain clustering\n    using the \"louvain\" package of Traag (2017)\n    finished: found 8 clusters and added\n    'louvain_0.4', the cluster labels (adata.obs, categorical) (0:00:00)\nrunning Louvain clustering\n    using the \"louvain\" package of Traag (2017)\n    finished: found 20 clusters and added\n    'louvain_1.4', the cluster labels (adata.obs, categorical) (0:00:00)\n    using 'X_pca' with n_pcs = 50\nStoring dendrogram info using `.uns['dendrogram_louvain_0.6']`"
  },
  {
    "objectID": "labs/scanpy/scanpy_04_clustering.html#meta-clust_kmean",
    "href": "labs/scanpy/scanpy_04_clustering.html#meta-clust_kmean",
    "title": " Clustering",
    "section": "2 K-means clustering",
    "text": "2 K-means clustering\nK-means is a generic clustering algorithm that has been used in many application areas. In R, it can be applied via the kmeans() function. Typically, it is applied to a reduced dimension representation of the expression data (most often PCA, because of the interpretability of the low-dimensional distances). We need to define the number of clusters in advance. Since the results depend on the initialization of the cluster centers, it is typically recommended to run K-means with multiple starting configurations (via the nstart argument).\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\n# extract pca coordinates\nX_pca = adata.obsm['Scanorama'] \n\n# kmeans with k=5\nkmeans = KMeans(n_clusters=5, random_state=0).fit(X_pca) \nadata.obs['kmeans5'] = kmeans.labels_.astype(str)\n\n# kmeans with k=10\nkmeans = KMeans(n_clusters=10, random_state=0).fit(X_pca) \nadata.obs['kmeans10'] = kmeans.labels_.astype(str)\n\n# kmeans with k=15\nkmeans = KMeans(n_clusters=15, random_state=0).fit(X_pca)\nadata.obs['kmeans15'] = kmeans.labels_.astype(str)\n\nsc.pl.umap(adata, color=['kmeans5', 'kmeans10', 'kmeans15'])\n\nadata.obsm\n\n\n\n\n\n\n\n\nAxisArrays with keys: Scanorama, X_pca, X_tsne, X_umap"
  },
  {
    "objectID": "labs/scanpy/scanpy_04_clustering.html#meta-clust_hier",
    "href": "labs/scanpy/scanpy_04_clustering.html#meta-clust_hier",
    "title": " Clustering",
    "section": "3 Hierarchical clustering",
    "text": "3 Hierarchical clustering\nHierarchical clustering is another generic form of clustering that can be applied also to scRNA-seq data. As K-means, it is typically applied to a reduced dimension representation of the data. Hierarchical clustering returns an entire hierarchy of partitionings (a dendrogram) that can be cut at different levels. Hierarchical clustering is done in these steps:\n\nDefine the distances between samples. The most common are Euclidean distance (a.k.a. straight line between two points) or correlation coefficients.\nDefine a measure of distances between clusters, called linkage criteria. It can for example be average distances between clusters. Commonly used methods are single, complete, average, median, centroid and ward.\nDefine the dendrogram among all samples using Bottom-up or Top-down approach. Bottom-up is where samples start with their own cluster which end up merged pair-by-pair until only one cluster is left. Top-down is where samples start all in the same cluster that end up being split by 2 until each sample has its own cluster.\n\nAs you might have realized, correlation is not a method implemented in the dist() function. However, we can create our own distances and transform them to a distance object. We can first compute sample correlations using the cor function.\nAs you already know, correlation range from -1 to 1, where 1 indicates that two samples are closest, -1 indicates that two samples are the furthest and 0 is somewhat in between. This, however, creates a problem in defining distances because a distance of 0 indicates that two samples are closest, 1 indicates that two samples are the furthest and distance of -1 is not meaningful. We thus need to transform the correlations to a positive scale (a.k.a. adjacency):\n\\[adj = \\frac{1- cor}{2}\\]\nOnce we transformed the correlations to a 0-1 scale, we can simply convert it to a distance object using as.dist() function. The transformation does not need to have a maximum of 1, but it is more intuitive to have it at 1, rather than at any other number.\nThe function AgglomerativeClustering has the option of running with disntance metrics “euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or “precomputed”. However, with ward linkage only euklidean distances works. Here we will try out euclidean distance and ward linkage calculated in PCA space.\n\nfrom sklearn.cluster import AgglomerativeClustering\n\ncluster = AgglomerativeClustering(n_clusters=5, linkage='ward')\nadata.obs['hclust_5'] = cluster.fit_predict(X_pca).astype(str)\n\ncluster = AgglomerativeClustering(n_clusters=10, linkage='ward')\nadata.obs['hclust_10'] = cluster.fit_predict(X_pca).astype(str)\n\ncluster = AgglomerativeClustering(n_clusters=15, linkage='ward')\nadata.obs['hclust_15'] = cluster.fit_predict(X_pca).astype(str)\n\nsc.pl.umap(adata, color=['hclust_5', 'hclust_10', 'hclust_15'])\n\n\n\n\n\n\n\n\nFinally, lets save the clustered data for further analysis.\n\nadata.write_h5ad('./data/covid/results/scanpy_covid_qc_dr_scanorama_cl.h5ad')"
  },
  {
    "objectID": "labs/scanpy/scanpy_04_clustering.html#meta-clust_distribution",
    "href": "labs/scanpy/scanpy_04_clustering.html#meta-clust_distribution",
    "title": " Clustering",
    "section": "4 Distribution of clusters",
    "text": "4 Distribution of clusters\nNow, we can select one of our clustering methods and compare the proportion of samples across the clusters.\nSelect the “leiden_0.6” and plot proportion of samples per cluster and also proportion covid vs ctrl.\nPlot proportion of cells from each condition per cluster.\n\ntmp = pd.crosstab(adata.obs['leiden_0.6'],adata.obs['type'], normalize='index')\ntmp.plot.bar(stacked=True).legend(bbox_to_anchor=(1.4, 1), loc='upper right')\n\ntmp = pd.crosstab(adata.obs['leiden_0.6'],adata.obs['sample'], normalize='index')\ntmp.plot.bar(stacked=True).legend(bbox_to_anchor=(1.4, 1),loc='upper right')\n\n&lt;matplotlib.legend.Legend at 0x7f7c62595090&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this case we have quite good representation of each sample in each cluster. But there are clearly some biases with more cells from one sample in some clusters and also more covid cells in some of the clusters.\nWe can also plot it in the other direction, the proportion of each cluster per sample.\n\ntmp = pd.crosstab(adata.obs['sample'],adata.obs['leiden_0.6'], normalize='index')\ntmp.plot.bar(stacked=True).legend(bbox_to_anchor=(1.4, 1), loc='upper right')\n\n&lt;matplotlib.legend.Legend at 0x7f7c9a9a3dc0&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nBy now you should know how to plot different features onto your data. Take the QC metrics that were calculated in the first exercise, that should be stored in your data object, and plot it as violin plots per cluster using the clustering method of your choice. For example, plot number of UMIS, detected genes, percent mitochondrial reads. Then, check carefully if there is any bias in how your data is separated by quality metrics. Could it be explained biologically, or could there be a technical bias there?"
  },
  {
    "objectID": "labs/scanpy/scanpy_04_clustering.html#meta-session",
    "href": "labs/scanpy/scanpy_04_clustering.html#meta-session",
    "title": " Clustering",
    "section": "5 Session info",
    "text": "5 Session info\n\n\nClick here\n\n\nsc.logging.print_versions()\n\n-----\nanndata     0.10.5.post1\nscanpy      1.9.6\n-----\nPIL                 10.0.0\nanyio               NA\nasttokens           NA\nattr                23.1.0\nbabel               2.12.1\nbackcall            0.2.0\ncertifi             2023.11.17\ncffi                1.15.1\ncharset_normalizer  3.1.0\ncolorama            0.4.6\ncomm                0.1.3\ncycler              0.12.1\ncython_runtime      NA\ndateutil            2.8.2\ndebugpy             1.6.7\ndecorator           5.1.1\ndefusedxml          0.7.1\nexceptiongroup      1.2.0\nexecuting           1.2.0\nfastjsonschema      NA\ngmpy2               2.1.2\nh5py                3.9.0\nidna                3.4\nigraph              0.10.8\nipykernel           6.23.1\nipython_genutils    0.2.0\njedi                0.18.2\njinja2              3.1.2\njoblib              1.3.2\njson5               NA\njsonpointer         2.0\njsonschema          4.17.3\njupyter_events      0.6.3\njupyter_server      2.6.0\njupyterlab_server   2.22.1\nkiwisolver          1.4.5\nleidenalg           0.10.1\nllvmlite            0.41.1\nlouvain             0.8.1\nmarkupsafe          2.1.2\nmatplotlib          3.8.0\nmatplotlib_inline   0.1.6\nmpl_toolkits        NA\nmpmath              1.3.0\nnatsort             8.4.0\nnbformat            5.8.0\nnumba               0.58.1\nnumpy               1.26.3\nopt_einsum          v3.3.0\noverrides           NA\npackaging           23.1\npandas              2.2.0\nparso               0.8.3\npexpect             4.8.0\npickleshare         0.7.5\npkg_resources       NA\nplatformdirs        3.5.1\nprometheus_client   NA\nprompt_toolkit      3.0.38\npsutil              5.9.5\nptyprocess          0.7.0\npure_eval           0.2.2\npvectorc            NA\npydev_ipython       NA\npydevconsole        NA\npydevd              2.9.5\npydevd_file_utils   NA\npydevd_plugins      NA\npydevd_tracing      NA\npygments            2.15.1\npyparsing           3.1.1\npyrsistent          NA\npythonjsonlogger    NA\npytz                2023.3\nrequests            2.31.0\nrfc3339_validator   0.1.4\nrfc3986_validator   0.1.1\nscipy               1.12.0\nsend2trash          NA\nsession_info        1.0.0\nsix                 1.16.0\nsklearn             1.4.0\nsniffio             1.3.0\nsocks               1.7.1\nstack_data          0.6.2\nsympy               1.12\ntexttable           1.7.0\nthreadpoolctl       3.2.0\ntorch               2.0.0\ntornado             6.3.2\ntqdm                4.65.0\ntraitlets           5.9.0\ntyping_extensions   NA\nurllib3             2.0.2\nwcwidth             0.2.6\nwebsocket           1.5.2\nyaml                6.0\nzmq                 25.0.2\nzoneinfo            NA\nzstandard           0.19.0\n-----\nIPython             8.13.2\njupyter_client      8.2.0\njupyter_core        5.3.0\njupyterlab          4.0.1\nnotebook            6.5.4\n-----\nPython 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) [GCC 11.3.0]\nLinux-5.15.0-92-generic-x86_64-with-glibc2.35\n-----\nSession information updated at 2024-02-06 00:39"
  },
  {
    "objectID": "labs/scanpy/scanpy_05_dge.html",
    "href": "labs/scanpy/scanpy_05_dge.html",
    "title": " Differential gene expression",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run Python commands unless it starts with %%bash, in which case, those chunks run shell commands.\nIn this tutorial we will cover differential gene expression, which comprises an extensive range of topics and methods. In single cell, differential expresison can have multiple functionalities such as identifying marker genes for cell populations, as well as identifying differentially regulated genes across conditions (healthy vs control). We will also cover controlling batch effect in your test.\nDifferential expression is performed with the function rank_genes_group. The default method to compute differential expression is the t-test_overestim_var. Other implemented methods are: logreg, t-test and wilcoxon.\nBy default, the .raw attribute of AnnData is used in case it has been initialized, it can be changed by setting use_raw=False.\nThe clustering with resolution 0.6 seems to give a reasonable number of clusters, so we will use that clustering for all DE tests.\nFirst, let’s import libraries and fetch the clustered data from the previous lab.\nimport numpy as np\nimport pandas as pd\nimport scanpy as sc\nimport gseapy\nimport matplotlib.pyplot as plt\nimport warnings\nimport os\nimport urllib.request\n\nwarnings.simplefilter(action=\"ignore\", category=Warning)\n\n# verbosity: errors (0), warnings (1), info (2), hints (3)\nsc.settings.verbosity = 2\n\nsc.settings.set_figure_params(dpi=80)\nRead in the clustered data object.\n# download pre-computed data if missing or long compute\nfetch_data = True\n\n# url for source and intermediate data\npath_data = \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\n\npath_results = \"data/covid/results\"\nif not os.path.exists(path_results):\n    os.makedirs(path_results, exist_ok=True)\n\n# path_file = \"data/covid/results/scanpy_covid_qc_dr_scanorama_cl.h5ad\"\npath_file = \"data/covid/results/scanpy_covid_qc_dr_scanorama_cl.h5ad\"\nif fetch_data and not os.path.exists(path_file):\n    urllib.request.urlretrieve(os.path.join(\n        path_data, 'covid/results/scanpy_covid_qc_dr_scanorama_cl.h5ad'), path_file)\n\nadata = sc.read_h5ad(path_file)\nadata\n\nAnnData object with n_obs × n_vars = 7222 × 19468\n    obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info', 'leiden', 'leiden_0.4', 'leiden_0.6', 'leiden_1.0', 'leiden_1.4', 'kmeans5', 'kmeans10', 'kmeans15', 'hclust_5', 'hclust_10', 'hclust_15'\n    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n    uns: 'dendrogram_leiden_0.6', 'doublet_info_colors', 'hclust_10_colors', 'hclust_15_colors', 'hclust_5_colors', 'hvg', 'kmeans10_colors', 'kmeans15_colors', 'kmeans5_colors', 'leiden', 'leiden_0.4', 'leiden_0.4_colors', 'leiden_0.6', 'leiden_0.6_colors', 'leiden_1.0', 'leiden_1.0_colors', 'leiden_1.4', 'leiden_1.4_colors', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap'\n    obsm: 'Scanorama', 'X_pca', 'X_pca_harmony', 'X_tsne', 'X_tsne_bbknn', 'X_tsne_harmony', 'X_tsne_scanorama', 'X_tsne_uncorr', 'X_umap', 'X_umap_bbknn', 'X_umap_harmony', 'X_umap_scanorama', 'X_umap_uncorr'\n    obsp: 'connectivities', 'distances'\nCheck what you have in the different matrices.\nprint(adata.X.shape)\nprint(type(adata.raw))\nprint(adata.X[:10,:10])\n\n(7222, 19468)\n&lt;class 'NoneType'&gt;\n&lt;Compressed Sparse Row sparse matrix of dtype 'float64'\n    with 2 stored elements and shape (10, 10)&gt;\n  Coords    Values\n  (1, 4)    0.7825693876867097\n  (8, 7)    1.1311041336746985\nAs you can see, the X matrix contains all genes and the data looks logtransformed.\nFor DGE analysis we would like to run with all genes, on normalized values, so if you did subset the adata.X for variable genes you would have to revert back to the raw matrix with adata = adata.raw.to_adata(). In case you have raw counts in the matrix you also have to renormalize and logtransform.\nNow lets look at the clustering of the object we loaded in the umap. We will use leiden_0.6 clustering in this exercise. If you recall from the previous exercise, we set the default umap to the umap created with Harmony.\nsc.pl.umap(adata, color='leiden_0.6')"
  },
  {
    "objectID": "labs/scanpy/scanpy_05_dge.html#t-test",
    "href": "labs/scanpy/scanpy_05_dge.html#t-test",
    "title": " Differential gene expression",
    "section": "1 T-test",
    "text": "1 T-test\n\nsc.tl.rank_genes_groups(adata, 'leiden_0.6', method='t-test', key_added = \"t-test\")\nsc.pl.rank_genes_groups(adata, n_genes=25, sharey=False, key = \"t-test\")\n\n# results are stored in the adata.uns[\"t-test\"] slot\nadata.uns.keys()\n\nranking genes\n    finished (0:00:01)\n\n\n\n\n\n\n\n\n\ndict_keys(['dendrogram_leiden_0.6', 'doublet_info_colors', 'hclust_10_colors', 'hclust_15_colors', 'hclust_5_colors', 'hvg', 'kmeans10_colors', 'kmeans15_colors', 'kmeans5_colors', 'leiden', 'leiden_0.4', 'leiden_0.4_colors', 'leiden_0.6', 'leiden_0.6_colors', 'leiden_1.0', 'leiden_1.0_colors', 'leiden_1.4', 'leiden_1.4_colors', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap', 't-test'])"
  },
  {
    "objectID": "labs/scanpy/scanpy_05_dge.html#t-test-overestimated_variance",
    "href": "labs/scanpy/scanpy_05_dge.html#t-test-overestimated_variance",
    "title": " Differential gene expression",
    "section": "2 T-test overestimated_variance",
    "text": "2 T-test overestimated_variance\n\nsc.tl.rank_genes_groups(adata, 'leiden_0.6', method='t-test_overestim_var', key_added = \"t-test_ov\")\nsc.pl.rank_genes_groups(adata, n_genes=25, sharey=False, key = \"t-test_ov\")\n\nranking genes\n    finished (0:00:00)"
  },
  {
    "objectID": "labs/scanpy/scanpy_05_dge.html#wilcoxon-rank-sum",
    "href": "labs/scanpy/scanpy_05_dge.html#wilcoxon-rank-sum",
    "title": " Differential gene expression",
    "section": "3 Wilcoxon rank-sum",
    "text": "3 Wilcoxon rank-sum\nThe result of a Wilcoxon rank-sum (Mann-Whitney-U) test is very similar. We recommend using the latter in publications, see e.g., Sonison & Robinson (2018). You might also consider much more powerful differential testing packages like MAST, limma, DESeq2 and, for python, the recent diffxpy.\n\nsc.tl.rank_genes_groups(adata, 'leiden_0.6', method='wilcoxon', key_added = \"wilcoxon\")\nsc.pl.rank_genes_groups(adata, n_genes=25, sharey=False, key=\"wilcoxon\")\n\nranking genes\n    finished (0:00:06)"
  },
  {
    "objectID": "labs/scanpy/scanpy_05_dge.html#logistic-regression-test",
    "href": "labs/scanpy/scanpy_05_dge.html#logistic-regression-test",
    "title": " Differential gene expression",
    "section": "4 Logistic regression test",
    "text": "4 Logistic regression test\nAs an alternative, let us rank genes using logistic regression. For instance, this has been suggested by Natranos et al. (2018). The essential difference is that here, we use a multi-variate appraoch whereas conventional differential tests are uni-variate. Clark et al. (2014) has more details.\n\nsc.tl.rank_genes_groups(adata, 'leiden_0.6', method='logreg',key_added = \"logreg\")\nsc.pl.rank_genes_groups(adata, n_genes=25, sharey=False, key = \"logreg\")\n\nranking genes\n    finished (0:00:22)"
  },
  {
    "objectID": "labs/scanpy/scanpy_05_dge.html#compare-genes",
    "href": "labs/scanpy/scanpy_05_dge.html#compare-genes",
    "title": " Differential gene expression",
    "section": "5 Compare genes",
    "text": "5 Compare genes\nTake all significant DE genes for cluster0 with each test and compare the overlap.\n\n#compare cluster1 genes, only stores top 100 by default\n\nwc = sc.get.rank_genes_groups_df(adata, group='0', key='wilcoxon', pval_cutoff=0.01, log2fc_min=0)['names']\ntt = sc.get.rank_genes_groups_df(adata, group='0', key='t-test', pval_cutoff=0.01, log2fc_min=0)['names']\ntt_ov = sc.get.rank_genes_groups_df(adata, group='0', key='t-test_ov', pval_cutoff=0.01, log2fc_min=0)['names']\n\nfrom matplotlib_venn import venn3\n\nvenn3([set(wc),set(tt),set(tt_ov)], ('Wilcox','T-test','T-test_ov') )\nplt.show()\n\n\n\n\n\n\n\n\nAs you can see, the Wilcoxon test and the T-test with overestimated variance gives very similar result. Also the regular T-test has good overlap."
  },
  {
    "objectID": "labs/scanpy/scanpy_05_dge.html#visualization",
    "href": "labs/scanpy/scanpy_05_dge.html#visualization",
    "title": " Differential gene expression",
    "section": "6 Visualization",
    "text": "6 Visualization\nThere are several ways to visualize the expression of top DE genes. Here we will plot top 5 genes per cluster from Wilcoxon test as heatmap, dotplot, violin plots or a matrix with average expression.\n\nsc.pl.rank_genes_groups_heatmap(adata, n_genes=5, key=\"wilcoxon\", groupby=\"leiden_0.6\", show_gene_labels=True)\nsc.pl.rank_genes_groups_dotplot(adata, n_genes=5, key=\"wilcoxon\", groupby=\"leiden_0.6\")\nsc.pl.rank_genes_groups_stacked_violin(adata, n_genes=5, key=\"wilcoxon\", groupby=\"leiden_0.6\")\nsc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, key=\"wilcoxon\", groupby=\"leiden_0.6\")"
  },
  {
    "objectID": "labs/scanpy/scanpy_05_dge.html#compare-specific-clusters",
    "href": "labs/scanpy/scanpy_05_dge.html#compare-specific-clusters",
    "title": " Differential gene expression",
    "section": "7 Compare specific clusters",
    "text": "7 Compare specific clusters\nWe can also do pairwise comparisons of individual clusters on one vs many clusters. For instance, clusters 1 & 2 have very similar expression profiles.\n\nsc.tl.rank_genes_groups(adata, 'leiden_0.6', groups=['1'], reference='2', method='wilcoxon')\nsc.pl.rank_genes_groups(adata, groups=['1'], n_genes=20)\n\nranking genes\n    finished (0:00:01)\n\n\n\n\n\n\n\n\n\nPlot as violins for those two groups, or across all the clusters.\n\nsc.pl.rank_genes_groups_violin(adata, groups='1', n_genes=10)\n\n# plot the same genes as violins across all the datasets.\n\n# convert numpy.recarray to list\nmynames = [x[0] for x in adata.uns['rank_genes_groups']['names'][:10]]\nsc.pl.stacked_violin(adata, mynames, groupby = 'leiden_0.6')"
  },
  {
    "objectID": "labs/scanpy/scanpy_05_dge.html#meta-dge_cond",
    "href": "labs/scanpy/scanpy_05_dge.html#meta-dge_cond",
    "title": " Differential gene expression",
    "section": "8 DGE across conditions",
    "text": "8 DGE across conditions\nThe second way of computing differential expression is to answer which genes are differentially expressed within a cluster. For example, in our case we have libraries comming from patients and controls and we would like to know which genes are influenced the most in a particular cell type. For this end, we will first subset our data for the desired cell cluster, then change the cell identities to the variable of comparison (which now in our case is the type, e.g. Covid/Ctrl).\n\ncl1 = adata[adata.obs['louvain_0.6'] == '4',:]\ncl1.obs['type'].value_counts()\n\nsc.tl.rank_genes_groups(cl1, 'type', method='wilcoxon', key_added = \"wilcoxon\")\nsc.pl.rank_genes_groups(cl1, n_genes=25, sharey=False, key=\"wilcoxon\")\n\nranking genes\n    finished (0:00:01)\n\n\n\n\n\n\n\n\n\n\nsc.pl.rank_genes_groups_violin(cl1, n_genes=10, key=\"wilcoxon\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also plot these genes across all clusters, but split by “type”, to check if the genes are also up/downregulated in other celltypes.\n\nimport seaborn as sns\n\ngenes1 = sc.get.rank_genes_groups_df(cl1, group='Covid', key='wilcoxon')['names'][:5]\ngenes2 = sc.get.rank_genes_groups_df(cl1, group='Ctrl', key='wilcoxon')['names'][:5]\ngenes = genes1.tolist() +  genes2.tolist() \ndf = sc.get.obs_df(adata, genes + ['louvain_0.6','type'], use_raw=False)\ndf2 = df.melt(id_vars=[\"louvain_0.6\",'type'], value_vars=genes)\n\nsns.catplot(x = \"louvain_0.6\", y = \"value\", hue = \"type\", kind = 'violin', col = \"variable\", data = df2, col_wrap=4, inner=None)\n\n\n\n\n\n\n\n\nAs you can see, we have many sex chromosome related genes among the top DE genes. And if you remember from the QC lab, we have inbalanced sex distribution among our subjects, so this may not be related to covid at all.\n\n8.1 Remove sex chromosome genes\nTo remove some of the bias due to inbalanced sex in the subjects we can remove the sex chromosome related genes.\n\nannot = sc.queries.biomart_annotations(\n        \"hsapiens\",\n        [\"ensembl_gene_id\", \"external_gene_name\", \"start_position\", \"end_position\", \"chromosome_name\"],\n    ).set_index(\"external_gene_name\")\n\nchrY_genes = adata.var_names.intersection(annot.index[annot.chromosome_name == \"Y\"])\nchrX_genes = adata.var_names.intersection(annot.index[annot.chromosome_name == \"X\"])\n\nsex_genes = chrY_genes.union(chrX_genes)\nprint(len(sex_genes))\nall_genes = cl1.var.index.tolist()\nprint(len(all_genes))\n\nkeep_genes = [x for x in all_genes if x not in sex_genes]\nprint(len(keep_genes))\n\ncl1 = cl1[:,keep_genes]\n\n551\n19468\n18917\n\n\nRerun differential expression.\n\nsc.tl.rank_genes_groups(cl1, 'type', method='wilcoxon', key_added = \"wilcoxon\")\nsc.pl.rank_genes_groups(cl1, n_genes=25, sharey=False, key=\"wilcoxon\")\n\nranking genes\n    finished (0:00:01)\n\n\n\n\n\n\n\n\n\n\n\n8.2 Patient batch effects\nWhen we are testing for Covid vs Control we are running a DGE test for 3 vs 3 individuals. That will be very sensitive to sample differences unless we find a way to control for it. So first, lets check how the top DGEs are expressed across the individuals:\n\ngenes1 = sc.get.rank_genes_groups_df(cl1, group='Covid', key='wilcoxon')['names'][:5]\ngenes2 = sc.get.rank_genes_groups_df(cl1, group='Ctrl', key='wilcoxon')['names'][:5]\ngenes = genes1.tolist() +  genes2.tolist() \n\nsc.pl.violin(cl1, genes1, groupby='sample')\nsc.pl.violin(cl1, genes2, groupby='sample')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, many of the genes detected as DGE in Covid are unique to one or 2 patients.\nWe can also plot the top Covid and top Ctrl genes as a dotplot:\n\ngenes1 = sc.get.rank_genes_groups_df(cl1, group='Covid', key='wilcoxon')['names'][:20]\ngenes2 = sc.get.rank_genes_groups_df(cl1, group='Ctrl', key='wilcoxon')['names'][:20]\ngenes = genes1.tolist() +  genes2.tolist() \n\nsc.pl.dotplot(cl1,genes, groupby='sample')\n\n\n\n\n\n\n\n\nClearly many of the top Covid genes are only high in the covid_17 sample, and not a general feature of covid patients.\nThis is also the patient with the highest number of cells in this cluster:\n\ncl1.obs['sample'].value_counts()\n\nsample\ncovid_17    130\nctrl_5      114\ncovid_1     109\nctrl_13      65\nctrl_14      62\nctrl_19      57\ncovid_16     38\ncovid_15     37\nName: count, dtype: int64\n\n\n\n\n8.3 Subsample\nSo one obvious thing to consider is an equal amount of cells per individual so that the DGE results are not dominated by a single sample.\nSo we will downsample to an equal number of cells per sample, in this case 34 cells per sample as it is the lowest number among all samples\n\ntarget_cells = 37\n\ntmp = [cl1[cl1.obs['sample'] == s] for s in cl1.obs['sample'].cat.categories]\n\nfor dat in tmp:\n    if dat.n_obs &gt; target_cells:\n            sc.pp.subsample(dat, n_obs=target_cells)\n\ncl1_sub = tmp[0].concatenate(*tmp[1:])\n\ncl1_sub.obs['sample'].value_counts()\n\nsample\ncovid_1     37\ncovid_15    37\ncovid_16    37\ncovid_17    37\nctrl_5      37\nctrl_13     37\nctrl_14     37\nctrl_19     37\nName: count, dtype: int64\n\n\n\nsc.tl.rank_genes_groups(cl1_sub, 'type', method='wilcoxon', key_added = \"wilcoxon\")\nsc.pl.rank_genes_groups(cl1_sub, n_genes=25, sharey=False, key=\"wilcoxon\")\n\nranking genes\n    finished (0:00:00)\n\n\n\n\n\n\n\n\n\n\ngenes1 = sc.get.rank_genes_groups_df(cl1_sub, group='Covid', key='wilcoxon')['names'][:20]\ngenes2 = sc.get.rank_genes_groups_df(cl1_sub, group='Ctrl', key='wilcoxon')['names'][:20]\ngenes = genes1.tolist() +  genes2.tolist() \n\nsc.pl.dotplot(cl1,genes, groupby='sample')\n\n\n\n\n\n\n\n\nIt looks much better now. But if we look per patient you can see that we still have some genes that are dominated by a single patient. Still, it is often a good idea to control the number of cells from each sample when doing differential expression.\nThere are many different ways to try and resolve the issue of patient batch effects, however most of them require R packages. These can be run via rpy2 as is demonstraded in this compendium: https://www.sc-best-practices.org/conditions/differential_gene_expression.html\nHowever, we have not included it here as of now. So please have a look at the patient batch effect section in the seurat DGE tutorial where we run EdgeR on pseudobulk and MAST with random effect."
  },
  {
    "objectID": "labs/scanpy/scanpy_05_dge.html#meta-dge_gsa",
    "href": "labs/scanpy/scanpy_05_dge.html#meta-dge_gsa",
    "title": " Differential gene expression",
    "section": "9 Gene Set Analysis (GSA)",
    "text": "9 Gene Set Analysis (GSA)\n\n9.1 Hypergeometric enrichment test\nHaving a defined list of differentially expressed genes, you can now look for their combined function using hypergeometric test.\n\n#Available databases : ‘Human’, ‘Mouse’, ‘Yeast’, ‘Fly’, ‘Fish’, ‘Worm’ \ngene_set_names = gseapy.get_library_name(organism='Human')\nprint(gene_set_names)\n\n['ARCHS4_Cell-lines', 'ARCHS4_IDG_Coexp', 'ARCHS4_Kinases_Coexp', 'ARCHS4_TFs_Coexp', 'ARCHS4_Tissues', 'Achilles_fitness_decrease', 'Achilles_fitness_increase', 'Aging_Perturbations_from_GEO_down', 'Aging_Perturbations_from_GEO_up', 'Allen_Brain_Atlas_10x_scRNA_2021', 'Allen_Brain_Atlas_down', 'Allen_Brain_Atlas_up', 'Azimuth_2023', 'Azimuth_Cell_Types_2021', 'BioCarta_2013', 'BioCarta_2015', 'BioCarta_2016', 'BioPlanet_2019', 'BioPlex_2017', 'CCLE_Proteomics_2020', 'CORUM', 'COVID-19_Related_Gene_Sets', 'COVID-19_Related_Gene_Sets_2021', 'Cancer_Cell_Line_Encyclopedia', 'CellMarker_Augmented_2021', 'ChEA_2013', 'ChEA_2015', 'ChEA_2016', 'ChEA_2022', 'Chromosome_Location', 'Chromosome_Location_hg19', 'ClinVar_2019', 'DSigDB', 'Data_Acquisition_Method_Most_Popular_Genes', 'DepMap_WG_CRISPR_Screens_Broad_CellLines_2019', 'DepMap_WG_CRISPR_Screens_Sanger_CellLines_2019', 'Descartes_Cell_Types_and_Tissue_2021', 'Diabetes_Perturbations_GEO_2022', 'DisGeNET', 'Disease_Perturbations_from_GEO_down', 'Disease_Perturbations_from_GEO_up', 'Disease_Signatures_from_GEO_down_2014', 'Disease_Signatures_from_GEO_up_2014', 'DrugMatrix', 'Drug_Perturbations_from_GEO_2014', 'Drug_Perturbations_from_GEO_down', 'Drug_Perturbations_from_GEO_up', 'ENCODE_Histone_Modifications_2013', 'ENCODE_Histone_Modifications_2015', 'ENCODE_TF_ChIP-seq_2014', 'ENCODE_TF_ChIP-seq_2015', 'ENCODE_and_ChEA_Consensus_TFs_from_ChIP-X', 'ESCAPE', 'Elsevier_Pathway_Collection', 'Enrichr_Libraries_Most_Popular_Genes', 'Enrichr_Submissions_TF-Gene_Coocurrence', 'Enrichr_Users_Contributed_Lists_2020', 'Epigenomics_Roadmap_HM_ChIP-seq', 'FANTOM6_lncRNA_KD_DEGs', 'GO_Biological_Process_2013', 'GO_Biological_Process_2015', 'GO_Biological_Process_2017', 'GO_Biological_Process_2017b', 'GO_Biological_Process_2018', 'GO_Biological_Process_2021', 'GO_Biological_Process_2023', 'GO_Cellular_Component_2013', 'GO_Cellular_Component_2015', 'GO_Cellular_Component_2017', 'GO_Cellular_Component_2017b', 'GO_Cellular_Component_2018', 'GO_Cellular_Component_2021', 'GO_Cellular_Component_2023', 'GO_Molecular_Function_2013', 'GO_Molecular_Function_2015', 'GO_Molecular_Function_2017', 'GO_Molecular_Function_2017b', 'GO_Molecular_Function_2018', 'GO_Molecular_Function_2021', 'GO_Molecular_Function_2023', 'GTEx_Aging_Signatures_2021', 'GTEx_Tissue_Expression_Down', 'GTEx_Tissue_Expression_Up', 'GTEx_Tissues_V8_2023', 'GWAS_Catalog_2019', 'GWAS_Catalog_2023', 'GeDiPNet_2023', 'GeneSigDB', 'Gene_Perturbations_from_GEO_down', 'Gene_Perturbations_from_GEO_up', 'Genes_Associated_with_NIH_Grants', 'Genome_Browser_PWMs', 'GlyGen_Glycosylated_Proteins_2022', 'HDSigDB_Human_2021', 'HDSigDB_Mouse_2021', 'HMDB_Metabolites', 'HMS_LINCS_KinomeScan', 'HomoloGene', 'HuBMAP_ASCT_plus_B_augmented_w_RNAseq_Coexpression', 'HuBMAP_ASCTplusB_augmented_2022', 'HumanCyc_2015', 'HumanCyc_2016', 'Human_Gene_Atlas', 'Human_Phenotype_Ontology', 'IDG_Drug_Targets_2022', 'InterPro_Domains_2019', 'Jensen_COMPARTMENTS', 'Jensen_DISEASES', 'Jensen_TISSUES', 'KEA_2013', 'KEA_2015', 'KEGG_2013', 'KEGG_2015', 'KEGG_2016', 'KEGG_2019_Human', 'KEGG_2019_Mouse', 'KEGG_2021_Human', 'KOMP2_Mouse_Phenotypes_2022', 'Kinase_Perturbations_from_GEO_down', 'Kinase_Perturbations_from_GEO_up', 'L1000_Kinase_and_GPCR_Perturbations_down', 'L1000_Kinase_and_GPCR_Perturbations_up', 'LINCS_L1000_CRISPR_KO_Consensus_Sigs', 'LINCS_L1000_Chem_Pert_Consensus_Sigs', 'LINCS_L1000_Chem_Pert_down', 'LINCS_L1000_Chem_Pert_up', 'LINCS_L1000_Ligand_Perturbations_down', 'LINCS_L1000_Ligand_Perturbations_up', 'Ligand_Perturbations_from_GEO_down', 'Ligand_Perturbations_from_GEO_up', 'MAGMA_Drugs_and_Diseases', 'MAGNET_2023', 'MCF7_Perturbations_from_GEO_down', 'MCF7_Perturbations_from_GEO_up', 'MGI_Mammalian_Phenotype_2013', 'MGI_Mammalian_Phenotype_2017', 'MGI_Mammalian_Phenotype_Level_3', 'MGI_Mammalian_Phenotype_Level_4', 'MGI_Mammalian_Phenotype_Level_4_2019', 'MGI_Mammalian_Phenotype_Level_4_2021', 'MSigDB_Computational', 'MSigDB_Hallmark_2020', 'MSigDB_Oncogenic_Signatures', 'Metabolomics_Workbench_Metabolites_2022', 'Microbe_Perturbations_from_GEO_down', 'Microbe_Perturbations_from_GEO_up', 'MoTrPAC_2023', 'Mouse_Gene_Atlas', 'NCI-60_Cancer_Cell_Lines', 'NCI-Nature_2015', 'NCI-Nature_2016', 'NIH_Funded_PIs_2017_AutoRIF_ARCHS4_Predictions', 'NIH_Funded_PIs_2017_GeneRIF_ARCHS4_Predictions', 'NIH_Funded_PIs_2017_Human_AutoRIF', 'NIH_Funded_PIs_2017_Human_GeneRIF', 'NURSA_Human_Endogenous_Complexome', 'OMIM_Disease', 'OMIM_Expanded', 'Old_CMAP_down', 'Old_CMAP_up', 'Orphanet_Augmented_2021', 'PFOCR_Pathways', 'PFOCR_Pathways_2023', 'PPI_Hub_Proteins', 'PanglaoDB_Augmented_2021', 'Panther_2015', 'Panther_2016', 'Pfam_Domains_2019', 'Pfam_InterPro_Domains', 'PheWeb_2019', 'PhenGenI_Association_2021', 'Phosphatase_Substrates_from_DEPOD', 'ProteomicsDB_2020', 'Proteomics_Drug_Atlas_2023', 'RNA-Seq_Disease_Gene_and_Drug_Signatures_from_GEO', 'RNAseq_Automatic_GEO_Signatures_Human_Down', 'RNAseq_Automatic_GEO_Signatures_Human_Up', 'RNAseq_Automatic_GEO_Signatures_Mouse_Down', 'RNAseq_Automatic_GEO_Signatures_Mouse_Up', 'Rare_Diseases_AutoRIF_ARCHS4_Predictions', 'Rare_Diseases_AutoRIF_Gene_Lists', 'Rare_Diseases_GeneRIF_ARCHS4_Predictions', 'Rare_Diseases_GeneRIF_Gene_Lists', 'Reactome_2013', 'Reactome_2015', 'Reactome_2016', 'Reactome_2022', 'Rummagene_kinases', 'Rummagene_signatures', 'Rummagene_transcription_factors', 'SILAC_Phosphoproteomics', 'SubCell_BarCode', 'SynGO_2022', 'SysMyo_Muscle_Gene_Sets', 'TF-LOF_Expression_from_GEO', 'TF_Perturbations_Followed_by_Expression', 'TG_GATES_2020', 'TRANSFAC_and_JASPAR_PWMs', 'TRRUST_Transcription_Factors_2019', 'Table_Mining_of_CRISPR_Studies', 'Tabula_Muris', 'Tabula_Sapiens', 'TargetScan_microRNA', 'TargetScan_microRNA_2017', 'The_Kinase_Library_2023', 'Tissue_Protein_Expression_from_Human_Proteome_Map', 'Tissue_Protein_Expression_from_ProteomicsDB', 'Transcription_Factor_PPIs', 'UK_Biobank_GWAS_v1', 'Virus-Host_PPI_P-HIPSTer_2020', 'VirusMINT', 'Virus_Perturbations_from_GEO_down', 'Virus_Perturbations_from_GEO_up', 'WikiPathway_2021_Human', 'WikiPathway_2023_Human', 'WikiPathways_2013', 'WikiPathways_2015', 'WikiPathways_2016', 'WikiPathways_2019_Human', 'WikiPathways_2019_Mouse', 'dbGaP', 'huMAP', 'lncHUB_lncRNA_Co-Expression', 'miRTarBase_2017']\n\n\nGet the significant DEGs for the Covid patients.\n\n#?gseapy.enrichr\nglist = sc.get.rank_genes_groups_df(cl1_sub, group='Covid', key='wilcoxon', log2fc_min=0.25, pval_cutoff=0.05)['names'].squeeze().str.strip().tolist()\nprint(len(glist))\n\n7\n\n\n\nenr_res = gseapy.enrichr(gene_list=glist, organism='Human', gene_sets='GO_Biological_Process_2018', cutoff = 0.5)\nenr_res.results.head()\n\n\n\n\n\n\n\n\nGene_set\nTerm\nOverlap\nP-value\nAdjusted P-value\nOld P-value\nOld Adjusted P-value\nOdds Ratio\nCombined Score\nGenes\n\n\n\n\n0\nGO_Biological_Process_2018\npositive regulation of inflammatory response (...\n2/73\n0.000273\n0.021549\n0\n0\n112.236620\n921.142995\nNFKBIA;S100A9\n\n\n1\nGO_Biological_Process_2018\npositive regulation of defense response (GO:00...\n2/74\n0.000280\n0.021549\n0\n0\n110.672222\n905.289889\nNFKBIA;S100A9\n\n\n2\nGO_Biological_Process_2018\npositive regulation of response to external st...\n2/90\n0.000414\n0.021549\n0\n0\n90.477273\n704.697251\nNFKBIA;S100A9\n\n\n3\nGO_Biological_Process_2018\npositive regulation of NF-kappaB transcription...\n2/128\n0.000836\n0.032592\n0\n0\n63.069841\n446.990813\nNFKBIA;S100A9\n\n\n4\nGO_Biological_Process_2018\ncellular protein complex assembly (GO:0043623)\n2/144\n0.001056\n0.032941\n0\n0\n55.918310\n383.234256\nHSP90AB1;POMP\n\n\n\n\n\n\n\nSome databases of interest:\nGO_Biological_Process_2017bKEGG_2019_HumanKEGG_2019_MouseWikiPathways_2019_HumanWikiPathways_2019_Mouse\nYou visualize your results using a simple barplot, for example:\n\ngseapy.barplot(enr_res.res2d,title='GO_Biological_Process_2018')\n\n&lt;Axes: title={'center': 'GO_Biological_Process_2018'}, xlabel='$- \\\\log_{10}$ (Adjusted P-value)'&gt;"
  },
  {
    "objectID": "labs/scanpy/scanpy_05_dge.html#meta-dge_gsea",
    "href": "labs/scanpy/scanpy_05_dge.html#meta-dge_gsea",
    "title": " Differential gene expression",
    "section": "10 Gene Set Enrichment Analysis (GSEA)",
    "text": "10 Gene Set Enrichment Analysis (GSEA)\nBesides the enrichment using hypergeometric test, we can also perform gene set enrichment analysis (GSEA), which scores ranked genes list (usually based on fold changes) and computes permutation test to check if a particular gene set is more present in the Up-regulated genes, among the DOWN_regulated genes or not differentially regulated.\nWe need a table with all DEGs and their log foldchanges. However, many lowly expressed genes will have high foldchanges and just contribue noise, so also filter for expression in enough cells.\n\ngene_rank = sc.get.rank_genes_groups_df(cl1_sub, group='Covid', key='wilcoxon')[['names','logfoldchanges']]\ngene_rank.sort_values(by=['logfoldchanges'], inplace=True, ascending=False)\n\n# calculate_qc_metrics will calculate number of cells per gene\nsc.pp.calculate_qc_metrics(cl1, percent_top=None, log1p=False, inplace=True)\n\n# filter for genes expressed in at least 30 cells.\ngene_rank = gene_rank[gene_rank['names'].isin(cl1.var_names[cl1.var.n_cells_by_counts&gt;30])]\n\ngene_rank\n\n\n\n\n\n\n\n\nnames\nlogfoldchanges\n\n\n\n\n526\nSLFN5\n26.829697\n\n\n368\nCXCL8\n26.812254\n\n\n209\nEGR1\n5.063837\n\n\n38\nPPBP\n4.969337\n\n\n211\nPF4\n4.870691\n\n\n...\n...\n...\n\n\n18062\nNXPH4\n-2.804427\n\n\n18449\nMME\n-3.049736\n\n\n18380\nDHDDS\n-3.202402\n\n\n18282\nKDM1B\n-3.256811\n\n\n18607\nZNF296\n-4.392631\n\n\n\n\n7105 rows × 2 columns\n\n\n\nOnce our list of genes are sorted, we can proceed with the enrichment itself. We can use the package to get gene set from the Molecular Signature Database (MSigDB) and select KEGG pathways as an example.\n\n#Available databases : ‘Human’, ‘Mouse’, ‘Yeast’, ‘Fly’, ‘Fish’, ‘Worm’ \ngene_set_names = gseapy.get_library_name(organism='Human')\nprint(gene_set_names)\n\n['ARCHS4_Cell-lines', 'ARCHS4_IDG_Coexp', 'ARCHS4_Kinases_Coexp', 'ARCHS4_TFs_Coexp', 'ARCHS4_Tissues', 'Achilles_fitness_decrease', 'Achilles_fitness_increase', 'Aging_Perturbations_from_GEO_down', 'Aging_Perturbations_from_GEO_up', 'Allen_Brain_Atlas_10x_scRNA_2021', 'Allen_Brain_Atlas_down', 'Allen_Brain_Atlas_up', 'Azimuth_2023', 'Azimuth_Cell_Types_2021', 'BioCarta_2013', 'BioCarta_2015', 'BioCarta_2016', 'BioPlanet_2019', 'BioPlex_2017', 'CCLE_Proteomics_2020', 'CORUM', 'COVID-19_Related_Gene_Sets', 'COVID-19_Related_Gene_Sets_2021', 'Cancer_Cell_Line_Encyclopedia', 'CellMarker_Augmented_2021', 'ChEA_2013', 'ChEA_2015', 'ChEA_2016', 'ChEA_2022', 'Chromosome_Location', 'Chromosome_Location_hg19', 'ClinVar_2019', 'DSigDB', 'Data_Acquisition_Method_Most_Popular_Genes', 'DepMap_WG_CRISPR_Screens_Broad_CellLines_2019', 'DepMap_WG_CRISPR_Screens_Sanger_CellLines_2019', 'Descartes_Cell_Types_and_Tissue_2021', 'Diabetes_Perturbations_GEO_2022', 'DisGeNET', 'Disease_Perturbations_from_GEO_down', 'Disease_Perturbations_from_GEO_up', 'Disease_Signatures_from_GEO_down_2014', 'Disease_Signatures_from_GEO_up_2014', 'DrugMatrix', 'Drug_Perturbations_from_GEO_2014', 'Drug_Perturbations_from_GEO_down', 'Drug_Perturbations_from_GEO_up', 'ENCODE_Histone_Modifications_2013', 'ENCODE_Histone_Modifications_2015', 'ENCODE_TF_ChIP-seq_2014', 'ENCODE_TF_ChIP-seq_2015', 'ENCODE_and_ChEA_Consensus_TFs_from_ChIP-X', 'ESCAPE', 'Elsevier_Pathway_Collection', 'Enrichr_Libraries_Most_Popular_Genes', 'Enrichr_Submissions_TF-Gene_Coocurrence', 'Enrichr_Users_Contributed_Lists_2020', 'Epigenomics_Roadmap_HM_ChIP-seq', 'FANTOM6_lncRNA_KD_DEGs', 'GO_Biological_Process_2013', 'GO_Biological_Process_2015', 'GO_Biological_Process_2017', 'GO_Biological_Process_2017b', 'GO_Biological_Process_2018', 'GO_Biological_Process_2021', 'GO_Biological_Process_2023', 'GO_Cellular_Component_2013', 'GO_Cellular_Component_2015', 'GO_Cellular_Component_2017', 'GO_Cellular_Component_2017b', 'GO_Cellular_Component_2018', 'GO_Cellular_Component_2021', 'GO_Cellular_Component_2023', 'GO_Molecular_Function_2013', 'GO_Molecular_Function_2015', 'GO_Molecular_Function_2017', 'GO_Molecular_Function_2017b', 'GO_Molecular_Function_2018', 'GO_Molecular_Function_2021', 'GO_Molecular_Function_2023', 'GTEx_Aging_Signatures_2021', 'GTEx_Tissue_Expression_Down', 'GTEx_Tissue_Expression_Up', 'GTEx_Tissues_V8_2023', 'GWAS_Catalog_2019', 'GWAS_Catalog_2023', 'GeDiPNet_2023', 'GeneSigDB', 'Gene_Perturbations_from_GEO_down', 'Gene_Perturbations_from_GEO_up', 'Genes_Associated_with_NIH_Grants', 'Genome_Browser_PWMs', 'GlyGen_Glycosylated_Proteins_2022', 'HDSigDB_Human_2021', 'HDSigDB_Mouse_2021', 'HMDB_Metabolites', 'HMS_LINCS_KinomeScan', 'HomoloGene', 'HuBMAP_ASCT_plus_B_augmented_w_RNAseq_Coexpression', 'HuBMAP_ASCTplusB_augmented_2022', 'HumanCyc_2015', 'HumanCyc_2016', 'Human_Gene_Atlas', 'Human_Phenotype_Ontology', 'IDG_Drug_Targets_2022', 'InterPro_Domains_2019', 'Jensen_COMPARTMENTS', 'Jensen_DISEASES', 'Jensen_TISSUES', 'KEA_2013', 'KEA_2015', 'KEGG_2013', 'KEGG_2015', 'KEGG_2016', 'KEGG_2019_Human', 'KEGG_2019_Mouse', 'KEGG_2021_Human', 'KOMP2_Mouse_Phenotypes_2022', 'Kinase_Perturbations_from_GEO_down', 'Kinase_Perturbations_from_GEO_up', 'L1000_Kinase_and_GPCR_Perturbations_down', 'L1000_Kinase_and_GPCR_Perturbations_up', 'LINCS_L1000_CRISPR_KO_Consensus_Sigs', 'LINCS_L1000_Chem_Pert_Consensus_Sigs', 'LINCS_L1000_Chem_Pert_down', 'LINCS_L1000_Chem_Pert_up', 'LINCS_L1000_Ligand_Perturbations_down', 'LINCS_L1000_Ligand_Perturbations_up', 'Ligand_Perturbations_from_GEO_down', 'Ligand_Perturbations_from_GEO_up', 'MAGMA_Drugs_and_Diseases', 'MAGNET_2023', 'MCF7_Perturbations_from_GEO_down', 'MCF7_Perturbations_from_GEO_up', 'MGI_Mammalian_Phenotype_2013', 'MGI_Mammalian_Phenotype_2017', 'MGI_Mammalian_Phenotype_Level_3', 'MGI_Mammalian_Phenotype_Level_4', 'MGI_Mammalian_Phenotype_Level_4_2019', 'MGI_Mammalian_Phenotype_Level_4_2021', 'MSigDB_Computational', 'MSigDB_Hallmark_2020', 'MSigDB_Oncogenic_Signatures', 'Metabolomics_Workbench_Metabolites_2022', 'Microbe_Perturbations_from_GEO_down', 'Microbe_Perturbations_from_GEO_up', 'MoTrPAC_2023', 'Mouse_Gene_Atlas', 'NCI-60_Cancer_Cell_Lines', 'NCI-Nature_2015', 'NCI-Nature_2016', 'NIH_Funded_PIs_2017_AutoRIF_ARCHS4_Predictions', 'NIH_Funded_PIs_2017_GeneRIF_ARCHS4_Predictions', 'NIH_Funded_PIs_2017_Human_AutoRIF', 'NIH_Funded_PIs_2017_Human_GeneRIF', 'NURSA_Human_Endogenous_Complexome', 'OMIM_Disease', 'OMIM_Expanded', 'Old_CMAP_down', 'Old_CMAP_up', 'Orphanet_Augmented_2021', 'PFOCR_Pathways', 'PFOCR_Pathways_2023', 'PPI_Hub_Proteins', 'PanglaoDB_Augmented_2021', 'Panther_2015', 'Panther_2016', 'Pfam_Domains_2019', 'Pfam_InterPro_Domains', 'PheWeb_2019', 'PhenGenI_Association_2021', 'Phosphatase_Substrates_from_DEPOD', 'ProteomicsDB_2020', 'Proteomics_Drug_Atlas_2023', 'RNA-Seq_Disease_Gene_and_Drug_Signatures_from_GEO', 'RNAseq_Automatic_GEO_Signatures_Human_Down', 'RNAseq_Automatic_GEO_Signatures_Human_Up', 'RNAseq_Automatic_GEO_Signatures_Mouse_Down', 'RNAseq_Automatic_GEO_Signatures_Mouse_Up', 'Rare_Diseases_AutoRIF_ARCHS4_Predictions', 'Rare_Diseases_AutoRIF_Gene_Lists', 'Rare_Diseases_GeneRIF_ARCHS4_Predictions', 'Rare_Diseases_GeneRIF_Gene_Lists', 'Reactome_2013', 'Reactome_2015', 'Reactome_2016', 'Reactome_2022', 'Rummagene_kinases', 'Rummagene_signatures', 'Rummagene_transcription_factors', 'SILAC_Phosphoproteomics', 'SubCell_BarCode', 'SynGO_2022', 'SysMyo_Muscle_Gene_Sets', 'TF-LOF_Expression_from_GEO', 'TF_Perturbations_Followed_by_Expression', 'TG_GATES_2020', 'TRANSFAC_and_JASPAR_PWMs', 'TRRUST_Transcription_Factors_2019', 'Table_Mining_of_CRISPR_Studies', 'Tabula_Muris', 'Tabula_Sapiens', 'TargetScan_microRNA', 'TargetScan_microRNA_2017', 'The_Kinase_Library_2023', 'Tissue_Protein_Expression_from_Human_Proteome_Map', 'Tissue_Protein_Expression_from_ProteomicsDB', 'Transcription_Factor_PPIs', 'UK_Biobank_GWAS_v1', 'Virus-Host_PPI_P-HIPSTer_2020', 'VirusMINT', 'Virus_Perturbations_from_GEO_down', 'Virus_Perturbations_from_GEO_up', 'WikiPathway_2021_Human', 'WikiPathway_2023_Human', 'WikiPathways_2013', 'WikiPathways_2015', 'WikiPathways_2016', 'WikiPathways_2019_Human', 'WikiPathways_2019_Mouse', 'dbGaP', 'huMAP', 'lncHUB_lncRNA_Co-Expression', 'miRTarBase_2017']\n\n\nNext, we will run GSEA. This will result in a table containing information for several pathways. We can then sort and filter those pathways to visualize only the top ones. You can select/filter them by either p-value or normalized enrichment score (NES).\n\nres = gseapy.prerank(rnk=gene_rank, gene_sets='KEGG_2021_Human')\n\nterms = res.res2d.Term\nterms[:10]\n\n0               Cytokine-cytokine receptor interaction\n1    AGE-RAGE signaling pathway in diabetic complic...\n2    Viral protein interaction with cytokine and cy...\n3                                 Rheumatoid arthritis\n4                              IL-17 signaling pathway\n5                                       Bladder cancer\n6                          Chemokine signaling pathway\n7                         NF-kappa B signaling pathway\n8                                        Legionellosis\n9                                       Chagas disease\nName: Term, dtype: object\n\n\n\ngseapy.gseaplot(rank_metric=res.ranking, term=terms[0], **res.results[terms[0]])\n\n[&lt;Axes: xlabel='Gene Rank', ylabel='Ranked metric'&gt;,\n &lt;Axes: &gt;,\n &lt;Axes: &gt;,\n &lt;Axes: ylabel='Enrichment Score'&gt;]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nWhich KEGG pathways are upregulated in this cluster? Which KEGG pathways are dowregulated in this cluster? Change the pathway source to another gene set (e.g. CP:WIKIPATHWAYS or CP:REACTOME or CP:BIOCARTA or GO:BP) and check the if you get similar results?\n\n\nFinally, let’s save the integrated data for further analysis.\n\nadata.write_h5ad('./data/covid/results/scanpy_covid_qc_dr_scanorama_cl_dge.h5ad')"
  },
  {
    "objectID": "labs/scanpy/scanpy_05_dge.html#meta-session",
    "href": "labs/scanpy/scanpy_05_dge.html#meta-session",
    "title": " Differential gene expression",
    "section": "11 Session info",
    "text": "11 Session info\n\n\nClick here\n\n\nsc.logging.print_versions()\n\n-----\nanndata     0.10.5.post1\nscanpy      1.9.6\n-----\nPIL                 10.0.0\nanyio               NA\nasttokens           NA\nattr                23.1.0\nbabel               2.12.1\nbackcall            0.2.0\ncertifi             2023.11.17\ncffi                1.15.1\ncharset_normalizer  3.1.0\ncolorama            0.4.6\ncomm                0.1.3\ncycler              0.12.1\ncython_runtime      NA\ndateutil            2.8.2\ndebugpy             1.6.7\ndecorator           5.1.1\ndefusedxml          0.7.1\nexceptiongroup      1.2.0\nexecuting           1.2.0\nfastjsonschema      NA\nfuture              0.18.3\ngmpy2               2.1.2\ngseapy              1.0.6\nh5py                3.9.0\nidna                3.4\nigraph              0.10.8\nipykernel           6.23.1\nipython_genutils    0.2.0\njedi                0.18.2\njinja2              3.1.2\njoblib              1.3.2\njson5               NA\njsonpointer         2.0\njsonschema          4.17.3\njupyter_events      0.6.3\njupyter_server      2.6.0\njupyterlab_server   2.22.1\nkiwisolver          1.4.5\nleidenalg           0.10.1\nllvmlite            0.41.1\nlouvain             0.8.1\nmarkupsafe          2.1.2\nmatplotlib          3.8.0\nmatplotlib_inline   0.1.6\nmatplotlib_venn     0.11.9\nmpl_toolkits        NA\nmpmath              1.3.0\nnatsort             8.4.0\nnbformat            5.8.0\nnumba               0.58.1\nnumpy               1.26.3\nopt_einsum          v3.3.0\noverrides           NA\npackaging           23.1\npandas              2.2.0\nparso               0.8.3\npatsy               0.5.6\npexpect             4.8.0\npickleshare         0.7.5\npkg_resources       NA\nplatformdirs        3.5.1\nprometheus_client   NA\nprompt_toolkit      3.0.38\npsutil              5.9.5\nptyprocess          0.7.0\npure_eval           0.2.2\npvectorc            NA\npybiomart           0.2.0\npycparser           2.21\npydev_ipython       NA\npydevconsole        NA\npydevd              2.9.5\npydevd_file_utils   NA\npydevd_plugins      NA\npydevd_tracing      NA\npygments            2.15.1\npyparsing           3.1.1\npyrsistent          NA\npythonjsonlogger    NA\npytz                2023.3\nrequests            2.31.0\nrequests_cache      0.4.13\nrfc3339_validator   0.1.4\nrfc3986_validator   0.1.1\nscipy               1.12.0\nseaborn             0.13.2\nsend2trash          NA\nsession_info        1.0.0\nsix                 1.16.0\nsklearn             1.4.0\nsniffio             1.3.0\nsocks               1.7.1\nsparse              0.15.1\nstack_data          0.6.2\nstatsmodels         0.14.1\nsympy               1.12\ntexttable           1.7.0\nthreadpoolctl       3.2.0\ntorch               2.0.0\ntornado             6.3.2\ntqdm                4.65.0\ntraitlets           5.9.0\ntyping_extensions   NA\nurllib3             2.0.2\nwcwidth             0.2.6\nwebsocket           1.5.2\nyaml                6.0\nzmq                 25.0.2\nzoneinfo            NA\nzstandard           0.19.0\n-----\nIPython             8.13.2\njupyter_client      8.2.0\njupyter_core        5.3.0\njupyterlab          4.0.1\nnotebook            6.5.4\n-----\nPython 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) [GCC 11.3.0]\nLinux-5.15.0-92-generic-x86_64-with-glibc2.35\n-----\nSession information updated at 2024-02-06 00:41"
  },
  {
    "objectID": "labs/scanpy/scanpy_06_celltyping.html",
    "href": "labs/scanpy/scanpy_06_celltyping.html",
    "title": " Celltype prediction",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run Python commands unless it starts with %%bash, in which case, those chunks run shell commands.\nCelltype prediction can either be performed on indiviudal cells where each cell gets a predicted celltype label, or on the level of clusters. All methods are based on similarity to other datasets, single cell or sorted bulk RNAseq, or uses known marker genes for each cell type.\nIdeally celltype predictions should be run on each sample separately and not using the integrated data. In this case we will select one sample from the Covid data, ctrl_13 and predict celltype by cell on that sample.\nSome methods will predict a celltype to each cell based on what it is most similar to, even if that celltype is not included in the reference. Other methods include an uncertainty so that cells with low similarity scores will be unclassified.\nThere are multiple different methods to predict celltypes, here we will just cover a few of those.\nHere we will use a reference PBMC dataset that we get from scanpy datasets and classify celltypes based on two methods:\nFirst, lets load required libraries\nimport numpy as np\nimport pandas as pd\nimport scanpy as sc\nimport matplotlib.pyplot as plt\nimport warnings\nimport os\nimport urllib.request\n\nwarnings.simplefilter(action=\"ignore\", category=Warning)\n\n# verbosity: errors (0), warnings (1), info (2), hints (3)\nsc.settings.verbosity = 2\nsc.settings.set_figure_params(dpi=80)\nLet’s read in the saved Covid-19 data object from the clustering step.\n# download pre-computed data if missing or long compute\nfetch_data = True\n\n# url for source and intermediate data\npath_data = \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\n\npath_results = \"data/covid/results\"\nif not os.path.exists(path_results):\n    os.makedirs(path_results, exist_ok=True)\n\n# path_file = \"data/covid/results/scanpy_covid_qc_dr_scanorama_cl.h5ad\"\npath_file = \"data/covid/results/scanpy_covid_qc_dr_scanorama_cl.h5ad\"\nif fetch_data and not os.path.exists(path_file):\n    urllib.request.urlretrieve(os.path.join(\n        path_data, 'covid/results/scanpy_covid_qc_dr_scanorama_cl.h5ad'), path_file)\n\nadata = sc.read_h5ad(path_file)\nadata\n\nAnnData object with n_obs × n_vars = 7222 × 19468\n    obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info', 'leiden', 'leiden_0.4', 'leiden_0.6', 'leiden_1.0', 'leiden_1.4', 'kmeans5', 'kmeans10', 'kmeans15', 'hclust_5', 'hclust_10', 'hclust_15'\n    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n    uns: 'dendrogram_leiden_0.6', 'doublet_info_colors', 'hclust_10_colors', 'hclust_15_colors', 'hclust_5_colors', 'hvg', 'kmeans10_colors', 'kmeans15_colors', 'kmeans5_colors', 'leiden', 'leiden_0.4', 'leiden_0.4_colors', 'leiden_0.6', 'leiden_0.6_colors', 'leiden_1.0', 'leiden_1.0_colors', 'leiden_1.4', 'leiden_1.4_colors', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap'\n    obsm: 'Scanorama', 'X_pca', 'X_pca_harmony', 'X_tsne', 'X_tsne_bbknn', 'X_tsne_harmony', 'X_tsne_scanorama', 'X_tsne_uncorr', 'X_umap', 'X_umap_bbknn', 'X_umap_harmony', 'X_umap_scanorama', 'X_umap_uncorr'\n    obsp: 'connectivities', 'distances'\nadata.uns['log1p']['base']=None\nprint(adata.shape)\n\n(7222, 19468)\nSubset one patient.\nadata = adata[adata.obs[\"sample\"] == \"ctrl_13\",:]\nprint(adata.shape)\n\n(1121, 19468)\nadata.obs[\"leiden_0.6\"].value_counts()\n\n1     272\n0     217\n3     205\n5     129\n2     120\n4      63\n7      35\n6      28\n10     27\n8      13\n9      12\nName: leiden_0.6, dtype: int64\nSome clusters have very few cells from this individual, so any cluster comparisons may be biased by this.\nsc.pl.umap(\n    adata, color=[\"leiden_0.6\"], palette=sc.pl.palettes.default_20\n)"
  },
  {
    "objectID": "labs/scanpy/scanpy_06_celltyping.html#meta-ct_ref",
    "href": "labs/scanpy/scanpy_06_celltyping.html#meta-ct_ref",
    "title": " Celltype prediction",
    "section": "1 Reference data",
    "text": "1 Reference data\nLoad the reference data from scanpy.datasets. It is the annotated and processed pbmc3k dataset from 10x.\n\nadata_ref = sc.datasets.pbmc3k_processed() \n\nadata_ref.obs['sample']='pbmc3k'\n\nprint(adata_ref.shape)\nadata_ref.obs\n\n(2638, 1838)\n\n\n\n\n\n\n\n\n\nn_genes\npercent_mito\nn_counts\nlouvain\nsample\n\n\nindex\n\n\n\n\n\n\n\n\n\nAAACATACAACCAC-1\n781\n0.030178\n2419.0\nCD4 T cells\npbmc3k\n\n\nAAACATTGAGCTAC-1\n1352\n0.037936\n4903.0\nB cells\npbmc3k\n\n\nAAACATTGATCAGC-1\n1131\n0.008897\n3147.0\nCD4 T cells\npbmc3k\n\n\nAAACCGTGCTTCCG-1\n960\n0.017431\n2639.0\nCD14+ Monocytes\npbmc3k\n\n\nAAACCGTGTATGCG-1\n522\n0.012245\n980.0\nNK cells\npbmc3k\n\n\n...\n...\n...\n...\n...\n...\n\n\nTTTCGAACTCTCAT-1\n1155\n0.021104\n3459.0\nCD14+ Monocytes\npbmc3k\n\n\nTTTCTACTGAGGCA-1\n1227\n0.009294\n3443.0\nB cells\npbmc3k\n\n\nTTTCTACTTCCTCG-1\n622\n0.021971\n1684.0\nB cells\npbmc3k\n\n\nTTTGCATGAGAGGC-1\n454\n0.020548\n1022.0\nB cells\npbmc3k\n\n\nTTTGCATGCCTCAC-1\n724\n0.008065\n1984.0\nCD4 T cells\npbmc3k\n\n\n\n\n2638 rows × 5 columns\n\n\n\n\nsc.pl.umap(adata_ref, color='louvain')\n\n\n\n\n\n\n\n\nMake sure we have the same genes in both datset by taking the intersection\n\nprint(adata_ref.shape[1])\nprint(adata.shape[1])\nvar_names = adata_ref.var_names.intersection(adata.var_names)\nprint(len(var_names))\n\nadata_ref = adata_ref[:, var_names]\nadata = adata[:, var_names]\n\n1838\n2626\n419\n\n\nFirst we need to rerun pca and umap with the same gene set for both datasets.\n\nsc.pp.pca(adata_ref)\nsc.pp.neighbors(adata_ref)\nsc.tl.umap(adata_ref)\nsc.pl.umap(adata_ref, color='louvain')\n\ncomputing PCA\n    with n_comps=50\n    finished (0:00:00)\ncomputing neighbors\n    using 'X_pca' with n_pcs = 50\n    finished (0:00:12)\ncomputing UMAP\n    finished (0:00:04)\n\n\n\n\n\n\n\n\n\n\nsc.pp.pca(adata)\nsc.pp.neighbors(adata)\nsc.tl.umap(adata)\nsc.pl.umap(adata, color='louvain_0.6')\n\ncomputing PCA\n    on highly variable genes\n    with n_comps=50\n    finished (0:00:00)\ncomputing neighbors\n    using 'X_pca' with n_pcs = 50\n    finished (0:00:00)\ncomputing UMAP\n    finished (0:00:02)"
  },
  {
    "objectID": "labs/scanpy/scanpy_06_celltyping.html#integrate-with-scanorama",
    "href": "labs/scanpy/scanpy_06_celltyping.html#integrate-with-scanorama",
    "title": " Celltype prediction",
    "section": "2 Integrate with scanorama",
    "text": "2 Integrate with scanorama\n\nimport scanorama\n\n#subset the individual dataset to the same variable genes as in MNN-correct.\nalldata = dict()\nalldata['ctrl']=adata\nalldata['ref']=adata_ref\n\n#convert to list of AnnData objects\nadatas = list(alldata.values())\n\n# run scanorama.integrate\nscanorama.integrate_scanpy(adatas, dimred = 50)\n\nFound 1676 genes among all datasets\n[[0.         0.49509367]\n [0.         0.        ]]\nProcessing datasets (0, 1)\n\n\n\n# add in sample info\nadata_ref.obs['sample']='pbmc3k'\n\n# create a merged scanpy object and add in the scanorama \nadata_merged = alldata['ctrl'].concatenate(alldata['ref'], batch_key='sample', batch_categories=['ctrl','pbmc3k'])\n\nembedding = np.concatenate([ad.obsm['X_scanorama'] for ad in adatas], axis=0)\nadata_merged.obsm['Scanorama'] = embedding\n\n\n#run  umap.\nsc.pp.neighbors(adata_merged, n_pcs =50, use_rep = \"Scanorama\")\nsc.tl.umap(adata_merged)\n\ncomputing neighbors\n    finished (0:00:00)\ncomputing UMAP\n    finished (0:00:05)\n\n\n\nsc.pl.umap(adata_merged, color=[\"sample\",\"louvain\"])\n\n\n\n\n\n\n\n\n\n2.1 Label transfer\nUsing the functions from the Spatial tutorial from Scanpy we will calculate normalized cosine distances between the two datasets and tranfer labels to the celltype with the highest scores.\n\nfrom sklearn.metrics.pairwise import cosine_distances\n\ndistances = 1 - cosine_distances(\n    adata_merged[adata_merged.obs['sample'] == \"pbmc3k\"].obsm[\"Scanorama\"],\n    adata_merged[adata_merged.obs['sample'] == \"ctrl\"].obsm[\"Scanorama\"],\n)\n\ndef label_transfer(dist, labels, index):\n    lab = pd.get_dummies(labels)\n    class_prob = lab.to_numpy().T @ dist\n    norm = np.linalg.norm(class_prob, 2, axis=0)\n    class_prob = class_prob / norm\n    class_prob = (class_prob.T - class_prob.min(1)) / class_prob.ptp(1)\n    # convert to df\n    cp_df = pd.DataFrame(\n        class_prob, columns=lab.columns\n    )\n    cp_df.index = index\n    # classify as max score\n    m = cp_df.idxmax(axis=1)\n    \n    return m\n\nclass_def = label_transfer(distances, adata_ref.obs.louvain, adata.obs.index)\n\n# add to obs section of the original object\nadata.obs['label_trans'] = class_def\n\nsc.pl.umap(adata, color=\"label_trans\")\n\n\n\n\n\n\n\n\n\n# add to merged object.\nadata_merged.obs[\"label_trans\"] = pd.concat(\n    [class_def, adata_ref.obs[\"louvain\"]], axis=0\n).tolist()\n\nsc.pl.umap(adata_merged, color=[\"sample\",\"louvain\",'label_trans'])\n#plot only ctrl cells.\nsc.pl.umap(adata_merged[adata_merged.obs['sample']=='ctrl'], color='label_trans')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow plot how many cells of each celltypes can be found in each cluster.\n\ntmp = pd.crosstab(adata.obs['leiden_0.6'],adata.obs['label_trans'], normalize='index')\ntmp.plot.bar(stacked=True).legend(bbox_to_anchor=(1.8, 1),loc='upper right')"
  },
  {
    "objectID": "labs/scanpy/scanpy_06_celltyping.html#ingest",
    "href": "labs/scanpy/scanpy_06_celltyping.html#ingest",
    "title": " Celltype prediction",
    "section": "3 Ingest",
    "text": "3 Ingest\nAnother method for celltype prediction is Ingest, for more information, please look at https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html\n\nsc.tl.ingest(adata, adata_ref, obs='louvain')\nsc.pl.umap(adata, color=['louvain','leiden_0.6'], wspace=0.5)\n\nrunning ingest\n    finished (0:00:18)\n\n\n\n\n\n\n\n\n\nAs you can see, ingest has created a new umap for us, so to get consistent plotting, lets revert back to the old one for further plotting:\n\nadata.obsm[\"X_umap\"] = adata.obsm[\"X_umap_uncorr\"]\n\nsc.pl.umap(adata, color=['louvain','leiden_0.6'], wspace=0.5)\n\n\n\n\n\n\n\n\nNow plot how many cells of each celltypes can be found in each cluster.\n\ntmp = pd.crosstab(adata.obs['leiden_0.6'],adata.obs['louvain'], normalize='index')\ntmp.plot.bar(stacked=True).legend(bbox_to_anchor=(1.8, 1),loc='upper right')"
  },
  {
    "objectID": "labs/scanpy/scanpy_06_celltyping.html#compare-results",
    "href": "labs/scanpy/scanpy_06_celltyping.html#compare-results",
    "title": " Celltype prediction",
    "section": "5 Compare results",
    "text": "5 Compare results\nThe predictions from ingest is stored in the column ‘louvain’ while we named the label transfer with scanorama as ‘predicted’\n\nsc.pl.umap(adata, color=['louvain','label_trans','majority_voting', 'majority_voting_ref'], wspace=0.5, ncols=3)\n\n\n\n\n\n\n\n\nAs you can see, the main celltypes are generally the same, but there are clearly differences, especially with regards to the cells predicted as either ILC/NK/CD8 T-cells.\nThe only way to make sure which method you trust is to look at what genes the different celltypes express and use your biological knowledge to make decisions."
  },
  {
    "objectID": "labs/scanpy/scanpy_06_celltyping.html#gene-set-analysis",
    "href": "labs/scanpy/scanpy_06_celltyping.html#gene-set-analysis",
    "title": " Celltype prediction",
    "section": "6 Gene set analysis",
    "text": "6 Gene set analysis\nAnother way of predicting celltypes is to use the differentially expressed genes per cluster and compare to lists of known cell marker genes. This requires a list of genes that you trust and that is relevant for the tissue you are working on.\nYou can either run it with a marker list from the ontology or a list of your choice as in the example below.\n\npath_file = 'data/human_cell_markers.txt'\nif not os.path.exists(path_file):\n    urllib.request.urlretrieve(os.path.join(\n        path_data, 'human_cell_markers.txt'), path_file)\n\n\ndf = pd.read_table(path_file)\ndf\n\nprint(df.shape)\n\n(2868, 15)\n\n\n\n# Filter for number of genes per celltype\ndf['nG'] = df.geneSymbol.str.split(\",\").str.len()\n\ndf = df[df['nG'] &gt; 5]\ndf = df[df['nG'] &lt; 100]\nd = df[df['cancerType'] == \"Normal\"]\nprint(df.shape)\n\n(445, 16)\n\n\n\ndf.index = df.cellName\ngene_dict = df.geneSymbol.str.split(\",\").to_dict()\n\n\n# run differential expression per cluster\nsc.tl.rank_genes_groups(adata, 'leiden_0.6', method='wilcoxon', key_added = \"wilcoxon\")\n\nranking genes\n    finished (0:00:01)\n\n\n\n# do gene set overlap to the groups in the gene list and top 300 DEGs.\nimport gseapy\n\ngsea_res = dict()\npred = dict()\n\nfor cl in adata.obs['leiden_0.6'].cat.categories.tolist():\n    print(cl)\n    glist = sc.get.rank_genes_groups_df(adata, group=cl, key='wilcoxon')[\n        'names'].squeeze().str.strip().tolist()\n    enr_res = gseapy.enrichr(gene_list=glist[:300],\n                             organism='Human',\n                             gene_sets=gene_dict,\n                             background=adata.shape[1],\n                             cutoff=1)\n    if enr_res.results.shape[0] == 0:\n        pred[cl] = \"Unass\"\n    else:\n        enr_res.results.sort_values(\n            by=\"P-value\", axis=0, ascending=True, inplace=True)\n        print(enr_res.results.head(2))\n        gsea_res[cl] = enr_res\n        pred[cl] = enr_res.results[\"Term\"][0]\n\n0\n    Gene_set                      Term Overlap   P-value  Adjusted P-value  \\\n1   gs_ind_0     Cancer stem-like cell     1/6  0.088981          0.203113   \n21  gs_ind_0  Spermatogonial stem cell     1/6  0.088981          0.203113   \n\n    Odds Ratio  Combined Score  Genes  \n1    17.450448       42.218477  ANPEP  \n21   17.450448       42.218477   BCL6  \n1\n   Gene_set                    Term Overlap   P-value  Adjusted P-value  \\\n0  gs_ind_0         Effector T cell    1/13  0.182865          0.255621   \n2  gs_ind_0  Lake et al.Science.Ex1    1/14  0.195465          0.255621   \n\n   Odds Ratio  Combined Score    Genes  \n0    7.675392       13.040543    IL2RB  \n2    7.106474       11.600406  CCDC88C  \n2\n   Gene_set                      Term Overlap   P-value  Adjusted P-value  \\\n3  gs_ind_0                  Monocyte     1/7  0.103024          0.206048   \n4  gs_ind_0  Parietal progenitor cell     1/7  0.103024          0.206048   \n\n   Odds Ratio  Combined Score  Genes  \n3   14.764993       33.557802   CD52  \n4   14.764993       33.557802  ANXA1  \n3\n   Gene_set                    Term Overlap   P-value  Adjusted P-value  \\\n6  gs_ind_0  Effector memory T cell     1/7  0.103024          0.226084   \n8  gs_ind_0            Naive T cell     1/7  0.103024          0.226084   \n\n   Odds Ratio  Combined Score Genes  \n6   14.764993       33.557802  IL7R  \n8   14.764993       33.557802  CCR7  \n4\n   Gene_set      Term Overlap   P-value  Adjusted P-value  Odds Ratio  \\\n0  gs_ind_0    B cell     1/6  0.088981          0.116851   17.450448   \n4  gs_ind_0  Monocyte     1/7  0.103024          0.116851   14.764993   \n\n   Combined Score Genes  \n0       42.218477  CD19  \n4       33.557802  CD52  \n5\n   Gene_set                             Term Overlap   P-value  \\\n2  gs_ind_0                       Macrophage     1/6  0.088981   \n3  gs_ind_0  Monocyte derived dendritic cell     1/8  0.116851   \n\n   Adjusted P-value  Odds Ratio  Combined Score  Genes  \n2          0.233702   17.450448       42.218477   AIF1  \n3          0.233702   12.795659       27.470422  ITGAX  \n6\n   Gene_set        Term Overlap   P-value  Adjusted P-value  Odds Ratio  \\\n3  gs_ind_0  Macrophage     1/6  0.088981          0.154536   17.450448   \n4  gs_ind_0  Myeloblast     1/6  0.088981          0.154536   17.450448   \n\n   Combined Score  Genes  \n3       42.218477   AIF1  \n4       42.218477  CSF3R  \n7\n   Gene_set      Term Overlap   P-value  Adjusted P-value  Odds Ratio  \\\n0  gs_ind_0    B cell     1/6  0.088981          0.155801   17.450448   \n3  gs_ind_0  Monocyte     1/7  0.103024          0.155801   14.764993   \n\n   Combined Score Genes  \n0       42.218477  CD19  \n3       33.557802  CD52  \n8\n   Gene_set                    Term Overlap   P-value  Adjusted P-value  \\\n1  gs_ind_0         Effector T cell    1/13  0.182865          0.365730   \n0  gs_ind_0  Circulating fetal cell    1/40  0.463035          0.463035   \n\n   Odds Ratio  Combined Score  Genes  \n1    7.675392       13.040543  IL2RB  \n0    2.425498        1.867518   ACTB  \n9\n   Gene_set                    Term Overlap   P-value  Adjusted P-value  \\\n2  gs_ind_0  Lake et al.Science.In4     1/6  0.088981          0.212585   \n1  gs_ind_0             Immune cell     1/7  0.103024          0.212585   \n\n   Odds Ratio  Combined Score    Genes  \n2   17.450448       42.218477  ADAMTS2  \n1   14.764993       33.557802     CD14  \n10\n   Gene_set        Term Overlap   P-value  Adjusted P-value  Odds Ratio  \\\n1  gs_ind_0  Macrophage     1/6  0.088981          0.171706   17.450448   \n2  gs_ind_0  Myeloblast     1/6  0.088981          0.171706   17.450448   \n\n   Combined Score  Genes  \n1       42.218477   AIF1  \n2       42.218477  CSF3R  \n\n\n\n# prediction per cluster\npred\n\n{'0': 'CD16+ dendritic cell',\n '1': 'Effector T cell',\n '2': 'CD8+ T cell',\n '3': 'Activated T cell',\n '4': 'B cell',\n '5': 'Circulating fetal cell',\n '6': 'Circulating fetal cell',\n '7': 'B cell',\n '8': 'Circulating fetal cell',\n '9': 'Circulating fetal cell',\n '10': 'Circulating fetal cell'}\n\n\n\nprediction = [pred[x] for x in adata.obs['leiden_0.6']]\nadata.obs[\"GS_overlap_pred\"] = prediction\n\nsc.pl.umap(adata, color='GS_overlap_pred')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nAs you can see, it agrees to some extent with the predictions from the methods above, but there are clear differences, which do you think looks better?"
  },
  {
    "objectID": "labs/scanpy/scanpy_06_celltyping.html#meta-session",
    "href": "labs/scanpy/scanpy_06_celltyping.html#meta-session",
    "title": " Celltype prediction",
    "section": "6 Session info",
    "text": "6 Session info\n\n\nClick here\n\n\nsc.logging.print_versions()\n\n-----\nanndata     0.10.5.post1\nscanpy      1.9.6\n-----\nPIL                 10.0.0\nannoy               NA\nanyio               NA\narray_api_compat    1.4.1\nasttokens           NA\nattr                23.1.0\nbabel               2.12.1\nbackcall            0.2.0\ncertifi             2023.11.17\ncffi                1.15.1\ncharset_normalizer  3.1.0\ncolorama            0.4.6\ncomm                0.1.3\ncycler              0.12.1\ncython_runtime      NA\ndateutil            2.8.2\ndebugpy             1.6.7\ndecorator           5.1.1\ndefusedxml          0.7.1\nexceptiongroup      1.2.0\nexecuting           1.2.0\nfastjsonschema      NA\nfbpca               NA\ngmpy2               2.1.2\ngseapy              1.0.6\nh5py                3.9.0\nidna                3.4\nigraph              0.10.8\nintervaltree        NA\nipykernel           6.23.1\nipython_genutils    0.2.0\njedi                0.18.2\njinja2              3.1.2\njoblib              1.3.2\njson5               NA\njsonpointer         2.0\njsonschema          4.17.3\njupyter_events      0.6.3\njupyter_server      2.6.0\njupyterlab_server   2.22.1\nkiwisolver          1.4.5\nleidenalg           0.10.1\nllvmlite            0.41.1\nlouvain             0.8.1\nmarkupsafe          2.1.2\nmatplotlib          3.8.0\nmatplotlib_inline   0.1.6\nmpl_toolkits        NA\nmpmath              1.3.0\nnatsort             8.4.0\nnbformat            5.8.0\nnumba               0.58.1\nnumpy               1.26.3\nopt_einsum          v3.3.0\noverrides           NA\npackaging           23.1\npandas              2.2.0\nparso               0.8.3\npatsy               0.5.6\npexpect             4.8.0\npickleshare         0.7.5\npkg_resources       NA\nplatformdirs        3.5.1\nprometheus_client   NA\nprompt_toolkit      3.0.38\npsutil              5.9.5\nptyprocess          0.7.0\npure_eval           0.2.2\npvectorc            NA\npycparser           2.21\npydev_ipython       NA\npydevconsole        NA\npydevd              2.9.5\npydevd_file_utils   NA\npydevd_plugins      NA\npydevd_tracing      NA\npygments            2.15.1\npynndescent         0.5.11\npyparsing           3.1.1\npyrsistent          NA\npythonjsonlogger    NA\npytz                2023.3\nrequests            2.31.0\nrfc3339_validator   0.1.4\nrfc3986_validator   0.1.1\nscanorama           1.7.4\nscipy               1.12.0\nsend2trash          NA\nsession_info        1.0.0\nsix                 1.16.0\nsklearn             1.4.0\nsniffio             1.3.0\nsocks               1.7.1\nsortedcontainers    2.4.0\nsparse              0.15.1\nstack_data          0.6.2\nstatsmodels         0.14.1\nsympy               1.12\ntexttable           1.7.0\nthreadpoolctl       3.2.0\ntorch               2.0.0\ntornado             6.3.2\ntqdm                4.65.0\ntraitlets           5.9.0\ntyping_extensions   NA\numap                0.5.5\nurllib3             2.0.2\nwcwidth             0.2.6\nwebsocket           1.5.2\nyaml                6.0\nzmq                 25.0.2\nzoneinfo            NA\nzstandard           0.19.0\n-----\nIPython             8.13.2\njupyter_client      8.2.0\njupyter_core        5.3.0\njupyterlab          4.0.1\nnotebook            6.5.4\n-----\nPython 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) [GCC 11.3.0]\nLinux-5.15.0-92-generic-x86_64-with-glibc2.35\n-----\nSession information updated at 2024-02-06 00:43"
  },
  {
    "objectID": "labs/scanpy/scanpy_07_trajectory.html",
    "href": "labs/scanpy/scanpy_07_trajectory.html",
    "title": " Trajectory inference using PAGA",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run Python commands unless it starts with %%bash, in which case, those chunks run shell commands.\nPartly following this PAGA tutorial with some modifications."
  },
  {
    "objectID": "labs/scanpy/scanpy_07_trajectory.html#loading-libraries",
    "href": "labs/scanpy/scanpy_07_trajectory.html#loading-libraries",
    "title": " Trajectory inference using PAGA",
    "section": "1 Loading libraries",
    "text": "1 Loading libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as pl\nfrom matplotlib import rcParams\nimport scanpy as sc\n\nimport scipy\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.simplefilter(action=\"ignore\", category=Warning)\n\n# verbosity: errors (0), warnings (1), info (2), hints (3)\nsc.settings.verbosity = 3\nsc.settings.set_figure_params(dpi=100, frameon=False, figsize=(5, 5), facecolor='white', color_map = 'viridis_r')"
  },
  {
    "objectID": "labs/scanpy/scanpy_07_trajectory.html#preparing-data",
    "href": "labs/scanpy/scanpy_07_trajectory.html#preparing-data",
    "title": " Trajectory inference using PAGA",
    "section": "2 Preparing data",
    "text": "2 Preparing data\nIn order to speed up the computations during the exercises, we will be using a subset of a bone marrow dataset (originally containing about 100K cells). The bone marrow is the source of adult immune cells, and contains virtually all differentiation stages of cell from the immune system which later circulate in the blood to all other organs.\n\n\n\n\n\nIf you have been using the Seurat, Bioconductor or Scanpy toolkits with your own data, you need to reach to the point where can find get:\n\nA dimensionality reduction where to perform the trajectory (for example: PCA, ICA, MNN, harmony, Diffusion Maps, UMAP)\nThe cell clustering information (for example: from Louvain, k-means)\nA KNN/SNN graph (this is useful to inspect and sanity-check your trajectories)\n\nIn this case, all the data has been preprocessed with Seurat with standard pipelines. In addition there was some manual filtering done to remove clusters that are disconnected and cells that are hard to cluster, which can be seen in this script\nThe file trajectory_scanpy_filtered.h5ad was converted from the Seurat object using the SeuratDisk package. For more information on how it was done, have a look at the script: convert_to_h5ad.R in the github repo.\nYou can download the data with the commands:\n\nimport os\nimport urllib.request\n\n# download pre-computed data if missing or long compute\nfetch_data = True\n\n# url for source and intermediate data\npath_data = \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\n\npath_results = \"data/trajectory\"\nif not os.path.exists(path_results):\n    os.makedirs(path_results, exist_ok=True)\n\npath_file = \"data/trajectory/trajectory_seurat_filtered.h5ad\"\nif not os.path.exists(path_file):\n    file_url = os.path.join(\n        path_data, \"trajectory/trajectory_seurat_filtered.h5ad\")\n    urllib.request.urlretrieve(file_url, path_file)"
  },
  {
    "objectID": "labs/scanpy/scanpy_07_trajectory.html#reading-data",
    "href": "labs/scanpy/scanpy_07_trajectory.html#reading-data",
    "title": " Trajectory inference using PAGA",
    "section": "3 Reading data",
    "text": "3 Reading data\nWe already have pre-computed and subsetted the dataset (with 6688 cells and 3585 genes) following the analysis steps in this course. We then saved the objects, so you can use common tools to open and start to work with them (either in R or Python).\n\nadata = sc.read_h5ad(\"data/trajectory/trajectory_seurat_filtered.h5ad\")\nadata.var\n\n\n\n\n\n\n\n\nfeatures\n\n\n\n\n0610040J01Rik\n0610040J01Rik\n\n\n1190007I07Rik\n1190007I07Rik\n\n\n1500009L16Rik\n1500009L16Rik\n\n\n1700012B09Rik\n1700012B09Rik\n\n\n1700020L24Rik\n1700020L24Rik\n\n\n...\n...\n\n\nSqor\nSqor\n\n\nSting1\nSting1\n\n\nTent5a\nTent5a\n\n\nTlcd4\nTlcd4\n\n\nZnrd2\nZnrd2\n\n\n\n\n3585 rows × 1 columns\n\n\n\n\n# check what you have in the X matrix, should be lognormalized counts.\nprint(adata.X[:10,:10])\n\n  (0, 4)    0.11622072805743532\n  (0, 8)    0.4800893970571722\n  (1, 8)    0.2478910541698065\n  (1, 9)    0.17188973970230348\n  (2, 1)    0.09413397843954842\n  (2, 7)    0.18016412971724202\n  (3, 1)    0.08438841021254412\n  (3, 4)    0.08438841021254412\n  (3, 7)    0.08438841021254412\n  (3, 8)    0.3648216463668793\n  (4, 1)    0.14198147850903975\n  (4, 8)    0.14198147850903975\n  (5, 1)    0.17953169693896723\n  (5, 8)    0.17953169693896723\n  (5, 9)    0.17953169693896723\n  (6, 4)    0.2319546390006887\n  (6, 8)    0.42010741700351195\n  (7, 1)    0.1775659421407816\n  (7, 8)    0.39593115482156394\n  (7, 9)    0.09271901219711086\n  (8, 1)    0.12089079757716388\n  (8, 8)    0.22873058755480363\n  (9, 1)    0.08915380247493314\n  (9, 4)    0.08915380247493314\n  (9, 8)    0.38270398718590104"
  },
  {
    "objectID": "labs/scanpy/scanpy_07_trajectory.html#explore-the-data",
    "href": "labs/scanpy/scanpy_07_trajectory.html#explore-the-data",
    "title": " Trajectory inference using PAGA",
    "section": "4 Explore the data",
    "text": "4 Explore the data\nThere is a umap and clusters provided with the object, first plot some information from the previous analysis onto the umap.\n\nsc.pl.umap(adata, color = ['clusters','dataset','batches','Phase'],legend_loc = 'on data', legend_fontsize = 'xx-small', ncols = 2)\n\n\n\n\n\n\n\n\nIt is crucial that you performing analysis of a dataset understands what is going on, what are the clusters you see in your data and most importantly How are the clusters related to each other?. Well, let’s explore the data a bit. With the help of this table, write down which cluster numbers in your dataset express these key markers.\n\n\n\nMarker\nCell Type\n\n\n\n\nCd34\nHSC progenitor\n\n\nMs4a1\nB cell lineage\n\n\nCd3e\nT cell lineage\n\n\nLtf\nGranulocyte lineage\n\n\nCst3\nMonocyte lineage\n\n\nMcpt8\nMast Cell lineage\n\n\nAlas2\nRBC lineage\n\n\nSiglech\nDendritic cell lineage\n\n\nC1qc\nMacrophage cell lineage\n\n\nPf4\nMegakaryocyte cell lineage\n\n\n\n\nmarkers = [\"Cd34\",\"Alas2\",\"Pf4\",\"Mcpt8\",\"Ltf\",\"Cst3\", \"Siglech\", \"C1qc\", \"Ms4a1\", \"Cd3e\", ]\nsc.pl.umap(adata, color = markers, use_raw = False, ncols = 4)"
  },
  {
    "objectID": "labs/scanpy/scanpy_07_trajectory.html#rerun-analysis-in-scanpy",
    "href": "labs/scanpy/scanpy_07_trajectory.html#rerun-analysis-in-scanpy",
    "title": " Trajectory inference using PAGA",
    "section": "5 Rerun analysis in Scanpy",
    "text": "5 Rerun analysis in Scanpy\nRedo clustering and umap using the basic Scanpy pipeline. Use the provided “X_harmony_Phase” dimensionality reduction as the staring point.\n\n# first, store the old umap with a new name so it is not overwritten\nadata.obsm['X_umap_old'] = adata.obsm['X_umap']\n\nsc.pp.neighbors(adata, n_pcs = 30, n_neighbors = 20, use_rep=\"X_harmony_Phase\")\nsc.tl.umap(adata, min_dist=0.4, spread=3)\n\ncomputing neighbors\n    finished: added to `.uns['neighbors']`\n    `.obsp['distances']`, distances for each pair of neighbors\n    `.obsp['connectivities']`, weighted adjacency matrix (0:00:12)\ncomputing UMAP\n    finished: added\n    'X_umap', UMAP coordinates (adata.obsm) (0:00:09)\n\n\n\n#sc.tl.umap(adata, min_dist=0.6, spread=1.5)\nsc.pl.umap(adata, color = ['clusters'],legend_loc = 'on data', legend_fontsize = 'xx-small', edges = True)\n\nsc.pl.umap(adata, color = markers, use_raw = False, ncols = 4)\n\n# Redo clustering as well\nsc.tl.leiden(adata, key_added = \"leiden_1.0\", resolution = 1.0) # default resolution in 1.0\nsc.tl.leiden(adata, key_added = \"leiden_1.2\", resolution = 1.2) # default resolution in 1.0\nsc.tl.leiden(adata, key_added = \"leiden_1.4\", resolution = 1.4) # default resolution in 1.0\n\n#sc.tl.louvain(adata, key_added = \"leiden_1.0\") # default resolution in 1.0\nsc.pl.umap(adata, color = ['leiden_1.0', 'leiden_1.2', 'leiden_1.4','clusters'],legend_loc = 'on data', legend_fontsize = 'xx-small', ncols =2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrunning Leiden clustering\n    finished: found 16 clusters and added\n    'leiden_1.0', the cluster labels (adata.obs, categorical) (0:00:03)\nrunning Leiden clustering\n    finished: found 17 clusters and added\n    'leiden_1.2', the cluster labels (adata.obs, categorical) (0:00:02)\nrunning Leiden clustering\n    finished: found 19 clusters and added\n    'leiden_1.4', the cluster labels (adata.obs, categorical) (0:00:02)\n\n\n\n\n\n\n\n\n\n\n#Rename clusters with really clear markers, the rest are left unlabelled.\n\nannot = pd.DataFrame(adata.obs['leiden_1.4'].astype('string'))\nannot[annot['leiden_1.4'] == '10'] = '10_megakaryo' #Pf4\nannot[annot['leiden_1.4'] == '17'] = '17_macro'  #C1qc\nannot[annot['leiden_1.4'] == '11'] = '11_eryth' #Alas2\nannot[annot['leiden_1.4'] == '18'] = '18_dend' #Siglech\nannot[annot['leiden_1.4'] == '13'] = '13_mast' #Mcpt8\nannot[annot['leiden_1.4'] == '0'] = '0_mono' #Cts3\nannot[annot['leiden_1.4'] == '1'] = '1_gran' #Ltf\nannot[annot['leiden_1.4'] == '9'] = '9_gran'\nannot[annot['leiden_1.4'] == '14'] = '14_TC' #Cd3e\nannot[annot['leiden_1.4'] == '16'] = '16_BC' #Ms4a1\nannot[annot['leiden_1.4'] == '8'] = '8_progen'  # Cd34\nannot[annot['leiden_1.4'] == '4'] = '4_progen' \nannot[annot['leiden_1.4'] == '5'] = '5_progen'\n\nadata.obs['annot']=annot['leiden_1.4'].astype('category')\n\nsc.pl.umap(adata, color = 'annot',legend_loc = 'on data', legend_fontsize = 'xx-small', ncols =2)\n\nannot.value_counts()\n#type(annot)\n\n# astype('category')\n\n\n\n\n\n\n\n\nleiden_1.4  \n0_mono          509\n1_gran          487\n2               479\n3               463\n4_progen        387\n5_progen        384\n7               368\n6               368\n8_progen        367\n9_gran          366\n10_megakaryo    301\n11_eryth        294\n12              276\n13_mast         159\n14_TC           151\n15              128\n16_BC           124\n17_macro        116\n18_dend         101\nName: count, dtype: int64\n\n\n\n# plot onto the Seurat embedding:\nsc.pl.embedding(adata, basis='X_umap_old', color = 'annot',legend_loc = 'on data', legend_fontsize = 'xx-small', ncols =2)"
  },
  {
    "objectID": "labs/scanpy/scanpy_07_trajectory.html#run-paga",
    "href": "labs/scanpy/scanpy_07_trajectory.html#run-paga",
    "title": " Trajectory inference using PAGA",
    "section": "6 Run PAGA",
    "text": "6 Run PAGA\nUse the clusters from leiden clustering with leiden_1.4 and run PAGA. First we create the graph and initialize the positions using the umap.\n\n# use the umap to initialize the graph layout.\nsc.tl.draw_graph(adata, init_pos='X_umap')\nsc.pl.draw_graph(adata, color='annot', legend_loc='on data', legend_fontsize = 'xx-small')\nsc.tl.paga(adata, groups='annot')\nsc.pl.paga(adata, color='annot', edge_width_scale = 0.3)\n\ndrawing single-cell graph using layout 'fa'\n    finished: added\n    'X_draw_graph_fa', graph_drawing coordinates (adata.obsm) (0:00:54)\nrunning PAGA\n    finished: added\n    'paga/connectivities', connectivities adjacency (adata.uns)\n    'paga/connectivities_tree', connectivities subtree (adata.uns) (0:00:00)\n--&gt; added 'pos', the PAGA positions (adata.uns['paga'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, we have edges between many clusters that we know are are unrelated, so we may need to clean up the data a bit more."
  },
  {
    "objectID": "labs/scanpy/scanpy_07_trajectory.html#filtering-graph-edges",
    "href": "labs/scanpy/scanpy_07_trajectory.html#filtering-graph-edges",
    "title": " Trajectory inference using PAGA",
    "section": "7 Filtering graph edges",
    "text": "7 Filtering graph edges\nFirst, lets explore the graph a bit. So we plot the umap with the graph connections on top.\n\nsc.pl.umap(adata, edges=True, color = 'annot', legend_loc= 'on data', legend_fontsize= 'xx-small')\n\n\n\n\n\n\n\n\nWe have many edges in the graph between unrelated clusters, so lets try with fewer neighbors.\n\nsc.pp.neighbors(adata, n_neighbors=5,  use_rep = 'X_harmony_Phase', n_pcs = 30)\nsc.pl.umap(adata, edges=True, color = 'annot', legend_loc= 'on data', legend_fontsize= 'xx-small')\n\ncomputing neighbors\n    finished: added to `.uns['neighbors']`\n    `.obsp['distances']`, distances for each pair of neighbors\n    `.obsp['connectivities']`, weighted adjacency matrix (0:00:00)\n\n\n\n\n\n\n\n\n\n\n7.1 Rerun PAGA again on the data\n\nsc.tl.draw_graph(adata, init_pos='X_umap')\nsc.pl.draw_graph(adata, color='annot', legend_loc='on data', legend_fontsize = 'xx-small')\n\ndrawing single-cell graph using layout 'fa'\n    finished: added\n    'X_draw_graph_fa', graph_drawing coordinates (adata.obsm) (0:00:51)\n\n\n\n\n\n\n\n\n\n\nsc.tl.paga(adata, groups='annot')\nsc.pl.paga(adata, color='annot', edge_width_scale = 0.3)\n\nrunning PAGA\n    finished: added\n    'paga/connectivities', connectivities adjacency (adata.uns)\n    'paga/connectivities_tree', connectivities subtree (adata.uns) (0:00:00)\n--&gt; added 'pos', the PAGA positions (adata.uns['paga'])"
  },
  {
    "objectID": "labs/scanpy/scanpy_07_trajectory.html#embedding-using-paga-initialization",
    "href": "labs/scanpy/scanpy_07_trajectory.html#embedding-using-paga-initialization",
    "title": " Trajectory inference using PAGA",
    "section": "8 Embedding using PAGA-initialization",
    "text": "8 Embedding using PAGA-initialization\nWe can now redraw the graph using another starting position from the paga layout. The following is just as well possible for a UMAP.\n\nsc.tl.draw_graph(adata, init_pos='paga')\n\ndrawing single-cell graph using layout 'fa'\n    finished: added\n    'X_draw_graph_fa', graph_drawing coordinates (adata.obsm) (0:00:54)\n\n\nNow we can see all marker genes also at single-cell resolution in a meaningful layout.\n\nsc.pl.draw_graph(adata, color=['annot'], legend_loc='on data', legend_fontsize=  'xx-small')\n\n\n\n\n\n\n\n\nCompare the 2 graphs\n\nsc.pl.paga_compare(\n    adata, threshold=0.03, title='', right_margin=0.2, size=10, edge_width_scale=0.5,\n    legend_fontsize=12, fontsize=12, frameon=False, edges=True)\n\n--&gt; added 'pos', the PAGA positions (adata.uns['paga'])"
  },
  {
    "objectID": "labs/scanpy/scanpy_07_trajectory.html#gene-changes",
    "href": "labs/scanpy/scanpy_07_trajectory.html#gene-changes",
    "title": " Trajectory inference using PAGA",
    "section": "9 Gene changes",
    "text": "9 Gene changes\nWe can reconstruct gene changes along PAGA paths for a given set of genes\nChoose a root cell for diffusion pseudotime. We have 3 progenitor clusters, but cluster 5 seems the most clear.\n\nadata.uns['iroot'] = np.flatnonzero(adata.obs['annot']  == '5_progen')[0]\n\nsc.tl.dpt(adata)\n\nWARNING: Trying to run `tl.dpt` without prior call of `tl.diffmap`. Falling back to `tl.diffmap` with default parameters.\ncomputing Diffusion Maps using n_comps=15(=n_dcs)\ncomputing transitions\n    finished (0:00:00)\n    eigenvalues of transition matrix\n    [1.         0.9989591  0.997628   0.9970365  0.9956704  0.99334306\n     0.9918951  0.9915921  0.99013233 0.98801893 0.9870309  0.9861044\n     0.9851118  0.9845008  0.9839531 ]\n    finished: added\n    'X_diffmap', diffmap coordinates (adata.obsm)\n    'diffmap_evals', eigenvalues of transition matrix (adata.uns) (0:00:00)\ncomputing Diffusion Pseudotime using n_dcs=10\n    finished: added\n    'dpt_pseudotime', the pseudotime (adata.obs) (0:00:00)\n\n\nUse the full raw data for visualization.\n\nsc.pl.draw_graph(adata, color=['annot', 'dpt_pseudotime'], legend_loc='on data', legend_fontsize= 'x-small')\n\n\n\n\n\n\n\n\nBy looking at the different know lineages and the layout of the graph we define manually some paths to the graph that corresponds to specific lineages.\n\n# Define paths\n\npaths = [('erythrocytes', ['5_progen', '8_progen', '6', '3', '7', '11_eryth']),\n         ('lympoid', ['5_progen', '12', '16_BC', '14_TC']),\n         ('granulo', ['5_progen', '4_progen', '2', '9_gran', '1_gran']),\n         ('mono', ['5_progen', '4_progen', '0_mono', '18_dend', '17_macro'])\n         ]\n\nadata.obs['distance'] = adata.obs['dpt_pseudotime']\n\nThen we select some genes that can vary in the lineages and plot onto the paths.\n\ngene_names = ['Gata2', 'Gata1', 'Klf1', 'Epor', 'Hba-a2',  # erythroid\n              'Elane', 'Cebpe', 'Gfi1',                    # neutrophil\n              'Irf8', 'Csf1r', 'Ctsg',                     # monocyte\n              'Itga2b','Prss34','Cma1','Procr',            # Megakaryo,Basophil,Mast,HPC\n              'C1qc','Siglech','Ms4a1','Cd3e','Cd34']\n\n\n_, axs = pl.subplots(ncols=4, figsize=(10, 4), gridspec_kw={\n                     'wspace': 0.05, 'left': 0.12})\npl.subplots_adjust(left=0.05, right=0.98, top=0.82, bottom=0.2)\nfor ipath, (descr, path) in enumerate(paths):\n    _, data = sc.pl.paga_path(\n        adata, path, gene_names,\n        show_node_names=False,\n        ax=axs[ipath],\n        ytick_fontsize=12,\n        left_margin=0.15,\n        n_avg=50,\n        annotations=['distance'],\n        show_yticks=True if ipath == 0 else False,\n        show_colorbar=False,\n        color_map='Greys',\n        groups_key='annot',\n        color_maps_annotations={'distance': 'viridis'},\n        title='{} path'.format(descr),\n        return_data=True,\n        use_raw=False,\n        show=False)\n    data.to_csv('data/trajectory/paga_path_{}.csv'.format(descr))\npl.savefig('data/trajectory/paga_path.pdf')\npl.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nAs you can see, we can manipulate the trajectory quite a bit by selecting different number of neighbors, components etc. to fit with our assumptions on the development of these celltypes.\nPlease explore further how you can tweak the trajectory. For instance, can you create a PAGA trajectory using the orignial umap from Seurat instead? Hint, you first need to compute the neighbors on the umap."
  },
  {
    "objectID": "labs/scanpy/scanpy_07_trajectory.html#session-info",
    "href": "labs/scanpy/scanpy_07_trajectory.html#session-info",
    "title": " Trajectory inference using PAGA",
    "section": "10 Session info",
    "text": "10 Session info\n\n\nClick here\n\n\nsc.logging.print_versions()\n\n-----\nanndata     0.10.5.post1\nscanpy      1.9.6\n-----\nPIL                 10.0.0\nanyio               NA\nasttokens           NA\nattr                23.1.0\nbabel               2.12.1\nbackcall            0.2.0\ncertifi             2023.11.17\ncffi                1.15.1\ncharset_normalizer  3.1.0\ncolorama            0.4.6\ncomm                0.1.3\ncycler              0.12.1\ncython_runtime      NA\ndateutil            2.8.2\ndebugpy             1.6.7\ndecorator           5.1.1\ndefusedxml          0.7.1\nexceptiongroup      1.2.0\nexecuting           1.2.0\nfa2                 NA\nfastjsonschema      NA\nfontTools           4.47.2\ngmpy2               2.1.2\nh5py                3.9.0\nidna                3.4\nigraph              0.10.8\nipykernel           6.23.1\nipython_genutils    0.2.0\njedi                0.18.2\njinja2              3.1.2\njoblib              1.3.2\njson5               NA\njsonpointer         2.0\njsonschema          4.17.3\njupyter_events      0.6.3\njupyter_server      2.6.0\njupyterlab_server   2.22.1\nkiwisolver          1.4.5\nleidenalg           0.10.1\nllvmlite            0.41.1\nlouvain             0.8.1\nmarkupsafe          2.1.2\nmatplotlib          3.8.0\nmatplotlib_inline   0.1.6\nmpl_toolkits        NA\nmpmath              1.3.0\nnatsort             8.4.0\nnbformat            5.8.0\nnetworkx            3.2.1\nnumba               0.58.1\nnumpy               1.26.3\nopt_einsum          v3.3.0\noverrides           NA\npackaging           23.1\npandas              2.2.0\nparso               0.8.3\npexpect             4.8.0\npickleshare         0.7.5\npkg_resources       NA\nplatformdirs        3.5.1\nprometheus_client   NA\nprompt_toolkit      3.0.38\npsutil              5.9.5\nptyprocess          0.7.0\npure_eval           0.2.2\npvectorc            NA\npycparser           2.21\npydev_ipython       NA\npydevconsole        NA\npydevd              2.9.5\npydevd_file_utils   NA\npydevd_plugins      NA\npydevd_tracing      NA\npygments            2.15.1\npynndescent         0.5.11\npyparsing           3.1.1\npyrsistent          NA\npythonjsonlogger    NA\npytz                2023.3\nrequests            2.31.0\nrfc3339_validator   0.1.4\nrfc3986_validator   0.1.1\nscipy               1.12.0\nsend2trash          NA\nsession_info        1.0.0\nsix                 1.16.0\nsklearn             1.4.0\nsniffio             1.3.0\nsocks               1.7.1\nsparse              0.15.1\nstack_data          0.6.2\nsympy               1.12\ntexttable           1.7.0\nthreadpoolctl       3.2.0\ntorch               2.0.0\ntornado             6.3.2\ntqdm                4.65.0\ntraitlets           5.9.0\ntyping_extensions   NA\numap                0.5.5\nurllib3             2.0.2\nwcwidth             0.2.6\nwebsocket           1.5.2\nyaml                6.0\nzmq                 25.0.2\nzoneinfo            NA\nzstandard           0.19.0\n-----\nIPython             8.13.2\njupyter_client      8.2.0\njupyter_core        5.3.0\njupyterlab          4.0.1\nnotebook            6.5.4\n-----\nPython 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) [GCC 11.3.0]\nLinux-5.15.0-92-generic-x86_64-with-glibc2.35\n-----\nSession information updated at 2024-02-06 00:46"
  },
  {
    "objectID": "labs/scanpy/scanpy_08_spatial.html",
    "href": "labs/scanpy/scanpy_08_spatial.html",
    "title": " Spatial Transcriptomics",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run Python commands unless it starts with %%bash, in which case, those chunks run shell commands.\nAdapted from tutorials by Giovanni Palla (https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html) and Carlos Talavera-López (https://docs.scvi-tools.org/en/latest/tutorials/notebooks/stereoscope_heart_LV_tutorial.html)\nSpatial transcriptomic data with the Visium platform is in many ways similar to scRNAseq data. It contains UMI counts for 5-20 cells instead of single cells, but is still quite sparse in the same way as scRNAseq data is, but with the additional information about spatial location in the tissue.\nHere we will first run quality control in a similar manner to scRNAseq data, then QC filtering, dimensionality reduction, integration and clustering. Then we will use scRNAseq data from mouse cortex to run label transfer to predict celltypes in the Visium spots.\nWe will use two Visium spatial transcriptomics dataset of the mouse brain (Sagittal), which are publicly available from the 10x genomics website. Note, that these dataset have already been filtered for spots that does not overlap with the tissue."
  },
  {
    "objectID": "labs/scanpy/scanpy_08_spatial.html#meta-st_prep",
    "href": "labs/scanpy/scanpy_08_spatial.html#meta-st_prep",
    "title": " Spatial Transcriptomics",
    "section": "1 Preparation",
    "text": "1 Preparation\nLoad packages\n\nimport scanpy as sc\nimport anndata as an\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scanorama\nimport warnings\nimport os\n\nwarnings.simplefilter(action=\"ignore\", category=Warning)\n\nsc.set_figure_params(facecolor=\"white\", figsize=(8, 8))\nsc.settings.verbosity = 3\n\nLoad ST data\nThe function datasets.visium_sge() downloads the dataset from 10x genomics and returns an AnnData object that contains counts, images and spatial coordinates. We will calculate standards QC metrics with pp.calculate_qc_metrics() and visualize them.\nWhen using your own Visium data, use Scanpy’s read_visium() function to import it.\n\n# download pre-computed data if missing or long compute\nfetch_data = True\n\n# url for source and intermediate data\npath_data = \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\n\nif not os.path.exists(\"./data/spatial/visium\"):\n    os.makedirs(\"./data/spatial/visium\")\n\n\nadata_anterior = sc.datasets.visium_sge(\n    sample_id=\"V1_Mouse_Brain_Sagittal_Anterior\"\n)\nadata_posterior = sc.datasets.visium_sge(\n    sample_id=\"V1_Mouse_Brain_Sagittal_Posterior\"\n)\n\nreading /work/labs/scanpy/data/V1_Mouse_Brain_Sagittal_Anterior/filtered_feature_bc_matrix.h5\n (0:00:00)\nreading /work/labs/scanpy/data/V1_Mouse_Brain_Sagittal_Posterior/filtered_feature_bc_matrix.h5\n (0:00:00)\n\n\n\nadata_anterior.var_names_make_unique()\nadata_posterior.var_names_make_unique()\n\nTo make sure that both images are included in the merged object, use uns_merge=“unique”.\n\n# merge into one dataset\nlibrary_names = [\"V1_Mouse_Brain_Sagittal_Anterior\", \"V1_Mouse_Brain_Sagittal_Posterior\"]\n\nadata = adata_anterior.concatenate(\n    adata_posterior,\n    batch_key=\"library_id\",\n    uns_merge=\"unique\",\n    batch_categories=library_names\n)\n\nadata\n\nAnnData object with n_obs × n_vars = 6050 × 32285\n    obs: 'in_tissue', 'array_row', 'array_col', 'library_id'\n    var: 'gene_ids', 'feature_types', 'genome'\n    uns: 'spatial'\n    obsm: 'spatial'\n\n\nAs you can see, we now have the slot spatial in obsm, which contains the spatial information from the Visium platform."
  },
  {
    "objectID": "labs/scanpy/scanpy_08_spatial.html#meta-st_qc",
    "href": "labs/scanpy/scanpy_08_spatial.html#meta-st_qc",
    "title": " Spatial Transcriptomics",
    "section": "2 Quality control",
    "text": "2 Quality control\nSimilar to scRNA-seq we use statistics on number of counts, number of features and percent mitochondria for quality control.\n\n# add info on mitochondrial and hemoglobin genes to the objects.\nadata.var['mt'] = adata.var_names.str.startswith('mt-') \nadata.var['hb'] = adata.var_names.str.contains((\"^Hb.*-\"))\n\nsc.pp.calculate_qc_metrics(adata, qc_vars=['mt','hb'], percent_top=None, log1p=False, inplace=True)\n\nsc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt', 'pct_counts_hb'], jitter=0.4, groupby = 'library_id', rotation= 45)\n\n\n\n\n\n\n\n\nWe can also plot the same data onto the tissue section.\nIn scanpy, this is a bit tricky when you have multiple sections, as you would have to subset and plot them separately.\n\n# need to plot the two sections separately and specify the library_id\nfor library in library_names:\n    sc.pl.spatial(adata[adata.obs.library_id == library,:], library_id=library, color = [\"total_counts\", \"n_genes_by_counts\",'pct_counts_mt', 'pct_counts_hb'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, the spots with low number of counts/features and high mitochondrial content are mainly towards the edges of the tissue. It is quite likely that these regions are damaged tissue. You may also see regions within a tissue with low quality if you have tears or folds in your section.\nBut remember, for some tissue types, the amount of genes expressed and proportion mitochondria may also be a biological features, so bear in mind what tissue you are working on and what these features mean.\n\n2.1 Filter spots\nSelect all spots with less than 25% mitocondrial reads, less than 20% hb-reads and 500 detected genes. You must judge for yourself based on your knowledge of the tissue what are appropriate filtering criteria for your dataset.\n\nkeep = (adata.obs['pct_counts_hb'] &lt; 20) & (adata.obs['pct_counts_mt'] &lt; 25) & (adata.obs['n_genes_by_counts'] &gt; 1000)\nprint(sum(keep))\n\nadata = adata[keep,:]\n\n5749\n\n\nAnd replot onto tissue sections.\n\nfor library in library_names:\n    sc.pl.spatial(adata[adata.obs.library_id == library,:], library_id=library, color = [\"total_counts\", \"n_genes_by_counts\",'pct_counts_mt', 'pct_counts_hb'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 Top expressed genes\nAs for scRNA-seq data, we will look at what the top expressed genes are.\n\nsc.pl.highest_expr_genes(adata, n_top=20)\n\nnormalizing counts per cell\n    finished (0:00:00)\n\n\n\n\n\n\n\n\n\nAs you can see, the mitochondrial genes are among the top expressed genes. Also the lncRNA gene Bc1 (brain cytoplasmic RNA 1). Also one hemoglobin gene.\n\n\n2.3 Filter genes\nWe will remove the Bc1 gene, hemoglobin genes (blood contamination) and the mitochondrial genes.\n\nmito_genes = adata.var_names.str.startswith('mt-')\nhb_genes = adata.var_names.str.contains('^Hb.*-')\n\nremove = np.add(mito_genes, hb_genes)\nremove[adata.var_names == \"Bc1\"] = True\nkeep = np.invert(remove)\nprint(sum(remove))\n\nadata = adata[:,keep]\n\nprint(adata.n_obs, adata.n_vars)\n\n22\n5749 32263"
  },
  {
    "objectID": "labs/scanpy/scanpy_08_spatial.html#meta-st_analysis",
    "href": "labs/scanpy/scanpy_08_spatial.html#meta-st_analysis",
    "title": " Spatial Transcriptomics",
    "section": "3 Analysis",
    "text": "3 Analysis\nWe will proceed with the data in a very similar manner to scRNA-seq data.\nAs we have two sections, we will select variable genes with batch_key=“library_id” and then take the union of variable genes for further analysis. The idea is to avoid including batch specific genes in the analysis.\n\n# save the counts to a separate object for later, we need the normalized counts in raw for DEG dete\ncounts_adata = adata.copy()\n\nsc.pp.normalize_total(adata, inplace=True)\nsc.pp.log1p(adata)\n# take 1500 variable genes per batch and then use the union of them.\nsc.pp.highly_variable_genes(adata, flavor=\"seurat\", n_top_genes=1500, inplace=True, batch_key=\"library_id\")\n\n# subset for variable genes\nadata.raw = adata\nadata = adata[:,adata.var.highly_variable_nbatches &gt; 0]\n\n# scale data\nsc.pp.scale(adata)\n\nnormalizing counts per cell\n    finished (0:00:00)\nIf you pass `n_top_genes`, all cutoffs are ignored.\nextracting highly variable genes\n    finished (0:00:02)\n--&gt; added\n    'highly_variable', boolean vector (adata.var)\n    'means', float vector (adata.var)\n    'dispersions', float vector (adata.var)\n    'dispersions_norm', float vector (adata.var)\n... as `zero_center=True`, sparse input is densified and may lead to large memory consumption\n\n\nNow we can plot gene expression of individual genes, the gene Hpca is a strong hippocampal marker and Ttr is a marker of the choroid plexus.\n\nfor library in library_names:\n    sc.pl.spatial(adata[adata.obs.library_id == library,:], library_id=library, color = [\"Ttr\", \"Hpca\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1 Dimensionality reduction and clustering\nWe can then now run dimensionality reduction and clustering using the same workflow as we use for scRNA-seq analysis.\n\nsc.pp.neighbors(adata)\nsc.tl.umap(adata)\nsc.tl.leiden(adata, key_added=\"clusters\")\n\ncomputing neighbors\nWARNING: You’re trying to run this on 2405 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n         Falling back to preprocessing with `sc.pp.pca` and default params.\ncomputing PCA\n    with n_comps=50\n    finished (0:00:03)\n    finished: added to `.uns['neighbors']`\n    `.obsp['distances']`, distances for each pair of neighbors\n    `.obsp['connectivities']`, weighted adjacency matrix (0:00:16)\ncomputing UMAP\n    finished: added\n    'X_umap', UMAP coordinates (adata.obsm) (0:00:08)\nrunning Leiden clustering\n    finished: found 23 clusters and added\n    'clusters', the cluster labels (adata.obs, categorical) (0:00:01)\n\n\nWe can then plot clusters onto umap or onto the tissue section.\n\nsc.pl.umap(\n    adata, color=[\"clusters\", \"library_id\"], palette=sc.pl.palettes.default_20\n)\n\nWARNING: Length of palette colors is smaller than the number of categories (palette length: 20, categories length: 23. Some categories will have the same color.\n\n\n\n\n\n\n\n\n\nAs we are plotting the two sections separately, we need to make sure that they get the same colors by fetching cluster colors from a dict.\n\nclusters_colors = dict(\n    zip([str(i) for i in range(len(adata.obs.clusters.cat.categories))], adata.uns[\"clusters_colors\"])\n)\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\n\nfor i, library in enumerate(\n    [\"V1_Mouse_Brain_Sagittal_Anterior\", \"V1_Mouse_Brain_Sagittal_Posterior\"]\n):\n    ad = adata[adata.obs.library_id == library, :].copy()\n    sc.pl.spatial(\n        ad,\n        img_key=\"hires\",\n        library_id=library,\n        color=\"clusters\",\n        size=1.5,\n        palette=[\n            v\n            for k, v in clusters_colors.items()\n            if k in ad.obs.clusters.unique().tolist()\n        ],\n        legend_loc=None,\n        show=False,\n        ax=axs[i],\n    )\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n3.2 Integration\nQuite often, there are strong batch effects between different ST sections, so it may be a good idea to integrate the data across sections.\nWe will do a similar integration as in the Data Integration lab, here we will use Scanorama for integration.\n\nadatas = {}\nfor batch in library_names:\n    adatas[batch] = adata[adata.obs['library_id'] == batch,]\n\nadatas \n\n{'V1_Mouse_Brain_Sagittal_Anterior': View of AnnData object with n_obs × n_vars = 2597 × 2405\n     obs: 'in_tissue', 'array_row', 'array_col', 'library_id', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_hb', 'pct_counts_hb', 'clusters'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection', 'mean', 'std'\n     uns: 'spatial', 'library_id_colors', 'log1p', 'hvg', 'neighbors', 'umap', 'leiden', 'clusters_colors'\n     obsm: 'spatial', 'X_pca', 'X_umap'\n     obsp: 'distances', 'connectivities',\n 'V1_Mouse_Brain_Sagittal_Posterior': View of AnnData object with n_obs × n_vars = 3152 × 2405\n     obs: 'in_tissue', 'array_row', 'array_col', 'library_id', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_hb', 'pct_counts_hb', 'clusters'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection', 'mean', 'std'\n     uns: 'spatial', 'library_id_colors', 'log1p', 'hvg', 'neighbors', 'umap', 'leiden', 'clusters_colors'\n     obsm: 'spatial', 'X_pca', 'X_umap'\n     obsp: 'distances', 'connectivities'}\n\n\n\nimport scanorama\n\n#convert to list of AnnData objects\nadatas = list(adatas.values())\n\n# run scanorama.integrate\nscanorama.integrate_scanpy(adatas, dimred = 50)\n\n# Get all the integrated matrices.\nscanorama_int = [ad.obsm['X_scanorama'] for ad in adatas]\n\n# make into one matrix.\nall_s = np.concatenate(scanorama_int)\nprint(all_s.shape)\n\n# add to the AnnData object\nadata.obsm[\"Scanorama\"] = all_s\n\nadata\n\nFound 2405 genes among all datasets\n[[0.         0.47824413]\n [0.         0.        ]]\nProcessing datasets (0, 1)\n(5749, 50)\n\n\nAnnData object with n_obs × n_vars = 5749 × 2405\n    obs: 'in_tissue', 'array_row', 'array_col', 'library_id', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_hb', 'pct_counts_hb', 'clusters'\n    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection', 'mean', 'std'\n    uns: 'spatial', 'library_id_colors', 'log1p', 'hvg', 'neighbors', 'umap', 'leiden', 'clusters_colors'\n    obsm: 'spatial', 'X_pca', 'X_umap', 'Scanorama'\n    obsp: 'distances', 'connectivities'\n\n\nThen we run dimensionality reduction and clustering as before.\n\nsc.pp.neighbors(adata, use_rep=\"Scanorama\")\nsc.tl.umap(adata)\nsc.tl.leiden(adata, key_added=\"clusters\")\n\nsc.pl.umap(\n    adata, color=[\"clusters\", \"library_id\"], palette=sc.pl.palettes.default_20\n)\n\ncomputing neighbors\n    finished: added to `.uns['neighbors']`\n    `.obsp['distances']`, distances for each pair of neighbors\n    `.obsp['connectivities']`, weighted adjacency matrix (0:00:00)\ncomputing UMAP\n    finished: added\n    'X_umap', UMAP coordinates (adata.obsm) (0:00:08)\nrunning Leiden clustering\n    finished: found 19 clusters and added\n    'clusters', the cluster labels (adata.obs, categorical) (0:00:02)\n\n\n\n\n\n\n\n\n\nAs we have new clusters, we again need to make a new dict for cluster colors\n\nclusters_colors = dict(\n    zip([str(i) for i in range(len(adata.obs.clusters.cat.categories))], adata.uns[\"clusters_colors\"])\n)\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\n\nfor i, library in enumerate(\n    [\"V1_Mouse_Brain_Sagittal_Anterior\", \"V1_Mouse_Brain_Sagittal_Posterior\"]\n):\n    ad = adata[adata.obs.library_id == library, :].copy()\n    sc.pl.spatial(\n        ad,\n        img_key=\"hires\",\n        library_id=library,\n        color=\"clusters\",\n        size=1.5,\n        palette=[\n            v\n            for k, v in clusters_colors.items()\n            if k in ad.obs.clusters.unique().tolist()\n        ],\n        legend_loc=None,\n        show=False,\n        ax=axs[i],\n    )\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nDo you see any differences between the integrated and non-integrated clustering? Judge for yourself, which of the clusterings do you think looks best? As a reference, you can compare to brain regions in the Allen brain atlas.\n\n\n\n\n3.3 Spatially Variable Features\nThere are two main workflows to identify molecular features that correlate with spatial location within a tissue. The first is to perform differential expression based on spatially distinct clusters, the other is to find features that have spatial patterning without taking clusters or spatial annotation into account. First, we will do differential expression between clusters just as we did for the scRNAseq data before.\n\n# run t-test \nsc.tl.rank_genes_groups(adata, \"clusters\", method=\"wilcoxon\")\n# plot as heatmap for cluster5 genes\nsc.pl.rank_genes_groups_heatmap(adata, groups=\"5\", n_genes=10, groupby=\"clusters\")\n\nranking genes\n    finished: added to `.uns['rank_genes_groups']`\n    'names', sorted np.recarray to be indexed by group ids\n    'scores', sorted np.recarray to be indexed by group ids\n    'logfoldchanges', sorted np.recarray to be indexed by group ids\n    'pvals', sorted np.recarray to be indexed by group ids\n    'pvals_adj', sorted np.recarray to be indexed by group ids (0:00:14)\nWARNING: dendrogram data not found (using key=dendrogram_clusters). Running `sc.tl.dendrogram` with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` independently.\n    using 'X_pca' with n_pcs = 50\nStoring dendrogram info using `.uns['dendrogram_clusters']`\nWARNING: Groups are not reordered because the `groupby` categories and the `var_group_labels` are different.\ncategories: 0, 1, 2, etc.\nvar_group_labels: 5\n\n\n\n\n\n\n\n\n\n\n# plot onto spatial location\ntop_genes = sc.get.rank_genes_groups_df(adata, group='5',log2fc_min=0)['names'][:3]\n\nfor library in [\"V1_Mouse_Brain_Sagittal_Anterior\", \"V1_Mouse_Brain_Sagittal_Posterior\"]:\n    sc.pl.spatial(adata[adata.obs.library_id == library,:], library_id=library, color = top_genes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial transcriptomics allows researchers to investigate how gene expression trends varies in space, thus identifying spatial patterns of gene expression. For this purpose there are multiple methods, such as SpatailDE, SPARK, Trendsceek, HMRF and Splotch.\nWe use SpatialDE Svensson et al., a Gaussian process-based statistical framework that aims to identify spatially variable genes.\n\n\n\n\n\n\nCaution\n\n\n\nThis is a slow compute intensive step, we will not run this now and instead use a pre-computed file in the step below.\n\n# this code is not executed\n\nif not fetch_data:\n    import NaiveDE\n    import SpatialDE\n\n    counts = sc.get.obs_df(adata, keys=list(adata.var_names), use_raw=True)\n    total_counts = sc.get.obs_df(adata, keys=[\"total_counts\"])\n    norm_expr = NaiveDE.stabilize(counts.T).T\n    resid_expr = NaiveDE.regress_out(\n        total_counts, norm_expr.T, \"np.log(total_counts)\").T\n    results = SpatialDE.run(adata.obsm[\"spatial\"], resid_expr)\n\n    import pickle\n    with open('data/spatial/visium/scanpy_spatialde.pkl', 'wb') as file:\n        pickle.dump(results, file)\n\n\n\nDownload precomputed file.\n\npath_file = \"data/spatial/visium/scanpy_spatialde.pkl\"\nif fetch_data and not os.path.exists(path_file):\n    import urllib.request\n    file_url = os.path.join(\n        path_data, \"spatial/visium/results/scanpy_spatialde.pkl\")\n    urllib.request.urlretrieve(file_url, path_file)\n\n\nimport pickle\nwith open('data/spatial/visium/scanpy_spatialde.pkl', 'rb') as file:\n    results = pickle.load(file)\n\n\n# skip for now.\n\n# We concatenate the results with the DataFrame of annotations of variables: `adata.var`.\nresults.index = results[\"g\"]\nadata.var = pd.concat(\n    [adata.var, results.loc[adata.var.index.values, :]], axis=1)\nadata.write_h5ad('./data/spatial/visium/adata_processed_sc.h5ad')\n\n# Then we can inspect significant genes that varies in space and visualize them with `sc.pl.spatial` function.\nresults.sort_values(\"qval\").head(10)"
  },
  {
    "objectID": "labs/scanpy/scanpy_08_spatial.html#meta-st_ss",
    "href": "labs/scanpy/scanpy_08_spatial.html#meta-st_ss",
    "title": " Spatial Transcriptomics",
    "section": "4 Single cell data",
    "text": "4 Single cell data\nWe can use a scRNA-seq dataset as a reference to predict the proportion of different celltypes in the Visium spots. Keep in mind that it is important to have a reference that contains all the celltypes you expect to find in your spots. Ideally it should be a scRNA-seq reference from the exact same tissue. We will use a reference scRNA-seq dataset of ~14,000 adult mouse cortical cell taxonomy from the Allen Institute, generated with the SMART-Seq2 protocol.\nConveniently, you can also download the pre-processed dataset in h5ad format.\n\nimport urllib.request\nimport os\n\npath_file = \"data/spatial/visium/allen_cortex.h5ad\"\nif not os.path.exists(path_file):\n    file_url = os.path.join(\n        path_data, \"spatial/visium/allen_cortex.h5ad\")\n    urllib.request.urlretrieve(file_url, path_file)\n\n\nadata_cortex = sc.read_h5ad(\"data/spatial/visium/allen_cortex.h5ad\")\nadata_cortex\n\nAnnData object with n_obs × n_vars = 14249 × 34617\n    obs: 'orig.ident', 'nCount_RNA', 'nFeature_RNA', 'sample_id', 'sample_type', 'organism', 'donor', 'sex', 'age_days', 'eye_condition', 'genotype', 'driver_lines', 'reporter_lines', 'brain_hemisphere', 'brain_region', 'brain_subregion', 'injection_label_direction', 'injection_primary', 'injection_secondary', 'injection_tract', 'injection_material', 'injection_exclusion_criterion', 'facs_date', 'facs_container', 'facs_sort_criteria', 'rna_amplification_set', 'library_prep_set', 'library_prep_avg_size_bp', 'seq_name', 'seq_tube', 'seq_batch', 'total_reads', 'percent_exon_reads', 'percent_intron_reads', 'percent_intergenic_reads', 'percent_rrna_reads', 'percent_mt_exon_reads', 'percent_reads_unique', 'percent_synth_reads', 'percent_ecoli_reads', 'percent_aligned_reads_total', 'complexity_cg', 'genes_detected_cpm_criterion', 'genes_detected_fpkm_criterion', 'tdt_cpm', 'gfp_cpm', 'class', 'subclass', 'cluster', 'confusion_score', 'cluster_correlation', 'core_intermediate_call'\n    var: 'features'\n\n\nHere is the metadata for the cell annotation:\n\nadata_cortex.obs\n\n\n\n\n\n\n\n\norig.ident\nnCount_RNA\nnFeature_RNA\nsample_id\nsample_type\norganism\ndonor\nsex\nage_days\neye_condition\n...\ngenes_detected_cpm_criterion\ngenes_detected_fpkm_criterion\ntdt_cpm\ngfp_cpm\nclass\nsubclass\ncluster\nconfusion_score\ncluster_correlation\ncore_intermediate_call\n\n\n\n\nF1S4_160108_001_A01\n0\n1730700.0\n9029\n527128530\nCells\nMus musculus\n225675\nM\n53\nNormal\n...\n10445\n9222\n248.86\n248.86\nGABAergic\nVip\nVip Arhgap36 Hmcn1\n0.4385\n0.837229\nIntermediate\n\n\nF1S4_160108_001_B01\n0\n1909605.0\n10207\n527128536\nCells\nMus musculus\n225675\nM\n53\nNormal\n...\n11600\n10370\n289.61\n289.61\nGABAergic\nLamp5\nLamp5 Lsp1\n0.1025\n0.878743\nCore\n\n\nF1S4_160108_001_C01\n0\n1984948.0\n10578\n527128542\nCells\nMus musculus\n225675\nM\n53\nNormal\n...\n11848\n10734\n281.06\n281.06\nGABAergic\nLamp5\nLamp5 Lsp1\n0.0195\n0.887084\nCore\n\n\nF1S4_160108_001_D01\n0\n2291552.0\n8482\n527128548\nCells\nMus musculus\n225675\nM\n53\nNormal\n...\n9494\n8561\n390.02\n390.02\nGABAergic\nVip\nVip Crispld2 Htr2c\n0.2734\n0.843552\nCore\n\n\nF1S4_160108_001_E01\n0\n1757463.0\n8697\n527128554\nCells\nMus musculus\n225675\nM\n53\nNormal\n...\n10012\n8791\n253.92\n253.92\nGABAergic\nLamp5\nLamp5 Plch2 Dock5\n0.7532\n0.854994\nCore\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nFYS4_171004_104_C01\n2\n949356.0\n9141\n645142562\nCells\nMus musculus\n350650\nM\n51\nNormal\n...\n9629\n9229\n432.15\n432.15\nGlutamatergic\nL5 PT\nL5 PT VISp C1ql2 Cdh13\n0.0477\n0.885255\nCore\n\n\nFYS4_171004_104_D01\n2\n998736.0\n6927\n645142573\nCells\nMus musculus\n350650\nM\n51\nNormal\n...\n7701\n7023\n217.83\n217.83\nGABAergic\nSst\nSst Hpse Sema3c\n0.1064\n0.854499\nCore\n\n\nFYS4_171004_104_F01\n2\n1002766.0\n6936\n645142613\nCells\nMus musculus\n350650\nM\n51\nNormal\n...\n7888\n7054\n91.88\n91.88\nGlutamatergic\nL5 PT\nL5 PT VISp Chrna6\n0.0095\n0.822625\nCore\n\n\nFYS4_171004_104_G01\n2\n1025804.0\n8027\n645142648\nCells\nMus musculus\n350650\nM\n51\nNormal\n...\n8933\n8146\n127.77\n127.77\nGABAergic\nSst\nSst Calb2 Pdlim5\n0.2852\n0.856322\nCore\n\n\nFYS4_171004_104_H01\n2\n882435.0\n6574\n645142673\nCells\nMus musculus\n350650\nM\n51\nNormal\n...\n7393\n6687\n310.17\n310.17\nGABAergic\nPvalb\nPvalb Reln Tac1\n0.6089\n0.799198\nCore\n\n\n\n\n14249 rows × 52 columns\n\n\n\nThere is an issue with the raw matrix in this object that the gene names are not in the index, so we will put them back in.\n\nadata_cortex.raw.var.index = adata_cortex.raw.var._index\nadata_cortex.raw.var\n\n\n\n\n\n\n\n\n_index\n\n\n_index\n\n\n\n\n\n0610005C13Rik\n0610005C13Rik\n\n\n0610006L08Rik\n0610006L08Rik\n\n\n0610007P14Rik\n0610007P14Rik\n\n\n0610009B22Rik\n0610009B22Rik\n\n\n0610009E02Rik\n0610009E02Rik\n\n\n...\n...\n\n\nZzef1\nZzef1\n\n\nZzz3\nZzz3\n\n\na\na\n\n\nl7Rn6\nl7Rn6\n\n\nn-R5s136\nn-R5s136\n\n\n\n\n34617 rows × 1 columns\n\n\n\nThen we run the regular pipline with normalization and dimensionality reduction.\n\nsc.pp.normalize_total(adata_cortex, target_sum=1e5)\nsc.pp.log1p(adata_cortex)\nsc.pp.highly_variable_genes(adata_cortex, min_mean=0.0125, max_mean=3, min_disp=0.5)\nsc.pp.scale(adata_cortex, max_value=10)\nsc.tl.pca(adata_cortex, svd_solver='arpack')\nsc.pp.neighbors(adata_cortex, n_pcs=30)\nsc.tl.umap(adata_cortex)\nsc.pl.umap(adata_cortex, color=\"subclass\", legend_loc='on data')\n\nnormalizing counts per cell\n    finished (0:00:00)\nextracting highly variable genes\n    finished (0:00:03)\n--&gt; added\n    'highly_variable', boolean vector (adata.var)\n    'means', float vector (adata.var)\n    'dispersions', float vector (adata.var)\n    'dispersions_norm', float vector (adata.var)\n... as `zero_center=True`, sparse input is densified and may lead to large memory consumption\ncomputing PCA\n    on highly variable genes\n    with n_comps=50\n    finished (0:00:16)\ncomputing neighbors\n    using 'X_pca' with n_pcs = 30\n    finished: added to `.uns['neighbors']`\n    `.obsp['distances']`, distances for each pair of neighbors\n    `.obsp['connectivities']`, weighted adjacency matrix (0:00:18)\ncomputing UMAP\n    finished: added\n    'X_umap', UMAP coordinates (adata.obsm) (0:00:11)\n\n\n\n\n\n\n\n\n\n\nadata_cortex.obs.subclass.value_counts()\n\nsubclass\nL6 IT         1872\nSst           1741\nVip           1728\nL4            1401\nPvalb         1337\nLamp5         1122\nL2/3 IT        982\nL6 CT          960\nL5 IT          880\nL5 PT          544\nAstro          368\nNP             362\nL6b            358\nSncg           125\nEndo            94\nOligo           91\nVLMC            67\nSMC             55\nMacrophage      51\nMeis2           45\nPeri            32\nSerpinf1        27\nCR               7\nName: count, dtype: int64\n\n\nFor speed, and for a more fair comparison of the celltypes, we will subsample all celltypes to a maximum of 200 cells per class (subclass).\n\ntarget_cells = 200\n\nadatas2 = [adata_cortex[adata_cortex.obs.subclass == clust] for clust in adata_cortex.obs.subclass.cat.categories]\n\nfor dat in adatas2:\n    if dat.n_obs &gt; target_cells:\n          sc.pp.subsample(dat, n_obs=target_cells)\n\nadata_cortex = adatas2[0].concatenate(*adatas2[1:])\n\nadata_cortex.obs.subclass.value_counts()\n\nsubclass\nAstro         200\nL6 IT         200\nSst           200\nPvalb         200\nNP            200\nLamp5         200\nL6b           200\nVip           200\nL6 CT         200\nL5 PT         200\nL5 IT         200\nL4            200\nL2/3 IT       200\nSncg          125\nEndo           94\nOligo          91\nVLMC           67\nSMC            55\nMacrophage     51\nMeis2          45\nPeri           32\nSerpinf1       27\nCR              7\nName: count, dtype: int64\n\n\n\nsc.pl.umap(\n    adata_cortex, color=[\"class\", \"subclass\", \"genotype\", \"brain_region\"], palette=sc.pl.palettes.default_28\n)\n\nWARNING: Length of palette colors is smaller than the number of categories (palette length: 28, categories length: 61. Some categories will have the same color.\n\n\n\n\n\n\n\n\n\n\nsc.pl.umap(adata_cortex, color=\"subclass\", legend_loc = 'on data')"
  },
  {
    "objectID": "labs/scanpy/scanpy_08_spatial.html#meta-st_sub",
    "href": "labs/scanpy/scanpy_08_spatial.html#meta-st_sub",
    "title": " Spatial Transcriptomics",
    "section": "5 Subset ST for cortex",
    "text": "5 Subset ST for cortex\nSince the scRNAseq dataset was generated from the mouse cortex, we will subset the visium dataset in order to select mainly the spots part of the cortex. Note that the integration can also be performed on the whole brain slice, but it would give rise to false positive cell type assignments and therefore it should be interpreted with more care.\nFor deconvolution we will need the counts data, so we will subset from the counts_adata object that we created earlier.\n\nlib_a = \"V1_Mouse_Brain_Sagittal_Anterior\"\n\ncounts_adata.obs['clusters'] = adata.obs.clusters\n\nadata_anterior_subset = counts_adata[\n    (counts_adata.obs.library_id == lib_a) \n    & (counts_adata.obsm[\"spatial\"][:, 1] &lt; 6000), :\n].copy()\n\n# select also the cortex clusters\nadata_anterior_subset = adata_anterior_subset[adata_anterior_subset.obs.clusters.isin(['3','5','6']),:]\n\n# plot to check that we have the correct regions\n\nsc.pl.spatial(\n    adata_anterior_subset,\n    img_key=\"hires\",\n    library_id = lib_a,\n    color=['clusters'],\n    size=1.5\n)"
  },
  {
    "objectID": "labs/scanpy/scanpy_08_spatial.html#meta-st_deconv",
    "href": "labs/scanpy/scanpy_08_spatial.html#meta-st_deconv",
    "title": " Spatial Transcriptomics",
    "section": "6 Deconvolution",
    "text": "6 Deconvolution\nDeconvolution is a method to estimate the abundance (or proportion) of different celltypes in a bulkRNAseq dataset using a single cell reference. As the Visium data can be seen as a small bulk, we can both use methods for traditional bulkRNAseq as well as methods especially developed for Visium data. Some methods for deconvolution are DWLS, cell2location, Tangram, Stereoscope, RCTD, SCDC and many more.\nHere, we will use deconvolution with Stereoscope implemented in the SCVI-tools package. To read more about Stereoscope please check out this github page (https://github.com/almaan/stereoscope)\n\n6.1 Select genes for deconvolution\nMost deconvolution methods does a prior gene selection and there are different options that are used: - Use variable genes in the SC data. - Use variable genes in both SC and ST data - DE genes between clusters in the SC data.\nIn this case we will use top DE genes per cluster, so first we have to run DGE detection on the scRNAseq data.\n\nsc.tl.rank_genes_groups(adata_cortex, 'subclass', method = \"t-test\", n_genes=100, use_raw=False)\nsc.pl.rank_genes_groups_dotplot(adata_cortex, n_genes=3)\n\nranking genes\n    finished: added to `.uns['rank_genes_groups']`\n    'names', sorted np.recarray to be indexed by group ids\n    'scores', sorted np.recarray to be indexed by group ids\n    'logfoldchanges', sorted np.recarray to be indexed by group ids\n    'pvals', sorted np.recarray to be indexed by group ids\n    'pvals_adj', sorted np.recarray to be indexed by group ids (0:00:09)\nWARNING: dendrogram data not found (using key=dendrogram_subclass). Running `sc.tl.dendrogram` with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` independently.\n    using 'X_pca' with n_pcs = 50\nStoring dendrogram info using `.uns['dendrogram_subclass']`\n\n\n\n\n\n\n\n\n\n\nsc.tl.filter_rank_genes_groups(adata_cortex, min_fold_change=1)\n\ngenes = sc.get.rank_genes_groups_df(adata_cortex, group = None)\ngenes\n\nFiltering genes using: min_in_group_fraction: 0.25 min_fold_change: 1, max_out_group_fraction: 0.5\n\n\n\n\n\n\n\n\n\ngroup\nnames\nscores\nlogfoldchanges\npvals\npvals_adj\n\n\n\n\n0\nAstro\nSlc1a3\n160.842041\nNaN\n3.174856e-300\n2.358455e-298\n\n\n1\nAstro\nNtsr2\n150.887924\nNaN\n1.097071e-231\n6.056987e-230\n\n\n2\nAstro\nAtp1a2\n138.091385\n11.185134\n0.000000e+00\n0.000000e+00\n\n\n3\nAstro\nAldoc\n131.687424\nNaN\n0.000000e+00\n0.000000e+00\n\n\n4\nAstro\nApoe\n124.696922\n9.052421\n0.000000e+00\n0.000000e+00\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2295\nVip\nSrrm4\n18.501577\nNaN\n2.355896e-47\n1.812313e-45\n\n\n2296\nVip\nDlx1as\n18.485483\nNaN\n7.043876e-47\n5.347322e-45\n\n\n2297\nVip\nAplp2\n18.252756\nNaN\n1.859496e-52\n1.671953e-50\n\n\n2298\nVip\nRit2\n18.057701\nNaN\n1.506046e-48\n1.201263e-46\n\n\n2299\nVip\nRab3a\n17.847469\nNaN\n4.922345e-66\n6.788718e-64\n\n\n\n\n2300 rows × 6 columns\n\n\n\n\ndeg = genes.names.unique().tolist()\nprint(len(deg))\n# check that the genes are also present in the ST data\n\ndeg = np.intersect1d(deg,adata_anterior_subset.var.index).tolist()\nprint(len(deg))\n\n1388\n1307\n\n\n\n\n6.2 Train the model\nFirst, train the model using scRNAseq data.\nStereoscope requires the data to be in counts, earlier in this tutorial we saved the spatial counts in a separate object counts_adata.\nIn the single cell data we have the raw counts in the raw.X matrix so that one will be used. So here we create a new object with all the correct slots for scVI.\n\nsc_adata = adata_cortex.copy()\nsc_adata.X = adata_cortex.raw.X.copy()\n\nSetup the anndata, the implementation requires the counts matrix to be in the “counts” layer as a copy.\n\nimport scvi\n# from scvi.data import register_tensor_from_anndata\nfrom scvi.external import RNAStereoscope, SpatialStereoscope\n\n# add counts layer\nsc_adata.layers[\"counts\"] = sc_adata.X.copy()\n\n# subset for the selected genes\nsc_adata = sc_adata[:, deg].copy()\n\n# create stereoscope object\nRNAStereoscope.setup_anndata(sc_adata, layer=\"counts\", labels_key=\"subclass\")\n\n\n# the model is saved to a file, so if is slow to run, you can simply reload it from disk by setting train = False\n\ntrain = True\nif train:\n    sc_model = RNAStereoscope(sc_adata)\n    sc_model.train(max_epochs=300)\n    sc_model.history[\"elbo_train\"][10:].plot()\n    sc_model.save(\"./data/spatial/visium/scanpy_scmodel\", overwrite=True)\nelse:\n    sc_model = RNAStereoscope.load(\"./data/spatial/visium/scanpy_scmodel\", sc_adata)\n    print(\"Loaded RNA model from file!\")\n\nTraining:   0%|          | 0/300 [00:00&lt;?, ?it/s]Epoch 1/300:   0%|          | 0/300 [00:00&lt;?, ?it/s]Epoch 1/300:   0%|          | 1/300 [00:00&lt;01:24,  3.54it/s]Epoch 1/300:   0%|          | 1/300 [00:00&lt;01:24,  3.54it/s, v_num=1, train_loss_step=3.84e+10, train_loss_epoch=4.64e+10]Epoch 2/300:   0%|          | 1/300 [00:00&lt;01:24,  3.54it/s, v_num=1, train_loss_step=3.84e+10, train_loss_epoch=4.64e+10]Epoch 2/300:   1%|          | 2/300 [00:00&lt;01:21,  3.64it/s, v_num=1, train_loss_step=3.84e+10, train_loss_epoch=4.64e+10]Epoch 2/300:   1%|          | 2/300 [00:00&lt;01:21,  3.64it/s, v_num=1, train_loss_step=2.78e+10, train_loss_epoch=3.44e+10]Epoch 3/300:   1%|          | 2/300 [00:00&lt;01:21,  3.64it/s, v_num=1, train_loss_step=2.78e+10, train_loss_epoch=3.44e+10]Epoch 3/300:   1%|          | 3/300 [00:00&lt;01:20,  3.70it/s, v_num=1, train_loss_step=2.78e+10, train_loss_epoch=3.44e+10]Epoch 3/300:   1%|          | 3/300 [00:00&lt;01:20,  3.70it/s, v_num=1, train_loss_step=2.27e+10, train_loss_epoch=2.61e+10]Epoch 4/300:   1%|          | 3/300 [00:00&lt;01:20,  3.70it/s, v_num=1, train_loss_step=2.27e+10, train_loss_epoch=2.61e+10]Epoch 4/300:   1%|▏         | 4/300 [00:01&lt;01:19,  3.71it/s, v_num=1, train_loss_step=2.27e+10, train_loss_epoch=2.61e+10]Epoch 4/300:   1%|▏         | 4/300 [00:01&lt;01:19,  3.71it/s, v_num=1, train_loss_step=1.72e+10, train_loss_epoch=2.02e+10]Epoch 5/300:   1%|▏         | 4/300 [00:01&lt;01:19,  3.71it/s, v_num=1, train_loss_step=1.72e+10, train_loss_epoch=2.02e+10]Epoch 5/300:   2%|▏         | 5/300 [00:01&lt;01:19,  3.70it/s, v_num=1, train_loss_step=1.72e+10, train_loss_epoch=2.02e+10]Epoch 5/300:   2%|▏         | 5/300 [00:01&lt;01:19,  3.70it/s, v_num=1, train_loss_step=1.34e+10, train_loss_epoch=1.6e+10] Epoch 6/300:   2%|▏         | 5/300 [00:01&lt;01:19,  3.70it/s, v_num=1, train_loss_step=1.34e+10, train_loss_epoch=1.6e+10]Epoch 6/300:   2%|▏         | 6/300 [00:01&lt;01:19,  3.69it/s, v_num=1, train_loss_step=1.34e+10, train_loss_epoch=1.6e+10]Epoch 6/300:   2%|▏         | 6/300 [00:01&lt;01:19,  3.69it/s, v_num=1, train_loss_step=1.06e+10, train_loss_epoch=1.3e+10]Epoch 7/300:   2%|▏         | 6/300 [00:01&lt;01:19,  3.69it/s, v_num=1, train_loss_step=1.06e+10, train_loss_epoch=1.3e+10]Epoch 7/300:   2%|▏         | 7/300 [00:01&lt;01:18,  3.74it/s, v_num=1, train_loss_step=1.06e+10, train_loss_epoch=1.3e+10]Epoch 7/300:   2%|▏         | 7/300 [00:01&lt;01:18,  3.74it/s, v_num=1, train_loss_step=9.05e+9, train_loss_epoch=1.07e+10]Epoch 8/300:   2%|▏         | 7/300 [00:01&lt;01:18,  3.74it/s, v_num=1, train_loss_step=9.05e+9, train_loss_epoch=1.07e+10]Epoch 8/300:   3%|▎         | 8/300 [00:02&lt;01:18,  3.74it/s, v_num=1, train_loss_step=9.05e+9, train_loss_epoch=1.07e+10]Epoch 8/300:   3%|▎         | 8/300 [00:02&lt;01:18,  3.74it/s, v_num=1, train_loss_step=7.03e+9, train_loss_epoch=8.91e+9] Epoch 9/300:   3%|▎         | 8/300 [00:02&lt;01:18,  3.74it/s, v_num=1, train_loss_step=7.03e+9, train_loss_epoch=8.91e+9]Epoch 9/300:   3%|▎         | 9/300 [00:02&lt;01:18,  3.71it/s, v_num=1, train_loss_step=7.03e+9, train_loss_epoch=8.91e+9]Epoch 9/300:   3%|▎         | 9/300 [00:02&lt;01:18,  3.71it/s, v_num=1, train_loss_step=6.47e+9, train_loss_epoch=7.54e+9]Epoch 10/300:   3%|▎         | 9/300 [00:02&lt;01:18,  3.71it/s, v_num=1, train_loss_step=6.47e+9, train_loss_epoch=7.54e+9]Epoch 10/300:   3%|▎         | 10/300 [00:02&lt;01:24,  3.43it/s, v_num=1, train_loss_step=6.47e+9, train_loss_epoch=7.54e+9]Epoch 10/300:   3%|▎         | 10/300 [00:02&lt;01:24,  3.43it/s, v_num=1, train_loss_step=5.5e+9, train_loss_epoch=6.45e+9] Epoch 11/300:   3%|▎         | 10/300 [00:02&lt;01:24,  3.43it/s, v_num=1, train_loss_step=5.5e+9, train_loss_epoch=6.45e+9]Epoch 11/300:   4%|▎         | 11/300 [00:03&lt;01:22,  3.51it/s, v_num=1, train_loss_step=5.5e+9, train_loss_epoch=6.45e+9]Epoch 11/300:   4%|▎         | 11/300 [00:03&lt;01:22,  3.51it/s, v_num=1, train_loss_step=4.97e+9, train_loss_epoch=5.58e+9]Epoch 12/300:   4%|▎         | 11/300 [00:03&lt;01:22,  3.51it/s, v_num=1, train_loss_step=4.97e+9, train_loss_epoch=5.58e+9]Epoch 12/300:   4%|▍         | 12/300 [00:03&lt;01:20,  3.59it/s, v_num=1, train_loss_step=4.97e+9, train_loss_epoch=5.58e+9]Epoch 12/300:   4%|▍         | 12/300 [00:03&lt;01:20,  3.59it/s, v_num=1, train_loss_step=4.38e+9, train_loss_epoch=4.87e+9]Epoch 13/300:   4%|▍         | 12/300 [00:03&lt;01:20,  3.59it/s, v_num=1, train_loss_step=4.38e+9, train_loss_epoch=4.87e+9]Epoch 13/300:   4%|▍         | 13/300 [00:03&lt;01:19,  3.63it/s, v_num=1, train_loss_step=4.38e+9, train_loss_epoch=4.87e+9]Epoch 13/300:   4%|▍         | 13/300 [00:03&lt;01:19,  3.63it/s, v_num=1, train_loss_step=3.69e+9, train_loss_epoch=4.29e+9]Epoch 14/300:   4%|▍         | 13/300 [00:03&lt;01:19,  3.63it/s, v_num=1, train_loss_step=3.69e+9, train_loss_epoch=4.29e+9]Epoch 14/300:   5%|▍         | 14/300 [00:03&lt;01:18,  3.65it/s, v_num=1, train_loss_step=3.69e+9, train_loss_epoch=4.29e+9]Epoch 14/300:   5%|▍         | 14/300 [00:03&lt;01:18,  3.65it/s, v_num=1, train_loss_step=3.35e+9, train_loss_epoch=3.8e+9] Epoch 15/300:   5%|▍         | 14/300 [00:03&lt;01:18,  3.65it/s, v_num=1, train_loss_step=3.35e+9, train_loss_epoch=3.8e+9]Epoch 15/300:   5%|▌         | 15/300 [00:04&lt;01:17,  3.67it/s, v_num=1, train_loss_step=3.35e+9, train_loss_epoch=3.8e+9]Epoch 15/300:   5%|▌         | 15/300 [00:04&lt;01:17,  3.67it/s, v_num=1, train_loss_step=3.12e+9, train_loss_epoch=3.39e+9]Epoch 16/300:   5%|▌         | 15/300 [00:04&lt;01:17,  3.67it/s, v_num=1, train_loss_step=3.12e+9, train_loss_epoch=3.39e+9]Epoch 16/300:   5%|▌         | 16/300 [00:04&lt;01:17,  3.69it/s, v_num=1, train_loss_step=3.12e+9, train_loss_epoch=3.39e+9]Epoch 16/300:   5%|▌         | 16/300 [00:04&lt;01:17,  3.69it/s, v_num=1, train_loss_step=2.98e+9, train_loss_epoch=3.04e+9]Epoch 17/300:   5%|▌         | 16/300 [00:04&lt;01:17,  3.69it/s, v_num=1, train_loss_step=2.98e+9, train_loss_epoch=3.04e+9]Epoch 17/300:   6%|▌         | 17/300 [00:04&lt;01:16,  3.71it/s, v_num=1, train_loss_step=2.98e+9, train_loss_epoch=3.04e+9]Epoch 17/300:   6%|▌         | 17/300 [00:04&lt;01:16,  3.71it/s, v_num=1, train_loss_step=2.49e+9, train_loss_epoch=2.74e+9]Epoch 18/300:   6%|▌         | 17/300 [00:04&lt;01:16,  3.71it/s, v_num=1, train_loss_step=2.49e+9, train_loss_epoch=2.74e+9]Epoch 18/300:   6%|▌         | 18/300 [00:04&lt;01:16,  3.69it/s, v_num=1, train_loss_step=2.49e+9, train_loss_epoch=2.74e+9]Epoch 18/300:   6%|▌         | 18/300 [00:04&lt;01:16,  3.69it/s, v_num=1, train_loss_step=2.19e+9, train_loss_epoch=2.49e+9]Epoch 19/300:   6%|▌         | 18/300 [00:04&lt;01:16,  3.69it/s, v_num=1, train_loss_step=2.19e+9, train_loss_epoch=2.49e+9]Epoch 19/300:   6%|▋         | 19/300 [00:05&lt;01:15,  3.70it/s, v_num=1, train_loss_step=2.19e+9, train_loss_epoch=2.49e+9]Epoch 19/300:   6%|▋         | 19/300 [00:05&lt;01:15,  3.70it/s, v_num=1, train_loss_step=2.1e+9, train_loss_epoch=2.27e+9] Epoch 20/300:   6%|▋         | 19/300 [00:05&lt;01:15,  3.70it/s, v_num=1, train_loss_step=2.1e+9, train_loss_epoch=2.27e+9]Epoch 20/300:   7%|▋         | 20/300 [00:05&lt;01:15,  3.71it/s, v_num=1, train_loss_step=2.1e+9, train_loss_epoch=2.27e+9]Epoch 20/300:   7%|▋         | 20/300 [00:05&lt;01:15,  3.71it/s, v_num=1, train_loss_step=1.8e+9, train_loss_epoch=2.07e+9]Epoch 21/300:   7%|▋         | 20/300 [00:05&lt;01:15,  3.71it/s, v_num=1, train_loss_step=1.8e+9, train_loss_epoch=2.07e+9]Epoch 21/300:   7%|▋         | 21/300 [00:05&lt;01:15,  3.70it/s, v_num=1, train_loss_step=1.8e+9, train_loss_epoch=2.07e+9]Epoch 21/300:   7%|▋         | 21/300 [00:05&lt;01:15,  3.70it/s, v_num=1, train_loss_step=1.7e+9, train_loss_epoch=1.9e+9] Epoch 22/300:   7%|▋         | 21/300 [00:05&lt;01:15,  3.70it/s, v_num=1, train_loss_step=1.7e+9, train_loss_epoch=1.9e+9]Epoch 22/300:   7%|▋         | 22/300 [00:06&lt;01:15,  3.69it/s, v_num=1, train_loss_step=1.7e+9, train_loss_epoch=1.9e+9]Epoch 22/300:   7%|▋         | 22/300 [00:06&lt;01:15,  3.69it/s, v_num=1, train_loss_step=1.57e+9, train_loss_epoch=1.75e+9]Epoch 23/300:   7%|▋         | 22/300 [00:06&lt;01:15,  3.69it/s, v_num=1, train_loss_step=1.57e+9, train_loss_epoch=1.75e+9]Epoch 23/300:   8%|▊         | 23/300 [00:06&lt;01:15,  3.67it/s, v_num=1, train_loss_step=1.57e+9, train_loss_epoch=1.75e+9]Epoch 23/300:   8%|▊         | 23/300 [00:06&lt;01:15,  3.67it/s, v_num=1, train_loss_step=1.5e+9, train_loss_epoch=1.62e+9] Epoch 24/300:   8%|▊         | 23/300 [00:06&lt;01:15,  3.67it/s, v_num=1, train_loss_step=1.5e+9, train_loss_epoch=1.62e+9]Epoch 24/300:   8%|▊         | 24/300 [00:06&lt;01:16,  3.63it/s, v_num=1, train_loss_step=1.5e+9, train_loss_epoch=1.62e+9]Epoch 24/300:   8%|▊         | 24/300 [00:06&lt;01:16,  3.63it/s, v_num=1, train_loss_step=1.39e+9, train_loss_epoch=1.5e+9]Epoch 25/300:   8%|▊         | 24/300 [00:06&lt;01:16,  3.63it/s, v_num=1, train_loss_step=1.39e+9, train_loss_epoch=1.5e+9]Epoch 25/300:   8%|▊         | 25/300 [00:06&lt;01:17,  3.56it/s, v_num=1, train_loss_step=1.39e+9, train_loss_epoch=1.5e+9]Epoch 25/300:   8%|▊         | 25/300 [00:06&lt;01:17,  3.56it/s, v_num=1, train_loss_step=1.31e+9, train_loss_epoch=1.39e+9]Epoch 26/300:   8%|▊         | 25/300 [00:06&lt;01:17,  3.56it/s, v_num=1, train_loss_step=1.31e+9, train_loss_epoch=1.39e+9]Epoch 26/300:   9%|▊         | 26/300 [00:07&lt;01:16,  3.59it/s, v_num=1, train_loss_step=1.31e+9, train_loss_epoch=1.39e+9]Epoch 26/300:   9%|▊         | 26/300 [00:07&lt;01:16,  3.59it/s, v_num=1, train_loss_step=1.17e+9, train_loss_epoch=1.3e+9] Epoch 27/300:   9%|▊         | 26/300 [00:07&lt;01:16,  3.59it/s, v_num=1, train_loss_step=1.17e+9, train_loss_epoch=1.3e+9]Epoch 27/300:   9%|▉         | 27/300 [00:07&lt;01:15,  3.60it/s, v_num=1, train_loss_step=1.17e+9, train_loss_epoch=1.3e+9]Epoch 27/300:   9%|▉         | 27/300 [00:07&lt;01:15,  3.60it/s, v_num=1, train_loss_step=1.15e+9, train_loss_epoch=1.21e+9]Epoch 28/300:   9%|▉         | 27/300 [00:07&lt;01:15,  3.60it/s, v_num=1, train_loss_step=1.15e+9, train_loss_epoch=1.21e+9]Epoch 28/300:   9%|▉         | 28/300 [00:07&lt;01:14,  3.64it/s, v_num=1, train_loss_step=1.15e+9, train_loss_epoch=1.21e+9]Epoch 28/300:   9%|▉         | 28/300 [00:07&lt;01:14,  3.64it/s, v_num=1, train_loss_step=1.03e+9, train_loss_epoch=1.13e+9]Epoch 29/300:   9%|▉         | 28/300 [00:07&lt;01:14,  3.64it/s, v_num=1, train_loss_step=1.03e+9, train_loss_epoch=1.13e+9]Epoch 29/300:  10%|▉         | 29/300 [00:07&lt;01:18,  3.47it/s, v_num=1, train_loss_step=1.03e+9, train_loss_epoch=1.13e+9]Epoch 29/300:  10%|▉         | 29/300 [00:07&lt;01:18,  3.47it/s, v_num=1, train_loss_step=1.04e+9, train_loss_epoch=1.06e+9]Epoch 30/300:  10%|▉         | 29/300 [00:07&lt;01:18,  3.47it/s, v_num=1, train_loss_step=1.04e+9, train_loss_epoch=1.06e+9]Epoch 30/300:  10%|█         | 30/300 [00:08&lt;01:18,  3.45it/s, v_num=1, train_loss_step=1.04e+9, train_loss_epoch=1.06e+9]Epoch 30/300:  10%|█         | 30/300 [00:08&lt;01:18,  3.45it/s, v_num=1, train_loss_step=9.26e+8, train_loss_epoch=9.98e+8]Epoch 31/300:  10%|█         | 30/300 [00:08&lt;01:18,  3.45it/s, v_num=1, train_loss_step=9.26e+8, train_loss_epoch=9.98e+8]Epoch 31/300:  10%|█         | 31/300 [00:08&lt;01:17,  3.49it/s, v_num=1, train_loss_step=9.26e+8, train_loss_epoch=9.98e+8]Epoch 31/300:  10%|█         | 31/300 [00:08&lt;01:17,  3.49it/s, v_num=1, train_loss_step=9.21e+8, train_loss_epoch=9.39e+8]Epoch 32/300:  10%|█         | 31/300 [00:08&lt;01:17,  3.49it/s, v_num=1, train_loss_step=9.21e+8, train_loss_epoch=9.39e+8]Epoch 32/300:  11%|█         | 32/300 [00:08&lt;01:15,  3.53it/s, v_num=1, train_loss_step=9.21e+8, train_loss_epoch=9.39e+8]Epoch 32/300:  11%|█         | 32/300 [00:08&lt;01:15,  3.53it/s, v_num=1, train_loss_step=8.56e+8, train_loss_epoch=8.85e+8]Epoch 33/300:  11%|█         | 32/300 [00:08&lt;01:15,  3.53it/s, v_num=1, train_loss_step=8.56e+8, train_loss_epoch=8.85e+8]Epoch 33/300:  11%|█         | 33/300 [00:09&lt;01:14,  3.58it/s, v_num=1, train_loss_step=8.56e+8, train_loss_epoch=8.85e+8]Epoch 33/300:  11%|█         | 33/300 [00:09&lt;01:14,  3.58it/s, v_num=1, train_loss_step=7.53e+8, train_loss_epoch=8.36e+8]Epoch 34/300:  11%|█         | 33/300 [00:09&lt;01:14,  3.58it/s, v_num=1, train_loss_step=7.53e+8, train_loss_epoch=8.36e+8]Epoch 34/300:  11%|█▏        | 34/300 [00:09&lt;01:15,  3.53it/s, v_num=1, train_loss_step=7.53e+8, train_loss_epoch=8.36e+8]Epoch 34/300:  11%|█▏        | 34/300 [00:09&lt;01:15,  3.53it/s, v_num=1, train_loss_step=7.56e+8, train_loss_epoch=7.9e+8] Epoch 35/300:  11%|█▏        | 34/300 [00:09&lt;01:15,  3.53it/s, v_num=1, train_loss_step=7.56e+8, train_loss_epoch=7.9e+8]Epoch 35/300:  12%|█▏        | 35/300 [00:09&lt;01:14,  3.55it/s, v_num=1, train_loss_step=7.56e+8, train_loss_epoch=7.9e+8]Epoch 35/300:  12%|█▏        | 35/300 [00:09&lt;01:14,  3.55it/s, v_num=1, train_loss_step=7.21e+8, train_loss_epoch=7.48e+8]Epoch 36/300:  12%|█▏        | 35/300 [00:09&lt;01:14,  3.55it/s, v_num=1, train_loss_step=7.21e+8, train_loss_epoch=7.48e+8]Epoch 36/300:  12%|█▏        | 36/300 [00:09&lt;01:13,  3.60it/s, v_num=1, train_loss_step=7.21e+8, train_loss_epoch=7.48e+8]Epoch 36/300:  12%|█▏        | 36/300 [00:09&lt;01:13,  3.60it/s, v_num=1, train_loss_step=6.46e+8, train_loss_epoch=7.09e+8]Epoch 37/300:  12%|█▏        | 36/300 [00:09&lt;01:13,  3.60it/s, v_num=1, train_loss_step=6.46e+8, train_loss_epoch=7.09e+8]Epoch 37/300:  12%|█▏        | 37/300 [00:10&lt;01:12,  3.61it/s, v_num=1, train_loss_step=6.46e+8, train_loss_epoch=7.09e+8]Epoch 37/300:  12%|█▏        | 37/300 [00:10&lt;01:12,  3.61it/s, v_num=1, train_loss_step=6.44e+8, train_loss_epoch=6.73e+8]Epoch 38/300:  12%|█▏        | 37/300 [00:10&lt;01:12,  3.61it/s, v_num=1, train_loss_step=6.44e+8, train_loss_epoch=6.73e+8]Epoch 38/300:  13%|█▎        | 38/300 [00:10&lt;01:11,  3.65it/s, v_num=1, train_loss_step=6.44e+8, train_loss_epoch=6.73e+8]Epoch 38/300:  13%|█▎        | 38/300 [00:10&lt;01:11,  3.65it/s, v_num=1, train_loss_step=5.71e+8, train_loss_epoch=6.4e+8] Epoch 39/300:  13%|█▎        | 38/300 [00:10&lt;01:11,  3.65it/s, v_num=1, train_loss_step=5.71e+8, train_loss_epoch=6.4e+8]Epoch 39/300:  13%|█▎        | 39/300 [00:10&lt;01:10,  3.69it/s, v_num=1, train_loss_step=5.71e+8, train_loss_epoch=6.4e+8]Epoch 39/300:  13%|█▎        | 39/300 [00:10&lt;01:10,  3.69it/s, v_num=1, train_loss_step=5.77e+8, train_loss_epoch=6.09e+8]Epoch 40/300:  13%|█▎        | 39/300 [00:10&lt;01:10,  3.69it/s, v_num=1, train_loss_step=5.77e+8, train_loss_epoch=6.09e+8]Epoch 40/300:  13%|█▎        | 40/300 [00:11&lt;01:10,  3.68it/s, v_num=1, train_loss_step=5.77e+8, train_loss_epoch=6.09e+8]Epoch 40/300:  13%|█▎        | 40/300 [00:11&lt;01:10,  3.68it/s, v_num=1, train_loss_step=5.22e+8, train_loss_epoch=5.8e+8] Epoch 41/300:  13%|█▎        | 40/300 [00:11&lt;01:10,  3.68it/s, v_num=1, train_loss_step=5.22e+8, train_loss_epoch=5.8e+8]Epoch 41/300:  14%|█▎        | 41/300 [00:11&lt;01:09,  3.71it/s, v_num=1, train_loss_step=5.22e+8, train_loss_epoch=5.8e+8]Epoch 41/300:  14%|█▎        | 41/300 [00:11&lt;01:09,  3.71it/s, v_num=1, train_loss_step=5.05e+8, train_loss_epoch=5.54e+8]Epoch 42/300:  14%|█▎        | 41/300 [00:11&lt;01:09,  3.71it/s, v_num=1, train_loss_step=5.05e+8, train_loss_epoch=5.54e+8]Epoch 42/300:  14%|█▍        | 42/300 [00:11&lt;01:10,  3.65it/s, v_num=1, train_loss_step=5.05e+8, train_loss_epoch=5.54e+8]Epoch 42/300:  14%|█▍        | 42/300 [00:11&lt;01:10,  3.65it/s, v_num=1, train_loss_step=4.95e+8, train_loss_epoch=5.28e+8]Epoch 43/300:  14%|█▍        | 42/300 [00:11&lt;01:10,  3.65it/s, v_num=1, train_loss_step=4.95e+8, train_loss_epoch=5.28e+8]Epoch 43/300:  14%|█▍        | 43/300 [00:11&lt;01:11,  3.58it/s, v_num=1, train_loss_step=4.95e+8, train_loss_epoch=5.28e+8]Epoch 43/300:  14%|█▍        | 43/300 [00:11&lt;01:11,  3.58it/s, v_num=1, train_loss_step=4.62e+8, train_loss_epoch=5.05e+8]Epoch 44/300:  14%|█▍        | 43/300 [00:11&lt;01:11,  3.58it/s, v_num=1, train_loss_step=4.62e+8, train_loss_epoch=5.05e+8]Epoch 44/300:  15%|█▍        | 44/300 [00:12&lt;01:12,  3.51it/s, v_num=1, train_loss_step=4.62e+8, train_loss_epoch=5.05e+8]Epoch 44/300:  15%|█▍        | 44/300 [00:12&lt;01:12,  3.51it/s, v_num=1, train_loss_step=4.71e+8, train_loss_epoch=4.83e+8]Epoch 45/300:  15%|█▍        | 44/300 [00:12&lt;01:12,  3.51it/s, v_num=1, train_loss_step=4.71e+8, train_loss_epoch=4.83e+8]Epoch 45/300:  15%|█▌        | 45/300 [00:12&lt;01:12,  3.52it/s, v_num=1, train_loss_step=4.71e+8, train_loss_epoch=4.83e+8]Epoch 45/300:  15%|█▌        | 45/300 [00:12&lt;01:12,  3.52it/s, v_num=1, train_loss_step=4.52e+8, train_loss_epoch=4.62e+8]Epoch 46/300:  15%|█▌        | 45/300 [00:12&lt;01:12,  3.52it/s, v_num=1, train_loss_step=4.52e+8, train_loss_epoch=4.62e+8]Epoch 46/300:  15%|█▌        | 46/300 [00:12&lt;01:10,  3.59it/s, v_num=1, train_loss_step=4.52e+8, train_loss_epoch=4.62e+8]Epoch 46/300:  15%|█▌        | 46/300 [00:12&lt;01:10,  3.59it/s, v_num=1, train_loss_step=4.1e+8, train_loss_epoch=4.43e+8] Epoch 47/300:  15%|█▌        | 46/300 [00:12&lt;01:10,  3.59it/s, v_num=1, train_loss_step=4.1e+8, train_loss_epoch=4.43e+8]Epoch 47/300:  16%|█▌        | 47/300 [00:12&lt;01:10,  3.59it/s, v_num=1, train_loss_step=4.1e+8, train_loss_epoch=4.43e+8]Epoch 47/300:  16%|█▌        | 47/300 [00:12&lt;01:10,  3.59it/s, v_num=1, train_loss_step=4.07e+8, train_loss_epoch=4.25e+8]Epoch 48/300:  16%|█▌        | 47/300 [00:12&lt;01:10,  3.59it/s, v_num=1, train_loss_step=4.07e+8, train_loss_epoch=4.25e+8]Epoch 48/300:  16%|█▌        | 48/300 [00:13&lt;01:13,  3.43it/s, v_num=1, train_loss_step=4.07e+8, train_loss_epoch=4.25e+8]Epoch 48/300:  16%|█▌        | 48/300 [00:13&lt;01:13,  3.43it/s, v_num=1, train_loss_step=3.68e+8, train_loss_epoch=4.08e+8]Epoch 49/300:  16%|█▌        | 48/300 [00:13&lt;01:13,  3.43it/s, v_num=1, train_loss_step=3.68e+8, train_loss_epoch=4.08e+8]Epoch 49/300:  16%|█▋        | 49/300 [00:13&lt;01:12,  3.48it/s, v_num=1, train_loss_step=3.68e+8, train_loss_epoch=4.08e+8]Epoch 49/300:  16%|█▋        | 49/300 [00:13&lt;01:12,  3.48it/s, v_num=1, train_loss_step=3.72e+8, train_loss_epoch=3.91e+8]Epoch 50/300:  16%|█▋        | 49/300 [00:13&lt;01:12,  3.48it/s, v_num=1, train_loss_step=3.72e+8, train_loss_epoch=3.91e+8]Epoch 50/300:  17%|█▋        | 50/300 [00:13&lt;01:10,  3.52it/s, v_num=1, train_loss_step=3.72e+8, train_loss_epoch=3.91e+8]Epoch 50/300:  17%|█▋        | 50/300 [00:13&lt;01:10,  3.52it/s, v_num=1, train_loss_step=3.45e+8, train_loss_epoch=3.76e+8]Epoch 51/300:  17%|█▋        | 50/300 [00:13&lt;01:10,  3.52it/s, v_num=1, train_loss_step=3.45e+8, train_loss_epoch=3.76e+8]Epoch 51/300:  17%|█▋        | 51/300 [00:14&lt;01:09,  3.56it/s, v_num=1, train_loss_step=3.45e+8, train_loss_epoch=3.76e+8]Epoch 51/300:  17%|█▋        | 51/300 [00:14&lt;01:09,  3.56it/s, v_num=1, train_loss_step=3.44e+8, train_loss_epoch=3.62e+8]Epoch 52/300:  17%|█▋        | 51/300 [00:14&lt;01:09,  3.56it/s, v_num=1, train_loss_step=3.44e+8, train_loss_epoch=3.62e+8]Epoch 52/300:  17%|█▋        | 52/300 [00:14&lt;01:09,  3.55it/s, v_num=1, train_loss_step=3.44e+8, train_loss_epoch=3.62e+8]Epoch 52/300:  17%|█▋        | 52/300 [00:14&lt;01:09,  3.55it/s, v_num=1, train_loss_step=3.04e+8, train_loss_epoch=3.48e+8]Epoch 53/300:  17%|█▋        | 52/300 [00:14&lt;01:09,  3.55it/s, v_num=1, train_loss_step=3.04e+8, train_loss_epoch=3.48e+8]Epoch 53/300:  18%|█▊        | 53/300 [00:14&lt;01:08,  3.62it/s, v_num=1, train_loss_step=3.04e+8, train_loss_epoch=3.48e+8]Epoch 53/300:  18%|█▊        | 53/300 [00:14&lt;01:08,  3.62it/s, v_num=1, train_loss_step=3.04e+8, train_loss_epoch=3.35e+8]Epoch 54/300:  18%|█▊        | 53/300 [00:14&lt;01:08,  3.62it/s, v_num=1, train_loss_step=3.04e+8, train_loss_epoch=3.35e+8]Epoch 54/300:  18%|█▊        | 54/300 [00:14&lt;01:07,  3.64it/s, v_num=1, train_loss_step=3.04e+8, train_loss_epoch=3.35e+8]Epoch 54/300:  18%|█▊        | 54/300 [00:14&lt;01:07,  3.64it/s, v_num=1, train_loss_step=3.15e+8, train_loss_epoch=3.23e+8]Epoch 55/300:  18%|█▊        | 54/300 [00:14&lt;01:07,  3.64it/s, v_num=1, train_loss_step=3.15e+8, train_loss_epoch=3.23e+8]Epoch 55/300:  18%|█▊        | 55/300 [00:15&lt;01:08,  3.60it/s, v_num=1, train_loss_step=3.15e+8, train_loss_epoch=3.23e+8]Epoch 55/300:  18%|█▊        | 55/300 [00:15&lt;01:08,  3.60it/s, v_num=1, train_loss_step=2.94e+8, train_loss_epoch=3.11e+8]Epoch 56/300:  18%|█▊        | 55/300 [00:15&lt;01:08,  3.60it/s, v_num=1, train_loss_step=2.94e+8, train_loss_epoch=3.11e+8]Epoch 56/300:  19%|█▊        | 56/300 [00:15&lt;01:11,  3.44it/s, v_num=1, train_loss_step=2.94e+8, train_loss_epoch=3.11e+8]Epoch 56/300:  19%|█▊        | 56/300 [00:15&lt;01:11,  3.44it/s, v_num=1, train_loss_step=2.75e+8, train_loss_epoch=3e+8]   Epoch 57/300:  19%|█▊        | 56/300 [00:15&lt;01:11,  3.44it/s, v_num=1, train_loss_step=2.75e+8, train_loss_epoch=3e+8]Epoch 57/300:  19%|█▉        | 57/300 [00:15&lt;01:10,  3.44it/s, v_num=1, train_loss_step=2.75e+8, train_loss_epoch=3e+8]Epoch 57/300:  19%|█▉        | 57/300 [00:15&lt;01:10,  3.44it/s, v_num=1, train_loss_step=2.71e+8, train_loss_epoch=2.9e+8]Epoch 58/300:  19%|█▉        | 57/300 [00:15&lt;01:10,  3.44it/s, v_num=1, train_loss_step=2.71e+8, train_loss_epoch=2.9e+8]Epoch 58/300:  19%|█▉        | 58/300 [00:16&lt;01:09,  3.47it/s, v_num=1, train_loss_step=2.71e+8, train_loss_epoch=2.9e+8]Epoch 58/300:  19%|█▉        | 58/300 [00:16&lt;01:09,  3.47it/s, v_num=1, train_loss_step=2.66e+8, train_loss_epoch=2.8e+8]Epoch 59/300:  19%|█▉        | 58/300 [00:16&lt;01:09,  3.47it/s, v_num=1, train_loss_step=2.66e+8, train_loss_epoch=2.8e+8]Epoch 59/300:  20%|█▉        | 59/300 [00:16&lt;01:09,  3.45it/s, v_num=1, train_loss_step=2.66e+8, train_loss_epoch=2.8e+8]Epoch 59/300:  20%|█▉        | 59/300 [00:16&lt;01:09,  3.45it/s, v_num=1, train_loss_step=2.57e+8, train_loss_epoch=2.71e+8]Epoch 60/300:  20%|█▉        | 59/300 [00:16&lt;01:09,  3.45it/s, v_num=1, train_loss_step=2.57e+8, train_loss_epoch=2.71e+8]Epoch 60/300:  20%|██        | 60/300 [00:16&lt;01:08,  3.52it/s, v_num=1, train_loss_step=2.57e+8, train_loss_epoch=2.71e+8]Epoch 60/300:  20%|██        | 60/300 [00:16&lt;01:08,  3.52it/s, v_num=1, train_loss_step=2.47e+8, train_loss_epoch=2.62e+8]Epoch 61/300:  20%|██        | 60/300 [00:16&lt;01:08,  3.52it/s, v_num=1, train_loss_step=2.47e+8, train_loss_epoch=2.62e+8]Epoch 61/300:  20%|██        | 61/300 [00:16&lt;01:07,  3.55it/s, v_num=1, train_loss_step=2.47e+8, train_loss_epoch=2.62e+8]Epoch 61/300:  20%|██        | 61/300 [00:16&lt;01:07,  3.55it/s, v_num=1, train_loss_step=2.43e+8, train_loss_epoch=2.53e+8]Epoch 62/300:  20%|██        | 61/300 [00:16&lt;01:07,  3.55it/s, v_num=1, train_loss_step=2.43e+8, train_loss_epoch=2.53e+8]Epoch 62/300:  21%|██        | 62/300 [00:17&lt;01:07,  3.52it/s, v_num=1, train_loss_step=2.43e+8, train_loss_epoch=2.53e+8]Epoch 62/300:  21%|██        | 62/300 [00:17&lt;01:07,  3.52it/s, v_num=1, train_loss_step=2.37e+8, train_loss_epoch=2.45e+8]Epoch 63/300:  21%|██        | 62/300 [00:17&lt;01:07,  3.52it/s, v_num=1, train_loss_step=2.37e+8, train_loss_epoch=2.45e+8]Epoch 63/300:  21%|██        | 63/300 [00:17&lt;01:06,  3.57it/s, v_num=1, train_loss_step=2.37e+8, train_loss_epoch=2.45e+8]Epoch 63/300:  21%|██        | 63/300 [00:17&lt;01:06,  3.57it/s, v_num=1, train_loss_step=2.14e+8, train_loss_epoch=2.37e+8]Epoch 64/300:  21%|██        | 63/300 [00:17&lt;01:06,  3.57it/s, v_num=1, train_loss_step=2.14e+8, train_loss_epoch=2.37e+8]Epoch 64/300:  21%|██▏       | 64/300 [00:17&lt;01:05,  3.61it/s, v_num=1, train_loss_step=2.14e+8, train_loss_epoch=2.37e+8]Epoch 64/300:  21%|██▏       | 64/300 [00:17&lt;01:05,  3.61it/s, v_num=1, train_loss_step=2.11e+8, train_loss_epoch=2.3e+8] Epoch 65/300:  21%|██▏       | 64/300 [00:17&lt;01:05,  3.61it/s, v_num=1, train_loss_step=2.11e+8, train_loss_epoch=2.3e+8]Epoch 65/300:  22%|██▏       | 65/300 [00:18&lt;01:05,  3.61it/s, v_num=1, train_loss_step=2.11e+8, train_loss_epoch=2.3e+8]Epoch 65/300:  22%|██▏       | 65/300 [00:18&lt;01:05,  3.61it/s, v_num=1, train_loss_step=2.13e+8, train_loss_epoch=2.22e+8]Epoch 66/300:  22%|██▏       | 65/300 [00:18&lt;01:05,  3.61it/s, v_num=1, train_loss_step=2.13e+8, train_loss_epoch=2.22e+8]Epoch 66/300:  22%|██▏       | 66/300 [00:18&lt;01:07,  3.48it/s, v_num=1, train_loss_step=2.13e+8, train_loss_epoch=2.22e+8]Epoch 66/300:  22%|██▏       | 66/300 [00:18&lt;01:07,  3.48it/s, v_num=1, train_loss_step=2.09e+8, train_loss_epoch=2.16e+8]Epoch 67/300:  22%|██▏       | 66/300 [00:18&lt;01:07,  3.48it/s, v_num=1, train_loss_step=2.09e+8, train_loss_epoch=2.16e+8]Epoch 67/300:  22%|██▏       | 67/300 [00:18&lt;01:07,  3.46it/s, v_num=1, train_loss_step=2.09e+8, train_loss_epoch=2.16e+8]Epoch 67/300:  22%|██▏       | 67/300 [00:18&lt;01:07,  3.46it/s, v_num=1, train_loss_step=2.08e+8, train_loss_epoch=2.09e+8]Epoch 68/300:  22%|██▏       | 67/300 [00:18&lt;01:07,  3.46it/s, v_num=1, train_loss_step=2.08e+8, train_loss_epoch=2.09e+8]Epoch 68/300:  23%|██▎       | 68/300 [00:18&lt;01:06,  3.51it/s, v_num=1, train_loss_step=2.08e+8, train_loss_epoch=2.09e+8]Epoch 68/300:  23%|██▎       | 68/300 [00:18&lt;01:06,  3.51it/s, v_num=1, train_loss_step=1.94e+8, train_loss_epoch=2.03e+8]Epoch 69/300:  23%|██▎       | 68/300 [00:18&lt;01:06,  3.51it/s, v_num=1, train_loss_step=1.94e+8, train_loss_epoch=2.03e+8]Epoch 69/300:  23%|██▎       | 69/300 [00:19&lt;01:05,  3.51it/s, v_num=1, train_loss_step=1.94e+8, train_loss_epoch=2.03e+8]Epoch 69/300:  23%|██▎       | 69/300 [00:19&lt;01:05,  3.51it/s, v_num=1, train_loss_step=1.85e+8, train_loss_epoch=1.97e+8]Epoch 70/300:  23%|██▎       | 69/300 [00:19&lt;01:05,  3.51it/s, v_num=1, train_loss_step=1.85e+8, train_loss_epoch=1.97e+8]Epoch 70/300:  23%|██▎       | 70/300 [00:19&lt;01:06,  3.44it/s, v_num=1, train_loss_step=1.85e+8, train_loss_epoch=1.97e+8]Epoch 70/300:  23%|██▎       | 70/300 [00:19&lt;01:06,  3.44it/s, v_num=1, train_loss_step=1.87e+8, train_loss_epoch=1.91e+8]Epoch 71/300:  23%|██▎       | 70/300 [00:19&lt;01:06,  3.44it/s, v_num=1, train_loss_step=1.87e+8, train_loss_epoch=1.91e+8]Epoch 71/300:  24%|██▎       | 71/300 [00:19&lt;01:07,  3.38it/s, v_num=1, train_loss_step=1.87e+8, train_loss_epoch=1.91e+8]Epoch 71/300:  24%|██▎       | 71/300 [00:19&lt;01:07,  3.38it/s, v_num=1, train_loss_step=1.81e+8, train_loss_epoch=1.86e+8]Epoch 72/300:  24%|██▎       | 71/300 [00:19&lt;01:07,  3.38it/s, v_num=1, train_loss_step=1.81e+8, train_loss_epoch=1.86e+8]Epoch 72/300:  24%|██▍       | 72/300 [00:20&lt;01:07,  3.40it/s, v_num=1, train_loss_step=1.81e+8, train_loss_epoch=1.86e+8]Epoch 72/300:  24%|██▍       | 72/300 [00:20&lt;01:07,  3.40it/s, v_num=1, train_loss_step=1.6e+8, train_loss_epoch=1.8e+8]  Epoch 73/300:  24%|██▍       | 72/300 [00:20&lt;01:07,  3.40it/s, v_num=1, train_loss_step=1.6e+8, train_loss_epoch=1.8e+8]Epoch 73/300:  24%|██▍       | 73/300 [00:20&lt;01:06,  3.44it/s, v_num=1, train_loss_step=1.6e+8, train_loss_epoch=1.8e+8]Epoch 73/300:  24%|██▍       | 73/300 [00:20&lt;01:06,  3.44it/s, v_num=1, train_loss_step=1.63e+8, train_loss_epoch=1.75e+8]Epoch 74/300:  24%|██▍       | 73/300 [00:20&lt;01:06,  3.44it/s, v_num=1, train_loss_step=1.63e+8, train_loss_epoch=1.75e+8]Epoch 74/300:  25%|██▍       | 74/300 [00:20&lt;01:04,  3.49it/s, v_num=1, train_loss_step=1.63e+8, train_loss_epoch=1.75e+8]Epoch 74/300:  25%|██▍       | 74/300 [00:20&lt;01:04,  3.49it/s, v_num=1, train_loss_step=1.61e+8, train_loss_epoch=1.7e+8] Epoch 75/300:  25%|██▍       | 74/300 [00:20&lt;01:04,  3.49it/s, v_num=1, train_loss_step=1.61e+8, train_loss_epoch=1.7e+8]Epoch 75/300:  25%|██▌       | 75/300 [00:21&lt;01:04,  3.48it/s, v_num=1, train_loss_step=1.61e+8, train_loss_epoch=1.7e+8]Epoch 75/300:  25%|██▌       | 75/300 [00:21&lt;01:04,  3.48it/s, v_num=1, train_loss_step=1.63e+8, train_loss_epoch=1.66e+8]Epoch 76/300:  25%|██▌       | 75/300 [00:21&lt;01:04,  3.48it/s, v_num=1, train_loss_step=1.63e+8, train_loss_epoch=1.66e+8]Epoch 76/300:  25%|██▌       | 76/300 [00:21&lt;01:04,  3.49it/s, v_num=1, train_loss_step=1.63e+8, train_loss_epoch=1.66e+8]Epoch 76/300:  25%|██▌       | 76/300 [00:21&lt;01:04,  3.49it/s, v_num=1, train_loss_step=1.53e+8, train_loss_epoch=1.61e+8]Epoch 77/300:  25%|██▌       | 76/300 [00:21&lt;01:04,  3.49it/s, v_num=1, train_loss_step=1.53e+8, train_loss_epoch=1.61e+8]Epoch 77/300:  26%|██▌       | 77/300 [00:21&lt;01:04,  3.44it/s, v_num=1, train_loss_step=1.53e+8, train_loss_epoch=1.61e+8]Epoch 77/300:  26%|██▌       | 77/300 [00:21&lt;01:04,  3.44it/s, v_num=1, train_loss_step=1.47e+8, train_loss_epoch=1.57e+8]Epoch 78/300:  26%|██▌       | 77/300 [00:21&lt;01:04,  3.44it/s, v_num=1, train_loss_step=1.47e+8, train_loss_epoch=1.57e+8]Epoch 78/300:  26%|██▌       | 78/300 [00:21&lt;01:03,  3.50it/s, v_num=1, train_loss_step=1.47e+8, train_loss_epoch=1.57e+8]Epoch 78/300:  26%|██▌       | 78/300 [00:21&lt;01:03,  3.50it/s, v_num=1, train_loss_step=1.49e+8, train_loss_epoch=1.53e+8]Epoch 79/300:  26%|██▌       | 78/300 [00:21&lt;01:03,  3.50it/s, v_num=1, train_loss_step=1.49e+8, train_loss_epoch=1.53e+8]Epoch 79/300:  26%|██▋       | 79/300 [00:22&lt;01:03,  3.47it/s, v_num=1, train_loss_step=1.49e+8, train_loss_epoch=1.53e+8]Epoch 79/300:  26%|██▋       | 79/300 [00:22&lt;01:03,  3.47it/s, v_num=1, train_loss_step=1.35e+8, train_loss_epoch=1.49e+8]Epoch 80/300:  26%|██▋       | 79/300 [00:22&lt;01:03,  3.47it/s, v_num=1, train_loss_step=1.35e+8, train_loss_epoch=1.49e+8]Epoch 80/300:  27%|██▋       | 80/300 [00:22&lt;01:02,  3.49it/s, v_num=1, train_loss_step=1.35e+8, train_loss_epoch=1.49e+8]Epoch 80/300:  27%|██▋       | 80/300 [00:22&lt;01:02,  3.49it/s, v_num=1, train_loss_step=1.33e+8, train_loss_epoch=1.45e+8]Epoch 81/300:  27%|██▋       | 80/300 [00:22&lt;01:02,  3.49it/s, v_num=1, train_loss_step=1.33e+8, train_loss_epoch=1.45e+8]Epoch 81/300:  27%|██▋       | 81/300 [00:22&lt;01:01,  3.55it/s, v_num=1, train_loss_step=1.33e+8, train_loss_epoch=1.45e+8]Epoch 81/300:  27%|██▋       | 81/300 [00:22&lt;01:01,  3.55it/s, v_num=1, train_loss_step=1.34e+8, train_loss_epoch=1.41e+8]Epoch 82/300:  27%|██▋       | 81/300 [00:22&lt;01:01,  3.55it/s, v_num=1, train_loss_step=1.34e+8, train_loss_epoch=1.41e+8]Epoch 82/300:  27%|██▋       | 82/300 [00:22&lt;01:01,  3.54it/s, v_num=1, train_loss_step=1.34e+8, train_loss_epoch=1.41e+8]Epoch 82/300:  27%|██▋       | 82/300 [00:22&lt;01:01,  3.54it/s, v_num=1, train_loss_step=1.31e+8, train_loss_epoch=1.38e+8]Epoch 83/300:  27%|██▋       | 82/300 [00:22&lt;01:01,  3.54it/s, v_num=1, train_loss_step=1.31e+8, train_loss_epoch=1.38e+8]Epoch 83/300:  28%|██▊       | 83/300 [00:23&lt;01:02,  3.49it/s, v_num=1, train_loss_step=1.31e+8, train_loss_epoch=1.38e+8]Epoch 83/300:  28%|██▊       | 83/300 [00:23&lt;01:02,  3.49it/s, v_num=1, train_loss_step=1.26e+8, train_loss_epoch=1.34e+8]Epoch 84/300:  28%|██▊       | 83/300 [00:23&lt;01:02,  3.49it/s, v_num=1, train_loss_step=1.26e+8, train_loss_epoch=1.34e+8]Epoch 84/300:  28%|██▊       | 84/300 [00:23&lt;01:02,  3.48it/s, v_num=1, train_loss_step=1.26e+8, train_loss_epoch=1.34e+8]Epoch 84/300:  28%|██▊       | 84/300 [00:23&lt;01:02,  3.48it/s, v_num=1, train_loss_step=1.26e+8, train_loss_epoch=1.31e+8]Epoch 85/300:  28%|██▊       | 84/300 [00:23&lt;01:02,  3.48it/s, v_num=1, train_loss_step=1.26e+8, train_loss_epoch=1.31e+8]Epoch 85/300:  28%|██▊       | 85/300 [00:23&lt;01:03,  3.41it/s, v_num=1, train_loss_step=1.26e+8, train_loss_epoch=1.31e+8]Epoch 85/300:  28%|██▊       | 85/300 [00:23&lt;01:03,  3.41it/s, v_num=1, train_loss_step=1.18e+8, train_loss_epoch=1.28e+8]Epoch 86/300:  28%|██▊       | 85/300 [00:23&lt;01:03,  3.41it/s, v_num=1, train_loss_step=1.18e+8, train_loss_epoch=1.28e+8]Epoch 86/300:  29%|██▊       | 86/300 [00:24&lt;01:03,  3.35it/s, v_num=1, train_loss_step=1.18e+8, train_loss_epoch=1.28e+8]Epoch 86/300:  29%|██▊       | 86/300 [00:24&lt;01:03,  3.35it/s, v_num=1, train_loss_step=1.16e+8, train_loss_epoch=1.24e+8]Epoch 87/300:  29%|██▊       | 86/300 [00:24&lt;01:03,  3.35it/s, v_num=1, train_loss_step=1.16e+8, train_loss_epoch=1.24e+8]Epoch 87/300:  29%|██▉       | 87/300 [00:24&lt;01:03,  3.36it/s, v_num=1, train_loss_step=1.16e+8, train_loss_epoch=1.24e+8]Epoch 87/300:  29%|██▉       | 87/300 [00:24&lt;01:03,  3.36it/s, v_num=1, train_loss_step=1.15e+8, train_loss_epoch=1.21e+8]Epoch 88/300:  29%|██▉       | 87/300 [00:24&lt;01:03,  3.36it/s, v_num=1, train_loss_step=1.15e+8, train_loss_epoch=1.21e+8]Epoch 88/300:  29%|██▉       | 88/300 [00:24&lt;01:01,  3.47it/s, v_num=1, train_loss_step=1.15e+8, train_loss_epoch=1.21e+8]Epoch 88/300:  29%|██▉       | 88/300 [00:24&lt;01:01,  3.47it/s, v_num=1, train_loss_step=1.15e+8, train_loss_epoch=1.19e+8]Epoch 89/300:  29%|██▉       | 88/300 [00:24&lt;01:01,  3.47it/s, v_num=1, train_loss_step=1.15e+8, train_loss_epoch=1.19e+8]Epoch 89/300:  30%|██▉       | 89/300 [00:25&lt;01:00,  3.47it/s, v_num=1, train_loss_step=1.15e+8, train_loss_epoch=1.19e+8]Epoch 89/300:  30%|██▉       | 89/300 [00:25&lt;01:00,  3.47it/s, v_num=1, train_loss_step=1.13e+8, train_loss_epoch=1.16e+8]Epoch 90/300:  30%|██▉       | 89/300 [00:25&lt;01:00,  3.47it/s, v_num=1, train_loss_step=1.13e+8, train_loss_epoch=1.16e+8]Epoch 90/300:  30%|███       | 90/300 [00:25&lt;01:00,  3.44it/s, v_num=1, train_loss_step=1.13e+8, train_loss_epoch=1.16e+8]Epoch 90/300:  30%|███       | 90/300 [00:25&lt;01:00,  3.44it/s, v_num=1, train_loss_step=1.08e+8, train_loss_epoch=1.13e+8]Epoch 91/300:  30%|███       | 90/300 [00:25&lt;01:00,  3.44it/s, v_num=1, train_loss_step=1.08e+8, train_loss_epoch=1.13e+8]Epoch 91/300:  30%|███       | 91/300 [00:25&lt;00:59,  3.50it/s, v_num=1, train_loss_step=1.08e+8, train_loss_epoch=1.13e+8]Epoch 91/300:  30%|███       | 91/300 [00:25&lt;00:59,  3.50it/s, v_num=1, train_loss_step=1.04e+8, train_loss_epoch=1.1e+8] Epoch 92/300:  30%|███       | 91/300 [00:25&lt;00:59,  3.50it/s, v_num=1, train_loss_step=1.04e+8, train_loss_epoch=1.1e+8]Epoch 92/300:  31%|███       | 92/300 [00:25&lt;00:58,  3.53it/s, v_num=1, train_loss_step=1.04e+8, train_loss_epoch=1.1e+8]Epoch 92/300:  31%|███       | 92/300 [00:25&lt;00:58,  3.53it/s, v_num=1, train_loss_step=1.04e+8, train_loss_epoch=1.08e+8]Epoch 93/300:  31%|███       | 92/300 [00:25&lt;00:58,  3.53it/s, v_num=1, train_loss_step=1.04e+8, train_loss_epoch=1.08e+8]Epoch 93/300:  31%|███       | 93/300 [00:26&lt;00:58,  3.57it/s, v_num=1, train_loss_step=1.04e+8, train_loss_epoch=1.08e+8]Epoch 93/300:  31%|███       | 93/300 [00:26&lt;00:58,  3.57it/s, v_num=1, train_loss_step=1.03e+8, train_loss_epoch=1.05e+8]Epoch 94/300:  31%|███       | 93/300 [00:26&lt;00:58,  3.57it/s, v_num=1, train_loss_step=1.03e+8, train_loss_epoch=1.05e+8]Epoch 94/300:  31%|███▏      | 94/300 [00:26&lt;00:57,  3.57it/s, v_num=1, train_loss_step=1.03e+8, train_loss_epoch=1.05e+8]Epoch 94/300:  31%|███▏      | 94/300 [00:26&lt;00:57,  3.57it/s, v_num=1, train_loss_step=9.73e+7, train_loss_epoch=1.03e+8]Epoch 95/300:  31%|███▏      | 94/300 [00:26&lt;00:57,  3.57it/s, v_num=1, train_loss_step=9.73e+7, train_loss_epoch=1.03e+8]Epoch 95/300:  32%|███▏      | 95/300 [00:26&lt;00:57,  3.58it/s, v_num=1, train_loss_step=9.73e+7, train_loss_epoch=1.03e+8]Epoch 95/300:  32%|███▏      | 95/300 [00:26&lt;00:57,  3.58it/s, v_num=1, train_loss_step=9.83e+7, train_loss_epoch=1.01e+8]Epoch 96/300:  32%|███▏      | 95/300 [00:26&lt;00:57,  3.58it/s, v_num=1, train_loss_step=9.83e+7, train_loss_epoch=1.01e+8]Epoch 96/300:  32%|███▏      | 96/300 [00:26&lt;00:56,  3.60it/s, v_num=1, train_loss_step=9.83e+7, train_loss_epoch=1.01e+8]Epoch 96/300:  32%|███▏      | 96/300 [00:26&lt;00:56,  3.60it/s, v_num=1, train_loss_step=9.41e+7, train_loss_epoch=9.86e+7]Epoch 97/300:  32%|███▏      | 96/300 [00:27&lt;00:56,  3.60it/s, v_num=1, train_loss_step=9.41e+7, train_loss_epoch=9.86e+7]Epoch 97/300:  32%|███▏      | 97/300 [00:27&lt;00:56,  3.59it/s, v_num=1, train_loss_step=9.41e+7, train_loss_epoch=9.86e+7]Epoch 97/300:  32%|███▏      | 97/300 [00:27&lt;00:56,  3.59it/s, v_num=1, train_loss_step=8.58e+7, train_loss_epoch=9.65e+7]Epoch 98/300:  32%|███▏      | 97/300 [00:27&lt;00:56,  3.59it/s, v_num=1, train_loss_step=8.58e+7, train_loss_epoch=9.65e+7]Epoch 98/300:  33%|███▎      | 98/300 [00:27&lt;00:56,  3.57it/s, v_num=1, train_loss_step=8.58e+7, train_loss_epoch=9.65e+7]Epoch 98/300:  33%|███▎      | 98/300 [00:27&lt;00:56,  3.57it/s, v_num=1, train_loss_step=8.56e+7, train_loss_epoch=9.44e+7]Epoch 99/300:  33%|███▎      | 98/300 [00:27&lt;00:56,  3.57it/s, v_num=1, train_loss_step=8.56e+7, train_loss_epoch=9.44e+7]Epoch 99/300:  33%|███▎      | 99/300 [00:27&lt;00:57,  3.51it/s, v_num=1, train_loss_step=8.56e+7, train_loss_epoch=9.44e+7]Epoch 99/300:  33%|███▎      | 99/300 [00:27&lt;00:57,  3.51it/s, v_num=1, train_loss_step=8.81e+7, train_loss_epoch=9.24e+7]Epoch 100/300:  33%|███▎      | 99/300 [00:27&lt;00:57,  3.51it/s, v_num=1, train_loss_step=8.81e+7, train_loss_epoch=9.24e+7]Epoch 100/300:  33%|███▎      | 100/300 [00:28&lt;00:58,  3.41it/s, v_num=1, train_loss_step=8.81e+7, train_loss_epoch=9.24e+7]Epoch 100/300:  33%|███▎      | 100/300 [00:28&lt;00:58,  3.41it/s, v_num=1, train_loss_step=8.91e+7, train_loss_epoch=9.04e+7]Epoch 101/300:  33%|███▎      | 100/300 [00:28&lt;00:58,  3.41it/s, v_num=1, train_loss_step=8.91e+7, train_loss_epoch=9.04e+7]Epoch 101/300:  34%|███▎      | 101/300 [00:28&lt;00:56,  3.50it/s, v_num=1, train_loss_step=8.91e+7, train_loss_epoch=9.04e+7]Epoch 101/300:  34%|███▎      | 101/300 [00:28&lt;00:56,  3.50it/s, v_num=1, train_loss_step=9.01e+7, train_loss_epoch=8.85e+7]Epoch 102/300:  34%|███▎      | 101/300 [00:28&lt;00:56,  3.50it/s, v_num=1, train_loss_step=9.01e+7, train_loss_epoch=8.85e+7]Epoch 102/300:  34%|███▍      | 102/300 [00:28&lt;00:56,  3.51it/s, v_num=1, train_loss_step=9.01e+7, train_loss_epoch=8.85e+7]Epoch 102/300:  34%|███▍      | 102/300 [00:28&lt;00:56,  3.51it/s, v_num=1, train_loss_step=8.53e+7, train_loss_epoch=8.67e+7]Epoch 103/300:  34%|███▍      | 102/300 [00:28&lt;00:56,  3.51it/s, v_num=1, train_loss_step=8.53e+7, train_loss_epoch=8.67e+7]Epoch 103/300:  34%|███▍      | 103/300 [00:29&lt;00:58,  3.38it/s, v_num=1, train_loss_step=8.53e+7, train_loss_epoch=8.67e+7]Epoch 103/300:  34%|███▍      | 103/300 [00:29&lt;00:58,  3.38it/s, v_num=1, train_loss_step=8.23e+7, train_loss_epoch=8.49e+7]Epoch 104/300:  34%|███▍      | 103/300 [00:29&lt;00:58,  3.38it/s, v_num=1, train_loss_step=8.23e+7, train_loss_epoch=8.49e+7]Epoch 104/300:  35%|███▍      | 104/300 [00:29&lt;00:59,  3.29it/s, v_num=1, train_loss_step=8.23e+7, train_loss_epoch=8.49e+7]Epoch 104/300:  35%|███▍      | 104/300 [00:29&lt;00:59,  3.29it/s, v_num=1, train_loss_step=7.08e+7, train_loss_epoch=8.32e+7]Epoch 105/300:  35%|███▍      | 104/300 [00:29&lt;00:59,  3.29it/s, v_num=1, train_loss_step=7.08e+7, train_loss_epoch=8.32e+7]Epoch 105/300:  35%|███▌      | 105/300 [00:29&lt;01:00,  3.25it/s, v_num=1, train_loss_step=7.08e+7, train_loss_epoch=8.32e+7]Epoch 105/300:  35%|███▌      | 105/300 [00:29&lt;01:00,  3.25it/s, v_num=1, train_loss_step=7.44e+7, train_loss_epoch=8.15e+7]Epoch 106/300:  35%|███▌      | 105/300 [00:29&lt;01:00,  3.25it/s, v_num=1, train_loss_step=7.44e+7, train_loss_epoch=8.15e+7]Epoch 106/300:  35%|███▌      | 106/300 [00:29&lt;00:58,  3.31it/s, v_num=1, train_loss_step=7.44e+7, train_loss_epoch=8.15e+7]Epoch 106/300:  35%|███▌      | 106/300 [00:29&lt;00:58,  3.31it/s, v_num=1, train_loss_step=7.74e+7, train_loss_epoch=7.98e+7]Epoch 107/300:  35%|███▌      | 106/300 [00:29&lt;00:58,  3.31it/s, v_num=1, train_loss_step=7.74e+7, train_loss_epoch=7.98e+7]Epoch 107/300:  36%|███▌      | 107/300 [00:30&lt;00:57,  3.34it/s, v_num=1, train_loss_step=7.74e+7, train_loss_epoch=7.98e+7]Epoch 107/300:  36%|███▌      | 107/300 [00:30&lt;00:57,  3.34it/s, v_num=1, train_loss_step=6.95e+7, train_loss_epoch=7.83e+7]Epoch 108/300:  36%|███▌      | 107/300 [00:30&lt;00:57,  3.34it/s, v_num=1, train_loss_step=6.95e+7, train_loss_epoch=7.83e+7]Epoch 108/300:  36%|███▌      | 108/300 [00:30&lt;00:57,  3.35it/s, v_num=1, train_loss_step=6.95e+7, train_loss_epoch=7.83e+7]Epoch 108/300:  36%|███▌      | 108/300 [00:30&lt;00:57,  3.35it/s, v_num=1, train_loss_step=7e+7, train_loss_epoch=7.67e+7]   Epoch 109/300:  36%|███▌      | 108/300 [00:30&lt;00:57,  3.35it/s, v_num=1, train_loss_step=7e+7, train_loss_epoch=7.67e+7]Epoch 109/300:  36%|███▋      | 109/300 [00:30&lt;00:55,  3.43it/s, v_num=1, train_loss_step=7e+7, train_loss_epoch=7.67e+7]Epoch 109/300:  36%|███▋      | 109/300 [00:30&lt;00:55,  3.43it/s, v_num=1, train_loss_step=7.16e+7, train_loss_epoch=7.52e+7]Epoch 110/300:  36%|███▋      | 109/300 [00:30&lt;00:55,  3.43it/s, v_num=1, train_loss_step=7.16e+7, train_loss_epoch=7.52e+7]Epoch 110/300:  37%|███▋      | 110/300 [00:31&lt;00:55,  3.41it/s, v_num=1, train_loss_step=7.16e+7, train_loss_epoch=7.52e+7]Epoch 110/300:  37%|███▋      | 110/300 [00:31&lt;00:55,  3.41it/s, v_num=1, train_loss_step=7.27e+7, train_loss_epoch=7.38e+7]Epoch 111/300:  37%|███▋      | 110/300 [00:31&lt;00:55,  3.41it/s, v_num=1, train_loss_step=7.27e+7, train_loss_epoch=7.38e+7]Epoch 111/300:  37%|███▋      | 111/300 [00:31&lt;00:54,  3.44it/s, v_num=1, train_loss_step=7.27e+7, train_loss_epoch=7.38e+7]Epoch 111/300:  37%|███▋      | 111/300 [00:31&lt;00:54,  3.44it/s, v_num=1, train_loss_step=6.67e+7, train_loss_epoch=7.24e+7]Epoch 112/300:  37%|███▋      | 111/300 [00:31&lt;00:54,  3.44it/s, v_num=1, train_loss_step=6.67e+7, train_loss_epoch=7.24e+7]Epoch 112/300:  37%|███▋      | 112/300 [00:31&lt;00:55,  3.39it/s, v_num=1, train_loss_step=6.67e+7, train_loss_epoch=7.24e+7]Epoch 112/300:  37%|███▋      | 112/300 [00:31&lt;00:55,  3.39it/s, v_num=1, train_loss_step=6.93e+7, train_loss_epoch=7.1e+7] Epoch 113/300:  37%|███▋      | 112/300 [00:31&lt;00:55,  3.39it/s, v_num=1, train_loss_step=6.93e+7, train_loss_epoch=7.1e+7]Epoch 113/300:  38%|███▊      | 113/300 [00:32&lt;00:54,  3.42it/s, v_num=1, train_loss_step=6.93e+7, train_loss_epoch=7.1e+7]Epoch 113/300:  38%|███▊      | 113/300 [00:32&lt;00:54,  3.42it/s, v_num=1, train_loss_step=6.54e+7, train_loss_epoch=6.97e+7]Epoch 114/300:  38%|███▊      | 113/300 [00:32&lt;00:54,  3.42it/s, v_num=1, train_loss_step=6.54e+7, train_loss_epoch=6.97e+7]Epoch 114/300:  38%|███▊      | 114/300 [00:32&lt;00:54,  3.42it/s, v_num=1, train_loss_step=6.54e+7, train_loss_epoch=6.97e+7]Epoch 114/300:  38%|███▊      | 114/300 [00:32&lt;00:54,  3.42it/s, v_num=1, train_loss_step=6.53e+7, train_loss_epoch=6.84e+7]Epoch 115/300:  38%|███▊      | 114/300 [00:32&lt;00:54,  3.42it/s, v_num=1, train_loss_step=6.53e+7, train_loss_epoch=6.84e+7]Epoch 115/300:  38%|███▊      | 115/300 [00:32&lt;00:53,  3.43it/s, v_num=1, train_loss_step=6.53e+7, train_loss_epoch=6.84e+7]Epoch 115/300:  38%|███▊      | 115/300 [00:32&lt;00:53,  3.43it/s, v_num=1, train_loss_step=6.36e+7, train_loss_epoch=6.71e+7]Epoch 116/300:  38%|███▊      | 115/300 [00:32&lt;00:53,  3.43it/s, v_num=1, train_loss_step=6.36e+7, train_loss_epoch=6.71e+7]Epoch 116/300:  39%|███▊      | 116/300 [00:32&lt;00:53,  3.41it/s, v_num=1, train_loss_step=6.36e+7, train_loss_epoch=6.71e+7]Epoch 116/300:  39%|███▊      | 116/300 [00:32&lt;00:53,  3.41it/s, v_num=1, train_loss_step=5.98e+7, train_loss_epoch=6.59e+7]Epoch 117/300:  39%|███▊      | 116/300 [00:32&lt;00:53,  3.41it/s, v_num=1, train_loss_step=5.98e+7, train_loss_epoch=6.59e+7]Epoch 117/300:  39%|███▉      | 117/300 [00:33&lt;00:53,  3.40it/s, v_num=1, train_loss_step=5.98e+7, train_loss_epoch=6.59e+7]Epoch 117/300:  39%|███▉      | 117/300 [00:33&lt;00:53,  3.40it/s, v_num=1, train_loss_step=6.26e+7, train_loss_epoch=6.47e+7]Epoch 118/300:  39%|███▉      | 117/300 [00:33&lt;00:53,  3.40it/s, v_num=1, train_loss_step=6.26e+7, train_loss_epoch=6.47e+7]Epoch 118/300:  39%|███▉      | 118/300 [00:33&lt;00:52,  3.46it/s, v_num=1, train_loss_step=6.26e+7, train_loss_epoch=6.47e+7]Epoch 118/300:  39%|███▉      | 118/300 [00:33&lt;00:52,  3.46it/s, v_num=1, train_loss_step=5.97e+7, train_loss_epoch=6.35e+7]Epoch 119/300:  39%|███▉      | 118/300 [00:33&lt;00:52,  3.46it/s, v_num=1, train_loss_step=5.97e+7, train_loss_epoch=6.35e+7]Epoch 119/300:  40%|███▉      | 119/300 [00:33&lt;00:51,  3.51it/s, v_num=1, train_loss_step=5.97e+7, train_loss_epoch=6.35e+7]Epoch 119/300:  40%|███▉      | 119/300 [00:33&lt;00:51,  3.51it/s, v_num=1, train_loss_step=6.2e+7, train_loss_epoch=6.24e+7] Epoch 120/300:  40%|███▉      | 119/300 [00:33&lt;00:51,  3.51it/s, v_num=1, train_loss_step=6.2e+7, train_loss_epoch=6.24e+7]Epoch 120/300:  40%|████      | 120/300 [00:34&lt;00:51,  3.51it/s, v_num=1, train_loss_step=6.2e+7, train_loss_epoch=6.24e+7]Epoch 120/300:  40%|████      | 120/300 [00:34&lt;00:51,  3.51it/s, v_num=1, train_loss_step=5.85e+7, train_loss_epoch=6.13e+7]Epoch 121/300:  40%|████      | 120/300 [00:34&lt;00:51,  3.51it/s, v_num=1, train_loss_step=5.85e+7, train_loss_epoch=6.13e+7]Epoch 121/300:  40%|████      | 121/300 [00:34&lt;00:54,  3.26it/s, v_num=1, train_loss_step=5.85e+7, train_loss_epoch=6.13e+7]Epoch 121/300:  40%|████      | 121/300 [00:34&lt;00:54,  3.26it/s, v_num=1, train_loss_step=5.64e+7, train_loss_epoch=6.02e+7]Epoch 122/300:  40%|████      | 121/300 [00:34&lt;00:54,  3.26it/s, v_num=1, train_loss_step=5.64e+7, train_loss_epoch=6.02e+7]Epoch 122/300:  41%|████      | 122/300 [00:34&lt;00:53,  3.35it/s, v_num=1, train_loss_step=5.64e+7, train_loss_epoch=6.02e+7]Epoch 122/300:  41%|████      | 122/300 [00:34&lt;00:53,  3.35it/s, v_num=1, train_loss_step=5.48e+7, train_loss_epoch=5.92e+7]Epoch 123/300:  41%|████      | 122/300 [00:34&lt;00:53,  3.35it/s, v_num=1, train_loss_step=5.48e+7, train_loss_epoch=5.92e+7]Epoch 123/300:  41%|████      | 123/300 [00:34&lt;00:52,  3.34it/s, v_num=1, train_loss_step=5.48e+7, train_loss_epoch=5.92e+7]Epoch 123/300:  41%|████      | 123/300 [00:34&lt;00:52,  3.34it/s, v_num=1, train_loss_step=5.59e+7, train_loss_epoch=5.82e+7]Epoch 124/300:  41%|████      | 123/300 [00:34&lt;00:52,  3.34it/s, v_num=1, train_loss_step=5.59e+7, train_loss_epoch=5.82e+7]Epoch 124/300:  41%|████▏     | 124/300 [00:35&lt;00:51,  3.40it/s, v_num=1, train_loss_step=5.59e+7, train_loss_epoch=5.82e+7]Epoch 124/300:  41%|████▏     | 124/300 [00:35&lt;00:51,  3.40it/s, v_num=1, train_loss_step=5.36e+7, train_loss_epoch=5.72e+7]Epoch 125/300:  41%|████▏     | 124/300 [00:35&lt;00:51,  3.40it/s, v_num=1, train_loss_step=5.36e+7, train_loss_epoch=5.72e+7]Epoch 125/300:  42%|████▏     | 125/300 [00:35&lt;00:50,  3.45it/s, v_num=1, train_loss_step=5.36e+7, train_loss_epoch=5.72e+7]Epoch 125/300:  42%|████▏     | 125/300 [00:35&lt;00:50,  3.45it/s, v_num=1, train_loss_step=5.21e+7, train_loss_epoch=5.62e+7]Epoch 126/300:  42%|████▏     | 125/300 [00:35&lt;00:50,  3.45it/s, v_num=1, train_loss_step=5.21e+7, train_loss_epoch=5.62e+7]Epoch 126/300:  42%|████▏     | 126/300 [00:35&lt;00:49,  3.52it/s, v_num=1, train_loss_step=5.21e+7, train_loss_epoch=5.62e+7]Epoch 126/300:  42%|████▏     | 126/300 [00:35&lt;00:49,  3.52it/s, v_num=1, train_loss_step=5.2e+7, train_loss_epoch=5.53e+7] Epoch 127/300:  42%|████▏     | 126/300 [00:35&lt;00:49,  3.52it/s, v_num=1, train_loss_step=5.2e+7, train_loss_epoch=5.53e+7]Epoch 127/300:  42%|████▏     | 127/300 [00:36&lt;00:48,  3.57it/s, v_num=1, train_loss_step=5.2e+7, train_loss_epoch=5.53e+7]Epoch 127/300:  42%|████▏     | 127/300 [00:36&lt;00:48,  3.57it/s, v_num=1, train_loss_step=4.8e+7, train_loss_epoch=5.43e+7]Epoch 128/300:  42%|████▏     | 127/300 [00:36&lt;00:48,  3.57it/s, v_num=1, train_loss_step=4.8e+7, train_loss_epoch=5.43e+7]Epoch 128/300:  43%|████▎     | 128/300 [00:36&lt;00:47,  3.59it/s, v_num=1, train_loss_step=4.8e+7, train_loss_epoch=5.43e+7]Epoch 128/300:  43%|████▎     | 128/300 [00:36&lt;00:47,  3.59it/s, v_num=1, train_loss_step=4.93e+7, train_loss_epoch=5.35e+7]Epoch 129/300:  43%|████▎     | 128/300 [00:36&lt;00:47,  3.59it/s, v_num=1, train_loss_step=4.93e+7, train_loss_epoch=5.35e+7]Epoch 129/300:  43%|████▎     | 129/300 [00:36&lt;00:47,  3.58it/s, v_num=1, train_loss_step=4.93e+7, train_loss_epoch=5.35e+7]Epoch 129/300:  43%|████▎     | 129/300 [00:36&lt;00:47,  3.58it/s, v_num=1, train_loss_step=4.9e+7, train_loss_epoch=5.26e+7] Epoch 130/300:  43%|████▎     | 129/300 [00:36&lt;00:47,  3.58it/s, v_num=1, train_loss_step=4.9e+7, train_loss_epoch=5.26e+7]Epoch 130/300:  43%|████▎     | 130/300 [00:36&lt;00:47,  3.61it/s, v_num=1, train_loss_step=4.9e+7, train_loss_epoch=5.26e+7]Epoch 130/300:  43%|████▎     | 130/300 [00:36&lt;00:47,  3.61it/s, v_num=1, train_loss_step=4.74e+7, train_loss_epoch=5.17e+7]Epoch 131/300:  43%|████▎     | 130/300 [00:36&lt;00:47,  3.61it/s, v_num=1, train_loss_step=4.74e+7, train_loss_epoch=5.17e+7]Epoch 131/300:  44%|████▎     | 131/300 [00:37&lt;00:47,  3.59it/s, v_num=1, train_loss_step=4.74e+7, train_loss_epoch=5.17e+7]Epoch 131/300:  44%|████▎     | 131/300 [00:37&lt;00:47,  3.59it/s, v_num=1, train_loss_step=5.01e+7, train_loss_epoch=5.09e+7]Epoch 132/300:  44%|████▎     | 131/300 [00:37&lt;00:47,  3.59it/s, v_num=1, train_loss_step=5.01e+7, train_loss_epoch=5.09e+7]Epoch 132/300:  44%|████▍     | 132/300 [00:37&lt;00:47,  3.53it/s, v_num=1, train_loss_step=5.01e+7, train_loss_epoch=5.09e+7]Epoch 132/300:  44%|████▍     | 132/300 [00:37&lt;00:47,  3.53it/s, v_num=1, train_loss_step=4.56e+7, train_loss_epoch=5.01e+7]Epoch 133/300:  44%|████▍     | 132/300 [00:37&lt;00:47,  3.53it/s, v_num=1, train_loss_step=4.56e+7, train_loss_epoch=5.01e+7]Epoch 133/300:  44%|████▍     | 133/300 [00:37&lt;00:46,  3.56it/s, v_num=1, train_loss_step=4.56e+7, train_loss_epoch=5.01e+7]Epoch 133/300:  44%|████▍     | 133/300 [00:37&lt;00:46,  3.56it/s, v_num=1, train_loss_step=5e+7, train_loss_epoch=4.93e+7]   Epoch 134/300:  44%|████▍     | 133/300 [00:37&lt;00:46,  3.56it/s, v_num=1, train_loss_step=5e+7, train_loss_epoch=4.93e+7]Epoch 134/300:  45%|████▍     | 134/300 [00:38&lt;00:46,  3.58it/s, v_num=1, train_loss_step=5e+7, train_loss_epoch=4.93e+7]Epoch 134/300:  45%|████▍     | 134/300 [00:38&lt;00:46,  3.58it/s, v_num=1, train_loss_step=4.31e+7, train_loss_epoch=4.85e+7]Epoch 135/300:  45%|████▍     | 134/300 [00:38&lt;00:46,  3.58it/s, v_num=1, train_loss_step=4.31e+7, train_loss_epoch=4.85e+7]Epoch 135/300:  45%|████▌     | 135/300 [00:38&lt;00:47,  3.51it/s, v_num=1, train_loss_step=4.31e+7, train_loss_epoch=4.85e+7]Epoch 135/300:  45%|████▌     | 135/300 [00:38&lt;00:47,  3.51it/s, v_num=1, train_loss_step=4.37e+7, train_loss_epoch=4.78e+7]Epoch 136/300:  45%|████▌     | 135/300 [00:38&lt;00:47,  3.51it/s, v_num=1, train_loss_step=4.37e+7, train_loss_epoch=4.78e+7]Epoch 136/300:  45%|████▌     | 136/300 [00:38&lt;00:46,  3.50it/s, v_num=1, train_loss_step=4.37e+7, train_loss_epoch=4.78e+7]Epoch 136/300:  45%|████▌     | 136/300 [00:38&lt;00:46,  3.50it/s, v_num=1, train_loss_step=4.44e+7, train_loss_epoch=4.7e+7] Epoch 137/300:  45%|████▌     | 136/300 [00:38&lt;00:46,  3.50it/s, v_num=1, train_loss_step=4.44e+7, train_loss_epoch=4.7e+7]Epoch 137/300:  46%|████▌     | 137/300 [00:38&lt;00:46,  3.53it/s, v_num=1, train_loss_step=4.44e+7, train_loss_epoch=4.7e+7]Epoch 137/300:  46%|████▌     | 137/300 [00:38&lt;00:46,  3.53it/s, v_num=1, train_loss_step=4.53e+7, train_loss_epoch=4.63e+7]Epoch 138/300:  46%|████▌     | 137/300 [00:38&lt;00:46,  3.53it/s, v_num=1, train_loss_step=4.53e+7, train_loss_epoch=4.63e+7]Epoch 138/300:  46%|████▌     | 138/300 [00:39&lt;00:45,  3.58it/s, v_num=1, train_loss_step=4.53e+7, train_loss_epoch=4.63e+7]Epoch 138/300:  46%|████▌     | 138/300 [00:39&lt;00:45,  3.58it/s, v_num=1, train_loss_step=3.98e+7, train_loss_epoch=4.56e+7]Epoch 139/300:  46%|████▌     | 138/300 [00:39&lt;00:45,  3.58it/s, v_num=1, train_loss_step=3.98e+7, train_loss_epoch=4.56e+7]Epoch 139/300:  46%|████▋     | 139/300 [00:39&lt;00:46,  3.48it/s, v_num=1, train_loss_step=3.98e+7, train_loss_epoch=4.56e+7]Epoch 139/300:  46%|████▋     | 139/300 [00:39&lt;00:46,  3.48it/s, v_num=1, train_loss_step=4.52e+7, train_loss_epoch=4.49e+7]Epoch 140/300:  46%|████▋     | 139/300 [00:39&lt;00:46,  3.48it/s, v_num=1, train_loss_step=4.52e+7, train_loss_epoch=4.49e+7]Epoch 140/300:  47%|████▋     | 140/300 [00:39&lt;00:47,  3.40it/s, v_num=1, train_loss_step=4.52e+7, train_loss_epoch=4.49e+7]Epoch 140/300:  47%|████▋     | 140/300 [00:39&lt;00:47,  3.40it/s, v_num=1, train_loss_step=4.07e+7, train_loss_epoch=4.43e+7]Epoch 141/300:  47%|████▋     | 140/300 [00:39&lt;00:47,  3.40it/s, v_num=1, train_loss_step=4.07e+7, train_loss_epoch=4.43e+7]Epoch 141/300:  47%|████▋     | 141/300 [00:40&lt;00:46,  3.40it/s, v_num=1, train_loss_step=4.07e+7, train_loss_epoch=4.43e+7]Epoch 141/300:  47%|████▋     | 141/300 [00:40&lt;00:46,  3.40it/s, v_num=1, train_loss_step=4.29e+7, train_loss_epoch=4.36e+7]Epoch 142/300:  47%|████▋     | 141/300 [00:40&lt;00:46,  3.40it/s, v_num=1, train_loss_step=4.29e+7, train_loss_epoch=4.36e+7]Epoch 142/300:  47%|████▋     | 142/300 [00:40&lt;00:46,  3.42it/s, v_num=1, train_loss_step=4.29e+7, train_loss_epoch=4.36e+7]Epoch 142/300:  47%|████▋     | 142/300 [00:40&lt;00:46,  3.42it/s, v_num=1, train_loss_step=4.11e+7, train_loss_epoch=4.3e+7] Epoch 143/300:  47%|████▋     | 142/300 [00:40&lt;00:46,  3.42it/s, v_num=1, train_loss_step=4.11e+7, train_loss_epoch=4.3e+7]Epoch 143/300:  48%|████▊     | 143/300 [00:40&lt;00:45,  3.43it/s, v_num=1, train_loss_step=4.11e+7, train_loss_epoch=4.3e+7]Epoch 143/300:  48%|████▊     | 143/300 [00:40&lt;00:45,  3.43it/s, v_num=1, train_loss_step=4.19e+7, train_loss_epoch=4.24e+7]Epoch 144/300:  48%|████▊     | 143/300 [00:40&lt;00:45,  3.43it/s, v_num=1, train_loss_step=4.19e+7, train_loss_epoch=4.24e+7]Epoch 144/300:  48%|████▊     | 144/300 [00:40&lt;00:46,  3.33it/s, v_num=1, train_loss_step=4.19e+7, train_loss_epoch=4.24e+7]Epoch 144/300:  48%|████▊     | 144/300 [00:40&lt;00:46,  3.33it/s, v_num=1, train_loss_step=3.85e+7, train_loss_epoch=4.18e+7]Epoch 145/300:  48%|████▊     | 144/300 [00:40&lt;00:46,  3.33it/s, v_num=1, train_loss_step=3.85e+7, train_loss_epoch=4.18e+7]Epoch 145/300:  48%|████▊     | 145/300 [00:41&lt;00:45,  3.40it/s, v_num=1, train_loss_step=3.85e+7, train_loss_epoch=4.18e+7]Epoch 145/300:  48%|████▊     | 145/300 [00:41&lt;00:45,  3.40it/s, v_num=1, train_loss_step=4.38e+7, train_loss_epoch=4.12e+7]Epoch 146/300:  48%|████▊     | 145/300 [00:41&lt;00:45,  3.40it/s, v_num=1, train_loss_step=4.38e+7, train_loss_epoch=4.12e+7]Epoch 146/300:  49%|████▊     | 146/300 [00:41&lt;00:44,  3.42it/s, v_num=1, train_loss_step=4.38e+7, train_loss_epoch=4.12e+7]Epoch 146/300:  49%|████▊     | 146/300 [00:41&lt;00:44,  3.42it/s, v_num=1, train_loss_step=3.82e+7, train_loss_epoch=4.06e+7]Epoch 147/300:  49%|████▊     | 146/300 [00:41&lt;00:44,  3.42it/s, v_num=1, train_loss_step=3.82e+7, train_loss_epoch=4.06e+7]Epoch 147/300:  49%|████▉     | 147/300 [00:41&lt;00:43,  3.48it/s, v_num=1, train_loss_step=3.82e+7, train_loss_epoch=4.06e+7]Epoch 147/300:  49%|████▉     | 147/300 [00:41&lt;00:43,  3.48it/s, v_num=1, train_loss_step=3.7e+7, train_loss_epoch=4e+7]    Epoch 148/300:  49%|████▉     | 147/300 [00:41&lt;00:43,  3.48it/s, v_num=1, train_loss_step=3.7e+7, train_loss_epoch=4e+7]Epoch 148/300:  49%|████▉     | 148/300 [00:42&lt;00:42,  3.54it/s, v_num=1, train_loss_step=3.7e+7, train_loss_epoch=4e+7]Epoch 148/300:  49%|████▉     | 148/300 [00:42&lt;00:42,  3.54it/s, v_num=1, train_loss_step=3.57e+7, train_loss_epoch=3.95e+7]Epoch 149/300:  49%|████▉     | 148/300 [00:42&lt;00:42,  3.54it/s, v_num=1, train_loss_step=3.57e+7, train_loss_epoch=3.95e+7]Epoch 149/300:  50%|████▉     | 149/300 [00:42&lt;00:42,  3.56it/s, v_num=1, train_loss_step=3.57e+7, train_loss_epoch=3.95e+7]Epoch 149/300:  50%|████▉     | 149/300 [00:42&lt;00:42,  3.56it/s, v_num=1, train_loss_step=3.46e+7, train_loss_epoch=3.9e+7] Epoch 150/300:  50%|████▉     | 149/300 [00:42&lt;00:42,  3.56it/s, v_num=1, train_loss_step=3.46e+7, train_loss_epoch=3.9e+7]Epoch 150/300:  50%|█████     | 150/300 [00:42&lt;00:42,  3.56it/s, v_num=1, train_loss_step=3.46e+7, train_loss_epoch=3.9e+7]Epoch 150/300:  50%|█████     | 150/300 [00:42&lt;00:42,  3.56it/s, v_num=1, train_loss_step=3.4e+7, train_loss_epoch=3.84e+7]Epoch 151/300:  50%|█████     | 150/300 [00:42&lt;00:42,  3.56it/s, v_num=1, train_loss_step=3.4e+7, train_loss_epoch=3.84e+7]Epoch 151/300:  50%|█████     | 151/300 [00:42&lt;00:41,  3.55it/s, v_num=1, train_loss_step=3.4e+7, train_loss_epoch=3.84e+7]Epoch 151/300:  50%|█████     | 151/300 [00:42&lt;00:41,  3.55it/s, v_num=1, train_loss_step=3.6e+7, train_loss_epoch=3.79e+7]Epoch 152/300:  50%|█████     | 151/300 [00:42&lt;00:41,  3.55it/s, v_num=1, train_loss_step=3.6e+7, train_loss_epoch=3.79e+7]Epoch 152/300:  51%|█████     | 152/300 [00:43&lt;00:42,  3.51it/s, v_num=1, train_loss_step=3.6e+7, train_loss_epoch=3.79e+7]Epoch 152/300:  51%|█████     | 152/300 [00:43&lt;00:42,  3.51it/s, v_num=1, train_loss_step=3.63e+7, train_loss_epoch=3.74e+7]Epoch 153/300:  51%|█████     | 152/300 [00:43&lt;00:42,  3.51it/s, v_num=1, train_loss_step=3.63e+7, train_loss_epoch=3.74e+7]Epoch 153/300:  51%|█████     | 153/300 [00:43&lt;00:42,  3.42it/s, v_num=1, train_loss_step=3.63e+7, train_loss_epoch=3.74e+7]Epoch 153/300:  51%|█████     | 153/300 [00:43&lt;00:42,  3.42it/s, v_num=1, train_loss_step=3.37e+7, train_loss_epoch=3.69e+7]Epoch 154/300:  51%|█████     | 153/300 [00:43&lt;00:42,  3.42it/s, v_num=1, train_loss_step=3.37e+7, train_loss_epoch=3.69e+7]Epoch 154/300:  51%|█████▏    | 154/300 [00:43&lt;00:43,  3.37it/s, v_num=1, train_loss_step=3.37e+7, train_loss_epoch=3.69e+7]Epoch 154/300:  51%|█████▏    | 154/300 [00:43&lt;00:43,  3.37it/s, v_num=1, train_loss_step=3.73e+7, train_loss_epoch=3.64e+7]Epoch 155/300:  51%|█████▏    | 154/300 [00:43&lt;00:43,  3.37it/s, v_num=1, train_loss_step=3.73e+7, train_loss_epoch=3.64e+7]Epoch 155/300:  52%|█████▏    | 155/300 [00:44&lt;00:42,  3.39it/s, v_num=1, train_loss_step=3.73e+7, train_loss_epoch=3.64e+7]Epoch 155/300:  52%|█████▏    | 155/300 [00:44&lt;00:42,  3.39it/s, v_num=1, train_loss_step=3.53e+7, train_loss_epoch=3.6e+7] Epoch 156/300:  52%|█████▏    | 155/300 [00:44&lt;00:42,  3.39it/s, v_num=1, train_loss_step=3.53e+7, train_loss_epoch=3.6e+7]Epoch 156/300:  52%|█████▏    | 156/300 [00:44&lt;00:41,  3.46it/s, v_num=1, train_loss_step=3.53e+7, train_loss_epoch=3.6e+7]Epoch 156/300:  52%|█████▏    | 156/300 [00:44&lt;00:41,  3.46it/s, v_num=1, train_loss_step=3.31e+7, train_loss_epoch=3.55e+7]Epoch 157/300:  52%|█████▏    | 156/300 [00:44&lt;00:41,  3.46it/s, v_num=1, train_loss_step=3.31e+7, train_loss_epoch=3.55e+7]Epoch 157/300:  52%|█████▏    | 157/300 [00:44&lt;00:42,  3.36it/s, v_num=1, train_loss_step=3.31e+7, train_loss_epoch=3.55e+7]Epoch 157/300:  52%|█████▏    | 157/300 [00:44&lt;00:42,  3.36it/s, v_num=1, train_loss_step=3.33e+7, train_loss_epoch=3.5e+7] Epoch 158/300:  52%|█████▏    | 157/300 [00:44&lt;00:42,  3.36it/s, v_num=1, train_loss_step=3.33e+7, train_loss_epoch=3.5e+7]Epoch 158/300:  53%|█████▎    | 158/300 [00:45&lt;00:42,  3.38it/s, v_num=1, train_loss_step=3.33e+7, train_loss_epoch=3.5e+7]Epoch 158/300:  53%|█████▎    | 158/300 [00:45&lt;00:42,  3.38it/s, v_num=1, train_loss_step=3.36e+7, train_loss_epoch=3.46e+7]Epoch 159/300:  53%|█████▎    | 158/300 [00:45&lt;00:42,  3.38it/s, v_num=1, train_loss_step=3.36e+7, train_loss_epoch=3.46e+7]Epoch 159/300:  53%|█████▎    | 159/300 [00:45&lt;00:40,  3.47it/s, v_num=1, train_loss_step=3.36e+7, train_loss_epoch=3.46e+7]Epoch 159/300:  53%|█████▎    | 159/300 [00:45&lt;00:40,  3.47it/s, v_num=1, train_loss_step=3.24e+7, train_loss_epoch=3.42e+7]Epoch 160/300:  53%|█████▎    | 159/300 [00:45&lt;00:40,  3.47it/s, v_num=1, train_loss_step=3.24e+7, train_loss_epoch=3.42e+7]Epoch 160/300:  53%|█████▎    | 160/300 [00:45&lt;00:40,  3.50it/s, v_num=1, train_loss_step=3.24e+7, train_loss_epoch=3.42e+7]Epoch 160/300:  53%|█████▎    | 160/300 [00:45&lt;00:40,  3.50it/s, v_num=1, train_loss_step=3.13e+7, train_loss_epoch=3.38e+7]Epoch 161/300:  53%|█████▎    | 160/300 [00:45&lt;00:40,  3.50it/s, v_num=1, train_loss_step=3.13e+7, train_loss_epoch=3.38e+7]Epoch 161/300:  54%|█████▎    | 161/300 [00:45&lt;00:39,  3.56it/s, v_num=1, train_loss_step=3.13e+7, train_loss_epoch=3.38e+7]Epoch 161/300:  54%|█████▎    | 161/300 [00:45&lt;00:39,  3.56it/s, v_num=1, train_loss_step=3e+7, train_loss_epoch=3.33e+7]   Epoch 162/300:  54%|█████▎    | 161/300 [00:45&lt;00:39,  3.56it/s, v_num=1, train_loss_step=3e+7, train_loss_epoch=3.33e+7]Epoch 162/300:  54%|█████▍    | 162/300 [00:46&lt;00:38,  3.59it/s, v_num=1, train_loss_step=3e+7, train_loss_epoch=3.33e+7]Epoch 162/300:  54%|█████▍    | 162/300 [00:46&lt;00:38,  3.59it/s, v_num=1, train_loss_step=2.99e+7, train_loss_epoch=3.29e+7]Epoch 163/300:  54%|█████▍    | 162/300 [00:46&lt;00:38,  3.59it/s, v_num=1, train_loss_step=2.99e+7, train_loss_epoch=3.29e+7]Epoch 163/300:  54%|█████▍    | 163/300 [00:46&lt;00:38,  3.58it/s, v_num=1, train_loss_step=2.99e+7, train_loss_epoch=3.29e+7]Epoch 163/300:  54%|█████▍    | 163/300 [00:46&lt;00:38,  3.58it/s, v_num=1, train_loss_step=3.03e+7, train_loss_epoch=3.25e+7]Epoch 164/300:  54%|█████▍    | 163/300 [00:46&lt;00:38,  3.58it/s, v_num=1, train_loss_step=3.03e+7, train_loss_epoch=3.25e+7]Epoch 164/300:  55%|█████▍    | 164/300 [00:46&lt;00:39,  3.48it/s, v_num=1, train_loss_step=3.03e+7, train_loss_epoch=3.25e+7]Epoch 164/300:  55%|█████▍    | 164/300 [00:46&lt;00:39,  3.48it/s, v_num=1, train_loss_step=3.11e+7, train_loss_epoch=3.21e+7]Epoch 165/300:  55%|█████▍    | 164/300 [00:46&lt;00:39,  3.48it/s, v_num=1, train_loss_step=3.11e+7, train_loss_epoch=3.21e+7]Epoch 165/300:  55%|█████▌    | 165/300 [00:46&lt;00:39,  3.39it/s, v_num=1, train_loss_step=3.11e+7, train_loss_epoch=3.21e+7]Epoch 165/300:  55%|█████▌    | 165/300 [00:46&lt;00:39,  3.39it/s, v_num=1, train_loss_step=2.9e+7, train_loss_epoch=3.18e+7] Epoch 166/300:  55%|█████▌    | 165/300 [00:47&lt;00:39,  3.39it/s, v_num=1, train_loss_step=2.9e+7, train_loss_epoch=3.18e+7]Epoch 166/300:  55%|█████▌    | 166/300 [00:47&lt;00:39,  3.39it/s, v_num=1, train_loss_step=2.9e+7, train_loss_epoch=3.18e+7]Epoch 166/300:  55%|█████▌    | 166/300 [00:47&lt;00:39,  3.39it/s, v_num=1, train_loss_step=2.79e+7, train_loss_epoch=3.14e+7]Epoch 167/300:  55%|█████▌    | 166/300 [00:47&lt;00:39,  3.39it/s, v_num=1, train_loss_step=2.79e+7, train_loss_epoch=3.14e+7]Epoch 167/300:  56%|█████▌    | 167/300 [00:47&lt;00:38,  3.42it/s, v_num=1, train_loss_step=2.79e+7, train_loss_epoch=3.14e+7]Epoch 167/300:  56%|█████▌    | 167/300 [00:47&lt;00:38,  3.42it/s, v_num=1, train_loss_step=3.1e+7, train_loss_epoch=3.1e+7]  Epoch 168/300:  56%|█████▌    | 167/300 [00:47&lt;00:38,  3.42it/s, v_num=1, train_loss_step=3.1e+7, train_loss_epoch=3.1e+7]Epoch 168/300:  56%|█████▌    | 168/300 [00:47&lt;00:38,  3.47it/s, v_num=1, train_loss_step=3.1e+7, train_loss_epoch=3.1e+7]Epoch 168/300:  56%|█████▌    | 168/300 [00:47&lt;00:38,  3.47it/s, v_num=1, train_loss_step=2.95e+7, train_loss_epoch=3.07e+7]Epoch 169/300:  56%|█████▌    | 168/300 [00:47&lt;00:38,  3.47it/s, v_num=1, train_loss_step=2.95e+7, train_loss_epoch=3.07e+7]Epoch 169/300:  56%|█████▋    | 169/300 [00:48&lt;00:37,  3.50it/s, v_num=1, train_loss_step=2.95e+7, train_loss_epoch=3.07e+7]Epoch 169/300:  56%|█████▋    | 169/300 [00:48&lt;00:37,  3.50it/s, v_num=1, train_loss_step=2.95e+7, train_loss_epoch=3.03e+7]Epoch 170/300:  56%|█████▋    | 169/300 [00:48&lt;00:37,  3.50it/s, v_num=1, train_loss_step=2.95e+7, train_loss_epoch=3.03e+7]Epoch 170/300:  57%|█████▋    | 170/300 [00:48&lt;00:36,  3.53it/s, v_num=1, train_loss_step=2.95e+7, train_loss_epoch=3.03e+7]Epoch 170/300:  57%|█████▋    | 170/300 [00:48&lt;00:36,  3.53it/s, v_num=1, train_loss_step=2.7e+7, train_loss_epoch=3e+7]    Epoch 171/300:  57%|█████▋    | 170/300 [00:48&lt;00:36,  3.53it/s, v_num=1, train_loss_step=2.7e+7, train_loss_epoch=3e+7]Epoch 171/300:  57%|█████▋    | 171/300 [00:48&lt;00:36,  3.52it/s, v_num=1, train_loss_step=2.7e+7, train_loss_epoch=3e+7]Epoch 171/300:  57%|█████▋    | 171/300 [00:48&lt;00:36,  3.52it/s, v_num=1, train_loss_step=2.76e+7, train_loss_epoch=2.96e+7]Epoch 172/300:  57%|█████▋    | 171/300 [00:48&lt;00:36,  3.52it/s, v_num=1, train_loss_step=2.76e+7, train_loss_epoch=2.96e+7]Epoch 172/300:  57%|█████▋    | 172/300 [00:48&lt;00:36,  3.52it/s, v_num=1, train_loss_step=2.76e+7, train_loss_epoch=2.96e+7]Epoch 172/300:  57%|█████▋    | 172/300 [00:48&lt;00:36,  3.52it/s, v_num=1, train_loss_step=2.82e+7, train_loss_epoch=2.93e+7]Epoch 173/300:  57%|█████▋    | 172/300 [00:48&lt;00:36,  3.52it/s, v_num=1, train_loss_step=2.82e+7, train_loss_epoch=2.93e+7]Epoch 173/300:  58%|█████▊    | 173/300 [00:49&lt;00:36,  3.52it/s, v_num=1, train_loss_step=2.82e+7, train_loss_epoch=2.93e+7]Epoch 173/300:  58%|█████▊    | 173/300 [00:49&lt;00:36,  3.52it/s, v_num=1, train_loss_step=2.77e+7, train_loss_epoch=2.9e+7] Epoch 174/300:  58%|█████▊    | 173/300 [00:49&lt;00:36,  3.52it/s, v_num=1, train_loss_step=2.77e+7, train_loss_epoch=2.9e+7]Epoch 174/300:  58%|█████▊    | 174/300 [00:49&lt;00:35,  3.51it/s, v_num=1, train_loss_step=2.77e+7, train_loss_epoch=2.9e+7]Epoch 174/300:  58%|█████▊    | 174/300 [00:49&lt;00:35,  3.51it/s, v_num=1, train_loss_step=2.79e+7, train_loss_epoch=2.87e+7]Epoch 175/300:  58%|█████▊    | 174/300 [00:49&lt;00:35,  3.51it/s, v_num=1, train_loss_step=2.79e+7, train_loss_epoch=2.87e+7]Epoch 175/300:  58%|█████▊    | 175/300 [00:49&lt;00:35,  3.52it/s, v_num=1, train_loss_step=2.79e+7, train_loss_epoch=2.87e+7]Epoch 175/300:  58%|█████▊    | 175/300 [00:49&lt;00:35,  3.52it/s, v_num=1, train_loss_step=2.68e+7, train_loss_epoch=2.84e+7]Epoch 176/300:  58%|█████▊    | 175/300 [00:49&lt;00:35,  3.52it/s, v_num=1, train_loss_step=2.68e+7, train_loss_epoch=2.84e+7]Epoch 176/300:  59%|█████▊    | 176/300 [00:50&lt;00:36,  3.36it/s, v_num=1, train_loss_step=2.68e+7, train_loss_epoch=2.84e+7]Epoch 176/300:  59%|█████▊    | 176/300 [00:50&lt;00:36,  3.36it/s, v_num=1, train_loss_step=2.89e+7, train_loss_epoch=2.81e+7]Epoch 177/300:  59%|█████▊    | 176/300 [00:50&lt;00:36,  3.36it/s, v_num=1, train_loss_step=2.89e+7, train_loss_epoch=2.81e+7]Epoch 177/300:  59%|█████▉    | 177/300 [00:50&lt;00:36,  3.40it/s, v_num=1, train_loss_step=2.89e+7, train_loss_epoch=2.81e+7]Epoch 177/300:  59%|█████▉    | 177/300 [00:50&lt;00:36,  3.40it/s, v_num=1, train_loss_step=2.58e+7, train_loss_epoch=2.78e+7]Epoch 178/300:  59%|█████▉    | 177/300 [00:50&lt;00:36,  3.40it/s, v_num=1, train_loss_step=2.58e+7, train_loss_epoch=2.78e+7]Epoch 178/300:  59%|█████▉    | 178/300 [00:50&lt;00:35,  3.44it/s, v_num=1, train_loss_step=2.58e+7, train_loss_epoch=2.78e+7]Epoch 178/300:  59%|█████▉    | 178/300 [00:50&lt;00:35,  3.44it/s, v_num=1, train_loss_step=2.68e+7, train_loss_epoch=2.75e+7]Epoch 179/300:  59%|█████▉    | 178/300 [00:50&lt;00:35,  3.44it/s, v_num=1, train_loss_step=2.68e+7, train_loss_epoch=2.75e+7]Epoch 179/300:  60%|█████▉    | 179/300 [00:51&lt;00:34,  3.46it/s, v_num=1, train_loss_step=2.68e+7, train_loss_epoch=2.75e+7]Epoch 179/300:  60%|█████▉    | 179/300 [00:51&lt;00:34,  3.46it/s, v_num=1, train_loss_step=2.81e+7, train_loss_epoch=2.72e+7]Epoch 180/300:  60%|█████▉    | 179/300 [00:51&lt;00:34,  3.46it/s, v_num=1, train_loss_step=2.81e+7, train_loss_epoch=2.72e+7]Epoch 180/300:  60%|██████    | 180/300 [00:51&lt;00:34,  3.51it/s, v_num=1, train_loss_step=2.81e+7, train_loss_epoch=2.72e+7]Epoch 180/300:  60%|██████    | 180/300 [00:51&lt;00:34,  3.51it/s, v_num=1, train_loss_step=2.64e+7, train_loss_epoch=2.69e+7]Epoch 181/300:  60%|██████    | 180/300 [00:51&lt;00:34,  3.51it/s, v_num=1, train_loss_step=2.64e+7, train_loss_epoch=2.69e+7]Epoch 181/300:  60%|██████    | 181/300 [00:51&lt;00:33,  3.54it/s, v_num=1, train_loss_step=2.64e+7, train_loss_epoch=2.69e+7]Epoch 181/300:  60%|██████    | 181/300 [00:51&lt;00:33,  3.54it/s, v_num=1, train_loss_step=2.55e+7, train_loss_epoch=2.66e+7]Epoch 182/300:  60%|██████    | 181/300 [00:51&lt;00:33,  3.54it/s, v_num=1, train_loss_step=2.55e+7, train_loss_epoch=2.66e+7]Epoch 182/300:  61%|██████    | 182/300 [00:51&lt;00:33,  3.54it/s, v_num=1, train_loss_step=2.55e+7, train_loss_epoch=2.66e+7]Epoch 182/300:  61%|██████    | 182/300 [00:51&lt;00:33,  3.54it/s, v_num=1, train_loss_step=2.7e+7, train_loss_epoch=2.64e+7] Epoch 183/300:  61%|██████    | 182/300 [00:51&lt;00:33,  3.54it/s, v_num=1, train_loss_step=2.7e+7, train_loss_epoch=2.64e+7]Epoch 183/300:  61%|██████    | 183/300 [00:52&lt;00:33,  3.52it/s, v_num=1, train_loss_step=2.7e+7, train_loss_epoch=2.64e+7]Epoch 183/300:  61%|██████    | 183/300 [00:52&lt;00:33,  3.52it/s, v_num=1, train_loss_step=2.7e+7, train_loss_epoch=2.61e+7]Epoch 184/300:  61%|██████    | 183/300 [00:52&lt;00:33,  3.52it/s, v_num=1, train_loss_step=2.7e+7, train_loss_epoch=2.61e+7]Epoch 184/300:  61%|██████▏   | 184/300 [00:52&lt;00:33,  3.51it/s, v_num=1, train_loss_step=2.7e+7, train_loss_epoch=2.61e+7]Epoch 184/300:  61%|██████▏   | 184/300 [00:52&lt;00:33,  3.51it/s, v_num=1, train_loss_step=2.29e+7, train_loss_epoch=2.59e+7]Epoch 185/300:  61%|██████▏   | 184/300 [00:52&lt;00:33,  3.51it/s, v_num=1, train_loss_step=2.29e+7, train_loss_epoch=2.59e+7]Epoch 185/300:  62%|██████▏   | 185/300 [00:52&lt;00:32,  3.52it/s, v_num=1, train_loss_step=2.29e+7, train_loss_epoch=2.59e+7]Epoch 185/300:  62%|██████▏   | 185/300 [00:52&lt;00:32,  3.52it/s, v_num=1, train_loss_step=2.39e+7, train_loss_epoch=2.56e+7]Epoch 186/300:  62%|██████▏   | 185/300 [00:52&lt;00:32,  3.52it/s, v_num=1, train_loss_step=2.39e+7, train_loss_epoch=2.56e+7]Epoch 186/300:  62%|██████▏   | 186/300 [00:53&lt;00:32,  3.48it/s, v_num=1, train_loss_step=2.39e+7, train_loss_epoch=2.56e+7]Epoch 186/300:  62%|██████▏   | 186/300 [00:53&lt;00:32,  3.48it/s, v_num=1, train_loss_step=2.37e+7, train_loss_epoch=2.54e+7]Epoch 187/300:  62%|██████▏   | 186/300 [00:53&lt;00:32,  3.48it/s, v_num=1, train_loss_step=2.37e+7, train_loss_epoch=2.54e+7]Epoch 187/300:  62%|██████▏   | 187/300 [00:53&lt;00:32,  3.52it/s, v_num=1, train_loss_step=2.37e+7, train_loss_epoch=2.54e+7]Epoch 187/300:  62%|██████▏   | 187/300 [00:53&lt;00:32,  3.52it/s, v_num=1, train_loss_step=2.42e+7, train_loss_epoch=2.51e+7]Epoch 188/300:  62%|██████▏   | 187/300 [00:53&lt;00:32,  3.52it/s, v_num=1, train_loss_step=2.42e+7, train_loss_epoch=2.51e+7]Epoch 188/300:  63%|██████▎   | 188/300 [00:53&lt;00:32,  3.46it/s, v_num=1, train_loss_step=2.42e+7, train_loss_epoch=2.51e+7]Epoch 188/300:  63%|██████▎   | 188/300 [00:53&lt;00:32,  3.46it/s, v_num=1, train_loss_step=2.44e+7, train_loss_epoch=2.49e+7]Epoch 189/300:  63%|██████▎   | 188/300 [00:53&lt;00:32,  3.46it/s, v_num=1, train_loss_step=2.44e+7, train_loss_epoch=2.49e+7]Epoch 189/300:  63%|██████▎   | 189/300 [00:53&lt;00:32,  3.42it/s, v_num=1, train_loss_step=2.44e+7, train_loss_epoch=2.49e+7]Epoch 189/300:  63%|██████▎   | 189/300 [00:53&lt;00:32,  3.42it/s, v_num=1, train_loss_step=2.44e+7, train_loss_epoch=2.46e+7]Epoch 190/300:  63%|██████▎   | 189/300 [00:53&lt;00:32,  3.42it/s, v_num=1, train_loss_step=2.44e+7, train_loss_epoch=2.46e+7]Epoch 190/300:  63%|██████▎   | 190/300 [00:54&lt;00:32,  3.41it/s, v_num=1, train_loss_step=2.44e+7, train_loss_epoch=2.46e+7]Epoch 190/300:  63%|██████▎   | 190/300 [00:54&lt;00:32,  3.41it/s, v_num=1, train_loss_step=2.39e+7, train_loss_epoch=2.44e+7]Epoch 191/300:  63%|██████▎   | 190/300 [00:54&lt;00:32,  3.41it/s, v_num=1, train_loss_step=2.39e+7, train_loss_epoch=2.44e+7]Epoch 191/300:  64%|██████▎   | 191/300 [00:54&lt;00:31,  3.42it/s, v_num=1, train_loss_step=2.39e+7, train_loss_epoch=2.44e+7]Epoch 191/300:  64%|██████▎   | 191/300 [00:54&lt;00:31,  3.42it/s, v_num=1, train_loss_step=2.49e+7, train_loss_epoch=2.42e+7]Epoch 192/300:  64%|██████▎   | 191/300 [00:54&lt;00:31,  3.42it/s, v_num=1, train_loss_step=2.49e+7, train_loss_epoch=2.42e+7]Epoch 192/300:  64%|██████▍   | 192/300 [00:54&lt;00:31,  3.44it/s, v_num=1, train_loss_step=2.49e+7, train_loss_epoch=2.42e+7]Epoch 192/300:  64%|██████▍   | 192/300 [00:54&lt;00:31,  3.44it/s, v_num=1, train_loss_step=2.16e+7, train_loss_epoch=2.4e+7] Epoch 193/300:  64%|██████▍   | 192/300 [00:54&lt;00:31,  3.44it/s, v_num=1, train_loss_step=2.16e+7, train_loss_epoch=2.4e+7]Epoch 193/300:  64%|██████▍   | 193/300 [00:55&lt;00:30,  3.46it/s, v_num=1, train_loss_step=2.16e+7, train_loss_epoch=2.4e+7]Epoch 193/300:  64%|██████▍   | 193/300 [00:55&lt;00:30,  3.46it/s, v_num=1, train_loss_step=2.28e+7, train_loss_epoch=2.38e+7]Epoch 194/300:  64%|██████▍   | 193/300 [00:55&lt;00:30,  3.46it/s, v_num=1, train_loss_step=2.28e+7, train_loss_epoch=2.38e+7]Epoch 194/300:  65%|██████▍   | 194/300 [00:55&lt;00:31,  3.36it/s, v_num=1, train_loss_step=2.28e+7, train_loss_epoch=2.38e+7]Epoch 194/300:  65%|██████▍   | 194/300 [00:55&lt;00:31,  3.36it/s, v_num=1, train_loss_step=2.26e+7, train_loss_epoch=2.35e+7]Epoch 195/300:  65%|██████▍   | 194/300 [00:55&lt;00:31,  3.36it/s, v_num=1, train_loss_step=2.26e+7, train_loss_epoch=2.35e+7]Epoch 195/300:  65%|██████▌   | 195/300 [00:55&lt;00:31,  3.30it/s, v_num=1, train_loss_step=2.26e+7, train_loss_epoch=2.35e+7]Epoch 195/300:  65%|██████▌   | 195/300 [00:55&lt;00:31,  3.30it/s, v_num=1, train_loss_step=2.16e+7, train_loss_epoch=2.33e+7]Epoch 196/300:  65%|██████▌   | 195/300 [00:55&lt;00:31,  3.30it/s, v_num=1, train_loss_step=2.16e+7, train_loss_epoch=2.33e+7]Epoch 196/300:  65%|██████▌   | 196/300 [00:55&lt;00:31,  3.30it/s, v_num=1, train_loss_step=2.16e+7, train_loss_epoch=2.33e+7]Epoch 196/300:  65%|██████▌   | 196/300 [00:55&lt;00:31,  3.30it/s, v_num=1, train_loss_step=2.07e+7, train_loss_epoch=2.31e+7]Epoch 197/300:  65%|██████▌   | 196/300 [00:55&lt;00:31,  3.30it/s, v_num=1, train_loss_step=2.07e+7, train_loss_epoch=2.31e+7]Epoch 197/300:  66%|██████▌   | 197/300 [00:56&lt;00:31,  3.29it/s, v_num=1, train_loss_step=2.07e+7, train_loss_epoch=2.31e+7]Epoch 197/300:  66%|██████▌   | 197/300 [00:56&lt;00:31,  3.29it/s, v_num=1, train_loss_step=2.26e+7, train_loss_epoch=2.29e+7]Epoch 198/300:  66%|██████▌   | 197/300 [00:56&lt;00:31,  3.29it/s, v_num=1, train_loss_step=2.26e+7, train_loss_epoch=2.29e+7]Epoch 198/300:  66%|██████▌   | 198/300 [00:56&lt;00:31,  3.27it/s, v_num=1, train_loss_step=2.26e+7, train_loss_epoch=2.29e+7]Epoch 198/300:  66%|██████▌   | 198/300 [00:56&lt;00:31,  3.27it/s, v_num=1, train_loss_step=2.08e+7, train_loss_epoch=2.27e+7]Epoch 199/300:  66%|██████▌   | 198/300 [00:56&lt;00:31,  3.27it/s, v_num=1, train_loss_step=2.08e+7, train_loss_epoch=2.27e+7]Epoch 199/300:  66%|██████▋   | 199/300 [00:56&lt;00:31,  3.23it/s, v_num=1, train_loss_step=2.08e+7, train_loss_epoch=2.27e+7]Epoch 199/300:  66%|██████▋   | 199/300 [00:56&lt;00:31,  3.23it/s, v_num=1, train_loss_step=2.18e+7, train_loss_epoch=2.25e+7]Epoch 200/300:  66%|██████▋   | 199/300 [00:56&lt;00:31,  3.23it/s, v_num=1, train_loss_step=2.18e+7, train_loss_epoch=2.25e+7]Epoch 200/300:  67%|██████▋   | 200/300 [00:57&lt;00:30,  3.28it/s, v_num=1, train_loss_step=2.18e+7, train_loss_epoch=2.25e+7]Epoch 200/300:  67%|██████▋   | 200/300 [00:57&lt;00:30,  3.28it/s, v_num=1, train_loss_step=2.18e+7, train_loss_epoch=2.24e+7]Epoch 201/300:  67%|██████▋   | 200/300 [00:57&lt;00:30,  3.28it/s, v_num=1, train_loss_step=2.18e+7, train_loss_epoch=2.24e+7]Epoch 201/300:  67%|██████▋   | 201/300 [00:57&lt;00:29,  3.33it/s, v_num=1, train_loss_step=2.18e+7, train_loss_epoch=2.24e+7]Epoch 201/300:  67%|██████▋   | 201/300 [00:57&lt;00:29,  3.33it/s, v_num=1, train_loss_step=2.05e+7, train_loss_epoch=2.22e+7]Epoch 202/300:  67%|██████▋   | 201/300 [00:57&lt;00:29,  3.33it/s, v_num=1, train_loss_step=2.05e+7, train_loss_epoch=2.22e+7]Epoch 202/300:  67%|██████▋   | 202/300 [00:57&lt;00:29,  3.34it/s, v_num=1, train_loss_step=2.05e+7, train_loss_epoch=2.22e+7]Epoch 202/300:  67%|██████▋   | 202/300 [00:57&lt;00:29,  3.34it/s, v_num=1, train_loss_step=2.04e+7, train_loss_epoch=2.2e+7] Epoch 203/300:  67%|██████▋   | 202/300 [00:57&lt;00:29,  3.34it/s, v_num=1, train_loss_step=2.04e+7, train_loss_epoch=2.2e+7]Epoch 203/300:  68%|██████▊   | 203/300 [00:58&lt;00:29,  3.32it/s, v_num=1, train_loss_step=2.04e+7, train_loss_epoch=2.2e+7]Epoch 203/300:  68%|██████▊   | 203/300 [00:58&lt;00:29,  3.32it/s, v_num=1, train_loss_step=2.03e+7, train_loss_epoch=2.18e+7]Epoch 204/300:  68%|██████▊   | 203/300 [00:58&lt;00:29,  3.32it/s, v_num=1, train_loss_step=2.03e+7, train_loss_epoch=2.18e+7]Epoch 204/300:  68%|██████▊   | 204/300 [00:58&lt;00:28,  3.38it/s, v_num=1, train_loss_step=2.03e+7, train_loss_epoch=2.18e+7]Epoch 204/300:  68%|██████▊   | 204/300 [00:58&lt;00:28,  3.38it/s, v_num=1, train_loss_step=2.17e+7, train_loss_epoch=2.16e+7]Epoch 205/300:  68%|██████▊   | 204/300 [00:58&lt;00:28,  3.38it/s, v_num=1, train_loss_step=2.17e+7, train_loss_epoch=2.16e+7]Epoch 205/300:  68%|██████▊   | 205/300 [00:58&lt;00:27,  3.45it/s, v_num=1, train_loss_step=2.17e+7, train_loss_epoch=2.16e+7]Epoch 205/300:  68%|██████▊   | 205/300 [00:58&lt;00:27,  3.45it/s, v_num=1, train_loss_step=1.94e+7, train_loss_epoch=2.15e+7]Epoch 206/300:  68%|██████▊   | 205/300 [00:58&lt;00:27,  3.45it/s, v_num=1, train_loss_step=1.94e+7, train_loss_epoch=2.15e+7]Epoch 206/300:  69%|██████▊   | 206/300 [00:58&lt;00:27,  3.47it/s, v_num=1, train_loss_step=1.94e+7, train_loss_epoch=2.15e+7]Epoch 206/300:  69%|██████▊   | 206/300 [00:58&lt;00:27,  3.47it/s, v_num=1, train_loss_step=2.16e+7, train_loss_epoch=2.13e+7]Epoch 207/300:  69%|██████▊   | 206/300 [00:58&lt;00:27,  3.47it/s, v_num=1, train_loss_step=2.16e+7, train_loss_epoch=2.13e+7]Epoch 207/300:  69%|██████▉   | 207/300 [00:59&lt;00:26,  3.46it/s, v_num=1, train_loss_step=2.16e+7, train_loss_epoch=2.13e+7]Epoch 207/300:  69%|██████▉   | 207/300 [00:59&lt;00:26,  3.46it/s, v_num=1, train_loss_step=1.89e+7, train_loss_epoch=2.11e+7]Epoch 208/300:  69%|██████▉   | 207/300 [00:59&lt;00:26,  3.46it/s, v_num=1, train_loss_step=1.89e+7, train_loss_epoch=2.11e+7]Epoch 208/300:  69%|██████▉   | 208/300 [00:59&lt;00:27,  3.38it/s, v_num=1, train_loss_step=1.89e+7, train_loss_epoch=2.11e+7]Epoch 208/300:  69%|██████▉   | 208/300 [00:59&lt;00:27,  3.38it/s, v_num=1, train_loss_step=1.9e+7, train_loss_epoch=2.1e+7]  Epoch 209/300:  69%|██████▉   | 208/300 [00:59&lt;00:27,  3.38it/s, v_num=1, train_loss_step=1.9e+7, train_loss_epoch=2.1e+7]Epoch 209/300:  70%|██████▉   | 209/300 [00:59&lt;00:27,  3.32it/s, v_num=1, train_loss_step=1.9e+7, train_loss_epoch=2.1e+7]Epoch 209/300:  70%|██████▉   | 209/300 [00:59&lt;00:27,  3.32it/s, v_num=1, train_loss_step=2.05e+7, train_loss_epoch=2.08e+7]Epoch 210/300:  70%|██████▉   | 209/300 [00:59&lt;00:27,  3.32it/s, v_num=1, train_loss_step=2.05e+7, train_loss_epoch=2.08e+7]Epoch 210/300:  70%|███████   | 210/300 [01:00&lt;00:27,  3.31it/s, v_num=1, train_loss_step=2.05e+7, train_loss_epoch=2.08e+7]Epoch 210/300:  70%|███████   | 210/300 [01:00&lt;00:27,  3.31it/s, v_num=1, train_loss_step=2.02e+7, train_loss_epoch=2.06e+7]Epoch 211/300:  70%|███████   | 210/300 [01:00&lt;00:27,  3.31it/s, v_num=1, train_loss_step=2.02e+7, train_loss_epoch=2.06e+7]Epoch 211/300:  70%|███████   | 211/300 [01:00&lt;00:27,  3.29it/s, v_num=1, train_loss_step=2.02e+7, train_loss_epoch=2.06e+7]Epoch 211/300:  70%|███████   | 211/300 [01:00&lt;00:27,  3.29it/s, v_num=1, train_loss_step=2.16e+7, train_loss_epoch=2.05e+7]Epoch 212/300:  70%|███████   | 211/300 [01:00&lt;00:27,  3.29it/s, v_num=1, train_loss_step=2.16e+7, train_loss_epoch=2.05e+7]Epoch 212/300:  71%|███████   | 212/300 [01:00&lt;00:27,  3.23it/s, v_num=1, train_loss_step=2.16e+7, train_loss_epoch=2.05e+7]Epoch 212/300:  71%|███████   | 212/300 [01:00&lt;00:27,  3.23it/s, v_num=1, train_loss_step=1.93e+7, train_loss_epoch=2.03e+7]Epoch 213/300:  71%|███████   | 212/300 [01:00&lt;00:27,  3.23it/s, v_num=1, train_loss_step=1.93e+7, train_loss_epoch=2.03e+7]Epoch 213/300:  71%|███████   | 213/300 [01:01&lt;00:26,  3.28it/s, v_num=1, train_loss_step=1.93e+7, train_loss_epoch=2.03e+7]Epoch 213/300:  71%|███████   | 213/300 [01:01&lt;00:26,  3.28it/s, v_num=1, train_loss_step=1.89e+7, train_loss_epoch=2.02e+7]Epoch 214/300:  71%|███████   | 213/300 [01:01&lt;00:26,  3.28it/s, v_num=1, train_loss_step=1.89e+7, train_loss_epoch=2.02e+7]Epoch 214/300:  71%|███████▏  | 214/300 [01:01&lt;00:25,  3.32it/s, v_num=1, train_loss_step=1.89e+7, train_loss_epoch=2.02e+7]Epoch 214/300:  71%|███████▏  | 214/300 [01:01&lt;00:25,  3.32it/s, v_num=1, train_loss_step=1.98e+7, train_loss_epoch=2e+7]   Epoch 215/300:  71%|███████▏  | 214/300 [01:01&lt;00:25,  3.32it/s, v_num=1, train_loss_step=1.98e+7, train_loss_epoch=2e+7]Epoch 215/300:  72%|███████▏  | 215/300 [01:01&lt;00:25,  3.38it/s, v_num=1, train_loss_step=1.98e+7, train_loss_epoch=2e+7]Epoch 215/300:  72%|███████▏  | 215/300 [01:01&lt;00:25,  3.38it/s, v_num=1, train_loss_step=1.78e+7, train_loss_epoch=1.99e+7]Epoch 216/300:  72%|███████▏  | 215/300 [01:01&lt;00:25,  3.38it/s, v_num=1, train_loss_step=1.78e+7, train_loss_epoch=1.99e+7]Epoch 216/300:  72%|███████▏  | 216/300 [01:01&lt;00:24,  3.40it/s, v_num=1, train_loss_step=1.78e+7, train_loss_epoch=1.99e+7]Epoch 216/300:  72%|███████▏  | 216/300 [01:01&lt;00:24,  3.40it/s, v_num=1, train_loss_step=1.82e+7, train_loss_epoch=1.98e+7]Epoch 217/300:  72%|███████▏  | 216/300 [01:01&lt;00:24,  3.40it/s, v_num=1, train_loss_step=1.82e+7, train_loss_epoch=1.98e+7]Epoch 217/300:  72%|███████▏  | 217/300 [01:02&lt;00:24,  3.40it/s, v_num=1, train_loss_step=1.82e+7, train_loss_epoch=1.98e+7]Epoch 217/300:  72%|███████▏  | 217/300 [01:02&lt;00:24,  3.40it/s, v_num=1, train_loss_step=2.02e+7, train_loss_epoch=1.96e+7]Epoch 218/300:  72%|███████▏  | 217/300 [01:02&lt;00:24,  3.40it/s, v_num=1, train_loss_step=2.02e+7, train_loss_epoch=1.96e+7]Epoch 218/300:  73%|███████▎  | 218/300 [01:02&lt;00:23,  3.43it/s, v_num=1, train_loss_step=2.02e+7, train_loss_epoch=1.96e+7]Epoch 218/300:  73%|███████▎  | 218/300 [01:02&lt;00:23,  3.43it/s, v_num=1, train_loss_step=1.78e+7, train_loss_epoch=1.95e+7]Epoch 219/300:  73%|███████▎  | 218/300 [01:02&lt;00:23,  3.43it/s, v_num=1, train_loss_step=1.78e+7, train_loss_epoch=1.95e+7]Epoch 219/300:  73%|███████▎  | 219/300 [01:02&lt;00:23,  3.47it/s, v_num=1, train_loss_step=1.78e+7, train_loss_epoch=1.95e+7]Epoch 219/300:  73%|███████▎  | 219/300 [01:02&lt;00:23,  3.47it/s, v_num=1, train_loss_step=1.86e+7, train_loss_epoch=1.93e+7]Epoch 220/300:  73%|███████▎  | 219/300 [01:02&lt;00:23,  3.47it/s, v_num=1, train_loss_step=1.86e+7, train_loss_epoch=1.93e+7]Epoch 220/300:  73%|███████▎  | 220/300 [01:03&lt;00:23,  3.40it/s, v_num=1, train_loss_step=1.86e+7, train_loss_epoch=1.93e+7]Epoch 220/300:  73%|███████▎  | 220/300 [01:03&lt;00:23,  3.40it/s, v_num=1, train_loss_step=1.72e+7, train_loss_epoch=1.92e+7]Epoch 221/300:  73%|███████▎  | 220/300 [01:03&lt;00:23,  3.40it/s, v_num=1, train_loss_step=1.72e+7, train_loss_epoch=1.92e+7]Epoch 221/300:  74%|███████▎  | 221/300 [01:03&lt;00:22,  3.44it/s, v_num=1, train_loss_step=1.72e+7, train_loss_epoch=1.92e+7]Epoch 221/300:  74%|███████▎  | 221/300 [01:03&lt;00:22,  3.44it/s, v_num=1, train_loss_step=1.77e+7, train_loss_epoch=1.91e+7]Epoch 222/300:  74%|███████▎  | 221/300 [01:03&lt;00:22,  3.44it/s, v_num=1, train_loss_step=1.77e+7, train_loss_epoch=1.91e+7]Epoch 222/300:  74%|███████▍  | 222/300 [01:03&lt;00:23,  3.38it/s, v_num=1, train_loss_step=1.77e+7, train_loss_epoch=1.91e+7]Epoch 222/300:  74%|███████▍  | 222/300 [01:03&lt;00:23,  3.38it/s, v_num=1, train_loss_step=1.68e+7, train_loss_epoch=1.9e+7] Epoch 223/300:  74%|███████▍  | 222/300 [01:03&lt;00:23,  3.38it/s, v_num=1, train_loss_step=1.68e+7, train_loss_epoch=1.9e+7]Epoch 223/300:  74%|███████▍  | 223/300 [01:03&lt;00:22,  3.43it/s, v_num=1, train_loss_step=1.68e+7, train_loss_epoch=1.9e+7]Epoch 223/300:  74%|███████▍  | 223/300 [01:03&lt;00:22,  3.43it/s, v_num=1, train_loss_step=1.88e+7, train_loss_epoch=1.88e+7]Epoch 224/300:  74%|███████▍  | 223/300 [01:03&lt;00:22,  3.43it/s, v_num=1, train_loss_step=1.88e+7, train_loss_epoch=1.88e+7]Epoch 224/300:  75%|███████▍  | 224/300 [01:04&lt;00:21,  3.46it/s, v_num=1, train_loss_step=1.88e+7, train_loss_epoch=1.88e+7]Epoch 224/300:  75%|███████▍  | 224/300 [01:04&lt;00:21,  3.46it/s, v_num=1, train_loss_step=1.68e+7, train_loss_epoch=1.87e+7]Epoch 225/300:  75%|███████▍  | 224/300 [01:04&lt;00:21,  3.46it/s, v_num=1, train_loss_step=1.68e+7, train_loss_epoch=1.87e+7]Epoch 225/300:  75%|███████▌  | 225/300 [01:04&lt;00:21,  3.48it/s, v_num=1, train_loss_step=1.68e+7, train_loss_epoch=1.87e+7]Epoch 225/300:  75%|███████▌  | 225/300 [01:04&lt;00:21,  3.48it/s, v_num=1, train_loss_step=1.71e+7, train_loss_epoch=1.86e+7]Epoch 226/300:  75%|███████▌  | 225/300 [01:04&lt;00:21,  3.48it/s, v_num=1, train_loss_step=1.71e+7, train_loss_epoch=1.86e+7]Epoch 226/300:  75%|███████▌  | 226/300 [01:04&lt;00:21,  3.46it/s, v_num=1, train_loss_step=1.71e+7, train_loss_epoch=1.86e+7]Epoch 226/300:  75%|███████▌  | 226/300 [01:04&lt;00:21,  3.46it/s, v_num=1, train_loss_step=1.69e+7, train_loss_epoch=1.85e+7]Epoch 227/300:  75%|███████▌  | 226/300 [01:04&lt;00:21,  3.46it/s, v_num=1, train_loss_step=1.69e+7, train_loss_epoch=1.85e+7]Epoch 227/300:  76%|███████▌  | 227/300 [01:05&lt;00:20,  3.48it/s, v_num=1, train_loss_step=1.69e+7, train_loss_epoch=1.85e+7]Epoch 227/300:  76%|███████▌  | 227/300 [01:05&lt;00:20,  3.48it/s, v_num=1, train_loss_step=1.9e+7, train_loss_epoch=1.83e+7] Epoch 228/300:  76%|███████▌  | 227/300 [01:05&lt;00:20,  3.48it/s, v_num=1, train_loss_step=1.9e+7, train_loss_epoch=1.83e+7]Epoch 228/300:  76%|███████▌  | 228/300 [01:05&lt;00:20,  3.48it/s, v_num=1, train_loss_step=1.9e+7, train_loss_epoch=1.83e+7]Epoch 228/300:  76%|███████▌  | 228/300 [01:05&lt;00:20,  3.48it/s, v_num=1, train_loss_step=1.71e+7, train_loss_epoch=1.82e+7]Epoch 229/300:  76%|███████▌  | 228/300 [01:05&lt;00:20,  3.48it/s, v_num=1, train_loss_step=1.71e+7, train_loss_epoch=1.82e+7]Epoch 229/300:  76%|███████▋  | 229/300 [01:05&lt;00:21,  3.37it/s, v_num=1, train_loss_step=1.71e+7, train_loss_epoch=1.82e+7]Epoch 229/300:  76%|███████▋  | 229/300 [01:05&lt;00:21,  3.37it/s, v_num=1, train_loss_step=1.72e+7, train_loss_epoch=1.81e+7]Epoch 230/300:  76%|███████▋  | 229/300 [01:05&lt;00:21,  3.37it/s, v_num=1, train_loss_step=1.72e+7, train_loss_epoch=1.81e+7]Epoch 230/300:  77%|███████▋  | 230/300 [01:06&lt;00:21,  3.33it/s, v_num=1, train_loss_step=1.72e+7, train_loss_epoch=1.81e+7]Epoch 230/300:  77%|███████▋  | 230/300 [01:06&lt;00:21,  3.33it/s, v_num=1, train_loss_step=1.73e+7, train_loss_epoch=1.8e+7] Epoch 231/300:  77%|███████▋  | 230/300 [01:06&lt;00:21,  3.33it/s, v_num=1, train_loss_step=1.73e+7, train_loss_epoch=1.8e+7]Epoch 231/300:  77%|███████▋  | 231/300 [01:06&lt;00:20,  3.37it/s, v_num=1, train_loss_step=1.73e+7, train_loss_epoch=1.8e+7]Epoch 231/300:  77%|███████▋  | 231/300 [01:06&lt;00:20,  3.37it/s, v_num=1, train_loss_step=1.68e+7, train_loss_epoch=1.79e+7]Epoch 232/300:  77%|███████▋  | 231/300 [01:06&lt;00:20,  3.37it/s, v_num=1, train_loss_step=1.68e+7, train_loss_epoch=1.79e+7]Epoch 232/300:  77%|███████▋  | 232/300 [01:06&lt;00:19,  3.45it/s, v_num=1, train_loss_step=1.68e+7, train_loss_epoch=1.79e+7]Epoch 232/300:  77%|███████▋  | 232/300 [01:06&lt;00:19,  3.45it/s, v_num=1, train_loss_step=1.72e+7, train_loss_epoch=1.78e+7]Epoch 233/300:  77%|███████▋  | 232/300 [01:06&lt;00:19,  3.45it/s, v_num=1, train_loss_step=1.72e+7, train_loss_epoch=1.78e+7]Epoch 233/300:  78%|███████▊  | 233/300 [01:06&lt;00:19,  3.49it/s, v_num=1, train_loss_step=1.72e+7, train_loss_epoch=1.78e+7]Epoch 233/300:  78%|███████▊  | 233/300 [01:06&lt;00:19,  3.49it/s, v_num=1, train_loss_step=1.9e+7, train_loss_epoch=1.77e+7] Epoch 234/300:  78%|███████▊  | 233/300 [01:06&lt;00:19,  3.49it/s, v_num=1, train_loss_step=1.9e+7, train_loss_epoch=1.77e+7]Epoch 234/300:  78%|███████▊  | 234/300 [01:07&lt;00:18,  3.49it/s, v_num=1, train_loss_step=1.9e+7, train_loss_epoch=1.77e+7]Epoch 234/300:  78%|███████▊  | 234/300 [01:07&lt;00:18,  3.49it/s, v_num=1, train_loss_step=1.55e+7, train_loss_epoch=1.76e+7]Epoch 235/300:  78%|███████▊  | 234/300 [01:07&lt;00:18,  3.49it/s, v_num=1, train_loss_step=1.55e+7, train_loss_epoch=1.76e+7]Epoch 235/300:  78%|███████▊  | 235/300 [01:07&lt;00:18,  3.46it/s, v_num=1, train_loss_step=1.55e+7, train_loss_epoch=1.76e+7]Epoch 235/300:  78%|███████▊  | 235/300 [01:07&lt;00:18,  3.46it/s, v_num=1, train_loss_step=1.64e+7, train_loss_epoch=1.75e+7]Epoch 236/300:  78%|███████▊  | 235/300 [01:07&lt;00:18,  3.46it/s, v_num=1, train_loss_step=1.64e+7, train_loss_epoch=1.75e+7]Epoch 236/300:  79%|███████▊  | 236/300 [01:07&lt;00:18,  3.49it/s, v_num=1, train_loss_step=1.64e+7, train_loss_epoch=1.75e+7]Epoch 236/300:  79%|███████▊  | 236/300 [01:07&lt;00:18,  3.49it/s, v_num=1, train_loss_step=1.62e+7, train_loss_epoch=1.74e+7]Epoch 237/300:  79%|███████▊  | 236/300 [01:07&lt;00:18,  3.49it/s, v_num=1, train_loss_step=1.62e+7, train_loss_epoch=1.74e+7]Epoch 237/300:  79%|███████▉  | 237/300 [01:08&lt;00:18,  3.48it/s, v_num=1, train_loss_step=1.62e+7, train_loss_epoch=1.74e+7]Epoch 237/300:  79%|███████▉  | 237/300 [01:08&lt;00:18,  3.48it/s, v_num=1, train_loss_step=1.52e+7, train_loss_epoch=1.73e+7]Epoch 238/300:  79%|███████▉  | 237/300 [01:08&lt;00:18,  3.48it/s, v_num=1, train_loss_step=1.52e+7, train_loss_epoch=1.73e+7]Epoch 238/300:  79%|███████▉  | 238/300 [01:08&lt;00:17,  3.52it/s, v_num=1, train_loss_step=1.52e+7, train_loss_epoch=1.73e+7]Epoch 238/300:  79%|███████▉  | 238/300 [01:08&lt;00:17,  3.52it/s, v_num=1, train_loss_step=1.76e+7, train_loss_epoch=1.72e+7]Epoch 239/300:  79%|███████▉  | 238/300 [01:08&lt;00:17,  3.52it/s, v_num=1, train_loss_step=1.76e+7, train_loss_epoch=1.72e+7]Epoch 239/300:  80%|███████▉  | 239/300 [01:08&lt;00:17,  3.49it/s, v_num=1, train_loss_step=1.76e+7, train_loss_epoch=1.72e+7]Epoch 239/300:  80%|███████▉  | 239/300 [01:08&lt;00:17,  3.49it/s, v_num=1, train_loss_step=1.58e+7, train_loss_epoch=1.71e+7]Epoch 240/300:  80%|███████▉  | 239/300 [01:08&lt;00:17,  3.49it/s, v_num=1, train_loss_step=1.58e+7, train_loss_epoch=1.71e+7]Epoch 240/300:  80%|████████  | 240/300 [01:08&lt;00:17,  3.45it/s, v_num=1, train_loss_step=1.58e+7, train_loss_epoch=1.71e+7]Epoch 240/300:  80%|████████  | 240/300 [01:08&lt;00:17,  3.45it/s, v_num=1, train_loss_step=1.67e+7, train_loss_epoch=1.7e+7] Epoch 241/300:  80%|████████  | 240/300 [01:08&lt;00:17,  3.45it/s, v_num=1, train_loss_step=1.67e+7, train_loss_epoch=1.7e+7]Epoch 241/300:  80%|████████  | 241/300 [01:09&lt;00:17,  3.42it/s, v_num=1, train_loss_step=1.67e+7, train_loss_epoch=1.7e+7]Epoch 241/300:  80%|████████  | 241/300 [01:09&lt;00:17,  3.42it/s, v_num=1, train_loss_step=1.59e+7, train_loss_epoch=1.69e+7]Epoch 242/300:  80%|████████  | 241/300 [01:09&lt;00:17,  3.42it/s, v_num=1, train_loss_step=1.59e+7, train_loss_epoch=1.69e+7]Epoch 242/300:  81%|████████  | 242/300 [01:09&lt;00:16,  3.49it/s, v_num=1, train_loss_step=1.59e+7, train_loss_epoch=1.69e+7]Epoch 242/300:  81%|████████  | 242/300 [01:09&lt;00:16,  3.49it/s, v_num=1, train_loss_step=1.56e+7, train_loss_epoch=1.68e+7]Epoch 243/300:  81%|████████  | 242/300 [01:09&lt;00:16,  3.49it/s, v_num=1, train_loss_step=1.56e+7, train_loss_epoch=1.68e+7]Epoch 243/300:  81%|████████  | 243/300 [01:09&lt;00:16,  3.52it/s, v_num=1, train_loss_step=1.56e+7, train_loss_epoch=1.68e+7]Epoch 243/300:  81%|████████  | 243/300 [01:09&lt;00:16,  3.52it/s, v_num=1, train_loss_step=1.61e+7, train_loss_epoch=1.67e+7]Epoch 244/300:  81%|████████  | 243/300 [01:09&lt;00:16,  3.52it/s, v_num=1, train_loss_step=1.61e+7, train_loss_epoch=1.67e+7]Epoch 244/300:  81%|████████▏ | 244/300 [01:10&lt;00:16,  3.43it/s, v_num=1, train_loss_step=1.61e+7, train_loss_epoch=1.67e+7]Epoch 244/300:  81%|████████▏ | 244/300 [01:10&lt;00:16,  3.43it/s, v_num=1, train_loss_step=1.52e+7, train_loss_epoch=1.66e+7]Epoch 245/300:  81%|████████▏ | 244/300 [01:10&lt;00:16,  3.43it/s, v_num=1, train_loss_step=1.52e+7, train_loss_epoch=1.66e+7]Epoch 245/300:  82%|████████▏ | 245/300 [01:10&lt;00:15,  3.47it/s, v_num=1, train_loss_step=1.52e+7, train_loss_epoch=1.66e+7]Epoch 245/300:  82%|████████▏ | 245/300 [01:10&lt;00:15,  3.47it/s, v_num=1, train_loss_step=1.57e+7, train_loss_epoch=1.66e+7]Epoch 246/300:  82%|████████▏ | 245/300 [01:10&lt;00:15,  3.47it/s, v_num=1, train_loss_step=1.57e+7, train_loss_epoch=1.66e+7]Epoch 246/300:  82%|████████▏ | 246/300 [01:10&lt;00:15,  3.51it/s, v_num=1, train_loss_step=1.57e+7, train_loss_epoch=1.66e+7]Epoch 246/300:  82%|████████▏ | 246/300 [01:10&lt;00:15,  3.51it/s, v_num=1, train_loss_step=1.57e+7, train_loss_epoch=1.65e+7]Epoch 247/300:  82%|████████▏ | 246/300 [01:10&lt;00:15,  3.51it/s, v_num=1, train_loss_step=1.57e+7, train_loss_epoch=1.65e+7]Epoch 247/300:  82%|████████▏ | 247/300 [01:10&lt;00:15,  3.42it/s, v_num=1, train_loss_step=1.57e+7, train_loss_epoch=1.65e+7]Epoch 247/300:  82%|████████▏ | 247/300 [01:10&lt;00:15,  3.42it/s, v_num=1, train_loss_step=1.45e+7, train_loss_epoch=1.64e+7]Epoch 248/300:  82%|████████▏ | 247/300 [01:10&lt;00:15,  3.42it/s, v_num=1, train_loss_step=1.45e+7, train_loss_epoch=1.64e+7]Epoch 248/300:  83%|████████▎ | 248/300 [01:11&lt;00:15,  3.38it/s, v_num=1, train_loss_step=1.45e+7, train_loss_epoch=1.64e+7]Epoch 248/300:  83%|████████▎ | 248/300 [01:11&lt;00:15,  3.38it/s, v_num=1, train_loss_step=1.56e+7, train_loss_epoch=1.63e+7]Epoch 249/300:  83%|████████▎ | 248/300 [01:11&lt;00:15,  3.38it/s, v_num=1, train_loss_step=1.56e+7, train_loss_epoch=1.63e+7]Epoch 249/300:  83%|████████▎ | 249/300 [01:11&lt;00:14,  3.41it/s, v_num=1, train_loss_step=1.56e+7, train_loss_epoch=1.63e+7]Epoch 249/300:  83%|████████▎ | 249/300 [01:11&lt;00:14,  3.41it/s, v_num=1, train_loss_step=1.51e+7, train_loss_epoch=1.62e+7]Epoch 250/300:  83%|████████▎ | 249/300 [01:11&lt;00:14,  3.41it/s, v_num=1, train_loss_step=1.51e+7, train_loss_epoch=1.62e+7]Epoch 250/300:  83%|████████▎ | 250/300 [01:11&lt;00:14,  3.44it/s, v_num=1, train_loss_step=1.51e+7, train_loss_epoch=1.62e+7]Epoch 250/300:  83%|████████▎ | 250/300 [01:11&lt;00:14,  3.44it/s, v_num=1, train_loss_step=1.62e+7, train_loss_epoch=1.61e+7]Epoch 251/300:  83%|████████▎ | 250/300 [01:11&lt;00:14,  3.44it/s, v_num=1, train_loss_step=1.62e+7, train_loss_epoch=1.61e+7]Epoch 251/300:  84%|████████▎ | 251/300 [01:12&lt;00:14,  3.43it/s, v_num=1, train_loss_step=1.62e+7, train_loss_epoch=1.61e+7]Epoch 251/300:  84%|████████▎ | 251/300 [01:12&lt;00:14,  3.43it/s, v_num=1, train_loss_step=1.54e+7, train_loss_epoch=1.61e+7]Epoch 252/300:  84%|████████▎ | 251/300 [01:12&lt;00:14,  3.43it/s, v_num=1, train_loss_step=1.54e+7, train_loss_epoch=1.61e+7]Epoch 252/300:  84%|████████▍ | 252/300 [01:12&lt;00:13,  3.46it/s, v_num=1, train_loss_step=1.54e+7, train_loss_epoch=1.61e+7]Epoch 252/300:  84%|████████▍ | 252/300 [01:12&lt;00:13,  3.46it/s, v_num=1, train_loss_step=1.54e+7, train_loss_epoch=1.6e+7] Epoch 253/300:  84%|████████▍ | 252/300 [01:12&lt;00:13,  3.46it/s, v_num=1, train_loss_step=1.54e+7, train_loss_epoch=1.6e+7]Epoch 253/300:  84%|████████▍ | 253/300 [01:12&lt;00:13,  3.48it/s, v_num=1, train_loss_step=1.54e+7, train_loss_epoch=1.6e+7]Epoch 253/300:  84%|████████▍ | 253/300 [01:12&lt;00:13,  3.48it/s, v_num=1, train_loss_step=1.53e+7, train_loss_epoch=1.59e+7]Epoch 254/300:  84%|████████▍ | 253/300 [01:12&lt;00:13,  3.48it/s, v_num=1, train_loss_step=1.53e+7, train_loss_epoch=1.59e+7]Epoch 254/300:  85%|████████▍ | 254/300 [01:12&lt;00:13,  3.47it/s, v_num=1, train_loss_step=1.53e+7, train_loss_epoch=1.59e+7]Epoch 254/300:  85%|████████▍ | 254/300 [01:12&lt;00:13,  3.47it/s, v_num=1, train_loss_step=1.55e+7, train_loss_epoch=1.58e+7]Epoch 255/300:  85%|████████▍ | 254/300 [01:12&lt;00:13,  3.47it/s, v_num=1, train_loss_step=1.55e+7, train_loss_epoch=1.58e+7]Epoch 255/300:  85%|████████▌ | 255/300 [01:13&lt;00:12,  3.46it/s, v_num=1, train_loss_step=1.55e+7, train_loss_epoch=1.58e+7]Epoch 255/300:  85%|████████▌ | 255/300 [01:13&lt;00:12,  3.46it/s, v_num=1, train_loss_step=1.56e+7, train_loss_epoch=1.58e+7]Epoch 256/300:  85%|████████▌ | 255/300 [01:13&lt;00:12,  3.46it/s, v_num=1, train_loss_step=1.56e+7, train_loss_epoch=1.58e+7]Epoch 256/300:  85%|████████▌ | 256/300 [01:13&lt;00:12,  3.48it/s, v_num=1, train_loss_step=1.56e+7, train_loss_epoch=1.58e+7]Epoch 256/300:  85%|████████▌ | 256/300 [01:13&lt;00:12,  3.48it/s, v_num=1, train_loss_step=1.56e+7, train_loss_epoch=1.57e+7]Epoch 257/300:  85%|████████▌ | 256/300 [01:13&lt;00:12,  3.48it/s, v_num=1, train_loss_step=1.56e+7, train_loss_epoch=1.57e+7]Epoch 257/300:  86%|████████▌ | 257/300 [01:13&lt;00:12,  3.52it/s, v_num=1, train_loss_step=1.56e+7, train_loss_epoch=1.57e+7]Epoch 257/300:  86%|████████▌ | 257/300 [01:13&lt;00:12,  3.52it/s, v_num=1, train_loss_step=1.42e+7, train_loss_epoch=1.56e+7]Epoch 258/300:  86%|████████▌ | 257/300 [01:13&lt;00:12,  3.52it/s, v_num=1, train_loss_step=1.42e+7, train_loss_epoch=1.56e+7]Epoch 258/300:  86%|████████▌ | 258/300 [01:14&lt;00:11,  3.51it/s, v_num=1, train_loss_step=1.42e+7, train_loss_epoch=1.56e+7]Epoch 258/300:  86%|████████▌ | 258/300 [01:14&lt;00:11,  3.51it/s, v_num=1, train_loss_step=1.43e+7, train_loss_epoch=1.56e+7]Epoch 259/300:  86%|████████▌ | 258/300 [01:14&lt;00:11,  3.51it/s, v_num=1, train_loss_step=1.43e+7, train_loss_epoch=1.56e+7]Epoch 259/300:  86%|████████▋ | 259/300 [01:14&lt;00:11,  3.50it/s, v_num=1, train_loss_step=1.43e+7, train_loss_epoch=1.56e+7]Epoch 259/300:  86%|████████▋ | 259/300 [01:14&lt;00:11,  3.50it/s, v_num=1, train_loss_step=1.42e+7, train_loss_epoch=1.55e+7]Epoch 260/300:  86%|████████▋ | 259/300 [01:14&lt;00:11,  3.50it/s, v_num=1, train_loss_step=1.42e+7, train_loss_epoch=1.55e+7]Epoch 260/300:  87%|████████▋ | 260/300 [01:14&lt;00:11,  3.42it/s, v_num=1, train_loss_step=1.42e+7, train_loss_epoch=1.55e+7]Epoch 260/300:  87%|████████▋ | 260/300 [01:14&lt;00:11,  3.42it/s, v_num=1, train_loss_step=1.35e+7, train_loss_epoch=1.54e+7]Epoch 261/300:  87%|████████▋ | 260/300 [01:14&lt;00:11,  3.42it/s, v_num=1, train_loss_step=1.35e+7, train_loss_epoch=1.54e+7]Epoch 261/300:  87%|████████▋ | 261/300 [01:14&lt;00:11,  3.46it/s, v_num=1, train_loss_step=1.35e+7, train_loss_epoch=1.54e+7]Epoch 261/300:  87%|████████▋ | 261/300 [01:14&lt;00:11,  3.46it/s, v_num=1, train_loss_step=1.47e+7, train_loss_epoch=1.54e+7]Epoch 262/300:  87%|████████▋ | 261/300 [01:14&lt;00:11,  3.46it/s, v_num=1, train_loss_step=1.47e+7, train_loss_epoch=1.54e+7]Epoch 262/300:  87%|████████▋ | 262/300 [01:15&lt;00:10,  3.48it/s, v_num=1, train_loss_step=1.47e+7, train_loss_epoch=1.54e+7]Epoch 262/300:  87%|████████▋ | 262/300 [01:15&lt;00:10,  3.48it/s, v_num=1, train_loss_step=1.28e+7, train_loss_epoch=1.53e+7]Epoch 263/300:  87%|████████▋ | 262/300 [01:15&lt;00:10,  3.48it/s, v_num=1, train_loss_step=1.28e+7, train_loss_epoch=1.53e+7]Epoch 263/300:  88%|████████▊ | 263/300 [01:15&lt;00:10,  3.45it/s, v_num=1, train_loss_step=1.28e+7, train_loss_epoch=1.53e+7]Epoch 263/300:  88%|████████▊ | 263/300 [01:15&lt;00:10,  3.45it/s, v_num=1, train_loss_step=1.38e+7, train_loss_epoch=1.52e+7]Epoch 264/300:  88%|████████▊ | 263/300 [01:15&lt;00:10,  3.45it/s, v_num=1, train_loss_step=1.38e+7, train_loss_epoch=1.52e+7]Epoch 264/300:  88%|████████▊ | 264/300 [01:15&lt;00:10,  3.47it/s, v_num=1, train_loss_step=1.38e+7, train_loss_epoch=1.52e+7]Epoch 264/300:  88%|████████▊ | 264/300 [01:15&lt;00:10,  3.47it/s, v_num=1, train_loss_step=1.33e+7, train_loss_epoch=1.52e+7]Epoch 265/300:  88%|████████▊ | 264/300 [01:15&lt;00:10,  3.47it/s, v_num=1, train_loss_step=1.33e+7, train_loss_epoch=1.52e+7]Epoch 265/300:  88%|████████▊ | 265/300 [01:16&lt;00:10,  3.39it/s, v_num=1, train_loss_step=1.33e+7, train_loss_epoch=1.52e+7]Epoch 265/300:  88%|████████▊ | 265/300 [01:16&lt;00:10,  3.39it/s, v_num=1, train_loss_step=1.43e+7, train_loss_epoch=1.51e+7]Epoch 266/300:  88%|████████▊ | 265/300 [01:16&lt;00:10,  3.39it/s, v_num=1, train_loss_step=1.43e+7, train_loss_epoch=1.51e+7]Epoch 266/300:  89%|████████▊ | 266/300 [01:16&lt;00:10,  3.36it/s, v_num=1, train_loss_step=1.43e+7, train_loss_epoch=1.51e+7]Epoch 266/300:  89%|████████▊ | 266/300 [01:16&lt;00:10,  3.36it/s, v_num=1, train_loss_step=1.5e+7, train_loss_epoch=1.5e+7]  Epoch 267/300:  89%|████████▊ | 266/300 [01:16&lt;00:10,  3.36it/s, v_num=1, train_loss_step=1.5e+7, train_loss_epoch=1.5e+7]Epoch 267/300:  89%|████████▉ | 267/300 [01:16&lt;00:09,  3.43it/s, v_num=1, train_loss_step=1.5e+7, train_loss_epoch=1.5e+7]Epoch 267/300:  89%|████████▉ | 267/300 [01:16&lt;00:09,  3.43it/s, v_num=1, train_loss_step=1.29e+7, train_loss_epoch=1.5e+7]Epoch 268/300:  89%|████████▉ | 267/300 [01:16&lt;00:09,  3.43it/s, v_num=1, train_loss_step=1.29e+7, train_loss_epoch=1.5e+7]Epoch 268/300:  89%|████████▉ | 268/300 [01:17&lt;00:09,  3.46it/s, v_num=1, train_loss_step=1.29e+7, train_loss_epoch=1.5e+7]Epoch 268/300:  89%|████████▉ | 268/300 [01:17&lt;00:09,  3.46it/s, v_num=1, train_loss_step=1.5e+7, train_loss_epoch=1.49e+7]Epoch 269/300:  89%|████████▉ | 268/300 [01:17&lt;00:09,  3.46it/s, v_num=1, train_loss_step=1.5e+7, train_loss_epoch=1.49e+7]Epoch 269/300:  90%|████████▉ | 269/300 [01:17&lt;00:08,  3.47it/s, v_num=1, train_loss_step=1.5e+7, train_loss_epoch=1.49e+7]Epoch 269/300:  90%|████████▉ | 269/300 [01:17&lt;00:08,  3.47it/s, v_num=1, train_loss_step=1.41e+7, train_loss_epoch=1.49e+7]Epoch 270/300:  90%|████████▉ | 269/300 [01:17&lt;00:08,  3.47it/s, v_num=1, train_loss_step=1.41e+7, train_loss_epoch=1.49e+7]Epoch 270/300:  90%|█████████ | 270/300 [01:17&lt;00:08,  3.50it/s, v_num=1, train_loss_step=1.41e+7, train_loss_epoch=1.49e+7]Epoch 270/300:  90%|█████████ | 270/300 [01:17&lt;00:08,  3.50it/s, v_num=1, train_loss_step=1.43e+7, train_loss_epoch=1.48e+7]Epoch 271/300:  90%|█████████ | 270/300 [01:17&lt;00:08,  3.50it/s, v_num=1, train_loss_step=1.43e+7, train_loss_epoch=1.48e+7]Epoch 271/300:  90%|█████████ | 271/300 [01:17&lt;00:08,  3.51it/s, v_num=1, train_loss_step=1.43e+7, train_loss_epoch=1.48e+7]Epoch 271/300:  90%|█████████ | 271/300 [01:17&lt;00:08,  3.51it/s, v_num=1, train_loss_step=1.45e+7, train_loss_epoch=1.47e+7]Epoch 272/300:  90%|█████████ | 271/300 [01:17&lt;00:08,  3.51it/s, v_num=1, train_loss_step=1.45e+7, train_loss_epoch=1.47e+7]Epoch 272/300:  91%|█████████ | 272/300 [01:18&lt;00:07,  3.51it/s, v_num=1, train_loss_step=1.45e+7, train_loss_epoch=1.47e+7]Epoch 272/300:  91%|█████████ | 272/300 [01:18&lt;00:07,  3.51it/s, v_num=1, train_loss_step=1.38e+7, train_loss_epoch=1.47e+7]Epoch 273/300:  91%|█████████ | 272/300 [01:18&lt;00:07,  3.51it/s, v_num=1, train_loss_step=1.38e+7, train_loss_epoch=1.47e+7]Epoch 273/300:  91%|█████████ | 273/300 [01:18&lt;00:07,  3.52it/s, v_num=1, train_loss_step=1.38e+7, train_loss_epoch=1.47e+7]Epoch 273/300:  91%|█████████ | 273/300 [01:18&lt;00:07,  3.52it/s, v_num=1, train_loss_step=1.35e+7, train_loss_epoch=1.46e+7]Epoch 274/300:  91%|█████████ | 273/300 [01:18&lt;00:07,  3.52it/s, v_num=1, train_loss_step=1.35e+7, train_loss_epoch=1.46e+7]Epoch 274/300:  91%|█████████▏| 274/300 [01:18&lt;00:07,  3.55it/s, v_num=1, train_loss_step=1.35e+7, train_loss_epoch=1.46e+7]Epoch 274/300:  91%|█████████▏| 274/300 [01:18&lt;00:07,  3.55it/s, v_num=1, train_loss_step=1.35e+7, train_loss_epoch=1.46e+7]Epoch 275/300:  91%|█████████▏| 274/300 [01:18&lt;00:07,  3.55it/s, v_num=1, train_loss_step=1.35e+7, train_loss_epoch=1.46e+7]Epoch 275/300:  92%|█████████▏| 275/300 [01:18&lt;00:07,  3.54it/s, v_num=1, train_loss_step=1.35e+7, train_loss_epoch=1.46e+7]Epoch 275/300:  92%|█████████▏| 275/300 [01:18&lt;00:07,  3.54it/s, v_num=1, train_loss_step=1.3e+7, train_loss_epoch=1.45e+7] Epoch 276/300:  92%|█████████▏| 275/300 [01:18&lt;00:07,  3.54it/s, v_num=1, train_loss_step=1.3e+7, train_loss_epoch=1.45e+7]Epoch 276/300:  92%|█████████▏| 276/300 [01:19&lt;00:06,  3.49it/s, v_num=1, train_loss_step=1.3e+7, train_loss_epoch=1.45e+7]Epoch 276/300:  92%|█████████▏| 276/300 [01:19&lt;00:06,  3.49it/s, v_num=1, train_loss_step=1.31e+7, train_loss_epoch=1.45e+7]Epoch 277/300:  92%|█████████▏| 276/300 [01:19&lt;00:06,  3.49it/s, v_num=1, train_loss_step=1.31e+7, train_loss_epoch=1.45e+7]Epoch 277/300:  92%|█████████▏| 277/300 [01:19&lt;00:06,  3.50it/s, v_num=1, train_loss_step=1.31e+7, train_loss_epoch=1.45e+7]Epoch 277/300:  92%|█████████▏| 277/300 [01:19&lt;00:06,  3.50it/s, v_num=1, train_loss_step=1.39e+7, train_loss_epoch=1.44e+7]Epoch 278/300:  92%|█████████▏| 277/300 [01:19&lt;00:06,  3.50it/s, v_num=1, train_loss_step=1.39e+7, train_loss_epoch=1.44e+7]Epoch 278/300:  93%|█████████▎| 278/300 [01:19&lt;00:06,  3.49it/s, v_num=1, train_loss_step=1.39e+7, train_loss_epoch=1.44e+7]Epoch 278/300:  93%|█████████▎| 278/300 [01:19&lt;00:06,  3.49it/s, v_num=1, train_loss_step=1.28e+7, train_loss_epoch=1.44e+7]Epoch 279/300:  93%|█████████▎| 278/300 [01:19&lt;00:06,  3.49it/s, v_num=1, train_loss_step=1.28e+7, train_loss_epoch=1.44e+7]Epoch 279/300:  93%|█████████▎| 279/300 [01:20&lt;00:06,  3.47it/s, v_num=1, train_loss_step=1.28e+7, train_loss_epoch=1.44e+7]Epoch 279/300:  93%|█████████▎| 279/300 [01:20&lt;00:06,  3.47it/s, v_num=1, train_loss_step=1.43e+7, train_loss_epoch=1.43e+7]Epoch 280/300:  93%|█████████▎| 279/300 [01:20&lt;00:06,  3.47it/s, v_num=1, train_loss_step=1.43e+7, train_loss_epoch=1.43e+7]Epoch 280/300:  93%|█████████▎| 280/300 [01:20&lt;00:05,  3.45it/s, v_num=1, train_loss_step=1.43e+7, train_loss_epoch=1.43e+7]Epoch 280/300:  93%|█████████▎| 280/300 [01:20&lt;00:05,  3.45it/s, v_num=1, train_loss_step=1.46e+7, train_loss_epoch=1.43e+7]Epoch 281/300:  93%|█████████▎| 280/300 [01:20&lt;00:05,  3.45it/s, v_num=1, train_loss_step=1.46e+7, train_loss_epoch=1.43e+7]Epoch 281/300:  94%|█████████▎| 281/300 [01:20&lt;00:05,  3.49it/s, v_num=1, train_loss_step=1.46e+7, train_loss_epoch=1.43e+7]Epoch 281/300:  94%|█████████▎| 281/300 [01:20&lt;00:05,  3.49it/s, v_num=1, train_loss_step=1.4e+7, train_loss_epoch=1.42e+7] Epoch 282/300:  94%|█████████▎| 281/300 [01:20&lt;00:05,  3.49it/s, v_num=1, train_loss_step=1.4e+7, train_loss_epoch=1.42e+7]Epoch 282/300:  94%|█████████▍| 282/300 [01:21&lt;00:05,  3.50it/s, v_num=1, train_loss_step=1.4e+7, train_loss_epoch=1.42e+7]Epoch 282/300:  94%|█████████▍| 282/300 [01:21&lt;00:05,  3.50it/s, v_num=1, train_loss_step=1.32e+7, train_loss_epoch=1.42e+7]Epoch 283/300:  94%|█████████▍| 282/300 [01:21&lt;00:05,  3.50it/s, v_num=1, train_loss_step=1.32e+7, train_loss_epoch=1.42e+7]Epoch 283/300:  94%|█████████▍| 283/300 [01:21&lt;00:05,  3.39it/s, v_num=1, train_loss_step=1.32e+7, train_loss_epoch=1.42e+7]Epoch 283/300:  94%|█████████▍| 283/300 [01:21&lt;00:05,  3.39it/s, v_num=1, train_loss_step=1.39e+7, train_loss_epoch=1.41e+7]Epoch 284/300:  94%|█████████▍| 283/300 [01:21&lt;00:05,  3.39it/s, v_num=1, train_loss_step=1.39e+7, train_loss_epoch=1.41e+7]Epoch 284/300:  95%|█████████▍| 284/300 [01:21&lt;00:04,  3.33it/s, v_num=1, train_loss_step=1.39e+7, train_loss_epoch=1.41e+7]Epoch 284/300:  95%|█████████▍| 284/300 [01:21&lt;00:04,  3.33it/s, v_num=1, train_loss_step=1.31e+7, train_loss_epoch=1.41e+7]Epoch 285/300:  95%|█████████▍| 284/300 [01:21&lt;00:04,  3.33it/s, v_num=1, train_loss_step=1.31e+7, train_loss_epoch=1.41e+7]Epoch 285/300:  95%|█████████▌| 285/300 [01:21&lt;00:04,  3.38it/s, v_num=1, train_loss_step=1.31e+7, train_loss_epoch=1.41e+7]Epoch 285/300:  95%|█████████▌| 285/300 [01:21&lt;00:04,  3.38it/s, v_num=1, train_loss_step=1.27e+7, train_loss_epoch=1.4e+7] Epoch 286/300:  95%|█████████▌| 285/300 [01:21&lt;00:04,  3.38it/s, v_num=1, train_loss_step=1.27e+7, train_loss_epoch=1.4e+7]Epoch 286/300:  95%|█████████▌| 286/300 [01:22&lt;00:04,  3.40it/s, v_num=1, train_loss_step=1.27e+7, train_loss_epoch=1.4e+7]Epoch 286/300:  95%|█████████▌| 286/300 [01:22&lt;00:04,  3.40it/s, v_num=1, train_loss_step=1.39e+7, train_loss_epoch=1.4e+7]Epoch 287/300:  95%|█████████▌| 286/300 [01:22&lt;00:04,  3.40it/s, v_num=1, train_loss_step=1.39e+7, train_loss_epoch=1.4e+7]Epoch 287/300:  96%|█████████▌| 287/300 [01:22&lt;00:03,  3.32it/s, v_num=1, train_loss_step=1.39e+7, train_loss_epoch=1.4e+7]Epoch 287/300:  96%|█████████▌| 287/300 [01:22&lt;00:03,  3.32it/s, v_num=1, train_loss_step=1.36e+7, train_loss_epoch=1.39e+7]Epoch 288/300:  96%|█████████▌| 287/300 [01:22&lt;00:03,  3.32it/s, v_num=1, train_loss_step=1.36e+7, train_loss_epoch=1.39e+7]Epoch 288/300:  96%|█████████▌| 288/300 [01:22&lt;00:03,  3.32it/s, v_num=1, train_loss_step=1.36e+7, train_loss_epoch=1.39e+7]Epoch 288/300:  96%|█████████▌| 288/300 [01:22&lt;00:03,  3.32it/s, v_num=1, train_loss_step=1.22e+7, train_loss_epoch=1.39e+7]Epoch 289/300:  96%|█████████▌| 288/300 [01:22&lt;00:03,  3.32it/s, v_num=1, train_loss_step=1.22e+7, train_loss_epoch=1.39e+7]Epoch 289/300:  96%|█████████▋| 289/300 [01:23&lt;00:03,  3.34it/s, v_num=1, train_loss_step=1.22e+7, train_loss_epoch=1.39e+7]Epoch 289/300:  96%|█████████▋| 289/300 [01:23&lt;00:03,  3.34it/s, v_num=1, train_loss_step=1.2e+7, train_loss_epoch=1.38e+7] Epoch 290/300:  96%|█████████▋| 289/300 [01:23&lt;00:03,  3.34it/s, v_num=1, train_loss_step=1.2e+7, train_loss_epoch=1.38e+7]Epoch 290/300:  97%|█████████▋| 290/300 [01:23&lt;00:02,  3.38it/s, v_num=1, train_loss_step=1.2e+7, train_loss_epoch=1.38e+7]Epoch 290/300:  97%|█████████▋| 290/300 [01:23&lt;00:02,  3.38it/s, v_num=1, train_loss_step=1.32e+7, train_loss_epoch=1.38e+7]Epoch 291/300:  97%|█████████▋| 290/300 [01:23&lt;00:02,  3.38it/s, v_num=1, train_loss_step=1.32e+7, train_loss_epoch=1.38e+7]Epoch 291/300:  97%|█████████▋| 291/300 [01:23&lt;00:02,  3.41it/s, v_num=1, train_loss_step=1.32e+7, train_loss_epoch=1.38e+7]Epoch 291/300:  97%|█████████▋| 291/300 [01:23&lt;00:02,  3.41it/s, v_num=1, train_loss_step=1.21e+7, train_loss_epoch=1.37e+7]Epoch 292/300:  97%|█████████▋| 291/300 [01:23&lt;00:02,  3.41it/s, v_num=1, train_loss_step=1.21e+7, train_loss_epoch=1.37e+7]Epoch 292/300:  97%|█████████▋| 292/300 [01:23&lt;00:02,  3.43it/s, v_num=1, train_loss_step=1.21e+7, train_loss_epoch=1.37e+7]Epoch 292/300:  97%|█████████▋| 292/300 [01:23&lt;00:02,  3.43it/s, v_num=1, train_loss_step=1.25e+7, train_loss_epoch=1.37e+7]Epoch 293/300:  97%|█████████▋| 292/300 [01:23&lt;00:02,  3.43it/s, v_num=1, train_loss_step=1.25e+7, train_loss_epoch=1.37e+7]Epoch 293/300:  98%|█████████▊| 293/300 [01:24&lt;00:02,  3.42it/s, v_num=1, train_loss_step=1.25e+7, train_loss_epoch=1.37e+7]Epoch 293/300:  98%|█████████▊| 293/300 [01:24&lt;00:02,  3.42it/s, v_num=1, train_loss_step=1.38e+7, train_loss_epoch=1.37e+7]Epoch 294/300:  98%|█████████▊| 293/300 [01:24&lt;00:02,  3.42it/s, v_num=1, train_loss_step=1.38e+7, train_loss_epoch=1.37e+7]Epoch 294/300:  98%|█████████▊| 294/300 [01:24&lt;00:01,  3.43it/s, v_num=1, train_loss_step=1.38e+7, train_loss_epoch=1.37e+7]Epoch 294/300:  98%|█████████▊| 294/300 [01:24&lt;00:01,  3.43it/s, v_num=1, train_loss_step=1.26e+7, train_loss_epoch=1.36e+7]Epoch 295/300:  98%|█████████▊| 294/300 [01:24&lt;00:01,  3.43it/s, v_num=1, train_loss_step=1.26e+7, train_loss_epoch=1.36e+7]Epoch 295/300:  98%|█████████▊| 295/300 [01:24&lt;00:01,  3.45it/s, v_num=1, train_loss_step=1.26e+7, train_loss_epoch=1.36e+7]Epoch 295/300:  98%|█████████▊| 295/300 [01:24&lt;00:01,  3.45it/s, v_num=1, train_loss_step=1.48e+7, train_loss_epoch=1.36e+7]Epoch 296/300:  98%|█████████▊| 295/300 [01:24&lt;00:01,  3.45it/s, v_num=1, train_loss_step=1.48e+7, train_loss_epoch=1.36e+7]Epoch 296/300:  99%|█████████▊| 296/300 [01:25&lt;00:01,  3.42it/s, v_num=1, train_loss_step=1.48e+7, train_loss_epoch=1.36e+7]Epoch 296/300:  99%|█████████▊| 296/300 [01:25&lt;00:01,  3.42it/s, v_num=1, train_loss_step=1.26e+7, train_loss_epoch=1.35e+7]Epoch 297/300:  99%|█████████▊| 296/300 [01:25&lt;00:01,  3.42it/s, v_num=1, train_loss_step=1.26e+7, train_loss_epoch=1.35e+7]Epoch 297/300:  99%|█████████▉| 297/300 [01:25&lt;00:00,  3.41it/s, v_num=1, train_loss_step=1.26e+7, train_loss_epoch=1.35e+7]Epoch 297/300:  99%|█████████▉| 297/300 [01:25&lt;00:00,  3.41it/s, v_num=1, train_loss_step=1.22e+7, train_loss_epoch=1.35e+7]Epoch 298/300:  99%|█████████▉| 297/300 [01:25&lt;00:00,  3.41it/s, v_num=1, train_loss_step=1.22e+7, train_loss_epoch=1.35e+7]Epoch 298/300:  99%|█████████▉| 298/300 [01:25&lt;00:00,  3.42it/s, v_num=1, train_loss_step=1.22e+7, train_loss_epoch=1.35e+7]Epoch 298/300:  99%|█████████▉| 298/300 [01:25&lt;00:00,  3.42it/s, v_num=1, train_loss_step=1.25e+7, train_loss_epoch=1.35e+7]Epoch 299/300:  99%|█████████▉| 298/300 [01:25&lt;00:00,  3.42it/s, v_num=1, train_loss_step=1.25e+7, train_loss_epoch=1.35e+7]Epoch 299/300: 100%|█████████▉| 299/300 [01:26&lt;00:00,  3.36it/s, v_num=1, train_loss_step=1.25e+7, train_loss_epoch=1.35e+7]Epoch 299/300: 100%|█████████▉| 299/300 [01:26&lt;00:00,  3.36it/s, v_num=1, train_loss_step=1.23e+7, train_loss_epoch=1.34e+7]Epoch 300/300: 100%|█████████▉| 299/300 [01:26&lt;00:00,  3.36it/s, v_num=1, train_loss_step=1.23e+7, train_loss_epoch=1.34e+7]Epoch 300/300: 100%|██████████| 300/300 [01:26&lt;00:00,  3.35it/s, v_num=1, train_loss_step=1.23e+7, train_loss_epoch=1.34e+7]Epoch 300/300: 100%|██████████| 300/300 [01:26&lt;00:00,  3.35it/s, v_num=1, train_loss_step=1.33e+7, train_loss_epoch=1.34e+7]Epoch 300/300: 100%|██████████| 300/300 [01:26&lt;00:00,  3.47it/s, v_num=1, train_loss_step=1.33e+7, train_loss_epoch=1.34e+7]\n\n\n\n\n\n\n\n\n\n\n\n6.3 Predict proportions on the spatial data\nFirst create a new st object with the correct genes and counts as a layer.\n\nst_adata = adata_anterior_subset.copy()\n\nst_adata.layers[\"counts\"] = st_adata.X.copy()\nst_adata = st_adata[:, deg].copy()\n\nSpatialStereoscope.setup_anndata(st_adata, layer=\"counts\")\n\n\ntrain = True\nif train:\n    spatial_model = SpatialStereoscope.from_rna_model(st_adata, sc_model)\n    spatial_model.train(max_epochs = 3000)\n    spatial_model.history[\"elbo_train\"][10:].plot()\n    spatial_model.save(\"./data/spatial/visium/scanpy_stmodel\", overwrite = True)\nelse:\n    spatial_model = SpatialStereoscope.load(\"./data/spatial/visium/scanpy_stmodel\", st_adata)\n    print(\"Loaded Spatial model from file!\")\n\nTraining:   0%|          | 0/3000 [00:00&lt;?, ?it/s]Epoch 1/3000:   0%|          | 0/3000 [00:00&lt;?, ?it/s]Epoch 1/3000:   0%|          | 1/3000 [00:00&lt;21:22,  2.34it/s]Epoch 1/3000:   0%|          | 1/3000 [00:00&lt;21:22,  2.34it/s, v_num=1, train_loss_step=1.11e+7, train_loss_epoch=1.17e+7]Epoch 2/3000:   0%|          | 1/3000 [00:00&lt;21:22,  2.34it/s, v_num=1, train_loss_step=1.11e+7, train_loss_epoch=1.17e+7]Epoch 2/3000:   0%|          | 2/3000 [00:00&lt;24:42,  2.02it/s, v_num=1, train_loss_step=1.11e+7, train_loss_epoch=1.17e+7]Epoch 2/3000:   0%|          | 2/3000 [00:00&lt;24:42,  2.02it/s, v_num=1, train_loss_step=1.2e+7, train_loss_epoch=1.17e+7] Epoch 3/3000:   0%|          | 2/3000 [00:00&lt;24:42,  2.02it/s, v_num=1, train_loss_step=1.2e+7, train_loss_epoch=1.17e+7]Epoch 3/3000:   0%|          | 3/3000 [00:01&lt;24:03,  2.08it/s, v_num=1, train_loss_step=1.2e+7, train_loss_epoch=1.17e+7]Epoch 3/3000:   0%|          | 3/3000 [00:01&lt;24:03,  2.08it/s, v_num=1, train_loss_step=1.16e+7, train_loss_epoch=1.16e+7]Epoch 4/3000:   0%|          | 3/3000 [00:01&lt;24:03,  2.08it/s, v_num=1, train_loss_step=1.16e+7, train_loss_epoch=1.16e+7]Epoch 4/3000:   0%|          | 4/3000 [00:01&lt;22:55,  2.18it/s, v_num=1, train_loss_step=1.16e+7, train_loss_epoch=1.16e+7]Epoch 4/3000:   0%|          | 4/3000 [00:01&lt;22:55,  2.18it/s, v_num=1, train_loss_step=1.14e+7, train_loss_epoch=1.16e+7]Epoch 5/3000:   0%|          | 4/3000 [00:01&lt;22:55,  2.18it/s, v_num=1, train_loss_step=1.14e+7, train_loss_epoch=1.16e+7]Epoch 5/3000:   0%|          | 5/3000 [00:02&lt;23:12,  2.15it/s, v_num=1, train_loss_step=1.14e+7, train_loss_epoch=1.16e+7]Epoch 5/3000:   0%|          | 5/3000 [00:02&lt;23:12,  2.15it/s, v_num=1, train_loss_step=1.12e+7, train_loss_epoch=1.16e+7]Epoch 6/3000:   0%|          | 5/3000 [00:02&lt;23:12,  2.15it/s, v_num=1, train_loss_step=1.12e+7, train_loss_epoch=1.16e+7]Epoch 6/3000:   0%|          | 6/3000 [00:02&lt;24:27,  2.04it/s, v_num=1, train_loss_step=1.12e+7, train_loss_epoch=1.16e+7]Epoch 6/3000:   0%|          | 6/3000 [00:02&lt;24:27,  2.04it/s, v_num=1, train_loss_step=1.14e+7, train_loss_epoch=1.15e+7]Epoch 7/3000:   0%|          | 6/3000 [00:02&lt;24:27,  2.04it/s, v_num=1, train_loss_step=1.14e+7, train_loss_epoch=1.15e+7]Epoch 7/3000:   0%|          | 7/3000 [00:03&lt;24:58,  2.00it/s, v_num=1, train_loss_step=1.14e+7, train_loss_epoch=1.15e+7]Epoch 7/3000:   0%|          | 7/3000 [00:03&lt;24:58,  2.00it/s, v_num=1, train_loss_step=1.2e+7, train_loss_epoch=1.15e+7] Epoch 8/3000:   0%|          | 7/3000 [00:03&lt;24:58,  2.00it/s, v_num=1, train_loss_step=1.2e+7, train_loss_epoch=1.15e+7]Epoch 8/3000:   0%|          | 8/3000 [00:03&lt;21:53,  2.28it/s, v_num=1, train_loss_step=1.2e+7, train_loss_epoch=1.15e+7]Epoch 8/3000:   0%|          | 8/3000 [00:03&lt;21:53,  2.28it/s, v_num=1, train_loss_step=1.14e+7, train_loss_epoch=1.14e+7]Epoch 9/3000:   0%|          | 8/3000 [00:03&lt;21:53,  2.28it/s, v_num=1, train_loss_step=1.14e+7, train_loss_epoch=1.14e+7]Epoch 9/3000:   0%|          | 9/3000 [00:04&lt;21:18,  2.34it/s, v_num=1, train_loss_step=1.14e+7, train_loss_epoch=1.14e+7]Epoch 9/3000:   0%|          | 9/3000 [00:04&lt;21:18,  2.34it/s, v_num=1, train_loss_step=1.17e+7, train_loss_epoch=1.14e+7]Epoch 10/3000:   0%|          | 9/3000 [00:04&lt;21:18,  2.34it/s, v_num=1, train_loss_step=1.17e+7, train_loss_epoch=1.14e+7]Epoch 10/3000:   0%|          | 10/3000 [00:04&lt;16:23,  3.04it/s, v_num=1, train_loss_step=1.17e+7, train_loss_epoch=1.14e+7]Epoch 10/3000:   0%|          | 10/3000 [00:04&lt;16:23,  3.04it/s, v_num=1, train_loss_step=1.15e+7, train_loss_epoch=1.14e+7]Epoch 11/3000:   0%|          | 10/3000 [00:04&lt;16:23,  3.04it/s, v_num=1, train_loss_step=1.15e+7, train_loss_epoch=1.14e+7]Epoch 11/3000:   0%|          | 11/3000 [00:04&lt;17:20,  2.87it/s, v_num=1, train_loss_step=1.15e+7, train_loss_epoch=1.14e+7]Epoch 11/3000:   0%|          | 11/3000 [00:04&lt;17:20,  2.87it/s, v_num=1, train_loss_step=1.18e+7, train_loss_epoch=1.13e+7]Epoch 12/3000:   0%|          | 11/3000 [00:04&lt;17:20,  2.87it/s, v_num=1, train_loss_step=1.18e+7, train_loss_epoch=1.13e+7]Epoch 12/3000:   0%|          | 12/3000 [00:05&lt;21:23,  2.33it/s, v_num=1, train_loss_step=1.18e+7, train_loss_epoch=1.13e+7]Epoch 12/3000:   0%|          | 12/3000 [00:05&lt;21:23,  2.33it/s, v_num=1, train_loss_step=1.12e+7, train_loss_epoch=1.13e+7]Epoch 13/3000:   0%|          | 12/3000 [00:05&lt;21:23,  2.33it/s, v_num=1, train_loss_step=1.12e+7, train_loss_epoch=1.13e+7]Epoch 13/3000:   0%|          | 13/3000 [00:05&lt;18:34,  2.68it/s, v_num=1, train_loss_step=1.12e+7, train_loss_epoch=1.13e+7]Epoch 13/3000:   0%|          | 13/3000 [00:05&lt;18:34,  2.68it/s, v_num=1, train_loss_step=1.16e+7, train_loss_epoch=1.12e+7]Epoch 14/3000:   0%|          | 13/3000 [00:05&lt;18:34,  2.68it/s, v_num=1, train_loss_step=1.16e+7, train_loss_epoch=1.12e+7]Epoch 14/3000:   0%|          | 14/3000 [00:05&lt;16:39,  2.99it/s, v_num=1, train_loss_step=1.16e+7, train_loss_epoch=1.12e+7]Epoch 14/3000:   0%|          | 14/3000 [00:05&lt;16:39,  2.99it/s, v_num=1, train_loss_step=1.16e+7, train_loss_epoch=1.12e+7]Epoch 15/3000:   0%|          | 14/3000 [00:05&lt;16:39,  2.99it/s, v_num=1, train_loss_step=1.16e+7, train_loss_epoch=1.12e+7]Epoch 15/3000:   0%|          | 15/3000 [00:06&lt;18:51,  2.64it/s, v_num=1, train_loss_step=1.16e+7, train_loss_epoch=1.12e+7]Epoch 15/3000:   0%|          | 15/3000 [00:06&lt;18:51,  2.64it/s, v_num=1, train_loss_step=1.16e+7, train_loss_epoch=1.12e+7]Epoch 16/3000:   0%|          | 15/3000 [00:06&lt;18:51,  2.64it/s, v_num=1, train_loss_step=1.16e+7, train_loss_epoch=1.12e+7]Epoch 16/3000:   1%|          | 16/3000 [00:06&lt;20:49,  2.39it/s, v_num=1, train_loss_step=1.16e+7, train_loss_epoch=1.12e+7]Epoch 16/3000:   1%|          | 16/3000 [00:06&lt;20:49,  2.39it/s, v_num=1, train_loss_step=1.13e+7, train_loss_epoch=1.11e+7]Epoch 17/3000:   1%|          | 16/3000 [00:06&lt;20:49,  2.39it/s, v_num=1, train_loss_step=1.13e+7, train_loss_epoch=1.11e+7]Epoch 17/3000:   1%|          | 17/3000 [00:07&lt;20:45,  2.40it/s, v_num=1, train_loss_step=1.13e+7, train_loss_epoch=1.11e+7]Epoch 17/3000:   1%|          | 17/3000 [00:07&lt;20:45,  2.40it/s, v_num=1, train_loss_step=1.06e+7, train_loss_epoch=1.11e+7]Epoch 18/3000:   1%|          | 17/3000 [00:07&lt;20:45,  2.40it/s, v_num=1, train_loss_step=1.06e+7, train_loss_epoch=1.11e+7]Epoch 18/3000:   1%|          | 18/3000 [00:07&lt;21:33,  2.31it/s, v_num=1, train_loss_step=1.06e+7, train_loss_epoch=1.11e+7]Epoch 18/3000:   1%|          | 18/3000 [00:07&lt;21:33,  2.31it/s, v_num=1, train_loss_step=1.09e+7, train_loss_epoch=1.1e+7] Epoch 19/3000:   1%|          | 18/3000 [00:07&lt;21:33,  2.31it/s, v_num=1, train_loss_step=1.09e+7, train_loss_epoch=1.1e+7]Epoch 19/3000:   1%|          | 19/3000 [00:07&lt;20:17,  2.45it/s, v_num=1, train_loss_step=1.09e+7, train_loss_epoch=1.1e+7]Epoch 19/3000:   1%|          | 19/3000 [00:07&lt;20:17,  2.45it/s, v_num=1, train_loss_step=1.16e+7, train_loss_epoch=1.1e+7]Epoch 20/3000:   1%|          | 19/3000 [00:07&lt;20:17,  2.45it/s, v_num=1, train_loss_step=1.16e+7, train_loss_epoch=1.1e+7]Epoch 20/3000:   1%|          | 20/3000 [00:08&lt;20:14,  2.45it/s, v_num=1, train_loss_step=1.16e+7, train_loss_epoch=1.1e+7]Epoch 20/3000:   1%|          | 20/3000 [00:08&lt;20:14,  2.45it/s, v_num=1, train_loss_step=1.12e+7, train_loss_epoch=1.1e+7]Epoch 21/3000:   1%|          | 20/3000 [00:08&lt;20:14,  2.45it/s, v_num=1, train_loss_step=1.12e+7, train_loss_epoch=1.1e+7]Epoch 21/3000:   1%|          | 21/3000 [00:08&lt;20:00,  2.48it/s, v_num=1, train_loss_step=1.12e+7, train_loss_epoch=1.1e+7]Epoch 21/3000:   1%|          | 21/3000 [00:08&lt;20:00,  2.48it/s, v_num=1, train_loss_step=1.15e+7, train_loss_epoch=1.09e+7]Epoch 22/3000:   1%|          | 21/3000 [00:08&lt;20:00,  2.48it/s, v_num=1, train_loss_step=1.15e+7, train_loss_epoch=1.09e+7]Epoch 22/3000:   1%|          | 22/3000 [00:09&lt;18:31,  2.68it/s, v_num=1, train_loss_step=1.15e+7, train_loss_epoch=1.09e+7]Epoch 22/3000:   1%|          | 22/3000 [00:09&lt;18:31,  2.68it/s, v_num=1, train_loss_step=1.09e+7, train_loss_epoch=1.09e+7]Epoch 23/3000:   1%|          | 22/3000 [00:09&lt;18:31,  2.68it/s, v_num=1, train_loss_step=1.09e+7, train_loss_epoch=1.09e+7]Epoch 23/3000:   1%|          | 23/3000 [00:09&lt;19:01,  2.61it/s, v_num=1, train_loss_step=1.09e+7, train_loss_epoch=1.09e+7]Epoch 23/3000:   1%|          | 23/3000 [00:09&lt;19:01,  2.61it/s, v_num=1, train_loss_step=1.06e+7, train_loss_epoch=1.08e+7]Epoch 24/3000:   1%|          | 23/3000 [00:09&lt;19:01,  2.61it/s, v_num=1, train_loss_step=1.06e+7, train_loss_epoch=1.08e+7]Epoch 24/3000:   1%|          | 24/3000 [00:09&lt;19:36,  2.53it/s, v_num=1, train_loss_step=1.06e+7, train_loss_epoch=1.08e+7]Epoch 24/3000:   1%|          | 24/3000 [00:09&lt;19:36,  2.53it/s, v_num=1, train_loss_step=1.06e+7, train_loss_epoch=1.08e+7]Epoch 25/3000:   1%|          | 24/3000 [00:09&lt;19:36,  2.53it/s, v_num=1, train_loss_step=1.06e+7, train_loss_epoch=1.08e+7]Epoch 25/3000:   1%|          | 25/3000 [00:10&lt;22:14,  2.23it/s, v_num=1, train_loss_step=1.06e+7, train_loss_epoch=1.08e+7]Epoch 25/3000:   1%|          | 25/3000 [00:10&lt;22:14,  2.23it/s, v_num=1, train_loss_step=1.03e+7, train_loss_epoch=1.07e+7]Epoch 26/3000:   1%|          | 25/3000 [00:10&lt;22:14,  2.23it/s, v_num=1, train_loss_step=1.03e+7, train_loss_epoch=1.07e+7]Epoch 26/3000:   1%|          | 26/3000 [00:11&lt;24:51,  1.99it/s, v_num=1, train_loss_step=1.03e+7, train_loss_epoch=1.07e+7]Epoch 26/3000:   1%|          | 26/3000 [00:11&lt;24:51,  1.99it/s, v_num=1, train_loss_step=1.11e+7, train_loss_epoch=1.07e+7]Epoch 27/3000:   1%|          | 26/3000 [00:11&lt;24:51,  1.99it/s, v_num=1, train_loss_step=1.11e+7, train_loss_epoch=1.07e+7]Epoch 27/3000:   1%|          | 27/3000 [00:11&lt;24:59,  1.98it/s, v_num=1, train_loss_step=1.11e+7, train_loss_epoch=1.07e+7]Epoch 27/3000:   1%|          | 27/3000 [00:11&lt;24:59,  1.98it/s, v_num=1, train_loss_step=1.08e+7, train_loss_epoch=1.06e+7]Epoch 28/3000:   1%|          | 27/3000 [00:11&lt;24:59,  1.98it/s, v_num=1, train_loss_step=1.08e+7, train_loss_epoch=1.06e+7]Epoch 28/3000:   1%|          | 28/3000 [00:11&lt;22:43,  2.18it/s, v_num=1, train_loss_step=1.08e+7, train_loss_epoch=1.06e+7]Epoch 28/3000:   1%|          | 28/3000 [00:11&lt;22:43,  2.18it/s, v_num=1, train_loss_step=1.08e+7, train_loss_epoch=1.06e+7]Epoch 29/3000:   1%|          | 28/3000 [00:11&lt;22:43,  2.18it/s, v_num=1, train_loss_step=1.08e+7, train_loss_epoch=1.06e+7]Epoch 29/3000:   1%|          | 29/3000 [00:12&lt;22:48,  2.17it/s, v_num=1, train_loss_step=1.08e+7, train_loss_epoch=1.06e+7]Epoch 29/3000:   1%|          | 29/3000 [00:12&lt;22:48,  2.17it/s, v_num=1, train_loss_step=1.06e+7, train_loss_epoch=1.05e+7]Epoch 30/3000:   1%|          | 29/3000 [00:12&lt;22:48,  2.17it/s, v_num=1, train_loss_step=1.06e+7, train_loss_epoch=1.05e+7]Epoch 30/3000:   1%|          | 30/3000 [00:12&lt;20:29,  2.42it/s, v_num=1, train_loss_step=1.06e+7, train_loss_epoch=1.05e+7]Epoch 30/3000:   1%|          | 30/3000 [00:12&lt;20:29,  2.42it/s, v_num=1, train_loss_step=1.05e+7, train_loss_epoch=1.05e+7]Epoch 31/3000:   1%|          | 30/3000 [00:12&lt;20:29,  2.42it/s, v_num=1, train_loss_step=1.05e+7, train_loss_epoch=1.05e+7]Epoch 31/3000:   1%|          | 31/3000 [00:13&lt;21:41,  2.28it/s, v_num=1, train_loss_step=1.05e+7, train_loss_epoch=1.05e+7]Epoch 31/3000:   1%|          | 31/3000 [00:13&lt;21:41,  2.28it/s, v_num=1, train_loss_step=1.04e+7, train_loss_epoch=1.05e+7]Epoch 32/3000:   1%|          | 31/3000 [00:13&lt;21:41,  2.28it/s, v_num=1, train_loss_step=1.04e+7, train_loss_epoch=1.05e+7]Epoch 32/3000:   1%|          | 32/3000 [00:13&lt;22:50,  2.17it/s, v_num=1, train_loss_step=1.04e+7, train_loss_epoch=1.05e+7]Epoch 32/3000:   1%|          | 32/3000 [00:13&lt;22:50,  2.17it/s, v_num=1, train_loss_step=1.05e+7, train_loss_epoch=1.04e+7]Epoch 33/3000:   1%|          | 32/3000 [00:13&lt;22:50,  2.17it/s, v_num=1, train_loss_step=1.05e+7, train_loss_epoch=1.04e+7]Epoch 33/3000:   1%|          | 33/3000 [00:14&lt;23:16,  2.12it/s, v_num=1, train_loss_step=1.05e+7, train_loss_epoch=1.04e+7]Epoch 33/3000:   1%|          | 33/3000 [00:14&lt;23:16,  2.12it/s, v_num=1, train_loss_step=1.02e+7, train_loss_epoch=1.04e+7]Epoch 34/3000:   1%|          | 33/3000 [00:14&lt;23:16,  2.12it/s, v_num=1, train_loss_step=1.02e+7, train_loss_epoch=1.04e+7]Epoch 34/3000:   1%|          | 34/3000 [00:14&lt;21:57,  2.25it/s, v_num=1, train_loss_step=1.02e+7, train_loss_epoch=1.04e+7]Epoch 34/3000:   1%|          | 34/3000 [00:14&lt;21:57,  2.25it/s, v_num=1, train_loss_step=1.01e+7, train_loss_epoch=1.03e+7]Epoch 35/3000:   1%|          | 34/3000 [00:14&lt;21:57,  2.25it/s, v_num=1, train_loss_step=1.01e+7, train_loss_epoch=1.03e+7]Epoch 35/3000:   1%|          | 35/3000 [00:14&lt;20:48,  2.38it/s, v_num=1, train_loss_step=1.01e+7, train_loss_epoch=1.03e+7]Epoch 35/3000:   1%|          | 35/3000 [00:14&lt;20:48,  2.38it/s, v_num=1, train_loss_step=1.05e+7, train_loss_epoch=1.03e+7]Epoch 36/3000:   1%|          | 35/3000 [00:14&lt;20:48,  2.38it/s, v_num=1, train_loss_step=1.05e+7, train_loss_epoch=1.03e+7]Epoch 36/3000:   1%|          | 36/3000 [00:15&lt;20:15,  2.44it/s, v_num=1, train_loss_step=1.05e+7, train_loss_epoch=1.03e+7]Epoch 36/3000:   1%|          | 36/3000 [00:15&lt;20:15,  2.44it/s, v_num=1, train_loss_step=9.99e+6, train_loss_epoch=1.02e+7]Epoch 37/3000:   1%|          | 36/3000 [00:15&lt;20:15,  2.44it/s, v_num=1, train_loss_step=9.99e+6, train_loss_epoch=1.02e+7]Epoch 37/3000:   1%|          | 37/3000 [00:15&lt;22:24,  2.20it/s, v_num=1, train_loss_step=9.99e+6, train_loss_epoch=1.02e+7]Epoch 37/3000:   1%|          | 37/3000 [00:15&lt;22:24,  2.20it/s, v_num=1, train_loss_step=9.85e+6, train_loss_epoch=1.02e+7]Epoch 38/3000:   1%|          | 37/3000 [00:15&lt;22:24,  2.20it/s, v_num=1, train_loss_step=9.85e+6, train_loss_epoch=1.02e+7]Epoch 38/3000:   1%|▏         | 38/3000 [00:16&lt;22:01,  2.24it/s, v_num=1, train_loss_step=9.85e+6, train_loss_epoch=1.02e+7]Epoch 38/3000:   1%|▏         | 38/3000 [00:16&lt;22:01,  2.24it/s, v_num=1, train_loss_step=9.69e+6, train_loss_epoch=1.01e+7]Epoch 39/3000:   1%|▏         | 38/3000 [00:16&lt;22:01,  2.24it/s, v_num=1, train_loss_step=9.69e+6, train_loss_epoch=1.01e+7]Epoch 39/3000:   1%|▏         | 39/3000 [00:16&lt;21:23,  2.31it/s, v_num=1, train_loss_step=9.69e+6, train_loss_epoch=1.01e+7]Epoch 39/3000:   1%|▏         | 39/3000 [00:16&lt;21:23,  2.31it/s, v_num=1, train_loss_step=1.05e+7, train_loss_epoch=1.01e+7]Epoch 40/3000:   1%|▏         | 39/3000 [00:16&lt;21:23,  2.31it/s, v_num=1, train_loss_step=1.05e+7, train_loss_epoch=1.01e+7]Epoch 40/3000:   1%|▏         | 40/3000 [00:17&lt;23:04,  2.14it/s, v_num=1, train_loss_step=1.05e+7, train_loss_epoch=1.01e+7]Epoch 40/3000:   1%|▏         | 40/3000 [00:17&lt;23:04,  2.14it/s, v_num=1, train_loss_step=9.93e+6, train_loss_epoch=1e+7]   Epoch 41/3000:   1%|▏         | 40/3000 [00:17&lt;23:04,  2.14it/s, v_num=1, train_loss_step=9.93e+6, train_loss_epoch=1e+7]Epoch 41/3000:   1%|▏         | 41/3000 [00:17&lt;17:41,  2.79it/s, v_num=1, train_loss_step=9.93e+6, train_loss_epoch=1e+7]Epoch 41/3000:   1%|▏         | 41/3000 [00:17&lt;17:41,  2.79it/s, v_num=1, train_loss_step=9.99e+6, train_loss_epoch=9.99e+6]Epoch 42/3000:   1%|▏         | 41/3000 [00:17&lt;17:41,  2.79it/s, v_num=1, train_loss_step=9.99e+6, train_loss_epoch=9.99e+6]Epoch 42/3000:   1%|▏         | 42/3000 [00:17&lt;13:53,  3.55it/s, v_num=1, train_loss_step=9.99e+6, train_loss_epoch=9.99e+6]Epoch 42/3000:   1%|▏         | 42/3000 [00:17&lt;13:53,  3.55it/s, v_num=1, train_loss_step=9.54e+6, train_loss_epoch=9.95e+6]Epoch 43/3000:   1%|▏         | 42/3000 [00:17&lt;13:53,  3.55it/s, v_num=1, train_loss_step=9.54e+6, train_loss_epoch=9.95e+6]Epoch 43/3000:   1%|▏         | 43/3000 [00:17&lt;11:22,  4.33it/s, v_num=1, train_loss_step=9.54e+6, train_loss_epoch=9.95e+6]Epoch 43/3000:   1%|▏         | 43/3000 [00:17&lt;11:22,  4.33it/s, v_num=1, train_loss_step=9.98e+6, train_loss_epoch=9.9e+6] Epoch 44/3000:   1%|▏         | 43/3000 [00:17&lt;11:22,  4.33it/s, v_num=1, train_loss_step=9.98e+6, train_loss_epoch=9.9e+6]Epoch 44/3000:   1%|▏         | 44/3000 [00:17&lt;09:29,  5.19it/s, v_num=1, train_loss_step=9.98e+6, train_loss_epoch=9.9e+6]Epoch 44/3000:   1%|▏         | 44/3000 [00:17&lt;09:29,  5.19it/s, v_num=1, train_loss_step=9.83e+6, train_loss_epoch=9.86e+6]Epoch 45/3000:   1%|▏         | 44/3000 [00:17&lt;09:29,  5.19it/s, v_num=1, train_loss_step=9.83e+6, train_loss_epoch=9.86e+6]Epoch 45/3000:   2%|▏         | 45/3000 [00:17&lt;10:30,  4.68it/s, v_num=1, train_loss_step=9.83e+6, train_loss_epoch=9.86e+6]Epoch 45/3000:   2%|▏         | 45/3000 [00:17&lt;10:30,  4.68it/s, v_num=1, train_loss_step=9.68e+6, train_loss_epoch=9.81e+6]Epoch 46/3000:   2%|▏         | 45/3000 [00:17&lt;10:30,  4.68it/s, v_num=1, train_loss_step=9.68e+6, train_loss_epoch=9.81e+6]Epoch 46/3000:   2%|▏         | 46/3000 [00:18&lt;13:30,  3.64it/s, v_num=1, train_loss_step=9.68e+6, train_loss_epoch=9.81e+6]Epoch 46/3000:   2%|▏         | 46/3000 [00:18&lt;13:30,  3.64it/s, v_num=1, train_loss_step=9.85e+6, train_loss_epoch=9.76e+6]Epoch 47/3000:   2%|▏         | 46/3000 [00:18&lt;13:30,  3.64it/s, v_num=1, train_loss_step=9.85e+6, train_loss_epoch=9.76e+6]Epoch 47/3000:   2%|▏         | 47/3000 [00:18&lt;16:31,  2.98it/s, v_num=1, train_loss_step=9.85e+6, train_loss_epoch=9.76e+6]Epoch 47/3000:   2%|▏         | 47/3000 [00:18&lt;16:31,  2.98it/s, v_num=1, train_loss_step=1.01e+7, train_loss_epoch=9.72e+6]Epoch 48/3000:   2%|▏         | 47/3000 [00:18&lt;16:31,  2.98it/s, v_num=1, train_loss_step=1.01e+7, train_loss_epoch=9.72e+6]Epoch 48/3000:   2%|▏         | 48/3000 [00:19&lt;20:02,  2.45it/s, v_num=1, train_loss_step=1.01e+7, train_loss_epoch=9.72e+6]Epoch 48/3000:   2%|▏         | 48/3000 [00:19&lt;20:02,  2.45it/s, v_num=1, train_loss_step=9.62e+6, train_loss_epoch=9.67e+6]Epoch 49/3000:   2%|▏         | 48/3000 [00:19&lt;20:02,  2.45it/s, v_num=1, train_loss_step=9.62e+6, train_loss_epoch=9.67e+6]Epoch 49/3000:   2%|▏         | 49/3000 [00:19&lt;20:41,  2.38it/s, v_num=1, train_loss_step=9.62e+6, train_loss_epoch=9.67e+6]Epoch 49/3000:   2%|▏         | 49/3000 [00:19&lt;20:41,  2.38it/s, v_num=1, train_loss_step=9.72e+6, train_loss_epoch=9.62e+6]Epoch 50/3000:   2%|▏         | 49/3000 [00:19&lt;20:41,  2.38it/s, v_num=1, train_loss_step=9.72e+6, train_loss_epoch=9.62e+6]Epoch 50/3000:   2%|▏         | 50/3000 [00:20&lt;20:30,  2.40it/s, v_num=1, train_loss_step=9.72e+6, train_loss_epoch=9.62e+6]Epoch 50/3000:   2%|▏         | 50/3000 [00:20&lt;20:30,  2.40it/s, v_num=1, train_loss_step=9.61e+6, train_loss_epoch=9.58e+6]Epoch 51/3000:   2%|▏         | 50/3000 [00:20&lt;20:30,  2.40it/s, v_num=1, train_loss_step=9.61e+6, train_loss_epoch=9.58e+6]Epoch 51/3000:   2%|▏         | 51/3000 [00:20&lt;16:21,  3.00it/s, v_num=1, train_loss_step=9.61e+6, train_loss_epoch=9.58e+6]Epoch 51/3000:   2%|▏         | 51/3000 [00:20&lt;16:21,  3.00it/s, v_num=1, train_loss_step=9.99e+6, train_loss_epoch=9.53e+6]Epoch 52/3000:   2%|▏         | 51/3000 [00:20&lt;16:21,  3.00it/s, v_num=1, train_loss_step=9.99e+6, train_loss_epoch=9.53e+6]Epoch 52/3000:   2%|▏         | 52/3000 [00:20&lt;14:14,  3.45it/s, v_num=1, train_loss_step=9.99e+6, train_loss_epoch=9.53e+6]Epoch 52/3000:   2%|▏         | 52/3000 [00:20&lt;14:14,  3.45it/s, v_num=1, train_loss_step=9.71e+6, train_loss_epoch=9.49e+6]Epoch 53/3000:   2%|▏         | 52/3000 [00:20&lt;14:14,  3.45it/s, v_num=1, train_loss_step=9.71e+6, train_loss_epoch=9.49e+6]Epoch 53/3000:   2%|▏         | 53/3000 [00:21&lt;18:10,  2.70it/s, v_num=1, train_loss_step=9.71e+6, train_loss_epoch=9.49e+6]Epoch 53/3000:   2%|▏         | 53/3000 [00:21&lt;18:10,  2.70it/s, v_num=1, train_loss_step=9.42e+6, train_loss_epoch=9.44e+6]Epoch 54/3000:   2%|▏         | 53/3000 [00:21&lt;18:10,  2.70it/s, v_num=1, train_loss_step=9.42e+6, train_loss_epoch=9.44e+6]Epoch 54/3000:   2%|▏         | 54/3000 [00:21&lt;19:20,  2.54it/s, v_num=1, train_loss_step=9.42e+6, train_loss_epoch=9.44e+6]Epoch 54/3000:   2%|▏         | 54/3000 [00:21&lt;19:20,  2.54it/s, v_num=1, train_loss_step=9.69e+6, train_loss_epoch=9.39e+6]Epoch 55/3000:   2%|▏         | 54/3000 [00:21&lt;19:20,  2.54it/s, v_num=1, train_loss_step=9.69e+6, train_loss_epoch=9.39e+6]Epoch 55/3000:   2%|▏         | 55/3000 [00:22&lt;21:29,  2.28it/s, v_num=1, train_loss_step=9.69e+6, train_loss_epoch=9.39e+6]Epoch 55/3000:   2%|▏         | 55/3000 [00:22&lt;21:29,  2.28it/s, v_num=1, train_loss_step=9.4e+6, train_loss_epoch=9.35e+6] Epoch 56/3000:   2%|▏         | 55/3000 [00:22&lt;21:29,  2.28it/s, v_num=1, train_loss_step=9.4e+6, train_loss_epoch=9.35e+6]Epoch 56/3000:   2%|▏         | 56/3000 [00:22&lt;21:18,  2.30it/s, v_num=1, train_loss_step=9.4e+6, train_loss_epoch=9.35e+6]Epoch 56/3000:   2%|▏         | 56/3000 [00:22&lt;21:18,  2.30it/s, v_num=1, train_loss_step=9.65e+6, train_loss_epoch=9.3e+6]Epoch 57/3000:   2%|▏         | 56/3000 [00:22&lt;21:18,  2.30it/s, v_num=1, train_loss_step=9.65e+6, train_loss_epoch=9.3e+6]Epoch 57/3000:   2%|▏         | 57/3000 [00:23&lt;22:56,  2.14it/s, v_num=1, train_loss_step=9.65e+6, train_loss_epoch=9.3e+6]Epoch 57/3000:   2%|▏         | 57/3000 [00:23&lt;22:56,  2.14it/s, v_num=1, train_loss_step=9.31e+6, train_loss_epoch=9.26e+6]Epoch 58/3000:   2%|▏         | 57/3000 [00:23&lt;22:56,  2.14it/s, v_num=1, train_loss_step=9.31e+6, train_loss_epoch=9.26e+6]Epoch 58/3000:   2%|▏         | 58/3000 [00:23&lt;23:07,  2.12it/s, v_num=1, train_loss_step=9.31e+6, train_loss_epoch=9.26e+6]Epoch 58/3000:   2%|▏         | 58/3000 [00:23&lt;23:07,  2.12it/s, v_num=1, train_loss_step=9.26e+6, train_loss_epoch=9.21e+6]Epoch 59/3000:   2%|▏         | 58/3000 [00:23&lt;23:07,  2.12it/s, v_num=1, train_loss_step=9.26e+6, train_loss_epoch=9.21e+6]Epoch 59/3000:   2%|▏         | 59/3000 [00:23&lt;21:40,  2.26it/s, v_num=1, train_loss_step=9.26e+6, train_loss_epoch=9.21e+6]Epoch 59/3000:   2%|▏         | 59/3000 [00:23&lt;21:40,  2.26it/s, v_num=1, train_loss_step=8.75e+6, train_loss_epoch=9.17e+6]Epoch 60/3000:   2%|▏         | 59/3000 [00:23&lt;21:40,  2.26it/s, v_num=1, train_loss_step=8.75e+6, train_loss_epoch=9.17e+6]Epoch 60/3000:   2%|▏         | 60/3000 [00:24&lt;22:32,  2.17it/s, v_num=1, train_loss_step=8.75e+6, train_loss_epoch=9.17e+6]Epoch 60/3000:   2%|▏         | 60/3000 [00:24&lt;22:32,  2.17it/s, v_num=1, train_loss_step=9.01e+6, train_loss_epoch=9.12e+6]Epoch 61/3000:   2%|▏         | 60/3000 [00:24&lt;22:32,  2.17it/s, v_num=1, train_loss_step=9.01e+6, train_loss_epoch=9.12e+6]Epoch 61/3000:   2%|▏         | 61/3000 [00:24&lt;23:12,  2.11it/s, v_num=1, train_loss_step=9.01e+6, train_loss_epoch=9.12e+6]Epoch 61/3000:   2%|▏         | 61/3000 [00:24&lt;23:12,  2.11it/s, v_num=1, train_loss_step=9.38e+6, train_loss_epoch=9.08e+6]Epoch 62/3000:   2%|▏         | 61/3000 [00:25&lt;23:12,  2.11it/s, v_num=1, train_loss_step=9.38e+6, train_loss_epoch=9.08e+6]Epoch 62/3000:   2%|▏         | 62/3000 [00:25&lt;23:51,  2.05it/s, v_num=1, train_loss_step=9.38e+6, train_loss_epoch=9.08e+6]Epoch 62/3000:   2%|▏         | 62/3000 [00:25&lt;23:51,  2.05it/s, v_num=1, train_loss_step=8.87e+6, train_loss_epoch=9.03e+6]Epoch 63/3000:   2%|▏         | 62/3000 [00:25&lt;23:51,  2.05it/s, v_num=1, train_loss_step=8.87e+6, train_loss_epoch=9.03e+6]Epoch 63/3000:   2%|▏         | 63/3000 [00:25&lt;23:45,  2.06it/s, v_num=1, train_loss_step=8.87e+6, train_loss_epoch=9.03e+6]Epoch 63/3000:   2%|▏         | 63/3000 [00:25&lt;23:45,  2.06it/s, v_num=1, train_loss_step=9.1e+6, train_loss_epoch=8.99e+6] Epoch 64/3000:   2%|▏         | 63/3000 [00:26&lt;23:45,  2.06it/s, v_num=1, train_loss_step=9.1e+6, train_loss_epoch=8.99e+6]Epoch 64/3000:   2%|▏         | 64/3000 [00:26&lt;23:02,  2.12it/s, v_num=1, train_loss_step=9.1e+6, train_loss_epoch=8.99e+6]Epoch 64/3000:   2%|▏         | 64/3000 [00:26&lt;23:02,  2.12it/s, v_num=1, train_loss_step=9.08e+6, train_loss_epoch=8.95e+6]Epoch 65/3000:   2%|▏         | 64/3000 [00:26&lt;23:02,  2.12it/s, v_num=1, train_loss_step=9.08e+6, train_loss_epoch=8.95e+6]Epoch 65/3000:   2%|▏         | 65/3000 [00:26&lt;24:20,  2.01it/s, v_num=1, train_loss_step=9.08e+6, train_loss_epoch=8.95e+6]Epoch 65/3000:   2%|▏         | 65/3000 [00:26&lt;24:20,  2.01it/s, v_num=1, train_loss_step=8.45e+6, train_loss_epoch=8.9e+6] Epoch 66/3000:   2%|▏         | 65/3000 [00:26&lt;24:20,  2.01it/s, v_num=1, train_loss_step=8.45e+6, train_loss_epoch=8.9e+6]Epoch 66/3000:   2%|▏         | 66/3000 [00:27&lt;19:57,  2.45it/s, v_num=1, train_loss_step=8.45e+6, train_loss_epoch=8.9e+6]Epoch 66/3000:   2%|▏         | 66/3000 [00:27&lt;19:57,  2.45it/s, v_num=1, train_loss_step=8.64e+6, train_loss_epoch=8.86e+6]Epoch 67/3000:   2%|▏         | 66/3000 [00:27&lt;19:57,  2.45it/s, v_num=1, train_loss_step=8.64e+6, train_loss_epoch=8.86e+6]Epoch 67/3000:   2%|▏         | 67/3000 [00:27&lt;20:46,  2.35it/s, v_num=1, train_loss_step=8.64e+6, train_loss_epoch=8.86e+6]Epoch 67/3000:   2%|▏         | 67/3000 [00:27&lt;20:46,  2.35it/s, v_num=1, train_loss_step=9.24e+6, train_loss_epoch=8.82e+6]Epoch 68/3000:   2%|▏         | 67/3000 [00:27&lt;20:46,  2.35it/s, v_num=1, train_loss_step=9.24e+6, train_loss_epoch=8.82e+6]Epoch 68/3000:   2%|▏         | 68/3000 [00:28&lt;22:21,  2.19it/s, v_num=1, train_loss_step=9.24e+6, train_loss_epoch=8.82e+6]Epoch 68/3000:   2%|▏         | 68/3000 [00:28&lt;22:21,  2.19it/s, v_num=1, train_loss_step=8.74e+6, train_loss_epoch=8.77e+6]Epoch 69/3000:   2%|▏         | 68/3000 [00:28&lt;22:21,  2.19it/s, v_num=1, train_loss_step=8.74e+6, train_loss_epoch=8.77e+6]Epoch 69/3000:   2%|▏         | 69/3000 [00:28&lt;23:24,  2.09it/s, v_num=1, train_loss_step=8.74e+6, train_loss_epoch=8.77e+6]Epoch 69/3000:   2%|▏         | 69/3000 [00:28&lt;23:24,  2.09it/s, v_num=1, train_loss_step=9.25e+6, train_loss_epoch=8.73e+6]Epoch 70/3000:   2%|▏         | 69/3000 [00:28&lt;23:24,  2.09it/s, v_num=1, train_loss_step=9.25e+6, train_loss_epoch=8.73e+6]Epoch 70/3000:   2%|▏         | 70/3000 [00:29&lt;20:37,  2.37it/s, v_num=1, train_loss_step=9.25e+6, train_loss_epoch=8.73e+6]Epoch 70/3000:   2%|▏         | 70/3000 [00:29&lt;20:37,  2.37it/s, v_num=1, train_loss_step=8.62e+6, train_loss_epoch=8.69e+6]Epoch 71/3000:   2%|▏         | 70/3000 [00:29&lt;20:37,  2.37it/s, v_num=1, train_loss_step=8.62e+6, train_loss_epoch=8.69e+6]Epoch 71/3000:   2%|▏         | 71/3000 [00:29&lt;20:11,  2.42it/s, v_num=1, train_loss_step=8.62e+6, train_loss_epoch=8.69e+6]Epoch 71/3000:   2%|▏         | 71/3000 [00:29&lt;20:11,  2.42it/s, v_num=1, train_loss_step=8.47e+6, train_loss_epoch=8.64e+6]Epoch 72/3000:   2%|▏         | 71/3000 [00:29&lt;20:11,  2.42it/s, v_num=1, train_loss_step=8.47e+6, train_loss_epoch=8.64e+6]Epoch 72/3000:   2%|▏         | 72/3000 [00:29&lt;17:08,  2.85it/s, v_num=1, train_loss_step=8.47e+6, train_loss_epoch=8.64e+6]Epoch 72/3000:   2%|▏         | 72/3000 [00:29&lt;17:08,  2.85it/s, v_num=1, train_loss_step=8.57e+6, train_loss_epoch=8.6e+6] Epoch 73/3000:   2%|▏         | 72/3000 [00:29&lt;17:08,  2.85it/s, v_num=1, train_loss_step=8.57e+6, train_loss_epoch=8.6e+6]Epoch 73/3000:   2%|▏         | 73/3000 [00:29&lt;15:10,  3.21it/s, v_num=1, train_loss_step=8.57e+6, train_loss_epoch=8.6e+6]Epoch 73/3000:   2%|▏         | 73/3000 [00:29&lt;15:10,  3.21it/s, v_num=1, train_loss_step=8.53e+6, train_loss_epoch=8.56e+6]Epoch 74/3000:   2%|▏         | 73/3000 [00:29&lt;15:10,  3.21it/s, v_num=1, train_loss_step=8.53e+6, train_loss_epoch=8.56e+6]Epoch 74/3000:   2%|▏         | 74/3000 [00:29&lt;13:08,  3.71it/s, v_num=1, train_loss_step=8.53e+6, train_loss_epoch=8.56e+6]Epoch 74/3000:   2%|▏         | 74/3000 [00:30&lt;13:08,  3.71it/s, v_num=1, train_loss_step=8.67e+6, train_loss_epoch=8.52e+6]Epoch 75/3000:   2%|▏         | 74/3000 [00:30&lt;13:08,  3.71it/s, v_num=1, train_loss_step=8.67e+6, train_loss_epoch=8.52e+6]Epoch 75/3000:   2%|▎         | 75/3000 [00:30&lt;10:48,  4.51it/s, v_num=1, train_loss_step=8.67e+6, train_loss_epoch=8.52e+6]Epoch 75/3000:   2%|▎         | 75/3000 [00:30&lt;10:48,  4.51it/s, v_num=1, train_loss_step=8.27e+6, train_loss_epoch=8.48e+6]Epoch 76/3000:   2%|▎         | 75/3000 [00:30&lt;10:48,  4.51it/s, v_num=1, train_loss_step=8.27e+6, train_loss_epoch=8.48e+6]Epoch 76/3000:   3%|▎         | 76/3000 [00:30&lt;12:07,  4.02it/s, v_num=1, train_loss_step=8.27e+6, train_loss_epoch=8.48e+6]Epoch 76/3000:   3%|▎         | 76/3000 [00:30&lt;12:07,  4.02it/s, v_num=1, train_loss_step=8.11e+6, train_loss_epoch=8.44e+6]Epoch 77/3000:   3%|▎         | 76/3000 [00:30&lt;12:07,  4.02it/s, v_num=1, train_loss_step=8.11e+6, train_loss_epoch=8.44e+6]Epoch 77/3000:   3%|▎         | 77/3000 [00:30&lt;11:06,  4.39it/s, v_num=1, train_loss_step=8.11e+6, train_loss_epoch=8.44e+6]Epoch 77/3000:   3%|▎         | 77/3000 [00:30&lt;11:06,  4.39it/s, v_num=1, train_loss_step=8.3e+6, train_loss_epoch=8.4e+6]  Epoch 78/3000:   3%|▎         | 77/3000 [00:30&lt;11:06,  4.39it/s, v_num=1, train_loss_step=8.3e+6, train_loss_epoch=8.4e+6]Epoch 78/3000:   3%|▎         | 78/3000 [00:30&lt;11:38,  4.19it/s, v_num=1, train_loss_step=8.3e+6, train_loss_epoch=8.4e+6]Epoch 78/3000:   3%|▎         | 78/3000 [00:30&lt;11:38,  4.19it/s, v_num=1, train_loss_step=8.44e+6, train_loss_epoch=8.35e+6]Epoch 79/3000:   3%|▎         | 78/3000 [00:30&lt;11:38,  4.19it/s, v_num=1, train_loss_step=8.44e+6, train_loss_epoch=8.35e+6]Epoch 79/3000:   3%|▎         | 79/3000 [00:31&lt;13:12,  3.69it/s, v_num=1, train_loss_step=8.44e+6, train_loss_epoch=8.35e+6]Epoch 79/3000:   3%|▎         | 79/3000 [00:31&lt;13:12,  3.69it/s, v_num=1, train_loss_step=8.47e+6, train_loss_epoch=8.31e+6]Epoch 80/3000:   3%|▎         | 79/3000 [00:31&lt;13:12,  3.69it/s, v_num=1, train_loss_step=8.47e+6, train_loss_epoch=8.31e+6]Epoch 80/3000:   3%|▎         | 80/3000 [00:31&lt;16:07,  3.02it/s, v_num=1, train_loss_step=8.47e+6, train_loss_epoch=8.31e+6]Epoch 80/3000:   3%|▎         | 80/3000 [00:31&lt;16:07,  3.02it/s, v_num=1, train_loss_step=8.14e+6, train_loss_epoch=8.27e+6]Epoch 81/3000:   3%|▎         | 80/3000 [00:31&lt;16:07,  3.02it/s, v_num=1, train_loss_step=8.14e+6, train_loss_epoch=8.27e+6]Epoch 81/3000:   3%|▎         | 81/3000 [00:32&lt;20:03,  2.43it/s, v_num=1, train_loss_step=8.14e+6, train_loss_epoch=8.27e+6]Epoch 81/3000:   3%|▎         | 81/3000 [00:32&lt;20:03,  2.43it/s, v_num=1, train_loss_step=8.02e+6, train_loss_epoch=8.23e+6]Epoch 82/3000:   3%|▎         | 81/3000 [00:32&lt;20:03,  2.43it/s, v_num=1, train_loss_step=8.02e+6, train_loss_epoch=8.23e+6]Epoch 82/3000:   3%|▎         | 82/3000 [00:32&lt;18:04,  2.69it/s, v_num=1, train_loss_step=8.02e+6, train_loss_epoch=8.23e+6]Epoch 82/3000:   3%|▎         | 82/3000 [00:32&lt;18:04,  2.69it/s, v_num=1, train_loss_step=8.09e+6, train_loss_epoch=8.19e+6]Epoch 83/3000:   3%|▎         | 82/3000 [00:32&lt;18:04,  2.69it/s, v_num=1, train_loss_step=8.09e+6, train_loss_epoch=8.19e+6]Epoch 83/3000:   3%|▎         | 83/3000 [00:33&lt;19:34,  2.48it/s, v_num=1, train_loss_step=8.09e+6, train_loss_epoch=8.19e+6]Epoch 83/3000:   3%|▎         | 83/3000 [00:33&lt;19:34,  2.48it/s, v_num=1, train_loss_step=8.27e+6, train_loss_epoch=8.15e+6]Epoch 84/3000:   3%|▎         | 83/3000 [00:33&lt;19:34,  2.48it/s, v_num=1, train_loss_step=8.27e+6, train_loss_epoch=8.15e+6]Epoch 84/3000:   3%|▎         | 84/3000 [00:33&lt;22:14,  2.18it/s, v_num=1, train_loss_step=8.27e+6, train_loss_epoch=8.15e+6]Epoch 84/3000:   3%|▎         | 84/3000 [00:33&lt;22:14,  2.18it/s, v_num=1, train_loss_step=8.29e+6, train_loss_epoch=8.12e+6]Epoch 85/3000:   3%|▎         | 84/3000 [00:33&lt;22:14,  2.18it/s, v_num=1, train_loss_step=8.29e+6, train_loss_epoch=8.12e+6]Epoch 85/3000:   3%|▎         | 85/3000 [00:33&lt;19:25,  2.50it/s, v_num=1, train_loss_step=8.29e+6, train_loss_epoch=8.12e+6]Epoch 85/3000:   3%|▎         | 85/3000 [00:33&lt;19:25,  2.50it/s, v_num=1, train_loss_step=8.3e+6, train_loss_epoch=8.08e+6] Epoch 86/3000:   3%|▎         | 85/3000 [00:33&lt;19:25,  2.50it/s, v_num=1, train_loss_step=8.3e+6, train_loss_epoch=8.08e+6]Epoch 86/3000:   3%|▎         | 86/3000 [00:34&lt;20:46,  2.34it/s, v_num=1, train_loss_step=8.3e+6, train_loss_epoch=8.08e+6]Epoch 86/3000:   3%|▎         | 86/3000 [00:34&lt;20:46,  2.34it/s, v_num=1, train_loss_step=7.84e+6, train_loss_epoch=8.04e+6]Epoch 87/3000:   3%|▎         | 86/3000 [00:34&lt;20:46,  2.34it/s, v_num=1, train_loss_step=7.84e+6, train_loss_epoch=8.04e+6]Epoch 87/3000:   3%|▎         | 87/3000 [00:34&lt;22:06,  2.20it/s, v_num=1, train_loss_step=7.84e+6, train_loss_epoch=8.04e+6]Epoch 87/3000:   3%|▎         | 87/3000 [00:34&lt;22:06,  2.20it/s, v_num=1, train_loss_step=7.76e+6, train_loss_epoch=8e+6]   Epoch 88/3000:   3%|▎         | 87/3000 [00:34&lt;22:06,  2.20it/s, v_num=1, train_loss_step=7.76e+6, train_loss_epoch=8e+6]Epoch 88/3000:   3%|▎         | 88/3000 [00:35&lt;21:55,  2.21it/s, v_num=1, train_loss_step=7.76e+6, train_loss_epoch=8e+6]Epoch 88/3000:   3%|▎         | 88/3000 [00:35&lt;21:55,  2.21it/s, v_num=1, train_loss_step=7.92e+6, train_loss_epoch=7.96e+6]Epoch 89/3000:   3%|▎         | 88/3000 [00:35&lt;21:55,  2.21it/s, v_num=1, train_loss_step=7.92e+6, train_loss_epoch=7.96e+6]Epoch 89/3000:   3%|▎         | 89/3000 [00:35&lt;20:01,  2.42it/s, v_num=1, train_loss_step=7.92e+6, train_loss_epoch=7.96e+6]Epoch 89/3000:   3%|▎         | 89/3000 [00:35&lt;20:01,  2.42it/s, v_num=1, train_loss_step=8.01e+6, train_loss_epoch=7.92e+6]Epoch 90/3000:   3%|▎         | 89/3000 [00:35&lt;20:01,  2.42it/s, v_num=1, train_loss_step=8.01e+6, train_loss_epoch=7.92e+6]Epoch 90/3000:   3%|▎         | 90/3000 [00:36&lt;19:24,  2.50it/s, v_num=1, train_loss_step=8.01e+6, train_loss_epoch=7.92e+6]Epoch 90/3000:   3%|▎         | 90/3000 [00:36&lt;19:24,  2.50it/s, v_num=1, train_loss_step=7.63e+6, train_loss_epoch=7.89e+6]Epoch 91/3000:   3%|▎         | 90/3000 [00:36&lt;19:24,  2.50it/s, v_num=1, train_loss_step=7.63e+6, train_loss_epoch=7.89e+6]Epoch 91/3000:   3%|▎         | 91/3000 [00:36&lt;19:02,  2.55it/s, v_num=1, train_loss_step=7.63e+6, train_loss_epoch=7.89e+6]Epoch 91/3000:   3%|▎         | 91/3000 [00:36&lt;19:02,  2.55it/s, v_num=1, train_loss_step=7.84e+6, train_loss_epoch=7.85e+6]Epoch 92/3000:   3%|▎         | 91/3000 [00:36&lt;19:02,  2.55it/s, v_num=1, train_loss_step=7.84e+6, train_loss_epoch=7.85e+6]Epoch 92/3000:   3%|▎         | 92/3000 [00:37&lt;22:14,  2.18it/s, v_num=1, train_loss_step=7.84e+6, train_loss_epoch=7.85e+6]Epoch 92/3000:   3%|▎         | 92/3000 [00:37&lt;22:14,  2.18it/s, v_num=1, train_loss_step=7.69e+6, train_loss_epoch=7.81e+6]Epoch 93/3000:   3%|▎         | 92/3000 [00:37&lt;22:14,  2.18it/s, v_num=1, train_loss_step=7.69e+6, train_loss_epoch=7.81e+6]Epoch 93/3000:   3%|▎         | 93/3000 [00:37&lt;23:10,  2.09it/s, v_num=1, train_loss_step=7.69e+6, train_loss_epoch=7.81e+6]Epoch 93/3000:   3%|▎         | 93/3000 [00:37&lt;23:10,  2.09it/s, v_num=1, train_loss_step=7.85e+6, train_loss_epoch=7.77e+6]Epoch 94/3000:   3%|▎         | 93/3000 [00:37&lt;23:10,  2.09it/s, v_num=1, train_loss_step=7.85e+6, train_loss_epoch=7.77e+6]Epoch 94/3000:   3%|▎         | 94/3000 [00:37&lt;20:05,  2.41it/s, v_num=1, train_loss_step=7.85e+6, train_loss_epoch=7.77e+6]Epoch 94/3000:   3%|▎         | 94/3000 [00:37&lt;20:05,  2.41it/s, v_num=1, train_loss_step=7.73e+6, train_loss_epoch=7.74e+6]Epoch 95/3000:   3%|▎         | 94/3000 [00:37&lt;20:05,  2.41it/s, v_num=1, train_loss_step=7.73e+6, train_loss_epoch=7.74e+6]Epoch 95/3000:   3%|▎         | 95/3000 [00:38&lt;22:17,  2.17it/s, v_num=1, train_loss_step=7.73e+6, train_loss_epoch=7.74e+6]Epoch 95/3000:   3%|▎         | 95/3000 [00:38&lt;22:17,  2.17it/s, v_num=1, train_loss_step=7.56e+6, train_loss_epoch=7.7e+6] Epoch 96/3000:   3%|▎         | 95/3000 [00:38&lt;22:17,  2.17it/s, v_num=1, train_loss_step=7.56e+6, train_loss_epoch=7.7e+6]Epoch 96/3000:   3%|▎         | 96/3000 [00:38&lt;22:12,  2.18it/s, v_num=1, train_loss_step=7.56e+6, train_loss_epoch=7.7e+6]Epoch 96/3000:   3%|▎         | 96/3000 [00:38&lt;22:12,  2.18it/s, v_num=1, train_loss_step=7.68e+6, train_loss_epoch=7.67e+6]Epoch 97/3000:   3%|▎         | 96/3000 [00:38&lt;22:12,  2.18it/s, v_num=1, train_loss_step=7.68e+6, train_loss_epoch=7.67e+6]Epoch 97/3000:   3%|▎         | 97/3000 [00:39&lt;21:26,  2.26it/s, v_num=1, train_loss_step=7.68e+6, train_loss_epoch=7.67e+6]Epoch 97/3000:   3%|▎         | 97/3000 [00:39&lt;21:26,  2.26it/s, v_num=1, train_loss_step=7.57e+6, train_loss_epoch=7.63e+6]Epoch 98/3000:   3%|▎         | 97/3000 [00:39&lt;21:26,  2.26it/s, v_num=1, train_loss_step=7.57e+6, train_loss_epoch=7.63e+6]Epoch 98/3000:   3%|▎         | 98/3000 [00:39&lt;20:39,  2.34it/s, v_num=1, train_loss_step=7.57e+6, train_loss_epoch=7.63e+6]Epoch 98/3000:   3%|▎         | 98/3000 [00:39&lt;20:39,  2.34it/s, v_num=1, train_loss_step=7.52e+6, train_loss_epoch=7.59e+6]Epoch 99/3000:   3%|▎         | 98/3000 [00:39&lt;20:39,  2.34it/s, v_num=1, train_loss_step=7.52e+6, train_loss_epoch=7.59e+6]Epoch 99/3000:   3%|▎         | 99/3000 [00:40&lt;20:21,  2.38it/s, v_num=1, train_loss_step=7.52e+6, train_loss_epoch=7.59e+6]Epoch 99/3000:   3%|▎         | 99/3000 [00:40&lt;20:21,  2.38it/s, v_num=1, train_loss_step=7.63e+6, train_loss_epoch=7.56e+6]Epoch 100/3000:   3%|▎         | 99/3000 [00:40&lt;20:21,  2.38it/s, v_num=1, train_loss_step=7.63e+6, train_loss_epoch=7.56e+6]Epoch 100/3000:   3%|▎         | 100/3000 [00:40&lt;18:25,  2.62it/s, v_num=1, train_loss_step=7.63e+6, train_loss_epoch=7.56e+6]Epoch 100/3000:   3%|▎         | 100/3000 [00:40&lt;18:25,  2.62it/s, v_num=1, train_loss_step=7.77e+6, train_loss_epoch=7.52e+6]Epoch 101/3000:   3%|▎         | 100/3000 [00:40&lt;18:25,  2.62it/s, v_num=1, train_loss_step=7.77e+6, train_loss_epoch=7.52e+6]Epoch 101/3000:   3%|▎         | 101/3000 [00:40&lt;17:45,  2.72it/s, v_num=1, train_loss_step=7.77e+6, train_loss_epoch=7.52e+6]Epoch 101/3000:   3%|▎         | 101/3000 [00:40&lt;17:45,  2.72it/s, v_num=1, train_loss_step=7.65e+6, train_loss_epoch=7.49e+6]Epoch 102/3000:   3%|▎         | 101/3000 [00:40&lt;17:45,  2.72it/s, v_num=1, train_loss_step=7.65e+6, train_loss_epoch=7.49e+6]Epoch 102/3000:   3%|▎         | 102/3000 [00:40&lt;17:10,  2.81it/s, v_num=1, train_loss_step=7.65e+6, train_loss_epoch=7.49e+6]Epoch 102/3000:   3%|▎         | 102/3000 [00:41&lt;17:10,  2.81it/s, v_num=1, train_loss_step=7.43e+6, train_loss_epoch=7.45e+6]Epoch 103/3000:   3%|▎         | 102/3000 [00:41&lt;17:10,  2.81it/s, v_num=1, train_loss_step=7.43e+6, train_loss_epoch=7.45e+6]Epoch 103/3000:   3%|▎         | 103/3000 [00:41&lt;20:40,  2.34it/s, v_num=1, train_loss_step=7.43e+6, train_loss_epoch=7.45e+6]Epoch 103/3000:   3%|▎         | 103/3000 [00:41&lt;20:40,  2.34it/s, v_num=1, train_loss_step=7.43e+6, train_loss_epoch=7.42e+6]Epoch 104/3000:   3%|▎         | 103/3000 [00:41&lt;20:40,  2.34it/s, v_num=1, train_loss_step=7.43e+6, train_loss_epoch=7.42e+6]Epoch 104/3000:   3%|▎         | 104/3000 [00:41&lt;19:24,  2.49it/s, v_num=1, train_loss_step=7.43e+6, train_loss_epoch=7.42e+6]Epoch 104/3000:   3%|▎         | 104/3000 [00:41&lt;19:24,  2.49it/s, v_num=1, train_loss_step=7.81e+6, train_loss_epoch=7.39e+6]Epoch 105/3000:   3%|▎         | 104/3000 [00:41&lt;19:24,  2.49it/s, v_num=1, train_loss_step=7.81e+6, train_loss_epoch=7.39e+6]Epoch 105/3000:   4%|▎         | 105/3000 [00:42&lt;17:30,  2.76it/s, v_num=1, train_loss_step=7.81e+6, train_loss_epoch=7.39e+6]Epoch 105/3000:   4%|▎         | 105/3000 [00:42&lt;17:30,  2.76it/s, v_num=1, train_loss_step=7.28e+6, train_loss_epoch=7.35e+6]Epoch 106/3000:   4%|▎         | 105/3000 [00:42&lt;17:30,  2.76it/s, v_num=1, train_loss_step=7.28e+6, train_loss_epoch=7.35e+6]Epoch 106/3000:   4%|▎         | 106/3000 [00:42&lt;17:52,  2.70it/s, v_num=1, train_loss_step=7.28e+6, train_loss_epoch=7.35e+6]Epoch 106/3000:   4%|▎         | 106/3000 [00:42&lt;17:52,  2.70it/s, v_num=1, train_loss_step=7.32e+6, train_loss_epoch=7.32e+6]Epoch 107/3000:   4%|▎         | 106/3000 [00:42&lt;17:52,  2.70it/s, v_num=1, train_loss_step=7.32e+6, train_loss_epoch=7.32e+6]Epoch 107/3000:   4%|▎         | 107/3000 [00:43&lt;18:30,  2.61it/s, v_num=1, train_loss_step=7.32e+6, train_loss_epoch=7.32e+6]Epoch 107/3000:   4%|▎         | 107/3000 [00:43&lt;18:30,  2.61it/s, v_num=1, train_loss_step=7.22e+6, train_loss_epoch=7.29e+6]Epoch 108/3000:   4%|▎         | 107/3000 [00:43&lt;18:30,  2.61it/s, v_num=1, train_loss_step=7.22e+6, train_loss_epoch=7.29e+6]Epoch 108/3000:   4%|▎         | 108/3000 [00:43&lt;22:00,  2.19it/s, v_num=1, train_loss_step=7.22e+6, train_loss_epoch=7.29e+6]Epoch 108/3000:   4%|▎         | 108/3000 [00:43&lt;22:00,  2.19it/s, v_num=1, train_loss_step=7.07e+6, train_loss_epoch=7.25e+6]Epoch 109/3000:   4%|▎         | 108/3000 [00:43&lt;22:00,  2.19it/s, v_num=1, train_loss_step=7.07e+6, train_loss_epoch=7.25e+6]Epoch 109/3000:   4%|▎         | 109/3000 [00:43&lt;19:27,  2.48it/s, v_num=1, train_loss_step=7.07e+6, train_loss_epoch=7.25e+6]Epoch 109/3000:   4%|▎         | 109/3000 [00:43&lt;19:27,  2.48it/s, v_num=1, train_loss_step=7.05e+6, train_loss_epoch=7.22e+6]Epoch 110/3000:   4%|▎         | 109/3000 [00:43&lt;19:27,  2.48it/s, v_num=1, train_loss_step=7.05e+6, train_loss_epoch=7.22e+6]Epoch 110/3000:   4%|▎         | 110/3000 [00:44&lt;18:16,  2.63it/s, v_num=1, train_loss_step=7.05e+6, train_loss_epoch=7.22e+6]Epoch 110/3000:   4%|▎         | 110/3000 [00:44&lt;18:16,  2.63it/s, v_num=1, train_loss_step=7.2e+6, train_loss_epoch=7.19e+6] Epoch 111/3000:   4%|▎         | 110/3000 [00:44&lt;18:16,  2.63it/s, v_num=1, train_loss_step=7.2e+6, train_loss_epoch=7.19e+6]Epoch 111/3000:   4%|▎         | 111/3000 [00:44&lt;17:16,  2.79it/s, v_num=1, train_loss_step=7.2e+6, train_loss_epoch=7.19e+6]Epoch 111/3000:   4%|▎         | 111/3000 [00:44&lt;17:16,  2.79it/s, v_num=1, train_loss_step=7.02e+6, train_loss_epoch=7.15e+6]Epoch 112/3000:   4%|▎         | 111/3000 [00:44&lt;17:16,  2.79it/s, v_num=1, train_loss_step=7.02e+6, train_loss_epoch=7.15e+6]Epoch 112/3000:   4%|▎         | 112/3000 [00:44&lt;18:34,  2.59it/s, v_num=1, train_loss_step=7.02e+6, train_loss_epoch=7.15e+6]Epoch 112/3000:   4%|▎         | 112/3000 [00:44&lt;18:34,  2.59it/s, v_num=1, train_loss_step=7.13e+6, train_loss_epoch=7.12e+6]Epoch 113/3000:   4%|▎         | 112/3000 [00:45&lt;18:34,  2.59it/s, v_num=1, train_loss_step=7.13e+6, train_loss_epoch=7.12e+6]Epoch 113/3000:   4%|▍         | 113/3000 [00:45&lt;20:01,  2.40it/s, v_num=1, train_loss_step=7.13e+6, train_loss_epoch=7.12e+6]Epoch 113/3000:   4%|▍         | 113/3000 [00:45&lt;20:01,  2.40it/s, v_num=1, train_loss_step=7.17e+6, train_loss_epoch=7.09e+6]Epoch 114/3000:   4%|▍         | 113/3000 [00:45&lt;20:01,  2.40it/s, v_num=1, train_loss_step=7.17e+6, train_loss_epoch=7.09e+6]Epoch 114/3000:   4%|▍         | 114/3000 [00:45&lt;19:29,  2.47it/s, v_num=1, train_loss_step=7.17e+6, train_loss_epoch=7.09e+6]Epoch 114/3000:   4%|▍         | 114/3000 [00:45&lt;19:29,  2.47it/s, v_num=1, train_loss_step=6.84e+6, train_loss_epoch=7.06e+6]Epoch 115/3000:   4%|▍         | 114/3000 [00:45&lt;19:29,  2.47it/s, v_num=1, train_loss_step=6.84e+6, train_loss_epoch=7.06e+6]Epoch 115/3000:   4%|▍         | 115/3000 [00:46&lt;18:30,  2.60it/s, v_num=1, train_loss_step=6.84e+6, train_loss_epoch=7.06e+6]Epoch 115/3000:   4%|▍         | 115/3000 [00:46&lt;18:30,  2.60it/s, v_num=1, train_loss_step=7.15e+6, train_loss_epoch=7.03e+6]Epoch 116/3000:   4%|▍         | 115/3000 [00:46&lt;18:30,  2.60it/s, v_num=1, train_loss_step=7.15e+6, train_loss_epoch=7.03e+6]Epoch 116/3000:   4%|▍         | 116/3000 [00:46&lt;21:13,  2.26it/s, v_num=1, train_loss_step=7.15e+6, train_loss_epoch=7.03e+6]Epoch 116/3000:   4%|▍         | 116/3000 [00:46&lt;21:13,  2.26it/s, v_num=1, train_loss_step=6.86e+6, train_loss_epoch=7e+6]   Epoch 117/3000:   4%|▍         | 116/3000 [00:46&lt;21:13,  2.26it/s, v_num=1, train_loss_step=6.86e+6, train_loss_epoch=7e+6]Epoch 117/3000:   4%|▍         | 117/3000 [00:47&lt;22:25,  2.14it/s, v_num=1, train_loss_step=6.86e+6, train_loss_epoch=7e+6]Epoch 117/3000:   4%|▍         | 117/3000 [00:47&lt;22:25,  2.14it/s, v_num=1, train_loss_step=6.74e+6, train_loss_epoch=6.97e+6]Epoch 118/3000:   4%|▍         | 117/3000 [00:47&lt;22:25,  2.14it/s, v_num=1, train_loss_step=6.74e+6, train_loss_epoch=6.97e+6]Epoch 118/3000:   4%|▍         | 118/3000 [00:47&lt;23:51,  2.01it/s, v_num=1, train_loss_step=6.74e+6, train_loss_epoch=6.97e+6]Epoch 118/3000:   4%|▍         | 118/3000 [00:47&lt;23:51,  2.01it/s, v_num=1, train_loss_step=6.85e+6, train_loss_epoch=6.93e+6]Epoch 119/3000:   4%|▍         | 118/3000 [00:47&lt;23:51,  2.01it/s, v_num=1, train_loss_step=6.85e+6, train_loss_epoch=6.93e+6]Epoch 119/3000:   4%|▍         | 119/3000 [00:48&lt;24:00,  2.00it/s, v_num=1, train_loss_step=6.85e+6, train_loss_epoch=6.93e+6]Epoch 119/3000:   4%|▍         | 119/3000 [00:48&lt;24:00,  2.00it/s, v_num=1, train_loss_step=6.81e+6, train_loss_epoch=6.9e+6] Epoch 120/3000:   4%|▍         | 119/3000 [00:48&lt;24:00,  2.00it/s, v_num=1, train_loss_step=6.81e+6, train_loss_epoch=6.9e+6]Epoch 120/3000:   4%|▍         | 120/3000 [00:48&lt;25:11,  1.91it/s, v_num=1, train_loss_step=6.81e+6, train_loss_epoch=6.9e+6]Epoch 120/3000:   4%|▍         | 120/3000 [00:48&lt;25:11,  1.91it/s, v_num=1, train_loss_step=6.83e+6, train_loss_epoch=6.87e+6]Epoch 121/3000:   4%|▍         | 120/3000 [00:48&lt;25:11,  1.91it/s, v_num=1, train_loss_step=6.83e+6, train_loss_epoch=6.87e+6]Epoch 121/3000:   4%|▍         | 121/3000 [00:49&lt;23:44,  2.02it/s, v_num=1, train_loss_step=6.83e+6, train_loss_epoch=6.87e+6]Epoch 121/3000:   4%|▍         | 121/3000 [00:49&lt;23:44,  2.02it/s, v_num=1, train_loss_step=6.67e+6, train_loss_epoch=6.84e+6]Epoch 122/3000:   4%|▍         | 121/3000 [00:49&lt;23:44,  2.02it/s, v_num=1, train_loss_step=6.67e+6, train_loss_epoch=6.84e+6]Epoch 122/3000:   4%|▍         | 122/3000 [00:49&lt;22:57,  2.09it/s, v_num=1, train_loss_step=6.67e+6, train_loss_epoch=6.84e+6]Epoch 122/3000:   4%|▍         | 122/3000 [00:49&lt;22:57,  2.09it/s, v_num=1, train_loss_step=6.83e+6, train_loss_epoch=6.81e+6]Epoch 123/3000:   4%|▍         | 122/3000 [00:49&lt;22:57,  2.09it/s, v_num=1, train_loss_step=6.83e+6, train_loss_epoch=6.81e+6]Epoch 123/3000:   4%|▍         | 123/3000 [00:50&lt;24:34,  1.95it/s, v_num=1, train_loss_step=6.83e+6, train_loss_epoch=6.81e+6]Epoch 123/3000:   4%|▍         | 123/3000 [00:50&lt;24:34,  1.95it/s, v_num=1, train_loss_step=6.68e+6, train_loss_epoch=6.78e+6]Epoch 124/3000:   4%|▍         | 123/3000 [00:50&lt;24:34,  1.95it/s, v_num=1, train_loss_step=6.68e+6, train_loss_epoch=6.78e+6]Epoch 124/3000:   4%|▍         | 124/3000 [00:50&lt;22:28,  2.13it/s, v_num=1, train_loss_step=6.68e+6, train_loss_epoch=6.78e+6]Epoch 124/3000:   4%|▍         | 124/3000 [00:50&lt;22:28,  2.13it/s, v_num=1, train_loss_step=6.68e+6, train_loss_epoch=6.76e+6]Epoch 125/3000:   4%|▍         | 124/3000 [00:50&lt;22:28,  2.13it/s, v_num=1, train_loss_step=6.68e+6, train_loss_epoch=6.76e+6]Epoch 125/3000:   4%|▍         | 125/3000 [00:51&lt;22:40,  2.11it/s, v_num=1, train_loss_step=6.68e+6, train_loss_epoch=6.76e+6]Epoch 125/3000:   4%|▍         | 125/3000 [00:51&lt;22:40,  2.11it/s, v_num=1, train_loss_step=6.42e+6, train_loss_epoch=6.73e+6]Epoch 126/3000:   4%|▍         | 125/3000 [00:51&lt;22:40,  2.11it/s, v_num=1, train_loss_step=6.42e+6, train_loss_epoch=6.73e+6]Epoch 126/3000:   4%|▍         | 126/3000 [00:51&lt;24:00,  1.99it/s, v_num=1, train_loss_step=6.42e+6, train_loss_epoch=6.73e+6]Epoch 126/3000:   4%|▍         | 126/3000 [00:51&lt;24:00,  1.99it/s, v_num=1, train_loss_step=6.83e+6, train_loss_epoch=6.7e+6] Epoch 127/3000:   4%|▍         | 126/3000 [00:51&lt;24:00,  1.99it/s, v_num=1, train_loss_step=6.83e+6, train_loss_epoch=6.7e+6]Epoch 127/3000:   4%|▍         | 127/3000 [00:52&lt;23:05,  2.07it/s, v_num=1, train_loss_step=6.83e+6, train_loss_epoch=6.7e+6]Epoch 127/3000:   4%|▍         | 127/3000 [00:52&lt;23:05,  2.07it/s, v_num=1, train_loss_step=6.89e+6, train_loss_epoch=6.67e+6]Epoch 128/3000:   4%|▍         | 127/3000 [00:52&lt;23:05,  2.07it/s, v_num=1, train_loss_step=6.89e+6, train_loss_epoch=6.67e+6]Epoch 128/3000:   4%|▍         | 128/3000 [00:52&lt;24:53,  1.92it/s, v_num=1, train_loss_step=6.89e+6, train_loss_epoch=6.67e+6]Epoch 128/3000:   4%|▍         | 128/3000 [00:52&lt;24:53,  1.92it/s, v_num=1, train_loss_step=6.69e+6, train_loss_epoch=6.64e+6]Epoch 129/3000:   4%|▍         | 128/3000 [00:52&lt;24:53,  1.92it/s, v_num=1, train_loss_step=6.69e+6, train_loss_epoch=6.64e+6]Epoch 129/3000:   4%|▍         | 129/3000 [00:53&lt;25:48,  1.85it/s, v_num=1, train_loss_step=6.69e+6, train_loss_epoch=6.64e+6]Epoch 129/3000:   4%|▍         | 129/3000 [00:53&lt;25:48,  1.85it/s, v_num=1, train_loss_step=6.45e+6, train_loss_epoch=6.61e+6]Epoch 130/3000:   4%|▍         | 129/3000 [00:53&lt;25:48,  1.85it/s, v_num=1, train_loss_step=6.45e+6, train_loss_epoch=6.61e+6]Epoch 130/3000:   4%|▍         | 130/3000 [00:54&lt;27:01,  1.77it/s, v_num=1, train_loss_step=6.45e+6, train_loss_epoch=6.61e+6]Epoch 130/3000:   4%|▍         | 130/3000 [00:54&lt;27:01,  1.77it/s, v_num=1, train_loss_step=6.41e+6, train_loss_epoch=6.58e+6]Epoch 131/3000:   4%|▍         | 130/3000 [00:54&lt;27:01,  1.77it/s, v_num=1, train_loss_step=6.41e+6, train_loss_epoch=6.58e+6]Epoch 131/3000:   4%|▍         | 131/3000 [00:54&lt;23:55,  2.00it/s, v_num=1, train_loss_step=6.41e+6, train_loss_epoch=6.58e+6]Epoch 131/3000:   4%|▍         | 131/3000 [00:54&lt;23:55,  2.00it/s, v_num=1, train_loss_step=6.68e+6, train_loss_epoch=6.56e+6]Epoch 132/3000:   4%|▍         | 131/3000 [00:54&lt;23:55,  2.00it/s, v_num=1, train_loss_step=6.68e+6, train_loss_epoch=6.56e+6]Epoch 132/3000:   4%|▍         | 132/3000 [00:54&lt;21:14,  2.25it/s, v_num=1, train_loss_step=6.68e+6, train_loss_epoch=6.56e+6]Epoch 132/3000:   4%|▍         | 132/3000 [00:54&lt;21:14,  2.25it/s, v_num=1, train_loss_step=6.49e+6, train_loss_epoch=6.53e+6]Epoch 133/3000:   4%|▍         | 132/3000 [00:54&lt;21:14,  2.25it/s, v_num=1, train_loss_step=6.49e+6, train_loss_epoch=6.53e+6]Epoch 133/3000:   4%|▍         | 133/3000 [00:55&lt;20:08,  2.37it/s, v_num=1, train_loss_step=6.49e+6, train_loss_epoch=6.53e+6]Epoch 133/3000:   4%|▍         | 133/3000 [00:55&lt;20:08,  2.37it/s, v_num=1, train_loss_step=6.25e+6, train_loss_epoch=6.5e+6] Epoch 134/3000:   4%|▍         | 133/3000 [00:55&lt;20:08,  2.37it/s, v_num=1, train_loss_step=6.25e+6, train_loss_epoch=6.5e+6]Epoch 134/3000:   4%|▍         | 134/3000 [00:55&lt;22:24,  2.13it/s, v_num=1, train_loss_step=6.25e+6, train_loss_epoch=6.5e+6]Epoch 134/3000:   4%|▍         | 134/3000 [00:55&lt;22:24,  2.13it/s, v_num=1, train_loss_step=6.54e+6, train_loss_epoch=6.47e+6]Epoch 135/3000:   4%|▍         | 134/3000 [00:55&lt;22:24,  2.13it/s, v_num=1, train_loss_step=6.54e+6, train_loss_epoch=6.47e+6]Epoch 135/3000:   4%|▍         | 135/3000 [00:56&lt;22:04,  2.16it/s, v_num=1, train_loss_step=6.54e+6, train_loss_epoch=6.47e+6]Epoch 135/3000:   4%|▍         | 135/3000 [00:56&lt;22:04,  2.16it/s, v_num=1, train_loss_step=6.46e+6, train_loss_epoch=6.45e+6]Epoch 136/3000:   4%|▍         | 135/3000 [00:56&lt;22:04,  2.16it/s, v_num=1, train_loss_step=6.46e+6, train_loss_epoch=6.45e+6]Epoch 136/3000:   5%|▍         | 136/3000 [00:56&lt;22:53,  2.09it/s, v_num=1, train_loss_step=6.46e+6, train_loss_epoch=6.45e+6]Epoch 136/3000:   5%|▍         | 136/3000 [00:56&lt;22:53,  2.09it/s, v_num=1, train_loss_step=6.44e+6, train_loss_epoch=6.42e+6]Epoch 137/3000:   5%|▍         | 136/3000 [00:56&lt;22:53,  2.09it/s, v_num=1, train_loss_step=6.44e+6, train_loss_epoch=6.42e+6]Epoch 137/3000:   5%|▍         | 137/3000 [00:57&lt;20:55,  2.28it/s, v_num=1, train_loss_step=6.44e+6, train_loss_epoch=6.42e+6]Epoch 137/3000:   5%|▍         | 137/3000 [00:57&lt;20:55,  2.28it/s, v_num=1, train_loss_step=6.47e+6, train_loss_epoch=6.39e+6]Epoch 138/3000:   5%|▍         | 137/3000 [00:57&lt;20:55,  2.28it/s, v_num=1, train_loss_step=6.47e+6, train_loss_epoch=6.39e+6]Epoch 138/3000:   5%|▍         | 138/3000 [00:57&lt;19:15,  2.48it/s, v_num=1, train_loss_step=6.47e+6, train_loss_epoch=6.39e+6]Epoch 138/3000:   5%|▍         | 138/3000 [00:57&lt;19:15,  2.48it/s, v_num=1, train_loss_step=6.38e+6, train_loss_epoch=6.37e+6]Epoch 139/3000:   5%|▍         | 138/3000 [00:57&lt;19:15,  2.48it/s, v_num=1, train_loss_step=6.38e+6, train_loss_epoch=6.37e+6]Epoch 139/3000:   5%|▍         | 139/3000 [00:57&lt;19:11,  2.48it/s, v_num=1, train_loss_step=6.38e+6, train_loss_epoch=6.37e+6]Epoch 139/3000:   5%|▍         | 139/3000 [00:57&lt;19:11,  2.48it/s, v_num=1, train_loss_step=6.23e+6, train_loss_epoch=6.34e+6]Epoch 140/3000:   5%|▍         | 139/3000 [00:57&lt;19:11,  2.48it/s, v_num=1, train_loss_step=6.23e+6, train_loss_epoch=6.34e+6]Epoch 140/3000:   5%|▍         | 140/3000 [00:58&lt;19:01,  2.50it/s, v_num=1, train_loss_step=6.23e+6, train_loss_epoch=6.34e+6]Epoch 140/3000:   5%|▍         | 140/3000 [00:58&lt;19:01,  2.50it/s, v_num=1, train_loss_step=6.66e+6, train_loss_epoch=6.31e+6]Epoch 141/3000:   5%|▍         | 140/3000 [00:58&lt;19:01,  2.50it/s, v_num=1, train_loss_step=6.66e+6, train_loss_epoch=6.31e+6]Epoch 141/3000:   5%|▍         | 141/3000 [00:58&lt;21:02,  2.26it/s, v_num=1, train_loss_step=6.66e+6, train_loss_epoch=6.31e+6]Epoch 141/3000:   5%|▍         | 141/3000 [00:58&lt;21:02,  2.26it/s, v_num=1, train_loss_step=6.39e+6, train_loss_epoch=6.29e+6]Epoch 142/3000:   5%|▍         | 141/3000 [00:58&lt;21:02,  2.26it/s, v_num=1, train_loss_step=6.39e+6, train_loss_epoch=6.29e+6]Epoch 142/3000:   5%|▍         | 142/3000 [00:59&lt;21:15,  2.24it/s, v_num=1, train_loss_step=6.39e+6, train_loss_epoch=6.29e+6]Epoch 142/3000:   5%|▍         | 142/3000 [00:59&lt;21:15,  2.24it/s, v_num=1, train_loss_step=6.44e+6, train_loss_epoch=6.26e+6]Epoch 143/3000:   5%|▍         | 142/3000 [00:59&lt;21:15,  2.24it/s, v_num=1, train_loss_step=6.44e+6, train_loss_epoch=6.26e+6]Epoch 143/3000:   5%|▍         | 143/3000 [00:59&lt;23:04,  2.06it/s, v_num=1, train_loss_step=6.44e+6, train_loss_epoch=6.26e+6]Epoch 143/3000:   5%|▍         | 143/3000 [00:59&lt;23:04,  2.06it/s, v_num=1, train_loss_step=6.53e+6, train_loss_epoch=6.24e+6]Epoch 144/3000:   5%|▍         | 143/3000 [00:59&lt;23:04,  2.06it/s, v_num=1, train_loss_step=6.53e+6, train_loss_epoch=6.24e+6]Epoch 144/3000:   5%|▍         | 144/3000 [01:00&lt;22:37,  2.10it/s, v_num=1, train_loss_step=6.53e+6, train_loss_epoch=6.24e+6]Epoch 144/3000:   5%|▍         | 144/3000 [01:00&lt;22:37,  2.10it/s, v_num=1, train_loss_step=5.77e+6, train_loss_epoch=6.21e+6]Epoch 145/3000:   5%|▍         | 144/3000 [01:00&lt;22:37,  2.10it/s, v_num=1, train_loss_step=5.77e+6, train_loss_epoch=6.21e+6]Epoch 145/3000:   5%|▍         | 145/3000 [01:00&lt;19:41,  2.42it/s, v_num=1, train_loss_step=5.77e+6, train_loss_epoch=6.21e+6]Epoch 145/3000:   5%|▍         | 145/3000 [01:00&lt;19:41,  2.42it/s, v_num=1, train_loss_step=5.85e+6, train_loss_epoch=6.19e+6]Epoch 146/3000:   5%|▍         | 145/3000 [01:00&lt;19:41,  2.42it/s, v_num=1, train_loss_step=5.85e+6, train_loss_epoch=6.19e+6]Epoch 146/3000:   5%|▍         | 146/3000 [01:00&lt;20:34,  2.31it/s, v_num=1, train_loss_step=5.85e+6, train_loss_epoch=6.19e+6]Epoch 146/3000:   5%|▍         | 146/3000 [01:00&lt;20:34,  2.31it/s, v_num=1, train_loss_step=5.9e+6, train_loss_epoch=6.16e+6] Epoch 147/3000:   5%|▍         | 146/3000 [01:00&lt;20:34,  2.31it/s, v_num=1, train_loss_step=5.9e+6, train_loss_epoch=6.16e+6]Epoch 147/3000:   5%|▍         | 147/3000 [01:01&lt;20:00,  2.38it/s, v_num=1, train_loss_step=5.9e+6, train_loss_epoch=6.16e+6]Epoch 147/3000:   5%|▍         | 147/3000 [01:01&lt;20:00,  2.38it/s, v_num=1, train_loss_step=6.22e+6, train_loss_epoch=6.14e+6]Epoch 148/3000:   5%|▍         | 147/3000 [01:01&lt;20:00,  2.38it/s, v_num=1, train_loss_step=6.22e+6, train_loss_epoch=6.14e+6]Epoch 148/3000:   5%|▍         | 148/3000 [01:01&lt;20:37,  2.31it/s, v_num=1, train_loss_step=6.22e+6, train_loss_epoch=6.14e+6]Epoch 148/3000:   5%|▍         | 148/3000 [01:01&lt;20:37,  2.31it/s, v_num=1, train_loss_step=6.08e+6, train_loss_epoch=6.11e+6]Epoch 149/3000:   5%|▍         | 148/3000 [01:01&lt;20:37,  2.31it/s, v_num=1, train_loss_step=6.08e+6, train_loss_epoch=6.11e+6]Epoch 149/3000:   5%|▍         | 149/3000 [01:02&lt;21:47,  2.18it/s, v_num=1, train_loss_step=6.08e+6, train_loss_epoch=6.11e+6]Epoch 149/3000:   5%|▍         | 149/3000 [01:02&lt;21:47,  2.18it/s, v_num=1, train_loss_step=5.97e+6, train_loss_epoch=6.09e+6]Epoch 150/3000:   5%|▍         | 149/3000 [01:02&lt;21:47,  2.18it/s, v_num=1, train_loss_step=5.97e+6, train_loss_epoch=6.09e+6]Epoch 150/3000:   5%|▌         | 150/3000 [01:02&lt;21:46,  2.18it/s, v_num=1, train_loss_step=5.97e+6, train_loss_epoch=6.09e+6]Epoch 150/3000:   5%|▌         | 150/3000 [01:02&lt;21:46,  2.18it/s, v_num=1, train_loss_step=5.7e+6, train_loss_epoch=6.06e+6] Epoch 151/3000:   5%|▌         | 150/3000 [01:02&lt;21:46,  2.18it/s, v_num=1, train_loss_step=5.7e+6, train_loss_epoch=6.06e+6]Epoch 151/3000:   5%|▌         | 151/3000 [01:03&lt;21:52,  2.17it/s, v_num=1, train_loss_step=5.7e+6, train_loss_epoch=6.06e+6]Epoch 151/3000:   5%|▌         | 151/3000 [01:03&lt;21:52,  2.17it/s, v_num=1, train_loss_step=5.88e+6, train_loss_epoch=6.04e+6]Epoch 152/3000:   5%|▌         | 151/3000 [01:03&lt;21:52,  2.17it/s, v_num=1, train_loss_step=5.88e+6, train_loss_epoch=6.04e+6]Epoch 152/3000:   5%|▌         | 152/3000 [01:03&lt;19:15,  2.46it/s, v_num=1, train_loss_step=5.88e+6, train_loss_epoch=6.04e+6]Epoch 152/3000:   5%|▌         | 152/3000 [01:03&lt;19:15,  2.46it/s, v_num=1, train_loss_step=6.16e+6, train_loss_epoch=6.02e+6]Epoch 153/3000:   5%|▌         | 152/3000 [01:03&lt;19:15,  2.46it/s, v_num=1, train_loss_step=6.16e+6, train_loss_epoch=6.02e+6]Epoch 153/3000:   5%|▌         | 153/3000 [01:03&lt;19:06,  2.48it/s, v_num=1, train_loss_step=6.16e+6, train_loss_epoch=6.02e+6]Epoch 153/3000:   5%|▌         | 153/3000 [01:03&lt;19:06,  2.48it/s, v_num=1, train_loss_step=6.09e+6, train_loss_epoch=5.99e+6]Epoch 154/3000:   5%|▌         | 153/3000 [01:03&lt;19:06,  2.48it/s, v_num=1, train_loss_step=6.09e+6, train_loss_epoch=5.99e+6]Epoch 154/3000:   5%|▌         | 154/3000 [01:04&lt;20:01,  2.37it/s, v_num=1, train_loss_step=6.09e+6, train_loss_epoch=5.99e+6]Epoch 154/3000:   5%|▌         | 154/3000 [01:04&lt;20:01,  2.37it/s, v_num=1, train_loss_step=6.21e+6, train_loss_epoch=5.97e+6]Epoch 155/3000:   5%|▌         | 154/3000 [01:04&lt;20:01,  2.37it/s, v_num=1, train_loss_step=6.21e+6, train_loss_epoch=5.97e+6]Epoch 155/3000:   5%|▌         | 155/3000 [01:04&lt;22:31,  2.10it/s, v_num=1, train_loss_step=6.21e+6, train_loss_epoch=5.97e+6]Epoch 155/3000:   5%|▌         | 155/3000 [01:04&lt;22:31,  2.10it/s, v_num=1, train_loss_step=6.27e+6, train_loss_epoch=5.94e+6]Epoch 156/3000:   5%|▌         | 155/3000 [01:04&lt;22:31,  2.10it/s, v_num=1, train_loss_step=6.27e+6, train_loss_epoch=5.94e+6]Epoch 156/3000:   5%|▌         | 156/3000 [01:05&lt;23:08,  2.05it/s, v_num=1, train_loss_step=6.27e+6, train_loss_epoch=5.94e+6]Epoch 156/3000:   5%|▌         | 156/3000 [01:05&lt;23:08,  2.05it/s, v_num=1, train_loss_step=5.82e+6, train_loss_epoch=5.92e+6]Epoch 157/3000:   5%|▌         | 156/3000 [01:05&lt;23:08,  2.05it/s, v_num=1, train_loss_step=5.82e+6, train_loss_epoch=5.92e+6]Epoch 157/3000:   5%|▌         | 157/3000 [01:05&lt;19:59,  2.37it/s, v_num=1, train_loss_step=5.82e+6, train_loss_epoch=5.92e+6]Epoch 157/3000:   5%|▌         | 157/3000 [01:05&lt;19:59,  2.37it/s, v_num=1, train_loss_step=5.84e+6, train_loss_epoch=5.9e+6] Epoch 158/3000:   5%|▌         | 157/3000 [01:05&lt;19:59,  2.37it/s, v_num=1, train_loss_step=5.84e+6, train_loss_epoch=5.9e+6]Epoch 158/3000:   5%|▌         | 158/3000 [01:06&lt;21:20,  2.22it/s, v_num=1, train_loss_step=5.84e+6, train_loss_epoch=5.9e+6]Epoch 158/3000:   5%|▌         | 158/3000 [01:06&lt;21:20,  2.22it/s, v_num=1, train_loss_step=5.85e+6, train_loss_epoch=5.88e+6]Epoch 159/3000:   5%|▌         | 158/3000 [01:06&lt;21:20,  2.22it/s, v_num=1, train_loss_step=5.85e+6, train_loss_epoch=5.88e+6]Epoch 159/3000:   5%|▌         | 159/3000 [01:06&lt;21:16,  2.23it/s, v_num=1, train_loss_step=5.85e+6, train_loss_epoch=5.88e+6]Epoch 159/3000:   5%|▌         | 159/3000 [01:06&lt;21:16,  2.23it/s, v_num=1, train_loss_step=5.99e+6, train_loss_epoch=5.85e+6]Epoch 160/3000:   5%|▌         | 159/3000 [01:06&lt;21:16,  2.23it/s, v_num=1, train_loss_step=5.99e+6, train_loss_epoch=5.85e+6]Epoch 160/3000:   5%|▌         | 160/3000 [01:07&lt;20:50,  2.27it/s, v_num=1, train_loss_step=5.99e+6, train_loss_epoch=5.85e+6]Epoch 160/3000:   5%|▌         | 160/3000 [01:07&lt;20:50,  2.27it/s, v_num=1, train_loss_step=5.94e+6, train_loss_epoch=5.83e+6]Epoch 161/3000:   5%|▌         | 160/3000 [01:07&lt;20:50,  2.27it/s, v_num=1, train_loss_step=5.94e+6, train_loss_epoch=5.83e+6]Epoch 161/3000:   5%|▌         | 161/3000 [01:07&lt;21:56,  2.16it/s, v_num=1, train_loss_step=5.94e+6, train_loss_epoch=5.83e+6]Epoch 161/3000:   5%|▌         | 161/3000 [01:07&lt;21:56,  2.16it/s, v_num=1, train_loss_step=5.63e+6, train_loss_epoch=5.81e+6]Epoch 162/3000:   5%|▌         | 161/3000 [01:07&lt;21:56,  2.16it/s, v_num=1, train_loss_step=5.63e+6, train_loss_epoch=5.81e+6]Epoch 162/3000:   5%|▌         | 162/3000 [01:08&lt;21:04,  2.24it/s, v_num=1, train_loss_step=5.63e+6, train_loss_epoch=5.81e+6]Epoch 162/3000:   5%|▌         | 162/3000 [01:08&lt;21:04,  2.24it/s, v_num=1, train_loss_step=5.72e+6, train_loss_epoch=5.79e+6]Epoch 163/3000:   5%|▌         | 162/3000 [01:08&lt;21:04,  2.24it/s, v_num=1, train_loss_step=5.72e+6, train_loss_epoch=5.79e+6]Epoch 163/3000:   5%|▌         | 163/3000 [01:08&lt;20:31,  2.30it/s, v_num=1, train_loss_step=5.72e+6, train_loss_epoch=5.79e+6]Epoch 163/3000:   5%|▌         | 163/3000 [01:08&lt;20:31,  2.30it/s, v_num=1, train_loss_step=5.61e+6, train_loss_epoch=5.76e+6]Epoch 164/3000:   5%|▌         | 163/3000 [01:08&lt;20:31,  2.30it/s, v_num=1, train_loss_step=5.61e+6, train_loss_epoch=5.76e+6]Epoch 164/3000:   5%|▌         | 164/3000 [01:08&lt;17:18,  2.73it/s, v_num=1, train_loss_step=5.61e+6, train_loss_epoch=5.76e+6]Epoch 164/3000:   5%|▌         | 164/3000 [01:08&lt;17:18,  2.73it/s, v_num=1, train_loss_step=5.77e+6, train_loss_epoch=5.74e+6]Epoch 165/3000:   5%|▌         | 164/3000 [01:08&lt;17:18,  2.73it/s, v_num=1, train_loss_step=5.77e+6, train_loss_epoch=5.74e+6]Epoch 165/3000:   6%|▌         | 165/3000 [01:08&lt;16:43,  2.82it/s, v_num=1, train_loss_step=5.77e+6, train_loss_epoch=5.74e+6]Epoch 165/3000:   6%|▌         | 165/3000 [01:08&lt;16:43,  2.82it/s, v_num=1, train_loss_step=5.58e+6, train_loss_epoch=5.72e+6]Epoch 166/3000:   6%|▌         | 165/3000 [01:08&lt;16:43,  2.82it/s, v_num=1, train_loss_step=5.58e+6, train_loss_epoch=5.72e+6]Epoch 166/3000:   6%|▌         | 166/3000 [01:09&lt;16:31,  2.86it/s, v_num=1, train_loss_step=5.58e+6, train_loss_epoch=5.72e+6]Epoch 166/3000:   6%|▌         | 166/3000 [01:09&lt;16:31,  2.86it/s, v_num=1, train_loss_step=5.64e+6, train_loss_epoch=5.7e+6] Epoch 167/3000:   6%|▌         | 166/3000 [01:09&lt;16:31,  2.86it/s, v_num=1, train_loss_step=5.64e+6, train_loss_epoch=5.7e+6]Epoch 167/3000:   6%|▌         | 167/3000 [01:09&lt;20:17,  2.33it/s, v_num=1, train_loss_step=5.64e+6, train_loss_epoch=5.7e+6]Epoch 167/3000:   6%|▌         | 167/3000 [01:09&lt;20:17,  2.33it/s, v_num=1, train_loss_step=5.5e+6, train_loss_epoch=5.68e+6]Epoch 168/3000:   6%|▌         | 167/3000 [01:09&lt;20:17,  2.33it/s, v_num=1, train_loss_step=5.5e+6, train_loss_epoch=5.68e+6]Epoch 168/3000:   6%|▌         | 168/3000 [01:10&lt;23:26,  2.01it/s, v_num=1, train_loss_step=5.5e+6, train_loss_epoch=5.68e+6]Epoch 168/3000:   6%|▌         | 168/3000 [01:10&lt;23:26,  2.01it/s, v_num=1, train_loss_step=5.8e+6, train_loss_epoch=5.65e+6]Epoch 169/3000:   6%|▌         | 168/3000 [01:10&lt;23:26,  2.01it/s, v_num=1, train_loss_step=5.8e+6, train_loss_epoch=5.65e+6]Epoch 169/3000:   6%|▌         | 169/3000 [01:10&lt;22:13,  2.12it/s, v_num=1, train_loss_step=5.8e+6, train_loss_epoch=5.65e+6]Epoch 169/3000:   6%|▌         | 169/3000 [01:10&lt;22:13,  2.12it/s, v_num=1, train_loss_step=5.28e+6, train_loss_epoch=5.63e+6]Epoch 170/3000:   6%|▌         | 169/3000 [01:10&lt;22:13,  2.12it/s, v_num=1, train_loss_step=5.28e+6, train_loss_epoch=5.63e+6]Epoch 170/3000:   6%|▌         | 170/3000 [01:11&lt;23:01,  2.05it/s, v_num=1, train_loss_step=5.28e+6, train_loss_epoch=5.63e+6]Epoch 170/3000:   6%|▌         | 170/3000 [01:11&lt;23:01,  2.05it/s, v_num=1, train_loss_step=5.36e+6, train_loss_epoch=5.61e+6]Epoch 171/3000:   6%|▌         | 170/3000 [01:11&lt;23:01,  2.05it/s, v_num=1, train_loss_step=5.36e+6, train_loss_epoch=5.61e+6]Epoch 171/3000:   6%|▌         | 171/3000 [01:12&lt;23:23,  2.02it/s, v_num=1, train_loss_step=5.36e+6, train_loss_epoch=5.61e+6]Epoch 171/3000:   6%|▌         | 171/3000 [01:12&lt;23:23,  2.02it/s, v_num=1, train_loss_step=5.97e+6, train_loss_epoch=5.59e+6]Epoch 172/3000:   6%|▌         | 171/3000 [01:12&lt;23:23,  2.02it/s, v_num=1, train_loss_step=5.97e+6, train_loss_epoch=5.59e+6]Epoch 172/3000:   6%|▌         | 172/3000 [01:12&lt;23:16,  2.03it/s, v_num=1, train_loss_step=5.97e+6, train_loss_epoch=5.59e+6]Epoch 172/3000:   6%|▌         | 172/3000 [01:12&lt;23:16,  2.03it/s, v_num=1, train_loss_step=5.49e+6, train_loss_epoch=5.57e+6]Epoch 173/3000:   6%|▌         | 172/3000 [01:12&lt;23:16,  2.03it/s, v_num=1, train_loss_step=5.49e+6, train_loss_epoch=5.57e+6]Epoch 173/3000:   6%|▌         | 173/3000 [01:13&lt;24:19,  1.94it/s, v_num=1, train_loss_step=5.49e+6, train_loss_epoch=5.57e+6]Epoch 173/3000:   6%|▌         | 173/3000 [01:13&lt;24:19,  1.94it/s, v_num=1, train_loss_step=5.57e+6, train_loss_epoch=5.55e+6]Epoch 174/3000:   6%|▌         | 173/3000 [01:13&lt;24:19,  1.94it/s, v_num=1, train_loss_step=5.57e+6, train_loss_epoch=5.55e+6]Epoch 174/3000:   6%|▌         | 174/3000 [01:13&lt;22:01,  2.14it/s, v_num=1, train_loss_step=5.57e+6, train_loss_epoch=5.55e+6]Epoch 174/3000:   6%|▌         | 174/3000 [01:13&lt;22:01,  2.14it/s, v_num=1, train_loss_step=5.59e+6, train_loss_epoch=5.53e+6]Epoch 175/3000:   6%|▌         | 174/3000 [01:13&lt;22:01,  2.14it/s, v_num=1, train_loss_step=5.59e+6, train_loss_epoch=5.53e+6]Epoch 175/3000:   6%|▌         | 175/3000 [01:13&lt;22:58,  2.05it/s, v_num=1, train_loss_step=5.59e+6, train_loss_epoch=5.53e+6]Epoch 175/3000:   6%|▌         | 175/3000 [01:13&lt;22:58,  2.05it/s, v_num=1, train_loss_step=5.76e+6, train_loss_epoch=5.51e+6]Epoch 176/3000:   6%|▌         | 175/3000 [01:13&lt;22:58,  2.05it/s, v_num=1, train_loss_step=5.76e+6, train_loss_epoch=5.51e+6]Epoch 176/3000:   6%|▌         | 176/3000 [01:14&lt;22:52,  2.06it/s, v_num=1, train_loss_step=5.76e+6, train_loss_epoch=5.51e+6]Epoch 176/3000:   6%|▌         | 176/3000 [01:14&lt;22:52,  2.06it/s, v_num=1, train_loss_step=5.27e+6, train_loss_epoch=5.49e+6]Epoch 177/3000:   6%|▌         | 176/3000 [01:14&lt;22:52,  2.06it/s, v_num=1, train_loss_step=5.27e+6, train_loss_epoch=5.49e+6]Epoch 177/3000:   6%|▌         | 177/3000 [01:14&lt;20:58,  2.24it/s, v_num=1, train_loss_step=5.27e+6, train_loss_epoch=5.49e+6]Epoch 177/3000:   6%|▌         | 177/3000 [01:14&lt;20:58,  2.24it/s, v_num=1, train_loss_step=5.21e+6, train_loss_epoch=5.47e+6]Epoch 178/3000:   6%|▌         | 177/3000 [01:14&lt;20:58,  2.24it/s, v_num=1, train_loss_step=5.21e+6, train_loss_epoch=5.47e+6]Epoch 178/3000:   6%|▌         | 178/3000 [01:15&lt;19:46,  2.38it/s, v_num=1, train_loss_step=5.21e+6, train_loss_epoch=5.47e+6]Epoch 178/3000:   6%|▌         | 178/3000 [01:15&lt;19:46,  2.38it/s, v_num=1, train_loss_step=5.42e+6, train_loss_epoch=5.45e+6]Epoch 179/3000:   6%|▌         | 178/3000 [01:15&lt;19:46,  2.38it/s, v_num=1, train_loss_step=5.42e+6, train_loss_epoch=5.45e+6]Epoch 179/3000:   6%|▌         | 179/3000 [01:15&lt;20:42,  2.27it/s, v_num=1, train_loss_step=5.42e+6, train_loss_epoch=5.45e+6]Epoch 179/3000:   6%|▌         | 179/3000 [01:15&lt;20:42,  2.27it/s, v_num=1, train_loss_step=5.42e+6, train_loss_epoch=5.43e+6]Epoch 180/3000:   6%|▌         | 179/3000 [01:15&lt;20:42,  2.27it/s, v_num=1, train_loss_step=5.42e+6, train_loss_epoch=5.43e+6]Epoch 180/3000:   6%|▌         | 180/3000 [01:16&lt;20:35,  2.28it/s, v_num=1, train_loss_step=5.42e+6, train_loss_epoch=5.43e+6]Epoch 180/3000:   6%|▌         | 180/3000 [01:16&lt;20:35,  2.28it/s, v_num=1, train_loss_step=5.7e+6, train_loss_epoch=5.41e+6] Epoch 181/3000:   6%|▌         | 180/3000 [01:16&lt;20:35,  2.28it/s, v_num=1, train_loss_step=5.7e+6, train_loss_epoch=5.41e+6]Epoch 181/3000:   6%|▌         | 181/3000 [01:16&lt;17:54,  2.62it/s, v_num=1, train_loss_step=5.7e+6, train_loss_epoch=5.41e+6]Epoch 181/3000:   6%|▌         | 181/3000 [01:16&lt;17:54,  2.62it/s, v_num=1, train_loss_step=5.3e+6, train_loss_epoch=5.39e+6]Epoch 182/3000:   6%|▌         | 181/3000 [01:16&lt;17:54,  2.62it/s, v_num=1, train_loss_step=5.3e+6, train_loss_epoch=5.39e+6]Epoch 182/3000:   6%|▌         | 182/3000 [01:16&lt;17:00,  2.76it/s, v_num=1, train_loss_step=5.3e+6, train_loss_epoch=5.39e+6]Epoch 182/3000:   6%|▌         | 182/3000 [01:16&lt;17:00,  2.76it/s, v_num=1, train_loss_step=5.43e+6, train_loss_epoch=5.37e+6]Epoch 183/3000:   6%|▌         | 182/3000 [01:16&lt;17:00,  2.76it/s, v_num=1, train_loss_step=5.43e+6, train_loss_epoch=5.37e+6]Epoch 183/3000:   6%|▌         | 183/3000 [01:16&lt;16:26,  2.86it/s, v_num=1, train_loss_step=5.43e+6, train_loss_epoch=5.37e+6]Epoch 183/3000:   6%|▌         | 183/3000 [01:16&lt;16:26,  2.86it/s, v_num=1, train_loss_step=5.33e+6, train_loss_epoch=5.35e+6]Epoch 184/3000:   6%|▌         | 183/3000 [01:16&lt;16:26,  2.86it/s, v_num=1, train_loss_step=5.33e+6, train_loss_epoch=5.35e+6]Epoch 184/3000:   6%|▌         | 184/3000 [01:17&lt;18:49,  2.49it/s, v_num=1, train_loss_step=5.33e+6, train_loss_epoch=5.35e+6]Epoch 184/3000:   6%|▌         | 184/3000 [01:17&lt;18:49,  2.49it/s, v_num=1, train_loss_step=5.35e+6, train_loss_epoch=5.33e+6]Epoch 185/3000:   6%|▌         | 184/3000 [01:17&lt;18:49,  2.49it/s, v_num=1, train_loss_step=5.35e+6, train_loss_epoch=5.33e+6]Epoch 185/3000:   6%|▌         | 185/3000 [01:17&lt;19:53,  2.36it/s, v_num=1, train_loss_step=5.35e+6, train_loss_epoch=5.33e+6]Epoch 185/3000:   6%|▌         | 185/3000 [01:17&lt;19:53,  2.36it/s, v_num=1, train_loss_step=5.14e+6, train_loss_epoch=5.31e+6]Epoch 186/3000:   6%|▌         | 185/3000 [01:17&lt;19:53,  2.36it/s, v_num=1, train_loss_step=5.14e+6, train_loss_epoch=5.31e+6]Epoch 186/3000:   6%|▌         | 186/3000 [01:18&lt;17:24,  2.69it/s, v_num=1, train_loss_step=5.14e+6, train_loss_epoch=5.31e+6]Epoch 186/3000:   6%|▌         | 186/3000 [01:18&lt;17:24,  2.69it/s, v_num=1, train_loss_step=5.48e+6, train_loss_epoch=5.29e+6]Epoch 187/3000:   6%|▌         | 186/3000 [01:18&lt;17:24,  2.69it/s, v_num=1, train_loss_step=5.48e+6, train_loss_epoch=5.29e+6]Epoch 187/3000:   6%|▌         | 187/3000 [01:18&lt;14:59,  3.13it/s, v_num=1, train_loss_step=5.48e+6, train_loss_epoch=5.29e+6]Epoch 187/3000:   6%|▌         | 187/3000 [01:18&lt;14:59,  3.13it/s, v_num=1, train_loss_step=5.25e+6, train_loss_epoch=5.28e+6]Epoch 188/3000:   6%|▌         | 187/3000 [01:18&lt;14:59,  3.13it/s, v_num=1, train_loss_step=5.25e+6, train_loss_epoch=5.28e+6]Epoch 188/3000:   6%|▋         | 188/3000 [01:18&lt;16:06,  2.91it/s, v_num=1, train_loss_step=5.25e+6, train_loss_epoch=5.28e+6]Epoch 188/3000:   6%|▋         | 188/3000 [01:18&lt;16:06,  2.91it/s, v_num=1, train_loss_step=5.17e+6, train_loss_epoch=5.26e+6]Epoch 189/3000:   6%|▋         | 188/3000 [01:18&lt;16:06,  2.91it/s, v_num=1, train_loss_step=5.17e+6, train_loss_epoch=5.26e+6]Epoch 189/3000:   6%|▋         | 189/3000 [01:19&lt;14:39,  3.20it/s, v_num=1, train_loss_step=5.17e+6, train_loss_epoch=5.26e+6]Epoch 189/3000:   6%|▋         | 189/3000 [01:19&lt;14:39,  3.20it/s, v_num=1, train_loss_step=5.27e+6, train_loss_epoch=5.24e+6]Epoch 190/3000:   6%|▋         | 189/3000 [01:19&lt;14:39,  3.20it/s, v_num=1, train_loss_step=5.27e+6, train_loss_epoch=5.24e+6]Epoch 190/3000:   6%|▋         | 190/3000 [01:19&lt;18:40,  2.51it/s, v_num=1, train_loss_step=5.27e+6, train_loss_epoch=5.24e+6]Epoch 190/3000:   6%|▋         | 190/3000 [01:19&lt;18:40,  2.51it/s, v_num=1, train_loss_step=5.18e+6, train_loss_epoch=5.22e+6]Epoch 191/3000:   6%|▋         | 190/3000 [01:19&lt;18:40,  2.51it/s, v_num=1, train_loss_step=5.18e+6, train_loss_epoch=5.22e+6]Epoch 191/3000:   6%|▋         | 191/3000 [01:20&lt;20:06,  2.33it/s, v_num=1, train_loss_step=5.18e+6, train_loss_epoch=5.22e+6]Epoch 191/3000:   6%|▋         | 191/3000 [01:20&lt;20:06,  2.33it/s, v_num=1, train_loss_step=4.75e+6, train_loss_epoch=5.2e+6] Epoch 192/3000:   6%|▋         | 191/3000 [01:20&lt;20:06,  2.33it/s, v_num=1, train_loss_step=4.75e+6, train_loss_epoch=5.2e+6]Epoch 192/3000:   6%|▋         | 192/3000 [01:20&lt;20:52,  2.24it/s, v_num=1, train_loss_step=4.75e+6, train_loss_epoch=5.2e+6]Epoch 192/3000:   6%|▋         | 192/3000 [01:20&lt;20:52,  2.24it/s, v_num=1, train_loss_step=5.26e+6, train_loss_epoch=5.18e+6]Epoch 193/3000:   6%|▋         | 192/3000 [01:20&lt;20:52,  2.24it/s, v_num=1, train_loss_step=5.26e+6, train_loss_epoch=5.18e+6]Epoch 193/3000:   6%|▋         | 193/3000 [01:21&lt;22:04,  2.12it/s, v_num=1, train_loss_step=5.26e+6, train_loss_epoch=5.18e+6]Epoch 193/3000:   6%|▋         | 193/3000 [01:21&lt;22:04,  2.12it/s, v_num=1, train_loss_step=4.95e+6, train_loss_epoch=5.17e+6]Epoch 194/3000:   6%|▋         | 193/3000 [01:21&lt;22:04,  2.12it/s, v_num=1, train_loss_step=4.95e+6, train_loss_epoch=5.17e+6]Epoch 194/3000:   6%|▋         | 194/3000 [01:21&lt;20:50,  2.24it/s, v_num=1, train_loss_step=4.95e+6, train_loss_epoch=5.17e+6]Epoch 194/3000:   6%|▋         | 194/3000 [01:21&lt;20:50,  2.24it/s, v_num=1, train_loss_step=5.51e+6, train_loss_epoch=5.15e+6]Epoch 195/3000:   6%|▋         | 194/3000 [01:21&lt;20:50,  2.24it/s, v_num=1, train_loss_step=5.51e+6, train_loss_epoch=5.15e+6]Epoch 195/3000:   6%|▋         | 195/3000 [01:21&lt;19:54,  2.35it/s, v_num=1, train_loss_step=5.51e+6, train_loss_epoch=5.15e+6]Epoch 195/3000:   6%|▋         | 195/3000 [01:21&lt;19:54,  2.35it/s, v_num=1, train_loss_step=4.85e+6, train_loss_epoch=5.13e+6]Epoch 196/3000:   6%|▋         | 195/3000 [01:21&lt;19:54,  2.35it/s, v_num=1, train_loss_step=4.85e+6, train_loss_epoch=5.13e+6]Epoch 196/3000:   7%|▋         | 196/3000 [01:22&lt;21:47,  2.14it/s, v_num=1, train_loss_step=4.85e+6, train_loss_epoch=5.13e+6]Epoch 196/3000:   7%|▋         | 196/3000 [01:22&lt;21:47,  2.14it/s, v_num=1, train_loss_step=5.15e+6, train_loss_epoch=5.11e+6]Epoch 197/3000:   7%|▋         | 196/3000 [01:22&lt;21:47,  2.14it/s, v_num=1, train_loss_step=5.15e+6, train_loss_epoch=5.11e+6]Epoch 197/3000:   7%|▋         | 197/3000 [01:22&lt;21:41,  2.15it/s, v_num=1, train_loss_step=5.15e+6, train_loss_epoch=5.11e+6]Epoch 197/3000:   7%|▋         | 197/3000 [01:22&lt;21:41,  2.15it/s, v_num=1, train_loss_step=5.32e+6, train_loss_epoch=5.09e+6]Epoch 198/3000:   7%|▋         | 197/3000 [01:22&lt;21:41,  2.15it/s, v_num=1, train_loss_step=5.32e+6, train_loss_epoch=5.09e+6]Epoch 198/3000:   7%|▋         | 198/3000 [01:23&lt;20:10,  2.31it/s, v_num=1, train_loss_step=5.32e+6, train_loss_epoch=5.09e+6]Epoch 198/3000:   7%|▋         | 198/3000 [01:23&lt;20:10,  2.31it/s, v_num=1, train_loss_step=5.26e+6, train_loss_epoch=5.08e+6]Epoch 199/3000:   7%|▋         | 198/3000 [01:23&lt;20:10,  2.31it/s, v_num=1, train_loss_step=5.26e+6, train_loss_epoch=5.08e+6]Epoch 199/3000:   7%|▋         | 199/3000 [01:23&lt;19:55,  2.34it/s, v_num=1, train_loss_step=5.26e+6, train_loss_epoch=5.08e+6]Epoch 199/3000:   7%|▋         | 199/3000 [01:23&lt;19:55,  2.34it/s, v_num=1, train_loss_step=5.05e+6, train_loss_epoch=5.06e+6]Epoch 200/3000:   7%|▋         | 199/3000 [01:23&lt;19:55,  2.34it/s, v_num=1, train_loss_step=5.05e+6, train_loss_epoch=5.06e+6]Epoch 200/3000:   7%|▋         | 200/3000 [01:24&lt;20:08,  2.32it/s, v_num=1, train_loss_step=5.05e+6, train_loss_epoch=5.06e+6]Epoch 200/3000:   7%|▋         | 200/3000 [01:24&lt;20:08,  2.32it/s, v_num=1, train_loss_step=5.05e+6, train_loss_epoch=5.04e+6]Epoch 201/3000:   7%|▋         | 200/3000 [01:24&lt;20:08,  2.32it/s, v_num=1, train_loss_step=5.05e+6, train_loss_epoch=5.04e+6]Epoch 201/3000:   7%|▋         | 201/3000 [01:24&lt;21:19,  2.19it/s, v_num=1, train_loss_step=5.05e+6, train_loss_epoch=5.04e+6]Epoch 201/3000:   7%|▋         | 201/3000 [01:24&lt;21:19,  2.19it/s, v_num=1, train_loss_step=5.04e+6, train_loss_epoch=5.03e+6]Epoch 202/3000:   7%|▋         | 201/3000 [01:24&lt;21:19,  2.19it/s, v_num=1, train_loss_step=5.04e+6, train_loss_epoch=5.03e+6]Epoch 202/3000:   7%|▋         | 202/3000 [01:25&lt;19:32,  2.39it/s, v_num=1, train_loss_step=5.04e+6, train_loss_epoch=5.03e+6]Epoch 202/3000:   7%|▋         | 202/3000 [01:25&lt;19:32,  2.39it/s, v_num=1, train_loss_step=4.92e+6, train_loss_epoch=5.01e+6]Epoch 203/3000:   7%|▋         | 202/3000 [01:25&lt;19:32,  2.39it/s, v_num=1, train_loss_step=4.92e+6, train_loss_epoch=5.01e+6]Epoch 203/3000:   7%|▋         | 203/3000 [01:25&lt;16:39,  2.80it/s, v_num=1, train_loss_step=4.92e+6, train_loss_epoch=5.01e+6]Epoch 203/3000:   7%|▋         | 203/3000 [01:25&lt;16:39,  2.80it/s, v_num=1, train_loss_step=5.01e+6, train_loss_epoch=4.99e+6]Epoch 204/3000:   7%|▋         | 203/3000 [01:25&lt;16:39,  2.80it/s, v_num=1, train_loss_step=5.01e+6, train_loss_epoch=4.99e+6]Epoch 204/3000:   7%|▋         | 204/3000 [01:25&lt;15:45,  2.96it/s, v_num=1, train_loss_step=5.01e+6, train_loss_epoch=4.99e+6]Epoch 204/3000:   7%|▋         | 204/3000 [01:25&lt;15:45,  2.96it/s, v_num=1, train_loss_step=4.81e+6, train_loss_epoch=4.97e+6]Epoch 205/3000:   7%|▋         | 204/3000 [01:25&lt;15:45,  2.96it/s, v_num=1, train_loss_step=4.81e+6, train_loss_epoch=4.97e+6]Epoch 205/3000:   7%|▋         | 205/3000 [01:25&lt;16:28,  2.83it/s, v_num=1, train_loss_step=4.81e+6, train_loss_epoch=4.97e+6]Epoch 205/3000:   7%|▋         | 205/3000 [01:25&lt;16:28,  2.83it/s, v_num=1, train_loss_step=4.99e+6, train_loss_epoch=4.96e+6]Epoch 206/3000:   7%|▋         | 205/3000 [01:25&lt;16:28,  2.83it/s, v_num=1, train_loss_step=4.99e+6, train_loss_epoch=4.96e+6]Epoch 206/3000:   7%|▋         | 206/3000 [01:26&lt;20:18,  2.29it/s, v_num=1, train_loss_step=4.99e+6, train_loss_epoch=4.96e+6]Epoch 206/3000:   7%|▋         | 206/3000 [01:26&lt;20:18,  2.29it/s, v_num=1, train_loss_step=5.15e+6, train_loss_epoch=4.94e+6]Epoch 207/3000:   7%|▋         | 206/3000 [01:26&lt;20:18,  2.29it/s, v_num=1, train_loss_step=5.15e+6, train_loss_epoch=4.94e+6]Epoch 207/3000:   7%|▋         | 207/3000 [01:27&lt;20:56,  2.22it/s, v_num=1, train_loss_step=5.15e+6, train_loss_epoch=4.94e+6]Epoch 207/3000:   7%|▋         | 207/3000 [01:27&lt;20:56,  2.22it/s, v_num=1, train_loss_step=5.1e+6, train_loss_epoch=4.92e+6] Epoch 208/3000:   7%|▋         | 207/3000 [01:27&lt;20:56,  2.22it/s, v_num=1, train_loss_step=5.1e+6, train_loss_epoch=4.92e+6]Epoch 208/3000:   7%|▋         | 208/3000 [01:27&lt;21:22,  2.18it/s, v_num=1, train_loss_step=5.1e+6, train_loss_epoch=4.92e+6]Epoch 208/3000:   7%|▋         | 208/3000 [01:27&lt;21:22,  2.18it/s, v_num=1, train_loss_step=4.96e+6, train_loss_epoch=4.91e+6]Epoch 209/3000:   7%|▋         | 208/3000 [01:27&lt;21:22,  2.18it/s, v_num=1, train_loss_step=4.96e+6, train_loss_epoch=4.91e+6]Epoch 209/3000:   7%|▋         | 209/3000 [01:28&lt;24:44,  1.88it/s, v_num=1, train_loss_step=4.96e+6, train_loss_epoch=4.91e+6]Epoch 209/3000:   7%|▋         | 209/3000 [01:28&lt;24:44,  1.88it/s, v_num=1, train_loss_step=4.93e+6, train_loss_epoch=4.89e+6]Epoch 210/3000:   7%|▋         | 209/3000 [01:28&lt;24:44,  1.88it/s, v_num=1, train_loss_step=4.93e+6, train_loss_epoch=4.89e+6]Epoch 210/3000:   7%|▋         | 210/3000 [01:28&lt;25:29,  1.82it/s, v_num=1, train_loss_step=4.93e+6, train_loss_epoch=4.89e+6]Epoch 210/3000:   7%|▋         | 210/3000 [01:28&lt;25:29,  1.82it/s, v_num=1, train_loss_step=4.83e+6, train_loss_epoch=4.88e+6]Epoch 211/3000:   7%|▋         | 210/3000 [01:28&lt;25:29,  1.82it/s, v_num=1, train_loss_step=4.83e+6, train_loss_epoch=4.88e+6]Epoch 211/3000:   7%|▋         | 211/3000 [01:29&lt;23:09,  2.01it/s, v_num=1, train_loss_step=4.83e+6, train_loss_epoch=4.88e+6]Epoch 211/3000:   7%|▋         | 211/3000 [01:29&lt;23:09,  2.01it/s, v_num=1, train_loss_step=4.79e+6, train_loss_epoch=4.86e+6]Epoch 212/3000:   7%|▋         | 211/3000 [01:29&lt;23:09,  2.01it/s, v_num=1, train_loss_step=4.79e+6, train_loss_epoch=4.86e+6]Epoch 212/3000:   7%|▋         | 212/3000 [01:29&lt;22:19,  2.08it/s, v_num=1, train_loss_step=4.79e+6, train_loss_epoch=4.86e+6]Epoch 212/3000:   7%|▋         | 212/3000 [01:29&lt;22:19,  2.08it/s, v_num=1, train_loss_step=5.07e+6, train_loss_epoch=4.84e+6]Epoch 213/3000:   7%|▋         | 212/3000 [01:29&lt;22:19,  2.08it/s, v_num=1, train_loss_step=5.07e+6, train_loss_epoch=4.84e+6]Epoch 213/3000:   7%|▋         | 213/3000 [01:30&lt;22:20,  2.08it/s, v_num=1, train_loss_step=5.07e+6, train_loss_epoch=4.84e+6]Epoch 213/3000:   7%|▋         | 213/3000 [01:30&lt;22:20,  2.08it/s, v_num=1, train_loss_step=4.6e+6, train_loss_epoch=4.83e+6] Epoch 214/3000:   7%|▋         | 213/3000 [01:30&lt;22:20,  2.08it/s, v_num=1, train_loss_step=4.6e+6, train_loss_epoch=4.83e+6]Epoch 214/3000:   7%|▋         | 214/3000 [01:30&lt;19:01,  2.44it/s, v_num=1, train_loss_step=4.6e+6, train_loss_epoch=4.83e+6]Epoch 214/3000:   7%|▋         | 214/3000 [01:30&lt;19:01,  2.44it/s, v_num=1, train_loss_step=4.8e+6, train_loss_epoch=4.81e+6]Epoch 215/3000:   7%|▋         | 214/3000 [01:30&lt;19:01,  2.44it/s, v_num=1, train_loss_step=4.8e+6, train_loss_epoch=4.81e+6]Epoch 215/3000:   7%|▋         | 215/3000 [01:30&lt;17:49,  2.60it/s, v_num=1, train_loss_step=4.8e+6, train_loss_epoch=4.81e+6]Epoch 215/3000:   7%|▋         | 215/3000 [01:30&lt;17:49,  2.60it/s, v_num=1, train_loss_step=4.91e+6, train_loss_epoch=4.8e+6]Epoch 216/3000:   7%|▋         | 215/3000 [01:30&lt;17:49,  2.60it/s, v_num=1, train_loss_step=4.91e+6, train_loss_epoch=4.8e+6]Epoch 216/3000:   7%|▋         | 216/3000 [01:30&lt;16:24,  2.83it/s, v_num=1, train_loss_step=4.91e+6, train_loss_epoch=4.8e+6]Epoch 216/3000:   7%|▋         | 216/3000 [01:30&lt;16:24,  2.83it/s, v_num=1, train_loss_step=4.77e+6, train_loss_epoch=4.78e+6]Epoch 217/3000:   7%|▋         | 216/3000 [01:30&lt;16:24,  2.83it/s, v_num=1, train_loss_step=4.77e+6, train_loss_epoch=4.78e+6]Epoch 217/3000:   7%|▋         | 217/3000 [01:31&lt;19:34,  2.37it/s, v_num=1, train_loss_step=4.77e+6, train_loss_epoch=4.78e+6]Epoch 217/3000:   7%|▋         | 217/3000 [01:31&lt;19:34,  2.37it/s, v_num=1, train_loss_step=4.56e+6, train_loss_epoch=4.76e+6]Epoch 218/3000:   7%|▋         | 217/3000 [01:31&lt;19:34,  2.37it/s, v_num=1, train_loss_step=4.56e+6, train_loss_epoch=4.76e+6]Epoch 218/3000:   7%|▋         | 218/3000 [01:32&lt;20:44,  2.23it/s, v_num=1, train_loss_step=4.56e+6, train_loss_epoch=4.76e+6]Epoch 218/3000:   7%|▋         | 218/3000 [01:32&lt;20:44,  2.23it/s, v_num=1, train_loss_step=4.56e+6, train_loss_epoch=4.75e+6]Epoch 219/3000:   7%|▋         | 218/3000 [01:32&lt;20:44,  2.23it/s, v_num=1, train_loss_step=4.56e+6, train_loss_epoch=4.75e+6]Epoch 219/3000:   7%|▋         | 219/3000 [01:32&lt;20:12,  2.29it/s, v_num=1, train_loss_step=4.56e+6, train_loss_epoch=4.75e+6]Epoch 219/3000:   7%|▋         | 219/3000 [01:32&lt;20:12,  2.29it/s, v_num=1, train_loss_step=4.83e+6, train_loss_epoch=4.73e+6]Epoch 220/3000:   7%|▋         | 219/3000 [01:32&lt;20:12,  2.29it/s, v_num=1, train_loss_step=4.83e+6, train_loss_epoch=4.73e+6]Epoch 220/3000:   7%|▋         | 220/3000 [01:32&lt;21:16,  2.18it/s, v_num=1, train_loss_step=4.83e+6, train_loss_epoch=4.73e+6]Epoch 220/3000:   7%|▋         | 220/3000 [01:32&lt;21:16,  2.18it/s, v_num=1, train_loss_step=4.46e+6, train_loss_epoch=4.72e+6]Epoch 221/3000:   7%|▋         | 220/3000 [01:32&lt;21:16,  2.18it/s, v_num=1, train_loss_step=4.46e+6, train_loss_epoch=4.72e+6]Epoch 221/3000:   7%|▋         | 221/3000 [01:33&lt;23:08,  2.00it/s, v_num=1, train_loss_step=4.46e+6, train_loss_epoch=4.72e+6]Epoch 221/3000:   7%|▋         | 221/3000 [01:33&lt;23:08,  2.00it/s, v_num=1, train_loss_step=4.64e+6, train_loss_epoch=4.7e+6] Epoch 222/3000:   7%|▋         | 221/3000 [01:33&lt;23:08,  2.00it/s, v_num=1, train_loss_step=4.64e+6, train_loss_epoch=4.7e+6]Epoch 222/3000:   7%|▋         | 222/3000 [01:33&lt;19:06,  2.42it/s, v_num=1, train_loss_step=4.64e+6, train_loss_epoch=4.7e+6]Epoch 222/3000:   7%|▋         | 222/3000 [01:33&lt;19:06,  2.42it/s, v_num=1, train_loss_step=4.65e+6, train_loss_epoch=4.69e+6]Epoch 223/3000:   7%|▋         | 222/3000 [01:33&lt;19:06,  2.42it/s, v_num=1, train_loss_step=4.65e+6, train_loss_epoch=4.69e+6]Epoch 223/3000:   7%|▋         | 223/3000 [01:34&lt;18:49,  2.46it/s, v_num=1, train_loss_step=4.65e+6, train_loss_epoch=4.69e+6]Epoch 223/3000:   7%|▋         | 223/3000 [01:34&lt;18:49,  2.46it/s, v_num=1, train_loss_step=4.85e+6, train_loss_epoch=4.67e+6]Epoch 224/3000:   7%|▋         | 223/3000 [01:34&lt;18:49,  2.46it/s, v_num=1, train_loss_step=4.85e+6, train_loss_epoch=4.67e+6]Epoch 224/3000:   7%|▋         | 224/3000 [01:34&lt;15:33,  2.97it/s, v_num=1, train_loss_step=4.85e+6, train_loss_epoch=4.67e+6]Epoch 224/3000:   7%|▋         | 224/3000 [01:34&lt;15:33,  2.97it/s, v_num=1, train_loss_step=4.76e+6, train_loss_epoch=4.66e+6]Epoch 225/3000:   7%|▋         | 224/3000 [01:34&lt;15:33,  2.97it/s, v_num=1, train_loss_step=4.76e+6, train_loss_epoch=4.66e+6]Epoch 225/3000:   8%|▊         | 225/3000 [01:34&lt;16:15,  2.84it/s, v_num=1, train_loss_step=4.76e+6, train_loss_epoch=4.66e+6]Epoch 225/3000:   8%|▊         | 225/3000 [01:34&lt;16:15,  2.84it/s, v_num=1, train_loss_step=4.71e+6, train_loss_epoch=4.64e+6]Epoch 226/3000:   8%|▊         | 225/3000 [01:34&lt;16:15,  2.84it/s, v_num=1, train_loss_step=4.71e+6, train_loss_epoch=4.64e+6]Epoch 226/3000:   8%|▊         | 226/3000 [01:35&lt;16:53,  2.74it/s, v_num=1, train_loss_step=4.71e+6, train_loss_epoch=4.64e+6]Epoch 226/3000:   8%|▊         | 226/3000 [01:35&lt;16:53,  2.74it/s, v_num=1, train_loss_step=4.52e+6, train_loss_epoch=4.63e+6]Epoch 227/3000:   8%|▊         | 226/3000 [01:35&lt;16:53,  2.74it/s, v_num=1, train_loss_step=4.52e+6, train_loss_epoch=4.63e+6]Epoch 227/3000:   8%|▊         | 227/3000 [01:35&lt;16:42,  2.77it/s, v_num=1, train_loss_step=4.52e+6, train_loss_epoch=4.63e+6]Epoch 227/3000:   8%|▊         | 227/3000 [01:35&lt;16:42,  2.77it/s, v_num=1, train_loss_step=4.49e+6, train_loss_epoch=4.61e+6]Epoch 228/3000:   8%|▊         | 227/3000 [01:35&lt;16:42,  2.77it/s, v_num=1, train_loss_step=4.49e+6, train_loss_epoch=4.61e+6]Epoch 228/3000:   8%|▊         | 228/3000 [01:35&lt;15:37,  2.96it/s, v_num=1, train_loss_step=4.49e+6, train_loss_epoch=4.61e+6]Epoch 228/3000:   8%|▊         | 228/3000 [01:35&lt;15:37,  2.96it/s, v_num=1, train_loss_step=4.39e+6, train_loss_epoch=4.6e+6] Epoch 229/3000:   8%|▊         | 228/3000 [01:35&lt;15:37,  2.96it/s, v_num=1, train_loss_step=4.39e+6, train_loss_epoch=4.6e+6]Epoch 229/3000:   8%|▊         | 229/3000 [01:36&lt;17:03,  2.71it/s, v_num=1, train_loss_step=4.39e+6, train_loss_epoch=4.6e+6]Epoch 229/3000:   8%|▊         | 229/3000 [01:36&lt;17:03,  2.71it/s, v_num=1, train_loss_step=4.52e+6, train_loss_epoch=4.59e+6]Epoch 230/3000:   8%|▊         | 229/3000 [01:36&lt;17:03,  2.71it/s, v_num=1, train_loss_step=4.52e+6, train_loss_epoch=4.59e+6]Epoch 230/3000:   8%|▊         | 230/3000 [01:36&lt;18:53,  2.44it/s, v_num=1, train_loss_step=4.52e+6, train_loss_epoch=4.59e+6]Epoch 230/3000:   8%|▊         | 230/3000 [01:36&lt;18:53,  2.44it/s, v_num=1, train_loss_step=4.53e+6, train_loss_epoch=4.57e+6]Epoch 231/3000:   8%|▊         | 230/3000 [01:36&lt;18:53,  2.44it/s, v_num=1, train_loss_step=4.53e+6, train_loss_epoch=4.57e+6]Epoch 231/3000:   8%|▊         | 231/3000 [01:37&lt;22:34,  2.04it/s, v_num=1, train_loss_step=4.53e+6, train_loss_epoch=4.57e+6]Epoch 231/3000:   8%|▊         | 231/3000 [01:37&lt;22:34,  2.04it/s, v_num=1, train_loss_step=4.48e+6, train_loss_epoch=4.56e+6]Epoch 232/3000:   8%|▊         | 231/3000 [01:37&lt;22:34,  2.04it/s, v_num=1, train_loss_step=4.48e+6, train_loss_epoch=4.56e+6]Epoch 232/3000:   8%|▊         | 232/3000 [01:37&lt;24:04,  1.92it/s, v_num=1, train_loss_step=4.48e+6, train_loss_epoch=4.56e+6]Epoch 232/3000:   8%|▊         | 232/3000 [01:37&lt;24:04,  1.92it/s, v_num=1, train_loss_step=4.62e+6, train_loss_epoch=4.54e+6]Epoch 233/3000:   8%|▊         | 232/3000 [01:37&lt;24:04,  1.92it/s, v_num=1, train_loss_step=4.62e+6, train_loss_epoch=4.54e+6]Epoch 233/3000:   8%|▊         | 233/3000 [01:38&lt;22:30,  2.05it/s, v_num=1, train_loss_step=4.62e+6, train_loss_epoch=4.54e+6]Epoch 233/3000:   8%|▊         | 233/3000 [01:38&lt;22:30,  2.05it/s, v_num=1, train_loss_step=4.59e+6, train_loss_epoch=4.53e+6]Epoch 234/3000:   8%|▊         | 233/3000 [01:38&lt;22:30,  2.05it/s, v_num=1, train_loss_step=4.59e+6, train_loss_epoch=4.53e+6]Epoch 234/3000:   8%|▊         | 234/3000 [01:38&lt;21:32,  2.14it/s, v_num=1, train_loss_step=4.59e+6, train_loss_epoch=4.53e+6]Epoch 234/3000:   8%|▊         | 234/3000 [01:38&lt;21:32,  2.14it/s, v_num=1, train_loss_step=4.66e+6, train_loss_epoch=4.51e+6]Epoch 235/3000:   8%|▊         | 234/3000 [01:38&lt;21:32,  2.14it/s, v_num=1, train_loss_step=4.66e+6, train_loss_epoch=4.51e+6]Epoch 235/3000:   8%|▊         | 235/3000 [01:39&lt;22:30,  2.05it/s, v_num=1, train_loss_step=4.66e+6, train_loss_epoch=4.51e+6]Epoch 235/3000:   8%|▊         | 235/3000 [01:39&lt;22:30,  2.05it/s, v_num=1, train_loss_step=4.58e+6, train_loss_epoch=4.5e+6] Epoch 236/3000:   8%|▊         | 235/3000 [01:39&lt;22:30,  2.05it/s, v_num=1, train_loss_step=4.58e+6, train_loss_epoch=4.5e+6]Epoch 236/3000:   8%|▊         | 236/3000 [01:39&lt;21:54,  2.10it/s, v_num=1, train_loss_step=4.58e+6, train_loss_epoch=4.5e+6]Epoch 236/3000:   8%|▊         | 236/3000 [01:39&lt;21:54,  2.10it/s, v_num=1, train_loss_step=4.41e+6, train_loss_epoch=4.49e+6]Epoch 237/3000:   8%|▊         | 236/3000 [01:39&lt;21:54,  2.10it/s, v_num=1, train_loss_step=4.41e+6, train_loss_epoch=4.49e+6]Epoch 237/3000:   8%|▊         | 237/3000 [01:40&lt;21:30,  2.14it/s, v_num=1, train_loss_step=4.41e+6, train_loss_epoch=4.49e+6]Epoch 237/3000:   8%|▊         | 237/3000 [01:40&lt;21:30,  2.14it/s, v_num=1, train_loss_step=4.51e+6, train_loss_epoch=4.47e+6]Epoch 238/3000:   8%|▊         | 237/3000 [01:40&lt;21:30,  2.14it/s, v_num=1, train_loss_step=4.51e+6, train_loss_epoch=4.47e+6]Epoch 238/3000:   8%|▊         | 238/3000 [01:40&lt;22:15,  2.07it/s, v_num=1, train_loss_step=4.51e+6, train_loss_epoch=4.47e+6]Epoch 238/3000:   8%|▊         | 238/3000 [01:40&lt;22:15,  2.07it/s, v_num=1, train_loss_step=4.22e+6, train_loss_epoch=4.46e+6]Epoch 239/3000:   8%|▊         | 238/3000 [01:40&lt;22:15,  2.07it/s, v_num=1, train_loss_step=4.22e+6, train_loss_epoch=4.46e+6]Epoch 239/3000:   8%|▊         | 239/3000 [01:41&lt;23:54,  1.92it/s, v_num=1, train_loss_step=4.22e+6, train_loss_epoch=4.46e+6]Epoch 239/3000:   8%|▊         | 239/3000 [01:41&lt;23:54,  1.92it/s, v_num=1, train_loss_step=4.49e+6, train_loss_epoch=4.45e+6]Epoch 240/3000:   8%|▊         | 239/3000 [01:41&lt;23:54,  1.92it/s, v_num=1, train_loss_step=4.49e+6, train_loss_epoch=4.45e+6]Epoch 240/3000:   8%|▊         | 240/3000 [01:41&lt;22:30,  2.04it/s, v_num=1, train_loss_step=4.49e+6, train_loss_epoch=4.45e+6]Epoch 240/3000:   8%|▊         | 240/3000 [01:41&lt;22:30,  2.04it/s, v_num=1, train_loss_step=4.37e+6, train_loss_epoch=4.43e+6]Epoch 241/3000:   8%|▊         | 240/3000 [01:41&lt;22:30,  2.04it/s, v_num=1, train_loss_step=4.37e+6, train_loss_epoch=4.43e+6]Epoch 241/3000:   8%|▊         | 241/3000 [01:42&lt;23:02,  2.00it/s, v_num=1, train_loss_step=4.37e+6, train_loss_epoch=4.43e+6]Epoch 241/3000:   8%|▊         | 241/3000 [01:42&lt;23:02,  2.00it/s, v_num=1, train_loss_step=4.74e+6, train_loss_epoch=4.42e+6]Epoch 242/3000:   8%|▊         | 241/3000 [01:42&lt;23:02,  2.00it/s, v_num=1, train_loss_step=4.74e+6, train_loss_epoch=4.42e+6]Epoch 242/3000:   8%|▊         | 242/3000 [01:42&lt;23:59,  1.92it/s, v_num=1, train_loss_step=4.74e+6, train_loss_epoch=4.42e+6]Epoch 242/3000:   8%|▊         | 242/3000 [01:42&lt;23:59,  1.92it/s, v_num=1, train_loss_step=4.46e+6, train_loss_epoch=4.41e+6]Epoch 243/3000:   8%|▊         | 242/3000 [01:42&lt;23:59,  1.92it/s, v_num=1, train_loss_step=4.46e+6, train_loss_epoch=4.41e+6]Epoch 243/3000:   8%|▊         | 243/3000 [01:43&lt;22:57,  2.00it/s, v_num=1, train_loss_step=4.46e+6, train_loss_epoch=4.41e+6]Epoch 243/3000:   8%|▊         | 243/3000 [01:43&lt;22:57,  2.00it/s, v_num=1, train_loss_step=4.29e+6, train_loss_epoch=4.39e+6]Epoch 244/3000:   8%|▊         | 243/3000 [01:43&lt;22:57,  2.00it/s, v_num=1, train_loss_step=4.29e+6, train_loss_epoch=4.39e+6]Epoch 244/3000:   8%|▊         | 244/3000 [01:43&lt;22:40,  2.03it/s, v_num=1, train_loss_step=4.29e+6, train_loss_epoch=4.39e+6]Epoch 244/3000:   8%|▊         | 244/3000 [01:43&lt;22:40,  2.03it/s, v_num=1, train_loss_step=4.49e+6, train_loss_epoch=4.38e+6]Epoch 245/3000:   8%|▊         | 244/3000 [01:43&lt;22:40,  2.03it/s, v_num=1, train_loss_step=4.49e+6, train_loss_epoch=4.38e+6]Epoch 245/3000:   8%|▊         | 245/3000 [01:44&lt;22:16,  2.06it/s, v_num=1, train_loss_step=4.49e+6, train_loss_epoch=4.38e+6]Epoch 245/3000:   8%|▊         | 245/3000 [01:44&lt;22:16,  2.06it/s, v_num=1, train_loss_step=4.29e+6, train_loss_epoch=4.37e+6]Epoch 246/3000:   8%|▊         | 245/3000 [01:44&lt;22:16,  2.06it/s, v_num=1, train_loss_step=4.29e+6, train_loss_epoch=4.37e+6]Epoch 246/3000:   8%|▊         | 246/3000 [01:44&lt;20:38,  2.22it/s, v_num=1, train_loss_step=4.29e+6, train_loss_epoch=4.37e+6]Epoch 246/3000:   8%|▊         | 246/3000 [01:44&lt;20:38,  2.22it/s, v_num=1, train_loss_step=4.34e+6, train_loss_epoch=4.35e+6]Epoch 247/3000:   8%|▊         | 246/3000 [01:44&lt;20:38,  2.22it/s, v_num=1, train_loss_step=4.34e+6, train_loss_epoch=4.35e+6]Epoch 247/3000:   8%|▊         | 247/3000 [01:45&lt;20:11,  2.27it/s, v_num=1, train_loss_step=4.34e+6, train_loss_epoch=4.35e+6]Epoch 247/3000:   8%|▊         | 247/3000 [01:45&lt;20:11,  2.27it/s, v_num=1, train_loss_step=4.21e+6, train_loss_epoch=4.34e+6]Epoch 248/3000:   8%|▊         | 247/3000 [01:45&lt;20:11,  2.27it/s, v_num=1, train_loss_step=4.21e+6, train_loss_epoch=4.34e+6]Epoch 248/3000:   8%|▊         | 248/3000 [01:45&lt;21:25,  2.14it/s, v_num=1, train_loss_step=4.21e+6, train_loss_epoch=4.34e+6]Epoch 248/3000:   8%|▊         | 248/3000 [01:45&lt;21:25,  2.14it/s, v_num=1, train_loss_step=4.33e+6, train_loss_epoch=4.33e+6]Epoch 249/3000:   8%|▊         | 248/3000 [01:45&lt;21:25,  2.14it/s, v_num=1, train_loss_step=4.33e+6, train_loss_epoch=4.33e+6]Epoch 249/3000:   8%|▊         | 249/3000 [01:46&lt;21:58,  2.09it/s, v_num=1, train_loss_step=4.33e+6, train_loss_epoch=4.33e+6]Epoch 249/3000:   8%|▊         | 249/3000 [01:46&lt;21:58,  2.09it/s, v_num=1, train_loss_step=4.27e+6, train_loss_epoch=4.31e+6]Epoch 250/3000:   8%|▊         | 249/3000 [01:46&lt;21:58,  2.09it/s, v_num=1, train_loss_step=4.27e+6, train_loss_epoch=4.31e+6]Epoch 250/3000:   8%|▊         | 250/3000 [01:46&lt;22:07,  2.07it/s, v_num=1, train_loss_step=4.27e+6, train_loss_epoch=4.31e+6]Epoch 250/3000:   8%|▊         | 250/3000 [01:46&lt;22:07,  2.07it/s, v_num=1, train_loss_step=4.38e+6, train_loss_epoch=4.3e+6] Epoch 251/3000:   8%|▊         | 250/3000 [01:46&lt;22:07,  2.07it/s, v_num=1, train_loss_step=4.38e+6, train_loss_epoch=4.3e+6]Epoch 251/3000:   8%|▊         | 251/3000 [01:46&lt;20:40,  2.22it/s, v_num=1, train_loss_step=4.38e+6, train_loss_epoch=4.3e+6]Epoch 251/3000:   8%|▊         | 251/3000 [01:46&lt;20:40,  2.22it/s, v_num=1, train_loss_step=4.37e+6, train_loss_epoch=4.29e+6]Epoch 252/3000:   8%|▊         | 251/3000 [01:46&lt;20:40,  2.22it/s, v_num=1, train_loss_step=4.37e+6, train_loss_epoch=4.29e+6]Epoch 252/3000:   8%|▊         | 252/3000 [01:47&lt;20:04,  2.28it/s, v_num=1, train_loss_step=4.37e+6, train_loss_epoch=4.29e+6]Epoch 252/3000:   8%|▊         | 252/3000 [01:47&lt;20:04,  2.28it/s, v_num=1, train_loss_step=4.41e+6, train_loss_epoch=4.28e+6]Epoch 253/3000:   8%|▊         | 252/3000 [01:47&lt;20:04,  2.28it/s, v_num=1, train_loss_step=4.41e+6, train_loss_epoch=4.28e+6]Epoch 253/3000:   8%|▊         | 253/3000 [01:47&lt;18:23,  2.49it/s, v_num=1, train_loss_step=4.41e+6, train_loss_epoch=4.28e+6]Epoch 253/3000:   8%|▊         | 253/3000 [01:47&lt;18:23,  2.49it/s, v_num=1, train_loss_step=4.38e+6, train_loss_epoch=4.26e+6]Epoch 254/3000:   8%|▊         | 253/3000 [01:47&lt;18:23,  2.49it/s, v_num=1, train_loss_step=4.38e+6, train_loss_epoch=4.26e+6]Epoch 254/3000:   8%|▊         | 254/3000 [01:47&lt;16:51,  2.71it/s, v_num=1, train_loss_step=4.38e+6, train_loss_epoch=4.26e+6]Epoch 254/3000:   8%|▊         | 254/3000 [01:47&lt;16:51,  2.71it/s, v_num=1, train_loss_step=4.3e+6, train_loss_epoch=4.25e+6] Epoch 255/3000:   8%|▊         | 254/3000 [01:47&lt;16:51,  2.71it/s, v_num=1, train_loss_step=4.3e+6, train_loss_epoch=4.25e+6]Epoch 255/3000:   8%|▊         | 255/3000 [01:48&lt;19:14,  2.38it/s, v_num=1, train_loss_step=4.3e+6, train_loss_epoch=4.25e+6]Epoch 255/3000:   8%|▊         | 255/3000 [01:48&lt;19:14,  2.38it/s, v_num=1, train_loss_step=4.27e+6, train_loss_epoch=4.24e+6]Epoch 256/3000:   8%|▊         | 255/3000 [01:48&lt;19:14,  2.38it/s, v_num=1, train_loss_step=4.27e+6, train_loss_epoch=4.24e+6]Epoch 256/3000:   9%|▊         | 256/3000 [01:49&lt;20:37,  2.22it/s, v_num=1, train_loss_step=4.27e+6, train_loss_epoch=4.24e+6]Epoch 256/3000:   9%|▊         | 256/3000 [01:49&lt;20:37,  2.22it/s, v_num=1, train_loss_step=4.25e+6, train_loss_epoch=4.23e+6]Epoch 257/3000:   9%|▊         | 256/3000 [01:49&lt;20:37,  2.22it/s, v_num=1, train_loss_step=4.25e+6, train_loss_epoch=4.23e+6]Epoch 257/3000:   9%|▊         | 257/3000 [01:49&lt;17:12,  2.66it/s, v_num=1, train_loss_step=4.25e+6, train_loss_epoch=4.23e+6]Epoch 257/3000:   9%|▊         | 257/3000 [01:49&lt;17:12,  2.66it/s, v_num=1, train_loss_step=3.98e+6, train_loss_epoch=4.21e+6]Epoch 258/3000:   9%|▊         | 257/3000 [01:49&lt;17:12,  2.66it/s, v_num=1, train_loss_step=3.98e+6, train_loss_epoch=4.21e+6]Epoch 258/3000:   9%|▊         | 258/3000 [01:49&lt;17:11,  2.66it/s, v_num=1, train_loss_step=4.41e+6, train_loss_epoch=4.2e+6] Epoch 259/3000:   9%|▊         | 258/3000 [01:49&lt;17:11,  2.66it/s, v_num=1, train_loss_step=4.41e+6, train_loss_epoch=4.2e+6]Epoch 259/3000:   9%|▊         | 259/3000 [01:49&lt;11:21,  4.02it/s, v_num=1, train_loss_step=4.41e+6, train_loss_epoch=4.2e+6]Epoch 259/3000:   9%|▊         | 259/3000 [01:49&lt;11:21,  4.02it/s, v_num=1, train_loss_step=4.08e+6, train_loss_epoch=4.19e+6]Epoch 260/3000:   9%|▊         | 259/3000 [01:49&lt;11:21,  4.02it/s, v_num=1, train_loss_step=4.08e+6, train_loss_epoch=4.19e+6]Epoch 260/3000:   9%|▊         | 260/3000 [01:49&lt;11:32,  3.96it/s, v_num=1, train_loss_step=4.08e+6, train_loss_epoch=4.19e+6]Epoch 260/3000:   9%|▊         | 260/3000 [01:49&lt;11:32,  3.96it/s, v_num=1, train_loss_step=4.02e+6, train_loss_epoch=4.18e+6]Epoch 261/3000:   9%|▊         | 260/3000 [01:49&lt;11:32,  3.96it/s, v_num=1, train_loss_step=4.02e+6, train_loss_epoch=4.18e+6]Epoch 261/3000:   9%|▊         | 261/3000 [01:49&lt;11:59,  3.81it/s, v_num=1, train_loss_step=4.02e+6, train_loss_epoch=4.18e+6]Epoch 261/3000:   9%|▊         | 261/3000 [01:49&lt;11:59,  3.81it/s, v_num=1, train_loss_step=4.17e+6, train_loss_epoch=4.16e+6]Epoch 262/3000:   9%|▊         | 261/3000 [01:49&lt;11:59,  3.81it/s, v_num=1, train_loss_step=4.17e+6, train_loss_epoch=4.16e+6]Epoch 262/3000:   9%|▊         | 262/3000 [01:50&lt;12:00,  3.80it/s, v_num=1, train_loss_step=4.17e+6, train_loss_epoch=4.16e+6]Epoch 262/3000:   9%|▊         | 262/3000 [01:50&lt;12:00,  3.80it/s, v_num=1, train_loss_step=4.08e+6, train_loss_epoch=4.15e+6]Epoch 263/3000:   9%|▊         | 262/3000 [01:50&lt;12:00,  3.80it/s, v_num=1, train_loss_step=4.08e+6, train_loss_epoch=4.15e+6]Epoch 263/3000:   9%|▉         | 263/3000 [01:50&lt;16:00,  2.85it/s, v_num=1, train_loss_step=4.08e+6, train_loss_epoch=4.15e+6]Epoch 263/3000:   9%|▉         | 263/3000 [01:50&lt;16:00,  2.85it/s, v_num=1, train_loss_step=4.29e+6, train_loss_epoch=4.14e+6]Epoch 264/3000:   9%|▉         | 263/3000 [01:50&lt;16:00,  2.85it/s, v_num=1, train_loss_step=4.29e+6, train_loss_epoch=4.14e+6]Epoch 264/3000:   9%|▉         | 264/3000 [01:51&lt;15:13,  3.00it/s, v_num=1, train_loss_step=4.29e+6, train_loss_epoch=4.14e+6]Epoch 264/3000:   9%|▉         | 264/3000 [01:51&lt;15:13,  3.00it/s, v_num=1, train_loss_step=4.26e+6, train_loss_epoch=4.13e+6]Epoch 265/3000:   9%|▉         | 264/3000 [01:51&lt;15:13,  3.00it/s, v_num=1, train_loss_step=4.26e+6, train_loss_epoch=4.13e+6]Epoch 265/3000:   9%|▉         | 265/3000 [01:51&lt;16:16,  2.80it/s, v_num=1, train_loss_step=4.26e+6, train_loss_epoch=4.13e+6]Epoch 265/3000:   9%|▉         | 265/3000 [01:51&lt;16:16,  2.80it/s, v_num=1, train_loss_step=3.89e+6, train_loss_epoch=4.12e+6]Epoch 266/3000:   9%|▉         | 265/3000 [01:51&lt;16:16,  2.80it/s, v_num=1, train_loss_step=3.89e+6, train_loss_epoch=4.12e+6]Epoch 266/3000:   9%|▉         | 266/3000 [01:52&lt;19:00,  2.40it/s, v_num=1, train_loss_step=3.89e+6, train_loss_epoch=4.12e+6]Epoch 266/3000:   9%|▉         | 266/3000 [01:52&lt;19:00,  2.40it/s, v_num=1, train_loss_step=4.32e+6, train_loss_epoch=4.11e+6]Epoch 267/3000:   9%|▉         | 266/3000 [01:52&lt;19:00,  2.40it/s, v_num=1, train_loss_step=4.32e+6, train_loss_epoch=4.11e+6]Epoch 267/3000:   9%|▉         | 267/3000 [01:52&lt;20:39,  2.21it/s, v_num=1, train_loss_step=4.32e+6, train_loss_epoch=4.11e+6]Epoch 267/3000:   9%|▉         | 267/3000 [01:52&lt;20:39,  2.21it/s, v_num=1, train_loss_step=4.38e+6, train_loss_epoch=4.09e+6]Epoch 268/3000:   9%|▉         | 267/3000 [01:52&lt;20:39,  2.21it/s, v_num=1, train_loss_step=4.38e+6, train_loss_epoch=4.09e+6]Epoch 268/3000:   9%|▉         | 268/3000 [01:53&lt;21:57,  2.07it/s, v_num=1, train_loss_step=4.38e+6, train_loss_epoch=4.09e+6]Epoch 268/3000:   9%|▉         | 268/3000 [01:53&lt;21:57,  2.07it/s, v_num=1, train_loss_step=4.15e+6, train_loss_epoch=4.08e+6]Epoch 269/3000:   9%|▉         | 268/3000 [01:53&lt;21:57,  2.07it/s, v_num=1, train_loss_step=4.15e+6, train_loss_epoch=4.08e+6]Epoch 269/3000:   9%|▉         | 269/3000 [01:53&lt;21:14,  2.14it/s, v_num=1, train_loss_step=4.15e+6, train_loss_epoch=4.08e+6]Epoch 269/3000:   9%|▉         | 269/3000 [01:53&lt;21:14,  2.14it/s, v_num=1, train_loss_step=4.23e+6, train_loss_epoch=4.07e+6]Epoch 270/3000:   9%|▉         | 269/3000 [01:53&lt;21:14,  2.14it/s, v_num=1, train_loss_step=4.23e+6, train_loss_epoch=4.07e+6]Epoch 270/3000:   9%|▉         | 270/3000 [01:54&lt;22:45,  2.00it/s, v_num=1, train_loss_step=4.23e+6, train_loss_epoch=4.07e+6]Epoch 270/3000:   9%|▉         | 270/3000 [01:54&lt;22:45,  2.00it/s, v_num=1, train_loss_step=4.13e+6, train_loss_epoch=4.06e+6]Epoch 271/3000:   9%|▉         | 270/3000 [01:54&lt;22:45,  2.00it/s, v_num=1, train_loss_step=4.13e+6, train_loss_epoch=4.06e+6]Epoch 271/3000:   9%|▉         | 271/3000 [01:54&lt;22:14,  2.04it/s, v_num=1, train_loss_step=4.13e+6, train_loss_epoch=4.06e+6]Epoch 271/3000:   9%|▉         | 271/3000 [01:54&lt;22:14,  2.04it/s, v_num=1, train_loss_step=4.13e+6, train_loss_epoch=4.05e+6]Epoch 272/3000:   9%|▉         | 271/3000 [01:54&lt;22:14,  2.04it/s, v_num=1, train_loss_step=4.13e+6, train_loss_epoch=4.05e+6]Epoch 272/3000:   9%|▉         | 272/3000 [01:54&lt;18:05,  2.51it/s, v_num=1, train_loss_step=4.13e+6, train_loss_epoch=4.05e+6]Epoch 272/3000:   9%|▉         | 272/3000 [01:54&lt;18:05,  2.51it/s, v_num=1, train_loss_step=4.07e+6, train_loss_epoch=4.04e+6]Epoch 273/3000:   9%|▉         | 272/3000 [01:54&lt;18:05,  2.51it/s, v_num=1, train_loss_step=4.07e+6, train_loss_epoch=4.04e+6]Epoch 273/3000:   9%|▉         | 273/3000 [01:55&lt;18:47,  2.42it/s, v_num=1, train_loss_step=4.07e+6, train_loss_epoch=4.04e+6]Epoch 273/3000:   9%|▉         | 273/3000 [01:55&lt;18:47,  2.42it/s, v_num=1, train_loss_step=4e+6, train_loss_epoch=4.02e+6]   Epoch 274/3000:   9%|▉         | 273/3000 [01:55&lt;18:47,  2.42it/s, v_num=1, train_loss_step=4e+6, train_loss_epoch=4.02e+6]Epoch 274/3000:   9%|▉         | 274/3000 [01:55&lt;20:28,  2.22it/s, v_num=1, train_loss_step=4e+6, train_loss_epoch=4.02e+6]Epoch 274/3000:   9%|▉         | 274/3000 [01:55&lt;20:28,  2.22it/s, v_num=1, train_loss_step=3.85e+6, train_loss_epoch=4.01e+6]Epoch 275/3000:   9%|▉         | 274/3000 [01:55&lt;20:28,  2.22it/s, v_num=1, train_loss_step=3.85e+6, train_loss_epoch=4.01e+6]Epoch 275/3000:   9%|▉         | 275/3000 [01:56&lt;21:16,  2.14it/s, v_num=1, train_loss_step=3.85e+6, train_loss_epoch=4.01e+6]Epoch 275/3000:   9%|▉         | 275/3000 [01:56&lt;21:16,  2.14it/s, v_num=1, train_loss_step=3.98e+6, train_loss_epoch=4e+6]   Epoch 276/3000:   9%|▉         | 275/3000 [01:56&lt;21:16,  2.14it/s, v_num=1, train_loss_step=3.98e+6, train_loss_epoch=4e+6]Epoch 276/3000:   9%|▉         | 276/3000 [01:56&lt;19:19,  2.35it/s, v_num=1, train_loss_step=3.98e+6, train_loss_epoch=4e+6]Epoch 276/3000:   9%|▉         | 276/3000 [01:56&lt;19:19,  2.35it/s, v_num=1, train_loss_step=4.1e+6, train_loss_epoch=3.99e+6]Epoch 277/3000:   9%|▉         | 276/3000 [01:56&lt;19:19,  2.35it/s, v_num=1, train_loss_step=4.1e+6, train_loss_epoch=3.99e+6]Epoch 277/3000:   9%|▉         | 277/3000 [01:57&lt;18:14,  2.49it/s, v_num=1, train_loss_step=4.1e+6, train_loss_epoch=3.99e+6]Epoch 277/3000:   9%|▉         | 277/3000 [01:57&lt;18:14,  2.49it/s, v_num=1, train_loss_step=4.15e+6, train_loss_epoch=3.98e+6]Epoch 278/3000:   9%|▉         | 277/3000 [01:57&lt;18:14,  2.49it/s, v_num=1, train_loss_step=4.15e+6, train_loss_epoch=3.98e+6]Epoch 278/3000:   9%|▉         | 278/3000 [01:57&lt;21:19,  2.13it/s, v_num=1, train_loss_step=4.15e+6, train_loss_epoch=3.98e+6]Epoch 278/3000:   9%|▉         | 278/3000 [01:57&lt;21:19,  2.13it/s, v_num=1, train_loss_step=4.06e+6, train_loss_epoch=3.97e+6]Epoch 279/3000:   9%|▉         | 278/3000 [01:57&lt;21:19,  2.13it/s, v_num=1, train_loss_step=4.06e+6, train_loss_epoch=3.97e+6]Epoch 279/3000:   9%|▉         | 279/3000 [01:58&lt;21:48,  2.08it/s, v_num=1, train_loss_step=4.06e+6, train_loss_epoch=3.97e+6]Epoch 279/3000:   9%|▉         | 279/3000 [01:58&lt;21:48,  2.08it/s, v_num=1, train_loss_step=3.96e+6, train_loss_epoch=3.96e+6]Epoch 280/3000:   9%|▉         | 279/3000 [01:58&lt;21:48,  2.08it/s, v_num=1, train_loss_step=3.96e+6, train_loss_epoch=3.96e+6]Epoch 280/3000:   9%|▉         | 280/3000 [01:58&lt;20:57,  2.16it/s, v_num=1, train_loss_step=3.96e+6, train_loss_epoch=3.96e+6]Epoch 280/3000:   9%|▉         | 280/3000 [01:58&lt;20:57,  2.16it/s, v_num=1, train_loss_step=3.83e+6, train_loss_epoch=3.95e+6]Epoch 281/3000:   9%|▉         | 280/3000 [01:58&lt;20:57,  2.16it/s, v_num=1, train_loss_step=3.83e+6, train_loss_epoch=3.95e+6]Epoch 281/3000:   9%|▉         | 281/3000 [01:58&lt;20:32,  2.21it/s, v_num=1, train_loss_step=3.83e+6, train_loss_epoch=3.95e+6]Epoch 281/3000:   9%|▉         | 281/3000 [01:59&lt;20:32,  2.21it/s, v_num=1, train_loss_step=4.02e+6, train_loss_epoch=3.94e+6]Epoch 282/3000:   9%|▉         | 281/3000 [01:59&lt;20:32,  2.21it/s, v_num=1, train_loss_step=4.02e+6, train_loss_epoch=3.94e+6]Epoch 282/3000:   9%|▉         | 282/3000 [01:59&lt;20:57,  2.16it/s, v_num=1, train_loss_step=4.02e+6, train_loss_epoch=3.94e+6]Epoch 282/3000:   9%|▉         | 282/3000 [01:59&lt;20:57,  2.16it/s, v_num=1, train_loss_step=3.89e+6, train_loss_epoch=3.93e+6]Epoch 283/3000:   9%|▉         | 282/3000 [01:59&lt;20:57,  2.16it/s, v_num=1, train_loss_step=3.89e+6, train_loss_epoch=3.93e+6]Epoch 283/3000:   9%|▉         | 283/3000 [01:59&lt;21:38,  2.09it/s, v_num=1, train_loss_step=3.89e+6, train_loss_epoch=3.93e+6]Epoch 283/3000:   9%|▉         | 283/3000 [02:00&lt;21:38,  2.09it/s, v_num=1, train_loss_step=3.94e+6, train_loss_epoch=3.91e+6]Epoch 284/3000:   9%|▉         | 283/3000 [02:00&lt;21:38,  2.09it/s, v_num=1, train_loss_step=3.94e+6, train_loss_epoch=3.91e+6]Epoch 284/3000:   9%|▉         | 284/3000 [02:00&lt;23:18,  1.94it/s, v_num=1, train_loss_step=3.94e+6, train_loss_epoch=3.91e+6]Epoch 284/3000:   9%|▉         | 284/3000 [02:00&lt;23:18,  1.94it/s, v_num=1, train_loss_step=3.97e+6, train_loss_epoch=3.9e+6] Epoch 285/3000:   9%|▉         | 284/3000 [02:00&lt;23:18,  1.94it/s, v_num=1, train_loss_step=3.97e+6, train_loss_epoch=3.9e+6]Epoch 285/3000:  10%|▉         | 285/3000 [02:00&lt;20:11,  2.24it/s, v_num=1, train_loss_step=3.97e+6, train_loss_epoch=3.9e+6]Epoch 285/3000:  10%|▉         | 285/3000 [02:00&lt;20:11,  2.24it/s, v_num=1, train_loss_step=3.95e+6, train_loss_epoch=3.89e+6]Epoch 286/3000:  10%|▉         | 285/3000 [02:00&lt;20:11,  2.24it/s, v_num=1, train_loss_step=3.95e+6, train_loss_epoch=3.89e+6]Epoch 286/3000:  10%|▉         | 286/3000 [02:01&lt;18:38,  2.43it/s, v_num=1, train_loss_step=3.95e+6, train_loss_epoch=3.89e+6]Epoch 286/3000:  10%|▉         | 286/3000 [02:01&lt;18:38,  2.43it/s, v_num=1, train_loss_step=3.8e+6, train_loss_epoch=3.88e+6] Epoch 287/3000:  10%|▉         | 286/3000 [02:01&lt;18:38,  2.43it/s, v_num=1, train_loss_step=3.8e+6, train_loss_epoch=3.88e+6]Epoch 287/3000:  10%|▉         | 287/3000 [02:01&lt;17:06,  2.64it/s, v_num=1, train_loss_step=3.8e+6, train_loss_epoch=3.88e+6]Epoch 287/3000:  10%|▉         | 287/3000 [02:01&lt;17:06,  2.64it/s, v_num=1, train_loss_step=3.92e+6, train_loss_epoch=3.87e+6]Epoch 288/3000:  10%|▉         | 287/3000 [02:01&lt;17:06,  2.64it/s, v_num=1, train_loss_step=3.92e+6, train_loss_epoch=3.87e+6]Epoch 288/3000:  10%|▉         | 288/3000 [02:02&lt;18:33,  2.44it/s, v_num=1, train_loss_step=3.92e+6, train_loss_epoch=3.87e+6]Epoch 288/3000:  10%|▉         | 288/3000 [02:02&lt;18:33,  2.44it/s, v_num=1, train_loss_step=4.01e+6, train_loss_epoch=3.86e+6]Epoch 289/3000:  10%|▉         | 288/3000 [02:02&lt;18:33,  2.44it/s, v_num=1, train_loss_step=4.01e+6, train_loss_epoch=3.86e+6]Epoch 289/3000:  10%|▉         | 289/3000 [02:02&lt;18:40,  2.42it/s, v_num=1, train_loss_step=4.01e+6, train_loss_epoch=3.86e+6]Epoch 289/3000:  10%|▉         | 289/3000 [02:02&lt;18:40,  2.42it/s, v_num=1, train_loss_step=3.99e+6, train_loss_epoch=3.85e+6]Epoch 290/3000:  10%|▉         | 289/3000 [02:02&lt;18:40,  2.42it/s, v_num=1, train_loss_step=3.99e+6, train_loss_epoch=3.85e+6]Epoch 290/3000:  10%|▉         | 290/3000 [02:02&lt;20:23,  2.21it/s, v_num=1, train_loss_step=3.99e+6, train_loss_epoch=3.85e+6]Epoch 290/3000:  10%|▉         | 290/3000 [02:02&lt;20:23,  2.21it/s, v_num=1, train_loss_step=3.91e+6, train_loss_epoch=3.84e+6]Epoch 291/3000:  10%|▉         | 290/3000 [02:02&lt;20:23,  2.21it/s, v_num=1, train_loss_step=3.91e+6, train_loss_epoch=3.84e+6]Epoch 291/3000:  10%|▉         | 291/3000 [02:03&lt;22:19,  2.02it/s, v_num=1, train_loss_step=3.91e+6, train_loss_epoch=3.84e+6]Epoch 291/3000:  10%|▉         | 291/3000 [02:03&lt;22:19,  2.02it/s, v_num=1, train_loss_step=3.64e+6, train_loss_epoch=3.83e+6]Epoch 292/3000:  10%|▉         | 291/3000 [02:03&lt;22:19,  2.02it/s, v_num=1, train_loss_step=3.64e+6, train_loss_epoch=3.83e+6]Epoch 292/3000:  10%|▉         | 292/3000 [02:04&lt;22:22,  2.02it/s, v_num=1, train_loss_step=3.64e+6, train_loss_epoch=3.83e+6]Epoch 292/3000:  10%|▉         | 292/3000 [02:04&lt;22:22,  2.02it/s, v_num=1, train_loss_step=3.69e+6, train_loss_epoch=3.82e+6]Epoch 293/3000:  10%|▉         | 292/3000 [02:04&lt;22:22,  2.02it/s, v_num=1, train_loss_step=3.69e+6, train_loss_epoch=3.82e+6]Epoch 293/3000:  10%|▉         | 293/3000 [02:04&lt;23:20,  1.93it/s, v_num=1, train_loss_step=3.69e+6, train_loss_epoch=3.82e+6]Epoch 293/3000:  10%|▉         | 293/3000 [02:04&lt;23:20,  1.93it/s, v_num=1, train_loss_step=3.83e+6, train_loss_epoch=3.81e+6]Epoch 294/3000:  10%|▉         | 293/3000 [02:04&lt;23:20,  1.93it/s, v_num=1, train_loss_step=3.83e+6, train_loss_epoch=3.81e+6]Epoch 294/3000:  10%|▉         | 294/3000 [02:05&lt;23:04,  1.95it/s, v_num=1, train_loss_step=3.83e+6, train_loss_epoch=3.81e+6]Epoch 294/3000:  10%|▉         | 294/3000 [02:05&lt;23:04,  1.95it/s, v_num=1, train_loss_step=4.16e+6, train_loss_epoch=3.8e+6] Epoch 295/3000:  10%|▉         | 294/3000 [02:05&lt;23:04,  1.95it/s, v_num=1, train_loss_step=4.16e+6, train_loss_epoch=3.8e+6]Epoch 295/3000:  10%|▉         | 295/3000 [02:05&lt;20:55,  2.15it/s, v_num=1, train_loss_step=4.16e+6, train_loss_epoch=3.8e+6]Epoch 295/3000:  10%|▉         | 295/3000 [02:05&lt;20:55,  2.15it/s, v_num=1, train_loss_step=3.86e+6, train_loss_epoch=3.79e+6]Epoch 296/3000:  10%|▉         | 295/3000 [02:05&lt;20:55,  2.15it/s, v_num=1, train_loss_step=3.86e+6, train_loss_epoch=3.79e+6]Epoch 296/3000:  10%|▉         | 296/3000 [02:05&lt;19:33,  2.30it/s, v_num=1, train_loss_step=3.86e+6, train_loss_epoch=3.79e+6]Epoch 296/3000:  10%|▉         | 296/3000 [02:05&lt;19:33,  2.30it/s, v_num=1, train_loss_step=3.76e+6, train_loss_epoch=3.78e+6]Epoch 297/3000:  10%|▉         | 296/3000 [02:05&lt;19:33,  2.30it/s, v_num=1, train_loss_step=3.76e+6, train_loss_epoch=3.78e+6]Epoch 297/3000:  10%|▉         | 297/3000 [02:06&lt;19:18,  2.33it/s, v_num=1, train_loss_step=3.76e+6, train_loss_epoch=3.78e+6]Epoch 297/3000:  10%|▉         | 297/3000 [02:06&lt;19:18,  2.33it/s, v_num=1, train_loss_step=3.76e+6, train_loss_epoch=3.77e+6]Epoch 298/3000:  10%|▉         | 297/3000 [02:06&lt;19:18,  2.33it/s, v_num=1, train_loss_step=3.76e+6, train_loss_epoch=3.77e+6]Epoch 298/3000:  10%|▉         | 298/3000 [02:06&lt;21:28,  2.10it/s, v_num=1, train_loss_step=3.76e+6, train_loss_epoch=3.77e+6]Epoch 298/3000:  10%|▉         | 298/3000 [02:06&lt;21:28,  2.10it/s, v_num=1, train_loss_step=3.79e+6, train_loss_epoch=3.76e+6]Epoch 299/3000:  10%|▉         | 298/3000 [02:06&lt;21:28,  2.10it/s, v_num=1, train_loss_step=3.79e+6, train_loss_epoch=3.76e+6]Epoch 299/3000:  10%|▉         | 299/3000 [02:07&lt;22:04,  2.04it/s, v_num=1, train_loss_step=3.79e+6, train_loss_epoch=3.76e+6]Epoch 299/3000:  10%|▉         | 299/3000 [02:07&lt;22:04,  2.04it/s, v_num=1, train_loss_step=3.75e+6, train_loss_epoch=3.75e+6]Epoch 300/3000:  10%|▉         | 299/3000 [02:07&lt;22:04,  2.04it/s, v_num=1, train_loss_step=3.75e+6, train_loss_epoch=3.75e+6]Epoch 300/3000:  10%|█         | 300/3000 [02:07&lt;22:19,  2.02it/s, v_num=1, train_loss_step=3.75e+6, train_loss_epoch=3.75e+6]Epoch 300/3000:  10%|█         | 300/3000 [02:07&lt;22:19,  2.02it/s, v_num=1, train_loss_step=3.8e+6, train_loss_epoch=3.74e+6] Epoch 301/3000:  10%|█         | 300/3000 [02:07&lt;22:19,  2.02it/s, v_num=1, train_loss_step=3.8e+6, train_loss_epoch=3.74e+6]Epoch 301/3000:  10%|█         | 301/3000 [02:08&lt;21:52,  2.06it/s, v_num=1, train_loss_step=3.8e+6, train_loss_epoch=3.74e+6]Epoch 301/3000:  10%|█         | 301/3000 [02:08&lt;21:52,  2.06it/s, v_num=1, train_loss_step=3.58e+6, train_loss_epoch=3.73e+6]Epoch 302/3000:  10%|█         | 301/3000 [02:08&lt;21:52,  2.06it/s, v_num=1, train_loss_step=3.58e+6, train_loss_epoch=3.73e+6]Epoch 302/3000:  10%|█         | 302/3000 [02:08&lt;21:20,  2.11it/s, v_num=1, train_loss_step=3.58e+6, train_loss_epoch=3.73e+6]Epoch 302/3000:  10%|█         | 302/3000 [02:08&lt;21:20,  2.11it/s, v_num=1, train_loss_step=3.66e+6, train_loss_epoch=3.72e+6]Epoch 303/3000:  10%|█         | 302/3000 [02:08&lt;21:20,  2.11it/s, v_num=1, train_loss_step=3.66e+6, train_loss_epoch=3.72e+6]Epoch 303/3000:  10%|█         | 303/3000 [02:09&lt;22:34,  1.99it/s, v_num=1, train_loss_step=3.66e+6, train_loss_epoch=3.72e+6]Epoch 303/3000:  10%|█         | 303/3000 [02:09&lt;22:34,  1.99it/s, v_num=1, train_loss_step=3.56e+6, train_loss_epoch=3.71e+6]Epoch 304/3000:  10%|█         | 303/3000 [02:09&lt;22:34,  1.99it/s, v_num=1, train_loss_step=3.56e+6, train_loss_epoch=3.71e+6]Epoch 304/3000:  10%|█         | 304/3000 [02:09&lt;19:45,  2.27it/s, v_num=1, train_loss_step=3.56e+6, train_loss_epoch=3.71e+6]Epoch 304/3000:  10%|█         | 304/3000 [02:09&lt;19:45,  2.27it/s, v_num=1, train_loss_step=3.5e+6, train_loss_epoch=3.7e+6]  Epoch 305/3000:  10%|█         | 304/3000 [02:09&lt;19:45,  2.27it/s, v_num=1, train_loss_step=3.5e+6, train_loss_epoch=3.7e+6]Epoch 305/3000:  10%|█         | 305/3000 [02:10&lt;20:03,  2.24it/s, v_num=1, train_loss_step=3.5e+6, train_loss_epoch=3.7e+6]Epoch 305/3000:  10%|█         | 305/3000 [02:10&lt;20:03,  2.24it/s, v_num=1, train_loss_step=3.76e+6, train_loss_epoch=3.69e+6]Epoch 306/3000:  10%|█         | 305/3000 [02:10&lt;20:03,  2.24it/s, v_num=1, train_loss_step=3.76e+6, train_loss_epoch=3.69e+6]Epoch 306/3000:  10%|█         | 306/3000 [02:10&lt;18:17,  2.46it/s, v_num=1, train_loss_step=3.76e+6, train_loss_epoch=3.69e+6]Epoch 306/3000:  10%|█         | 306/3000 [02:10&lt;18:17,  2.46it/s, v_num=1, train_loss_step=3.73e+6, train_loss_epoch=3.68e+6]Epoch 307/3000:  10%|█         | 306/3000 [02:10&lt;18:17,  2.46it/s, v_num=1, train_loss_step=3.73e+6, train_loss_epoch=3.68e+6]Epoch 307/3000:  10%|█         | 307/3000 [02:11&lt;20:33,  2.18it/s, v_num=1, train_loss_step=3.73e+6, train_loss_epoch=3.68e+6]Epoch 307/3000:  10%|█         | 307/3000 [02:11&lt;20:33,  2.18it/s, v_num=1, train_loss_step=3.43e+6, train_loss_epoch=3.67e+6]Epoch 308/3000:  10%|█         | 307/3000 [02:11&lt;20:33,  2.18it/s, v_num=1, train_loss_step=3.43e+6, train_loss_epoch=3.67e+6]Epoch 308/3000:  10%|█         | 308/3000 [02:11&lt;19:08,  2.34it/s, v_num=1, train_loss_step=3.43e+6, train_loss_epoch=3.67e+6]Epoch 308/3000:  10%|█         | 308/3000 [02:11&lt;19:08,  2.34it/s, v_num=1, train_loss_step=3.7e+6, train_loss_epoch=3.66e+6] Epoch 309/3000:  10%|█         | 308/3000 [02:11&lt;19:08,  2.34it/s, v_num=1, train_loss_step=3.7e+6, train_loss_epoch=3.66e+6]Epoch 309/3000:  10%|█         | 309/3000 [02:11&lt;18:01,  2.49it/s, v_num=1, train_loss_step=3.7e+6, train_loss_epoch=3.66e+6]Epoch 309/3000:  10%|█         | 309/3000 [02:11&lt;18:01,  2.49it/s, v_num=1, train_loss_step=3.61e+6, train_loss_epoch=3.66e+6]Epoch 310/3000:  10%|█         | 309/3000 [02:11&lt;18:01,  2.49it/s, v_num=1, train_loss_step=3.61e+6, train_loss_epoch=3.66e+6]Epoch 310/3000:  10%|█         | 310/3000 [02:12&lt;16:58,  2.64it/s, v_num=1, train_loss_step=3.61e+6, train_loss_epoch=3.66e+6]Epoch 310/3000:  10%|█         | 310/3000 [02:12&lt;16:58,  2.64it/s, v_num=1, train_loss_step=3.73e+6, train_loss_epoch=3.65e+6]Epoch 311/3000:  10%|█         | 310/3000 [02:12&lt;16:58,  2.64it/s, v_num=1, train_loss_step=3.73e+6, train_loss_epoch=3.65e+6]Epoch 311/3000:  10%|█         | 311/3000 [02:12&lt;14:42,  3.05it/s, v_num=1, train_loss_step=3.73e+6, train_loss_epoch=3.65e+6]Epoch 311/3000:  10%|█         | 311/3000 [02:12&lt;14:42,  3.05it/s, v_num=1, train_loss_step=3.72e+6, train_loss_epoch=3.64e+6]Epoch 312/3000:  10%|█         | 311/3000 [02:12&lt;14:42,  3.05it/s, v_num=1, train_loss_step=3.72e+6, train_loss_epoch=3.64e+6]Epoch 312/3000:  10%|█         | 312/3000 [02:12&lt;15:28,  2.89it/s, v_num=1, train_loss_step=3.72e+6, train_loss_epoch=3.64e+6]Epoch 312/3000:  10%|█         | 312/3000 [02:12&lt;15:28,  2.89it/s, v_num=1, train_loss_step=3.64e+6, train_loss_epoch=3.63e+6]Epoch 313/3000:  10%|█         | 312/3000 [02:12&lt;15:28,  2.89it/s, v_num=1, train_loss_step=3.64e+6, train_loss_epoch=3.63e+6]Epoch 313/3000:  10%|█         | 313/3000 [02:13&lt;17:46,  2.52it/s, v_num=1, train_loss_step=3.64e+6, train_loss_epoch=3.63e+6]Epoch 313/3000:  10%|█         | 313/3000 [02:13&lt;17:46,  2.52it/s, v_num=1, train_loss_step=3.67e+6, train_loss_epoch=3.62e+6]Epoch 314/3000:  10%|█         | 313/3000 [02:13&lt;17:46,  2.52it/s, v_num=1, train_loss_step=3.67e+6, train_loss_epoch=3.62e+6]Epoch 314/3000:  10%|█         | 314/3000 [02:13&lt;16:26,  2.72it/s, v_num=1, train_loss_step=3.67e+6, train_loss_epoch=3.62e+6]Epoch 314/3000:  10%|█         | 314/3000 [02:13&lt;16:26,  2.72it/s, v_num=1, train_loss_step=3.67e+6, train_loss_epoch=3.61e+6]Epoch 315/3000:  10%|█         | 314/3000 [02:13&lt;16:26,  2.72it/s, v_num=1, train_loss_step=3.67e+6, train_loss_epoch=3.61e+6]Epoch 315/3000:  10%|█         | 315/3000 [02:13&lt;17:38,  2.54it/s, v_num=1, train_loss_step=3.67e+6, train_loss_epoch=3.61e+6]Epoch 315/3000:  10%|█         | 315/3000 [02:13&lt;17:38,  2.54it/s, v_num=1, train_loss_step=3.46e+6, train_loss_epoch=3.6e+6] Epoch 316/3000:  10%|█         | 315/3000 [02:13&lt;17:38,  2.54it/s, v_num=1, train_loss_step=3.46e+6, train_loss_epoch=3.6e+6]Epoch 316/3000:  11%|█         | 316/3000 [02:14&lt;17:14,  2.59it/s, v_num=1, train_loss_step=3.46e+6, train_loss_epoch=3.6e+6]Epoch 316/3000:  11%|█         | 316/3000 [02:14&lt;17:14,  2.59it/s, v_num=1, train_loss_step=3.52e+6, train_loss_epoch=3.59e+6]Epoch 317/3000:  11%|█         | 316/3000 [02:14&lt;17:14,  2.59it/s, v_num=1, train_loss_step=3.52e+6, train_loss_epoch=3.59e+6]Epoch 317/3000:  11%|█         | 317/3000 [02:14&lt;16:12,  2.76it/s, v_num=1, train_loss_step=3.52e+6, train_loss_epoch=3.59e+6]Epoch 317/3000:  11%|█         | 317/3000 [02:14&lt;16:12,  2.76it/s, v_num=1, train_loss_step=3.43e+6, train_loss_epoch=3.58e+6]Epoch 318/3000:  11%|█         | 317/3000 [02:14&lt;16:12,  2.76it/s, v_num=1, train_loss_step=3.43e+6, train_loss_epoch=3.58e+6]Epoch 318/3000:  11%|█         | 318/3000 [02:15&lt;18:08,  2.46it/s, v_num=1, train_loss_step=3.43e+6, train_loss_epoch=3.58e+6]Epoch 318/3000:  11%|█         | 318/3000 [02:15&lt;18:08,  2.46it/s, v_num=1, train_loss_step=3.65e+6, train_loss_epoch=3.57e+6]Epoch 319/3000:  11%|█         | 318/3000 [02:15&lt;18:08,  2.46it/s, v_num=1, train_loss_step=3.65e+6, train_loss_epoch=3.57e+6]Epoch 319/3000:  11%|█         | 319/3000 [02:15&lt;19:16,  2.32it/s, v_num=1, train_loss_step=3.65e+6, train_loss_epoch=3.57e+6]Epoch 319/3000:  11%|█         | 319/3000 [02:15&lt;19:16,  2.32it/s, v_num=1, train_loss_step=3.45e+6, train_loss_epoch=3.56e+6]Epoch 320/3000:  11%|█         | 319/3000 [02:15&lt;19:16,  2.32it/s, v_num=1, train_loss_step=3.45e+6, train_loss_epoch=3.56e+6]Epoch 320/3000:  11%|█         | 320/3000 [02:16&lt;19:40,  2.27it/s, v_num=1, train_loss_step=3.45e+6, train_loss_epoch=3.56e+6]Epoch 320/3000:  11%|█         | 320/3000 [02:16&lt;19:40,  2.27it/s, v_num=1, train_loss_step=3.26e+6, train_loss_epoch=3.56e+6]Epoch 321/3000:  11%|█         | 320/3000 [02:16&lt;19:40,  2.27it/s, v_num=1, train_loss_step=3.26e+6, train_loss_epoch=3.56e+6]Epoch 321/3000:  11%|█         | 321/3000 [02:16&lt;19:50,  2.25it/s, v_num=1, train_loss_step=3.26e+6, train_loss_epoch=3.56e+6]Epoch 321/3000:  11%|█         | 321/3000 [02:16&lt;19:50,  2.25it/s, v_num=1, train_loss_step=3.4e+6, train_loss_epoch=3.55e+6] Epoch 322/3000:  11%|█         | 321/3000 [02:16&lt;19:50,  2.25it/s, v_num=1, train_loss_step=3.4e+6, train_loss_epoch=3.55e+6]Epoch 322/3000:  11%|█         | 322/3000 [02:16&lt;19:22,  2.30it/s, v_num=1, train_loss_step=3.4e+6, train_loss_epoch=3.55e+6]Epoch 322/3000:  11%|█         | 322/3000 [02:16&lt;19:22,  2.30it/s, v_num=1, train_loss_step=3.5e+6, train_loss_epoch=3.54e+6]Epoch 323/3000:  11%|█         | 322/3000 [02:16&lt;19:22,  2.30it/s, v_num=1, train_loss_step=3.5e+6, train_loss_epoch=3.54e+6]Epoch 323/3000:  11%|█         | 323/3000 [02:17&lt;20:44,  2.15it/s, v_num=1, train_loss_step=3.5e+6, train_loss_epoch=3.54e+6]Epoch 323/3000:  11%|█         | 323/3000 [02:17&lt;20:44,  2.15it/s, v_num=1, train_loss_step=3.79e+6, train_loss_epoch=3.53e+6]Epoch 324/3000:  11%|█         | 323/3000 [02:17&lt;20:44,  2.15it/s, v_num=1, train_loss_step=3.79e+6, train_loss_epoch=3.53e+6]Epoch 324/3000:  11%|█         | 324/3000 [02:17&lt;21:01,  2.12it/s, v_num=1, train_loss_step=3.79e+6, train_loss_epoch=3.53e+6]Epoch 324/3000:  11%|█         | 324/3000 [02:17&lt;21:01,  2.12it/s, v_num=1, train_loss_step=3.44e+6, train_loss_epoch=3.52e+6]Epoch 325/3000:  11%|█         | 324/3000 [02:17&lt;21:01,  2.12it/s, v_num=1, train_loss_step=3.44e+6, train_loss_epoch=3.52e+6]Epoch 325/3000:  11%|█         | 325/3000 [02:18&lt;21:26,  2.08it/s, v_num=1, train_loss_step=3.44e+6, train_loss_epoch=3.52e+6]Epoch 325/3000:  11%|█         | 325/3000 [02:18&lt;21:26,  2.08it/s, v_num=1, train_loss_step=3.48e+6, train_loss_epoch=3.51e+6]Epoch 326/3000:  11%|█         | 325/3000 [02:18&lt;21:26,  2.08it/s, v_num=1, train_loss_step=3.48e+6, train_loss_epoch=3.51e+6]Epoch 326/3000:  11%|█         | 326/3000 [02:18&lt;19:45,  2.25it/s, v_num=1, train_loss_step=3.48e+6, train_loss_epoch=3.51e+6]Epoch 326/3000:  11%|█         | 326/3000 [02:18&lt;19:45,  2.25it/s, v_num=1, train_loss_step=3.68e+6, train_loss_epoch=3.5e+6] Epoch 327/3000:  11%|█         | 326/3000 [02:18&lt;19:45,  2.25it/s, v_num=1, train_loss_step=3.68e+6, train_loss_epoch=3.5e+6]Epoch 327/3000:  11%|█         | 327/3000 [02:19&lt;21:00,  2.12it/s, v_num=1, train_loss_step=3.68e+6, train_loss_epoch=3.5e+6]Epoch 327/3000:  11%|█         | 327/3000 [02:19&lt;21:00,  2.12it/s, v_num=1, train_loss_step=3.45e+6, train_loss_epoch=3.49e+6]Epoch 328/3000:  11%|█         | 327/3000 [02:19&lt;21:00,  2.12it/s, v_num=1, train_loss_step=3.45e+6, train_loss_epoch=3.49e+6]Epoch 328/3000:  11%|█         | 328/3000 [02:19&lt;20:54,  2.13it/s, v_num=1, train_loss_step=3.45e+6, train_loss_epoch=3.49e+6]Epoch 328/3000:  11%|█         | 328/3000 [02:19&lt;20:54,  2.13it/s, v_num=1, train_loss_step=3.53e+6, train_loss_epoch=3.49e+6]Epoch 329/3000:  11%|█         | 328/3000 [02:19&lt;20:54,  2.13it/s, v_num=1, train_loss_step=3.53e+6, train_loss_epoch=3.49e+6]Epoch 329/3000:  11%|█         | 329/3000 [02:20&lt;21:20,  2.09it/s, v_num=1, train_loss_step=3.53e+6, train_loss_epoch=3.49e+6]Epoch 329/3000:  11%|█         | 329/3000 [02:20&lt;21:20,  2.09it/s, v_num=1, train_loss_step=3.48e+6, train_loss_epoch=3.48e+6]Epoch 330/3000:  11%|█         | 329/3000 [02:20&lt;21:20,  2.09it/s, v_num=1, train_loss_step=3.48e+6, train_loss_epoch=3.48e+6]Epoch 330/3000:  11%|█         | 330/3000 [02:20&lt;22:22,  1.99it/s, v_num=1, train_loss_step=3.48e+6, train_loss_epoch=3.48e+6]Epoch 330/3000:  11%|█         | 330/3000 [02:20&lt;22:22,  1.99it/s, v_num=1, train_loss_step=3.5e+6, train_loss_epoch=3.47e+6] Epoch 331/3000:  11%|█         | 330/3000 [02:20&lt;22:22,  1.99it/s, v_num=1, train_loss_step=3.5e+6, train_loss_epoch=3.47e+6]Epoch 331/3000:  11%|█         | 331/3000 [02:21&lt;22:15,  2.00it/s, v_num=1, train_loss_step=3.5e+6, train_loss_epoch=3.47e+6]Epoch 331/3000:  11%|█         | 331/3000 [02:21&lt;22:15,  2.00it/s, v_num=1, train_loss_step=3.54e+6, train_loss_epoch=3.46e+6]Epoch 332/3000:  11%|█         | 331/3000 [02:21&lt;22:15,  2.00it/s, v_num=1, train_loss_step=3.54e+6, train_loss_epoch=3.46e+6]Epoch 332/3000:  11%|█         | 332/3000 [02:21&lt;20:34,  2.16it/s, v_num=1, train_loss_step=3.54e+6, train_loss_epoch=3.46e+6]Epoch 332/3000:  11%|█         | 332/3000 [02:21&lt;20:34,  2.16it/s, v_num=1, train_loss_step=3.59e+6, train_loss_epoch=3.45e+6]Epoch 333/3000:  11%|█         | 332/3000 [02:21&lt;20:34,  2.16it/s, v_num=1, train_loss_step=3.59e+6, train_loss_epoch=3.45e+6]Epoch 333/3000:  11%|█         | 333/3000 [02:21&lt;17:46,  2.50it/s, v_num=1, train_loss_step=3.59e+6, train_loss_epoch=3.45e+6]Epoch 333/3000:  11%|█         | 333/3000 [02:21&lt;17:46,  2.50it/s, v_num=1, train_loss_step=3.33e+6, train_loss_epoch=3.44e+6]Epoch 334/3000:  11%|█         | 333/3000 [02:21&lt;17:46,  2.50it/s, v_num=1, train_loss_step=3.33e+6, train_loss_epoch=3.44e+6]Epoch 334/3000:  11%|█         | 334/3000 [02:22&lt;20:28,  2.17it/s, v_num=1, train_loss_step=3.33e+6, train_loss_epoch=3.44e+6]Epoch 334/3000:  11%|█         | 334/3000 [02:22&lt;20:28,  2.17it/s, v_num=1, train_loss_step=3.32e+6, train_loss_epoch=3.44e+6]Epoch 335/3000:  11%|█         | 334/3000 [02:22&lt;20:28,  2.17it/s, v_num=1, train_loss_step=3.32e+6, train_loss_epoch=3.44e+6]Epoch 335/3000:  11%|█         | 335/3000 [02:22&lt;16:30,  2.69it/s, v_num=1, train_loss_step=3.32e+6, train_loss_epoch=3.44e+6]Epoch 335/3000:  11%|█         | 335/3000 [02:22&lt;16:30,  2.69it/s, v_num=1, train_loss_step=3.3e+6, train_loss_epoch=3.43e+6] Epoch 336/3000:  11%|█         | 335/3000 [02:22&lt;16:30,  2.69it/s, v_num=1, train_loss_step=3.3e+6, train_loss_epoch=3.43e+6]Epoch 336/3000:  11%|█         | 336/3000 [02:22&lt;14:28,  3.07it/s, v_num=1, train_loss_step=3.3e+6, train_loss_epoch=3.43e+6]Epoch 336/3000:  11%|█         | 336/3000 [02:22&lt;14:28,  3.07it/s, v_num=1, train_loss_step=3.37e+6, train_loss_epoch=3.42e+6]Epoch 337/3000:  11%|█         | 336/3000 [02:22&lt;14:28,  3.07it/s, v_num=1, train_loss_step=3.37e+6, train_loss_epoch=3.42e+6]Epoch 337/3000:  11%|█         | 337/3000 [02:23&lt;12:30,  3.55it/s, v_num=1, train_loss_step=3.37e+6, train_loss_epoch=3.42e+6]Epoch 337/3000:  11%|█         | 337/3000 [02:23&lt;12:30,  3.55it/s, v_num=1, train_loss_step=3.47e+6, train_loss_epoch=3.41e+6]Epoch 338/3000:  11%|█         | 337/3000 [02:23&lt;12:30,  3.55it/s, v_num=1, train_loss_step=3.47e+6, train_loss_epoch=3.41e+6]Epoch 338/3000:  11%|█▏        | 338/3000 [02:23&lt;12:21,  3.59it/s, v_num=1, train_loss_step=3.47e+6, train_loss_epoch=3.41e+6]Epoch 338/3000:  11%|█▏        | 338/3000 [02:23&lt;12:21,  3.59it/s, v_num=1, train_loss_step=3.27e+6, train_loss_epoch=3.4e+6] Epoch 339/3000:  11%|█▏        | 338/3000 [02:23&lt;12:21,  3.59it/s, v_num=1, train_loss_step=3.27e+6, train_loss_epoch=3.4e+6]Epoch 339/3000:  11%|█▏        | 339/3000 [02:23&lt;13:39,  3.25it/s, v_num=1, train_loss_step=3.27e+6, train_loss_epoch=3.4e+6]Epoch 339/3000:  11%|█▏        | 339/3000 [02:23&lt;13:39,  3.25it/s, v_num=1, train_loss_step=3.49e+6, train_loss_epoch=3.39e+6]Epoch 340/3000:  11%|█▏        | 339/3000 [02:23&lt;13:39,  3.25it/s, v_num=1, train_loss_step=3.49e+6, train_loss_epoch=3.39e+6]Epoch 340/3000:  11%|█▏        | 340/3000 [02:24&lt;14:41,  3.02it/s, v_num=1, train_loss_step=3.49e+6, train_loss_epoch=3.39e+6]Epoch 340/3000:  11%|█▏        | 340/3000 [02:24&lt;14:41,  3.02it/s, v_num=1, train_loss_step=3.52e+6, train_loss_epoch=3.39e+6]Epoch 341/3000:  11%|█▏        | 340/3000 [02:24&lt;14:41,  3.02it/s, v_num=1, train_loss_step=3.52e+6, train_loss_epoch=3.39e+6]Epoch 341/3000:  11%|█▏        | 341/3000 [02:24&lt;15:13,  2.91it/s, v_num=1, train_loss_step=3.52e+6, train_loss_epoch=3.39e+6]Epoch 341/3000:  11%|█▏        | 341/3000 [02:24&lt;15:13,  2.91it/s, v_num=1, train_loss_step=3.28e+6, train_loss_epoch=3.38e+6]Epoch 342/3000:  11%|█▏        | 341/3000 [02:24&lt;15:13,  2.91it/s, v_num=1, train_loss_step=3.28e+6, train_loss_epoch=3.38e+6]Epoch 342/3000:  11%|█▏        | 342/3000 [02:24&lt;16:45,  2.64it/s, v_num=1, train_loss_step=3.28e+6, train_loss_epoch=3.38e+6]Epoch 342/3000:  11%|█▏        | 342/3000 [02:24&lt;16:45,  2.64it/s, v_num=1, train_loss_step=3.39e+6, train_loss_epoch=3.37e+6]Epoch 343/3000:  11%|█▏        | 342/3000 [02:24&lt;16:45,  2.64it/s, v_num=1, train_loss_step=3.39e+6, train_loss_epoch=3.37e+6]Epoch 343/3000:  11%|█▏        | 343/3000 [02:25&lt;17:59,  2.46it/s, v_num=1, train_loss_step=3.39e+6, train_loss_epoch=3.37e+6]Epoch 343/3000:  11%|█▏        | 343/3000 [02:25&lt;17:59,  2.46it/s, v_num=1, train_loss_step=3.41e+6, train_loss_epoch=3.36e+6]Epoch 344/3000:  11%|█▏        | 343/3000 [02:25&lt;17:59,  2.46it/s, v_num=1, train_loss_step=3.41e+6, train_loss_epoch=3.36e+6]Epoch 344/3000:  11%|█▏        | 344/3000 [02:25&lt;19:23,  2.28it/s, v_num=1, train_loss_step=3.41e+6, train_loss_epoch=3.36e+6]Epoch 344/3000:  11%|█▏        | 344/3000 [02:25&lt;19:23,  2.28it/s, v_num=1, train_loss_step=3.39e+6, train_loss_epoch=3.35e+6]Epoch 345/3000:  11%|█▏        | 344/3000 [02:25&lt;19:23,  2.28it/s, v_num=1, train_loss_step=3.39e+6, train_loss_epoch=3.35e+6]Epoch 345/3000:  12%|█▏        | 345/3000 [02:26&lt;20:14,  2.19it/s, v_num=1, train_loss_step=3.39e+6, train_loss_epoch=3.35e+6]Epoch 345/3000:  12%|█▏        | 345/3000 [02:26&lt;20:14,  2.19it/s, v_num=1, train_loss_step=3.27e+6, train_loss_epoch=3.35e+6]Epoch 346/3000:  12%|█▏        | 345/3000 [02:26&lt;20:14,  2.19it/s, v_num=1, train_loss_step=3.27e+6, train_loss_epoch=3.35e+6]Epoch 346/3000:  12%|█▏        | 346/3000 [02:26&lt;20:21,  2.17it/s, v_num=1, train_loss_step=3.27e+6, train_loss_epoch=3.35e+6]Epoch 346/3000:  12%|█▏        | 346/3000 [02:26&lt;20:21,  2.17it/s, v_num=1, train_loss_step=3.38e+6, train_loss_epoch=3.34e+6]Epoch 347/3000:  12%|█▏        | 346/3000 [02:26&lt;20:21,  2.17it/s, v_num=1, train_loss_step=3.38e+6, train_loss_epoch=3.34e+6]Epoch 347/3000:  12%|█▏        | 347/3000 [02:27&lt;18:51,  2.34it/s, v_num=1, train_loss_step=3.38e+6, train_loss_epoch=3.34e+6]Epoch 347/3000:  12%|█▏        | 347/3000 [02:27&lt;18:51,  2.34it/s, v_num=1, train_loss_step=3.26e+6, train_loss_epoch=3.33e+6]Epoch 348/3000:  12%|█▏        | 347/3000 [02:27&lt;18:51,  2.34it/s, v_num=1, train_loss_step=3.26e+6, train_loss_epoch=3.33e+6]Epoch 348/3000:  12%|█▏        | 348/3000 [02:27&lt;15:49,  2.79it/s, v_num=1, train_loss_step=3.26e+6, train_loss_epoch=3.33e+6]Epoch 348/3000:  12%|█▏        | 348/3000 [02:27&lt;15:49,  2.79it/s, v_num=1, train_loss_step=3.33e+6, train_loss_epoch=3.32e+6]Epoch 349/3000:  12%|█▏        | 348/3000 [02:27&lt;15:49,  2.79it/s, v_num=1, train_loss_step=3.33e+6, train_loss_epoch=3.32e+6]Epoch 349/3000:  12%|█▏        | 349/3000 [02:27&lt;13:06,  3.37it/s, v_num=1, train_loss_step=3.33e+6, train_loss_epoch=3.32e+6]Epoch 349/3000:  12%|█▏        | 349/3000 [02:27&lt;13:06,  3.37it/s, v_num=1, train_loss_step=3.47e+6, train_loss_epoch=3.32e+6]Epoch 350/3000:  12%|█▏        | 349/3000 [02:27&lt;13:06,  3.37it/s, v_num=1, train_loss_step=3.47e+6, train_loss_epoch=3.32e+6]Epoch 350/3000:  12%|█▏        | 350/3000 [02:27&lt;10:41,  4.13it/s, v_num=1, train_loss_step=3.47e+6, train_loss_epoch=3.32e+6]Epoch 350/3000:  12%|█▏        | 350/3000 [02:27&lt;10:41,  4.13it/s, v_num=1, train_loss_step=3.01e+6, train_loss_epoch=3.31e+6]Epoch 351/3000:  12%|█▏        | 350/3000 [02:27&lt;10:41,  4.13it/s, v_num=1, train_loss_step=3.01e+6, train_loss_epoch=3.31e+6]Epoch 351/3000:  12%|█▏        | 351/3000 [02:28&lt;13:01,  3.39it/s, v_num=1, train_loss_step=3.01e+6, train_loss_epoch=3.31e+6]Epoch 351/3000:  12%|█▏        | 351/3000 [02:28&lt;13:01,  3.39it/s, v_num=1, train_loss_step=3.35e+6, train_loss_epoch=3.3e+6] Epoch 352/3000:  12%|█▏        | 351/3000 [02:28&lt;13:01,  3.39it/s, v_num=1, train_loss_step=3.35e+6, train_loss_epoch=3.3e+6]Epoch 352/3000:  12%|█▏        | 352/3000 [02:28&lt;15:22,  2.87it/s, v_num=1, train_loss_step=3.35e+6, train_loss_epoch=3.3e+6]Epoch 352/3000:  12%|█▏        | 352/3000 [02:28&lt;15:22,  2.87it/s, v_num=1, train_loss_step=3.55e+6, train_loss_epoch=3.29e+6]Epoch 353/3000:  12%|█▏        | 352/3000 [02:28&lt;15:22,  2.87it/s, v_num=1, train_loss_step=3.55e+6, train_loss_epoch=3.29e+6]Epoch 353/3000:  12%|█▏        | 353/3000 [02:29&lt;16:59,  2.60it/s, v_num=1, train_loss_step=3.55e+6, train_loss_epoch=3.29e+6]Epoch 353/3000:  12%|█▏        | 353/3000 [02:29&lt;16:59,  2.60it/s, v_num=1, train_loss_step=3.48e+6, train_loss_epoch=3.28e+6]Epoch 354/3000:  12%|█▏        | 353/3000 [02:29&lt;16:59,  2.60it/s, v_num=1, train_loss_step=3.48e+6, train_loss_epoch=3.28e+6]Epoch 354/3000:  12%|█▏        | 354/3000 [02:29&lt;17:16,  2.55it/s, v_num=1, train_loss_step=3.48e+6, train_loss_epoch=3.28e+6]Epoch 354/3000:  12%|█▏        | 354/3000 [02:29&lt;17:16,  2.55it/s, v_num=1, train_loss_step=3.24e+6, train_loss_epoch=3.28e+6]Epoch 355/3000:  12%|█▏        | 354/3000 [02:29&lt;17:16,  2.55it/s, v_num=1, train_loss_step=3.24e+6, train_loss_epoch=3.28e+6]Epoch 355/3000:  12%|█▏        | 355/3000 [02:29&lt;18:14,  2.42it/s, v_num=1, train_loss_step=3.24e+6, train_loss_epoch=3.28e+6]Epoch 355/3000:  12%|█▏        | 355/3000 [02:29&lt;18:14,  2.42it/s, v_num=1, train_loss_step=3.25e+6, train_loss_epoch=3.27e+6]Epoch 356/3000:  12%|█▏        | 355/3000 [02:29&lt;18:14,  2.42it/s, v_num=1, train_loss_step=3.25e+6, train_loss_epoch=3.27e+6]Epoch 356/3000:  12%|█▏        | 356/3000 [02:30&lt;16:39,  2.64it/s, v_num=1, train_loss_step=3.25e+6, train_loss_epoch=3.27e+6]Epoch 356/3000:  12%|█▏        | 356/3000 [02:30&lt;16:39,  2.64it/s, v_num=1, train_loss_step=3.27e+6, train_loss_epoch=3.26e+6]Epoch 357/3000:  12%|█▏        | 356/3000 [02:30&lt;16:39,  2.64it/s, v_num=1, train_loss_step=3.27e+6, train_loss_epoch=3.26e+6]Epoch 357/3000:  12%|█▏        | 357/3000 [02:30&lt;17:56,  2.46it/s, v_num=1, train_loss_step=3.27e+6, train_loss_epoch=3.26e+6]Epoch 357/3000:  12%|█▏        | 357/3000 [02:30&lt;17:56,  2.46it/s, v_num=1, train_loss_step=3.18e+6, train_loss_epoch=3.25e+6]Epoch 358/3000:  12%|█▏        | 357/3000 [02:30&lt;17:56,  2.46it/s, v_num=1, train_loss_step=3.18e+6, train_loss_epoch=3.25e+6]Epoch 358/3000:  12%|█▏        | 358/3000 [02:31&lt;16:28,  2.67it/s, v_num=1, train_loss_step=3.18e+6, train_loss_epoch=3.25e+6]Epoch 358/3000:  12%|█▏        | 358/3000 [02:31&lt;16:28,  2.67it/s, v_num=1, train_loss_step=3.35e+6, train_loss_epoch=3.25e+6]Epoch 359/3000:  12%|█▏        | 358/3000 [02:31&lt;16:28,  2.67it/s, v_num=1, train_loss_step=3.35e+6, train_loss_epoch=3.25e+6]Epoch 359/3000:  12%|█▏        | 359/3000 [02:31&lt;19:00,  2.32it/s, v_num=1, train_loss_step=3.35e+6, train_loss_epoch=3.25e+6]Epoch 359/3000:  12%|█▏        | 359/3000 [02:31&lt;19:00,  2.32it/s, v_num=1, train_loss_step=3.31e+6, train_loss_epoch=3.24e+6]Epoch 360/3000:  12%|█▏        | 359/3000 [02:31&lt;19:00,  2.32it/s, v_num=1, train_loss_step=3.31e+6, train_loss_epoch=3.24e+6]Epoch 360/3000:  12%|█▏        | 360/3000 [02:32&lt;19:57,  2.20it/s, v_num=1, train_loss_step=3.31e+6, train_loss_epoch=3.24e+6]Epoch 360/3000:  12%|█▏        | 360/3000 [02:32&lt;19:57,  2.20it/s, v_num=1, train_loss_step=3.37e+6, train_loss_epoch=3.23e+6]Epoch 361/3000:  12%|█▏        | 360/3000 [02:32&lt;19:57,  2.20it/s, v_num=1, train_loss_step=3.37e+6, train_loss_epoch=3.23e+6]Epoch 361/3000:  12%|█▏        | 361/3000 [02:32&lt;18:24,  2.39it/s, v_num=1, train_loss_step=3.37e+6, train_loss_epoch=3.23e+6]Epoch 361/3000:  12%|█▏        | 361/3000 [02:32&lt;18:24,  2.39it/s, v_num=1, train_loss_step=3.32e+6, train_loss_epoch=3.23e+6]Epoch 362/3000:  12%|█▏        | 361/3000 [02:32&lt;18:24,  2.39it/s, v_num=1, train_loss_step=3.32e+6, train_loss_epoch=3.23e+6]Epoch 362/3000:  12%|█▏        | 362/3000 [02:32&lt;19:40,  2.23it/s, v_num=1, train_loss_step=3.32e+6, train_loss_epoch=3.23e+6]Epoch 362/3000:  12%|█▏        | 362/3000 [02:32&lt;19:40,  2.23it/s, v_num=1, train_loss_step=3.26e+6, train_loss_epoch=3.22e+6]Epoch 363/3000:  12%|█▏        | 362/3000 [02:32&lt;19:40,  2.23it/s, v_num=1, train_loss_step=3.26e+6, train_loss_epoch=3.22e+6]Epoch 363/3000:  12%|█▏        | 363/3000 [02:33&lt;19:13,  2.29it/s, v_num=1, train_loss_step=3.26e+6, train_loss_epoch=3.22e+6]Epoch 363/3000:  12%|█▏        | 363/3000 [02:33&lt;19:13,  2.29it/s, v_num=1, train_loss_step=3.09e+6, train_loss_epoch=3.21e+6]Epoch 364/3000:  12%|█▏        | 363/3000 [02:33&lt;19:13,  2.29it/s, v_num=1, train_loss_step=3.09e+6, train_loss_epoch=3.21e+6]Epoch 364/3000:  12%|█▏        | 364/3000 [02:33&lt;16:58,  2.59it/s, v_num=1, train_loss_step=3.09e+6, train_loss_epoch=3.21e+6]Epoch 364/3000:  12%|█▏        | 364/3000 [02:33&lt;16:58,  2.59it/s, v_num=1, train_loss_step=3.24e+6, train_loss_epoch=3.2e+6] Epoch 365/3000:  12%|█▏        | 364/3000 [02:33&lt;16:58,  2.59it/s, v_num=1, train_loss_step=3.24e+6, train_loss_epoch=3.2e+6]Epoch 365/3000:  12%|█▏        | 365/3000 [02:33&lt;15:17,  2.87it/s, v_num=1, train_loss_step=3.24e+6, train_loss_epoch=3.2e+6]Epoch 365/3000:  12%|█▏        | 365/3000 [02:33&lt;15:17,  2.87it/s, v_num=1, train_loss_step=3.33e+6, train_loss_epoch=3.2e+6]Epoch 366/3000:  12%|█▏        | 365/3000 [02:33&lt;15:17,  2.87it/s, v_num=1, train_loss_step=3.33e+6, train_loss_epoch=3.2e+6]Epoch 366/3000:  12%|█▏        | 366/3000 [02:34&lt;13:47,  3.18it/s, v_num=1, train_loss_step=3.33e+6, train_loss_epoch=3.2e+6]Epoch 366/3000:  12%|█▏        | 366/3000 [02:34&lt;13:47,  3.18it/s, v_num=1, train_loss_step=3.25e+6, train_loss_epoch=3.19e+6]Epoch 367/3000:  12%|█▏        | 366/3000 [02:34&lt;13:47,  3.18it/s, v_num=1, train_loss_step=3.25e+6, train_loss_epoch=3.19e+6]Epoch 367/3000:  12%|█▏        | 367/3000 [02:34&lt;16:17,  2.69it/s, v_num=1, train_loss_step=3.25e+6, train_loss_epoch=3.19e+6]Epoch 367/3000:  12%|█▏        | 367/3000 [02:34&lt;16:17,  2.69it/s, v_num=1, train_loss_step=3.27e+6, train_loss_epoch=3.18e+6]Epoch 368/3000:  12%|█▏        | 367/3000 [02:34&lt;16:17,  2.69it/s, v_num=1, train_loss_step=3.27e+6, train_loss_epoch=3.18e+6]Epoch 368/3000:  12%|█▏        | 368/3000 [02:35&lt;15:53,  2.76it/s, v_num=1, train_loss_step=3.27e+6, train_loss_epoch=3.18e+6]Epoch 368/3000:  12%|█▏        | 368/3000 [02:35&lt;15:53,  2.76it/s, v_num=1, train_loss_step=3.49e+6, train_loss_epoch=3.18e+6]Epoch 369/3000:  12%|█▏        | 368/3000 [02:35&lt;15:53,  2.76it/s, v_num=1, train_loss_step=3.49e+6, train_loss_epoch=3.18e+6]Epoch 369/3000:  12%|█▏        | 369/3000 [02:35&lt;17:03,  2.57it/s, v_num=1, train_loss_step=3.49e+6, train_loss_epoch=3.18e+6]Epoch 369/3000:  12%|█▏        | 369/3000 [02:35&lt;17:03,  2.57it/s, v_num=1, train_loss_step=3.12e+6, train_loss_epoch=3.17e+6]Epoch 370/3000:  12%|█▏        | 369/3000 [02:35&lt;17:03,  2.57it/s, v_num=1, train_loss_step=3.12e+6, train_loss_epoch=3.17e+6]Epoch 370/3000:  12%|█▏        | 370/3000 [02:36&lt;19:23,  2.26it/s, v_num=1, train_loss_step=3.12e+6, train_loss_epoch=3.17e+6]Epoch 370/3000:  12%|█▏        | 370/3000 [02:36&lt;19:23,  2.26it/s, v_num=1, train_loss_step=3e+6, train_loss_epoch=3.16e+6]   Epoch 371/3000:  12%|█▏        | 370/3000 [02:36&lt;19:23,  2.26it/s, v_num=1, train_loss_step=3e+6, train_loss_epoch=3.16e+6]Epoch 371/3000:  12%|█▏        | 371/3000 [02:36&lt;20:53,  2.10it/s, v_num=1, train_loss_step=3e+6, train_loss_epoch=3.16e+6]Epoch 371/3000:  12%|█▏        | 371/3000 [02:36&lt;20:53,  2.10it/s, v_num=1, train_loss_step=3.16e+6, train_loss_epoch=3.15e+6]Epoch 372/3000:  12%|█▏        | 371/3000 [02:36&lt;20:53,  2.10it/s, v_num=1, train_loss_step=3.16e+6, train_loss_epoch=3.15e+6]Epoch 372/3000:  12%|█▏        | 372/3000 [02:37&lt;21:39,  2.02it/s, v_num=1, train_loss_step=3.16e+6, train_loss_epoch=3.15e+6]Epoch 372/3000:  12%|█▏        | 372/3000 [02:37&lt;21:39,  2.02it/s, v_num=1, train_loss_step=3.35e+6, train_loss_epoch=3.15e+6]Epoch 373/3000:  12%|█▏        | 372/3000 [02:37&lt;21:39,  2.02it/s, v_num=1, train_loss_step=3.35e+6, train_loss_epoch=3.15e+6]Epoch 373/3000:  12%|█▏        | 373/3000 [02:37&lt;20:30,  2.14it/s, v_num=1, train_loss_step=3.35e+6, train_loss_epoch=3.15e+6]Epoch 373/3000:  12%|█▏        | 373/3000 [02:37&lt;20:30,  2.14it/s, v_num=1, train_loss_step=3.07e+6, train_loss_epoch=3.14e+6]Epoch 374/3000:  12%|█▏        | 373/3000 [02:37&lt;20:30,  2.14it/s, v_num=1, train_loss_step=3.07e+6, train_loss_epoch=3.14e+6]Epoch 374/3000:  12%|█▏        | 374/3000 [02:37&lt;18:24,  2.38it/s, v_num=1, train_loss_step=3.07e+6, train_loss_epoch=3.14e+6]Epoch 374/3000:  12%|█▏        | 374/3000 [02:37&lt;18:24,  2.38it/s, v_num=1, train_loss_step=3.18e+6, train_loss_epoch=3.13e+6]Epoch 375/3000:  12%|█▏        | 374/3000 [02:37&lt;18:24,  2.38it/s, v_num=1, train_loss_step=3.18e+6, train_loss_epoch=3.13e+6]Epoch 375/3000:  12%|█▎        | 375/3000 [02:38&lt;20:16,  2.16it/s, v_num=1, train_loss_step=3.18e+6, train_loss_epoch=3.13e+6]Epoch 375/3000:  12%|█▎        | 375/3000 [02:38&lt;20:16,  2.16it/s, v_num=1, train_loss_step=2.91e+6, train_loss_epoch=3.13e+6]Epoch 376/3000:  12%|█▎        | 375/3000 [02:38&lt;20:16,  2.16it/s, v_num=1, train_loss_step=2.91e+6, train_loss_epoch=3.13e+6]Epoch 376/3000:  13%|█▎        | 376/3000 [02:38&lt;19:35,  2.23it/s, v_num=1, train_loss_step=2.91e+6, train_loss_epoch=3.13e+6]Epoch 376/3000:  13%|█▎        | 376/3000 [02:38&lt;19:35,  2.23it/s, v_num=1, train_loss_step=3.25e+6, train_loss_epoch=3.12e+6]Epoch 377/3000:  13%|█▎        | 376/3000 [02:38&lt;19:35,  2.23it/s, v_num=1, train_loss_step=3.25e+6, train_loss_epoch=3.12e+6]Epoch 377/3000:  13%|█▎        | 377/3000 [02:39&lt;19:02,  2.30it/s, v_num=1, train_loss_step=3.25e+6, train_loss_epoch=3.12e+6]Epoch 377/3000:  13%|█▎        | 377/3000 [02:39&lt;19:02,  2.30it/s, v_num=1, train_loss_step=3.05e+6, train_loss_epoch=3.11e+6]Epoch 378/3000:  13%|█▎        | 377/3000 [02:39&lt;19:02,  2.30it/s, v_num=1, train_loss_step=3.05e+6, train_loss_epoch=3.11e+6]Epoch 378/3000:  13%|█▎        | 378/3000 [02:39&lt;16:45,  2.61it/s, v_num=1, train_loss_step=3.05e+6, train_loss_epoch=3.11e+6]Epoch 378/3000:  13%|█▎        | 378/3000 [02:39&lt;16:45,  2.61it/s, v_num=1, train_loss_step=3.16e+6, train_loss_epoch=3.11e+6]Epoch 379/3000:  13%|█▎        | 378/3000 [02:39&lt;16:45,  2.61it/s, v_num=1, train_loss_step=3.16e+6, train_loss_epoch=3.11e+6]Epoch 379/3000:  13%|█▎        | 379/3000 [02:39&lt;15:40,  2.79it/s, v_num=1, train_loss_step=3.16e+6, train_loss_epoch=3.11e+6]Epoch 379/3000:  13%|█▎        | 379/3000 [02:39&lt;15:40,  2.79it/s, v_num=1, train_loss_step=3.06e+6, train_loss_epoch=3.1e+6] Epoch 380/3000:  13%|█▎        | 379/3000 [02:39&lt;15:40,  2.79it/s, v_num=1, train_loss_step=3.06e+6, train_loss_epoch=3.1e+6]Epoch 380/3000:  13%|█▎        | 380/3000 [02:40&lt;16:26,  2.66it/s, v_num=1, train_loss_step=3.06e+6, train_loss_epoch=3.1e+6]Epoch 380/3000:  13%|█▎        | 380/3000 [02:40&lt;16:26,  2.66it/s, v_num=1, train_loss_step=2.95e+6, train_loss_epoch=3.09e+6]Epoch 381/3000:  13%|█▎        | 380/3000 [02:40&lt;16:26,  2.66it/s, v_num=1, train_loss_step=2.95e+6, train_loss_epoch=3.09e+6]Epoch 381/3000:  13%|█▎        | 381/3000 [02:40&lt;16:32,  2.64it/s, v_num=1, train_loss_step=2.95e+6, train_loss_epoch=3.09e+6]Epoch 381/3000:  13%|█▎        | 381/3000 [02:40&lt;16:32,  2.64it/s, v_num=1, train_loss_step=3.31e+6, train_loss_epoch=3.09e+6]Epoch 382/3000:  13%|█▎        | 381/3000 [02:40&lt;16:32,  2.64it/s, v_num=1, train_loss_step=3.31e+6, train_loss_epoch=3.09e+6]Epoch 382/3000:  13%|█▎        | 382/3000 [02:40&lt;15:42,  2.78it/s, v_num=1, train_loss_step=3.31e+6, train_loss_epoch=3.09e+6]Epoch 382/3000:  13%|█▎        | 382/3000 [02:40&lt;15:42,  2.78it/s, v_num=1, train_loss_step=3.18e+6, train_loss_epoch=3.08e+6]Epoch 383/3000:  13%|█▎        | 382/3000 [02:40&lt;15:42,  2.78it/s, v_num=1, train_loss_step=3.18e+6, train_loss_epoch=3.08e+6]Epoch 383/3000:  13%|█▎        | 383/3000 [02:41&lt;16:18,  2.67it/s, v_num=1, train_loss_step=3.18e+6, train_loss_epoch=3.08e+6]Epoch 383/3000:  13%|█▎        | 383/3000 [02:41&lt;16:18,  2.67it/s, v_num=1, train_loss_step=3.1e+6, train_loss_epoch=3.07e+6] Epoch 384/3000:  13%|█▎        | 383/3000 [02:41&lt;16:18,  2.67it/s, v_num=1, train_loss_step=3.1e+6, train_loss_epoch=3.07e+6]Epoch 384/3000:  13%|█▎        | 384/3000 [02:41&lt;18:19,  2.38it/s, v_num=1, train_loss_step=3.1e+6, train_loss_epoch=3.07e+6]Epoch 384/3000:  13%|█▎        | 384/3000 [02:41&lt;18:19,  2.38it/s, v_num=1, train_loss_step=3.06e+6, train_loss_epoch=3.07e+6]Epoch 385/3000:  13%|█▎        | 384/3000 [02:41&lt;18:19,  2.38it/s, v_num=1, train_loss_step=3.06e+6, train_loss_epoch=3.07e+6]Epoch 385/3000:  13%|█▎        | 385/3000 [02:42&lt;19:51,  2.20it/s, v_num=1, train_loss_step=3.06e+6, train_loss_epoch=3.07e+6]Epoch 385/3000:  13%|█▎        | 385/3000 [02:42&lt;19:51,  2.20it/s, v_num=1, train_loss_step=2.99e+6, train_loss_epoch=3.06e+6]Epoch 386/3000:  13%|█▎        | 385/3000 [02:42&lt;19:51,  2.20it/s, v_num=1, train_loss_step=2.99e+6, train_loss_epoch=3.06e+6]Epoch 386/3000:  13%|█▎        | 386/3000 [02:42&lt;21:34,  2.02it/s, v_num=1, train_loss_step=2.99e+6, train_loss_epoch=3.06e+6]Epoch 386/3000:  13%|█▎        | 386/3000 [02:42&lt;21:34,  2.02it/s, v_num=1, train_loss_step=3.11e+6, train_loss_epoch=3.05e+6]Epoch 387/3000:  13%|█▎        | 386/3000 [02:42&lt;21:34,  2.02it/s, v_num=1, train_loss_step=3.11e+6, train_loss_epoch=3.05e+6]Epoch 387/3000:  13%|█▎        | 387/3000 [02:43&lt;20:57,  2.08it/s, v_num=1, train_loss_step=3.11e+6, train_loss_epoch=3.05e+6]Epoch 387/3000:  13%|█▎        | 387/3000 [02:43&lt;20:57,  2.08it/s, v_num=1, train_loss_step=3.09e+6, train_loss_epoch=3.05e+6]Epoch 388/3000:  13%|█▎        | 387/3000 [02:43&lt;20:57,  2.08it/s, v_num=1, train_loss_step=3.09e+6, train_loss_epoch=3.05e+6]Epoch 388/3000:  13%|█▎        | 388/3000 [02:43&lt;21:16,  2.05it/s, v_num=1, train_loss_step=3.09e+6, train_loss_epoch=3.05e+6]Epoch 388/3000:  13%|█▎        | 388/3000 [02:43&lt;21:16,  2.05it/s, v_num=1, train_loss_step=2.95e+6, train_loss_epoch=3.04e+6]Epoch 389/3000:  13%|█▎        | 388/3000 [02:43&lt;21:16,  2.05it/s, v_num=1, train_loss_step=2.95e+6, train_loss_epoch=3.04e+6]Epoch 389/3000:  13%|█▎        | 389/3000 [02:44&lt;20:01,  2.17it/s, v_num=1, train_loss_step=2.95e+6, train_loss_epoch=3.04e+6]Epoch 389/3000:  13%|█▎        | 389/3000 [02:44&lt;20:01,  2.17it/s, v_num=1, train_loss_step=2.96e+6, train_loss_epoch=3.03e+6]Epoch 390/3000:  13%|█▎        | 389/3000 [02:44&lt;20:01,  2.17it/s, v_num=1, train_loss_step=2.96e+6, train_loss_epoch=3.03e+6]Epoch 390/3000:  13%|█▎        | 390/3000 [02:44&lt;20:09,  2.16it/s, v_num=1, train_loss_step=2.96e+6, train_loss_epoch=3.03e+6]Epoch 390/3000:  13%|█▎        | 390/3000 [02:44&lt;20:09,  2.16it/s, v_num=1, train_loss_step=3.15e+6, train_loss_epoch=3.03e+6]Epoch 391/3000:  13%|█▎        | 390/3000 [02:44&lt;20:09,  2.16it/s, v_num=1, train_loss_step=3.15e+6, train_loss_epoch=3.03e+6]Epoch 391/3000:  13%|█▎        | 391/3000 [02:45&lt;20:55,  2.08it/s, v_num=1, train_loss_step=3.15e+6, train_loss_epoch=3.03e+6]Epoch 391/3000:  13%|█▎        | 391/3000 [02:45&lt;20:55,  2.08it/s, v_num=1, train_loss_step=2.9e+6, train_loss_epoch=3.02e+6] Epoch 392/3000:  13%|█▎        | 391/3000 [02:45&lt;20:55,  2.08it/s, v_num=1, train_loss_step=2.9e+6, train_loss_epoch=3.02e+6]Epoch 392/3000:  13%|█▎        | 392/3000 [02:45&lt;20:10,  2.15it/s, v_num=1, train_loss_step=2.9e+6, train_loss_epoch=3.02e+6]Epoch 392/3000:  13%|█▎        | 392/3000 [02:45&lt;20:10,  2.15it/s, v_num=1, train_loss_step=3.09e+6, train_loss_epoch=3.01e+6]Epoch 393/3000:  13%|█▎        | 392/3000 [02:45&lt;20:10,  2.15it/s, v_num=1, train_loss_step=3.09e+6, train_loss_epoch=3.01e+6]Epoch 393/3000:  13%|█▎        | 393/3000 [02:46&lt;18:19,  2.37it/s, v_num=1, train_loss_step=3.09e+6, train_loss_epoch=3.01e+6]Epoch 393/3000:  13%|█▎        | 393/3000 [02:46&lt;18:19,  2.37it/s, v_num=1, train_loss_step=3.04e+6, train_loss_epoch=3.01e+6]Epoch 394/3000:  13%|█▎        | 393/3000 [02:46&lt;18:19,  2.37it/s, v_num=1, train_loss_step=3.04e+6, train_loss_epoch=3.01e+6]Epoch 394/3000:  13%|█▎        | 394/3000 [02:46&lt;18:09,  2.39it/s, v_num=1, train_loss_step=3.04e+6, train_loss_epoch=3.01e+6]Epoch 394/3000:  13%|█▎        | 394/3000 [02:46&lt;18:09,  2.39it/s, v_num=1, train_loss_step=3.06e+6, train_loss_epoch=3e+6]   Epoch 395/3000:  13%|█▎        | 394/3000 [02:46&lt;18:09,  2.39it/s, v_num=1, train_loss_step=3.06e+6, train_loss_epoch=3e+6]Epoch 395/3000:  13%|█▎        | 395/3000 [02:46&lt;18:23,  2.36it/s, v_num=1, train_loss_step=3.06e+6, train_loss_epoch=3e+6]Epoch 395/3000:  13%|█▎        | 395/3000 [02:46&lt;18:23,  2.36it/s, v_num=1, train_loss_step=3.21e+6, train_loss_epoch=3e+6]Epoch 396/3000:  13%|█▎        | 395/3000 [02:46&lt;18:23,  2.36it/s, v_num=1, train_loss_step=3.21e+6, train_loss_epoch=3e+6]Epoch 396/3000:  13%|█▎        | 396/3000 [02:47&lt;19:49,  2.19it/s, v_num=1, train_loss_step=3.21e+6, train_loss_epoch=3e+6]Epoch 396/3000:  13%|█▎        | 396/3000 [02:47&lt;19:49,  2.19it/s, v_num=1, train_loss_step=3.1e+6, train_loss_epoch=2.99e+6]Epoch 397/3000:  13%|█▎        | 396/3000 [02:47&lt;19:49,  2.19it/s, v_num=1, train_loss_step=3.1e+6, train_loss_epoch=2.99e+6]Epoch 397/3000:  13%|█▎        | 397/3000 [02:47&lt;18:01,  2.41it/s, v_num=1, train_loss_step=3.1e+6, train_loss_epoch=2.99e+6]Epoch 397/3000:  13%|█▎        | 397/3000 [02:47&lt;18:01,  2.41it/s, v_num=1, train_loss_step=2.98e+6, train_loss_epoch=2.98e+6]Epoch 398/3000:  13%|█▎        | 397/3000 [02:47&lt;18:01,  2.41it/s, v_num=1, train_loss_step=2.98e+6, train_loss_epoch=2.98e+6]Epoch 398/3000:  13%|█▎        | 398/3000 [02:48&lt;16:15,  2.67it/s, v_num=1, train_loss_step=2.98e+6, train_loss_epoch=2.98e+6]Epoch 398/3000:  13%|█▎        | 398/3000 [02:48&lt;16:15,  2.67it/s, v_num=1, train_loss_step=2.96e+6, train_loss_epoch=2.98e+6]Epoch 399/3000:  13%|█▎        | 398/3000 [02:48&lt;16:15,  2.67it/s, v_num=1, train_loss_step=2.96e+6, train_loss_epoch=2.98e+6]Epoch 399/3000:  13%|█▎        | 399/3000 [02:48&lt;17:43,  2.45it/s, v_num=1, train_loss_step=2.96e+6, train_loss_epoch=2.98e+6]Epoch 399/3000:  13%|█▎        | 399/3000 [02:48&lt;17:43,  2.45it/s, v_num=1, train_loss_step=3.03e+6, train_loss_epoch=2.97e+6]Epoch 400/3000:  13%|█▎        | 399/3000 [02:48&lt;17:43,  2.45it/s, v_num=1, train_loss_step=3.03e+6, train_loss_epoch=2.97e+6]Epoch 400/3000:  13%|█▎        | 400/3000 [02:48&lt;18:02,  2.40it/s, v_num=1, train_loss_step=3.03e+6, train_loss_epoch=2.97e+6]Epoch 400/3000:  13%|█▎        | 400/3000 [02:48&lt;18:02,  2.40it/s, v_num=1, train_loss_step=2.84e+6, train_loss_epoch=2.96e+6]Epoch 401/3000:  13%|█▎        | 400/3000 [02:48&lt;18:02,  2.40it/s, v_num=1, train_loss_step=2.84e+6, train_loss_epoch=2.96e+6]Epoch 401/3000:  13%|█▎        | 401/3000 [02:49&lt;19:27,  2.23it/s, v_num=1, train_loss_step=2.84e+6, train_loss_epoch=2.96e+6]Epoch 401/3000:  13%|█▎        | 401/3000 [02:49&lt;19:27,  2.23it/s, v_num=1, train_loss_step=2.99e+6, train_loss_epoch=2.96e+6]Epoch 402/3000:  13%|█▎        | 401/3000 [02:49&lt;19:27,  2.23it/s, v_num=1, train_loss_step=2.99e+6, train_loss_epoch=2.96e+6]Epoch 402/3000:  13%|█▎        | 402/3000 [02:49&lt;20:06,  2.15it/s, v_num=1, train_loss_step=2.99e+6, train_loss_epoch=2.96e+6]Epoch 402/3000:  13%|█▎        | 402/3000 [02:49&lt;20:06,  2.15it/s, v_num=1, train_loss_step=3.08e+6, train_loss_epoch=2.95e+6]Epoch 403/3000:  13%|█▎        | 402/3000 [02:49&lt;20:06,  2.15it/s, v_num=1, train_loss_step=3.08e+6, train_loss_epoch=2.95e+6]Epoch 403/3000:  13%|█▎        | 403/3000 [02:50&lt;20:13,  2.14it/s, v_num=1, train_loss_step=3.08e+6, train_loss_epoch=2.95e+6]Epoch 403/3000:  13%|█▎        | 403/3000 [02:50&lt;20:13,  2.14it/s, v_num=1, train_loss_step=2.96e+6, train_loss_epoch=2.95e+6]Epoch 404/3000:  13%|█▎        | 403/3000 [02:50&lt;20:13,  2.14it/s, v_num=1, train_loss_step=2.96e+6, train_loss_epoch=2.95e+6]Epoch 404/3000:  13%|█▎        | 404/3000 [02:50&lt;21:00,  2.06it/s, v_num=1, train_loss_step=2.96e+6, train_loss_epoch=2.95e+6]Epoch 404/3000:  13%|█▎        | 404/3000 [02:50&lt;21:00,  2.06it/s, v_num=1, train_loss_step=2.8e+6, train_loss_epoch=2.94e+6] Epoch 405/3000:  13%|█▎        | 404/3000 [02:50&lt;21:00,  2.06it/s, v_num=1, train_loss_step=2.8e+6, train_loss_epoch=2.94e+6]Epoch 405/3000:  14%|█▎        | 405/3000 [02:51&lt;20:23,  2.12it/s, v_num=1, train_loss_step=2.8e+6, train_loss_epoch=2.94e+6]Epoch 405/3000:  14%|█▎        | 405/3000 [02:51&lt;20:23,  2.12it/s, v_num=1, train_loss_step=3.18e+6, train_loss_epoch=2.93e+6]Epoch 406/3000:  14%|█▎        | 405/3000 [02:51&lt;20:23,  2.12it/s, v_num=1, train_loss_step=3.18e+6, train_loss_epoch=2.93e+6]Epoch 406/3000:  14%|█▎        | 406/3000 [02:51&lt;21:38,  2.00it/s, v_num=1, train_loss_step=3.18e+6, train_loss_epoch=2.93e+6]Epoch 406/3000:  14%|█▎        | 406/3000 [02:51&lt;21:38,  2.00it/s, v_num=1, train_loss_step=2.97e+6, train_loss_epoch=2.93e+6]Epoch 407/3000:  14%|█▎        | 406/3000 [02:51&lt;21:38,  2.00it/s, v_num=1, train_loss_step=2.97e+6, train_loss_epoch=2.93e+6]Epoch 407/3000:  14%|█▎        | 407/3000 [02:52&lt;19:25,  2.22it/s, v_num=1, train_loss_step=2.97e+6, train_loss_epoch=2.93e+6]Epoch 407/3000:  14%|█▎        | 407/3000 [02:52&lt;19:25,  2.22it/s, v_num=1, train_loss_step=2.84e+6, train_loss_epoch=2.92e+6]Epoch 408/3000:  14%|█▎        | 407/3000 [02:52&lt;19:25,  2.22it/s, v_num=1, train_loss_step=2.84e+6, train_loss_epoch=2.92e+6]Epoch 408/3000:  14%|█▎        | 408/3000 [02:52&lt;19:46,  2.19it/s, v_num=1, train_loss_step=2.84e+6, train_loss_epoch=2.92e+6]Epoch 408/3000:  14%|█▎        | 408/3000 [02:52&lt;19:46,  2.19it/s, v_num=1, train_loss_step=3.06e+6, train_loss_epoch=2.92e+6]Epoch 409/3000:  14%|█▎        | 408/3000 [02:52&lt;19:46,  2.19it/s, v_num=1, train_loss_step=3.06e+6, train_loss_epoch=2.92e+6]Epoch 409/3000:  14%|█▎        | 409/3000 [02:53&lt;19:45,  2.18it/s, v_num=1, train_loss_step=3.06e+6, train_loss_epoch=2.92e+6]Epoch 409/3000:  14%|█▎        | 409/3000 [02:53&lt;19:45,  2.18it/s, v_num=1, train_loss_step=3.12e+6, train_loss_epoch=2.91e+6]Epoch 410/3000:  14%|█▎        | 409/3000 [02:53&lt;19:45,  2.18it/s, v_num=1, train_loss_step=3.12e+6, train_loss_epoch=2.91e+6]Epoch 410/3000:  14%|█▎        | 410/3000 [02:53&lt;21:00,  2.05it/s, v_num=1, train_loss_step=3.12e+6, train_loss_epoch=2.91e+6]Epoch 410/3000:  14%|█▎        | 410/3000 [02:53&lt;21:00,  2.05it/s, v_num=1, train_loss_step=2.82e+6, train_loss_epoch=2.9e+6] Epoch 411/3000:  14%|█▎        | 410/3000 [02:53&lt;21:00,  2.05it/s, v_num=1, train_loss_step=2.82e+6, train_loss_epoch=2.9e+6]Epoch 411/3000:  14%|█▎        | 411/3000 [02:54&lt;22:36,  1.91it/s, v_num=1, train_loss_step=2.82e+6, train_loss_epoch=2.9e+6]Epoch 411/3000:  14%|█▎        | 411/3000 [02:54&lt;22:36,  1.91it/s, v_num=1, train_loss_step=2.73e+6, train_loss_epoch=2.9e+6]Epoch 412/3000:  14%|█▎        | 411/3000 [02:54&lt;22:36,  1.91it/s, v_num=1, train_loss_step=2.73e+6, train_loss_epoch=2.9e+6]Epoch 412/3000:  14%|█▎        | 412/3000 [02:54&lt;21:26,  2.01it/s, v_num=1, train_loss_step=2.73e+6, train_loss_epoch=2.9e+6]Epoch 412/3000:  14%|█▎        | 412/3000 [02:54&lt;21:26,  2.01it/s, v_num=1, train_loss_step=2.85e+6, train_loss_epoch=2.89e+6]Epoch 413/3000:  14%|█▎        | 412/3000 [02:54&lt;21:26,  2.01it/s, v_num=1, train_loss_step=2.85e+6, train_loss_epoch=2.89e+6]Epoch 413/3000:  14%|█▍        | 413/3000 [02:55&lt;21:48,  1.98it/s, v_num=1, train_loss_step=2.85e+6, train_loss_epoch=2.89e+6]Epoch 413/3000:  14%|█▍        | 413/3000 [02:55&lt;21:48,  1.98it/s, v_num=1, train_loss_step=2.87e+6, train_loss_epoch=2.89e+6]Epoch 414/3000:  14%|█▍        | 413/3000 [02:55&lt;21:48,  1.98it/s, v_num=1, train_loss_step=2.87e+6, train_loss_epoch=2.89e+6]Epoch 414/3000:  14%|█▍        | 414/3000 [02:56&lt;23:31,  1.83it/s, v_num=1, train_loss_step=2.87e+6, train_loss_epoch=2.89e+6]Epoch 414/3000:  14%|█▍        | 414/3000 [02:56&lt;23:31,  1.83it/s, v_num=1, train_loss_step=2.94e+6, train_loss_epoch=2.88e+6]Epoch 415/3000:  14%|█▍        | 414/3000 [02:56&lt;23:31,  1.83it/s, v_num=1, train_loss_step=2.94e+6, train_loss_epoch=2.88e+6]Epoch 415/3000:  14%|█▍        | 415/3000 [02:56&lt;22:50,  1.89it/s, v_num=1, train_loss_step=2.94e+6, train_loss_epoch=2.88e+6]Epoch 415/3000:  14%|█▍        | 415/3000 [02:56&lt;22:50,  1.89it/s, v_num=1, train_loss_step=2.89e+6, train_loss_epoch=2.88e+6]Epoch 416/3000:  14%|█▍        | 415/3000 [02:56&lt;22:50,  1.89it/s, v_num=1, train_loss_step=2.89e+6, train_loss_epoch=2.88e+6]Epoch 416/3000:  14%|█▍        | 416/3000 [02:56&lt;20:37,  2.09it/s, v_num=1, train_loss_step=2.89e+6, train_loss_epoch=2.88e+6]Epoch 416/3000:  14%|█▍        | 416/3000 [02:56&lt;20:37,  2.09it/s, v_num=1, train_loss_step=2.95e+6, train_loss_epoch=2.87e+6]Epoch 417/3000:  14%|█▍        | 416/3000 [02:56&lt;20:37,  2.09it/s, v_num=1, train_loss_step=2.95e+6, train_loss_epoch=2.87e+6]Epoch 417/3000:  14%|█▍        | 417/3000 [02:57&lt;19:22,  2.22it/s, v_num=1, train_loss_step=2.95e+6, train_loss_epoch=2.87e+6]Epoch 417/3000:  14%|█▍        | 417/3000 [02:57&lt;19:22,  2.22it/s, v_num=1, train_loss_step=2.77e+6, train_loss_epoch=2.86e+6]Epoch 418/3000:  14%|█▍        | 417/3000 [02:57&lt;19:22,  2.22it/s, v_num=1, train_loss_step=2.77e+6, train_loss_epoch=2.86e+6]Epoch 418/3000:  14%|█▍        | 418/3000 [02:57&lt;18:56,  2.27it/s, v_num=1, train_loss_step=2.77e+6, train_loss_epoch=2.86e+6]Epoch 418/3000:  14%|█▍        | 418/3000 [02:57&lt;18:56,  2.27it/s, v_num=1, train_loss_step=2.76e+6, train_loss_epoch=2.86e+6]Epoch 419/3000:  14%|█▍        | 418/3000 [02:57&lt;18:56,  2.27it/s, v_num=1, train_loss_step=2.76e+6, train_loss_epoch=2.86e+6]Epoch 419/3000:  14%|█▍        | 419/3000 [02:58&lt;18:54,  2.28it/s, v_num=1, train_loss_step=2.76e+6, train_loss_epoch=2.86e+6]Epoch 419/3000:  14%|█▍        | 419/3000 [02:58&lt;18:54,  2.28it/s, v_num=1, train_loss_step=2.89e+6, train_loss_epoch=2.85e+6]Epoch 420/3000:  14%|█▍        | 419/3000 [02:58&lt;18:54,  2.28it/s, v_num=1, train_loss_step=2.89e+6, train_loss_epoch=2.85e+6]Epoch 420/3000:  14%|█▍        | 420/3000 [02:58&lt;19:37,  2.19it/s, v_num=1, train_loss_step=2.89e+6, train_loss_epoch=2.85e+6]Epoch 420/3000:  14%|█▍        | 420/3000 [02:58&lt;19:37,  2.19it/s, v_num=1, train_loss_step=2.96e+6, train_loss_epoch=2.85e+6]Epoch 421/3000:  14%|█▍        | 420/3000 [02:58&lt;19:37,  2.19it/s, v_num=1, train_loss_step=2.96e+6, train_loss_epoch=2.85e+6]Epoch 421/3000:  14%|█▍        | 421/3000 [02:59&lt;20:54,  2.06it/s, v_num=1, train_loss_step=2.96e+6, train_loss_epoch=2.85e+6]Epoch 421/3000:  14%|█▍        | 421/3000 [02:59&lt;20:54,  2.06it/s, v_num=1, train_loss_step=2.89e+6, train_loss_epoch=2.84e+6]Epoch 422/3000:  14%|█▍        | 421/3000 [02:59&lt;20:54,  2.06it/s, v_num=1, train_loss_step=2.89e+6, train_loss_epoch=2.84e+6]Epoch 422/3000:  14%|█▍        | 422/3000 [02:59&lt;20:04,  2.14it/s, v_num=1, train_loss_step=2.89e+6, train_loss_epoch=2.84e+6]Epoch 422/3000:  14%|█▍        | 422/3000 [02:59&lt;20:04,  2.14it/s, v_num=1, train_loss_step=3e+6, train_loss_epoch=2.84e+6]   Epoch 423/3000:  14%|█▍        | 422/3000 [02:59&lt;20:04,  2.14it/s, v_num=1, train_loss_step=3e+6, train_loss_epoch=2.84e+6]Epoch 423/3000:  14%|█▍        | 423/3000 [02:59&lt;18:40,  2.30it/s, v_num=1, train_loss_step=3e+6, train_loss_epoch=2.84e+6]Epoch 423/3000:  14%|█▍        | 423/3000 [02:59&lt;18:40,  2.30it/s, v_num=1, train_loss_step=2.85e+6, train_loss_epoch=2.83e+6]Epoch 424/3000:  14%|█▍        | 423/3000 [02:59&lt;18:40,  2.30it/s, v_num=1, train_loss_step=2.85e+6, train_loss_epoch=2.83e+6]Epoch 424/3000:  14%|█▍        | 424/3000 [03:00&lt;19:36,  2.19it/s, v_num=1, train_loss_step=2.85e+6, train_loss_epoch=2.83e+6]Epoch 424/3000:  14%|█▍        | 424/3000 [03:00&lt;19:36,  2.19it/s, v_num=1, train_loss_step=2.9e+6, train_loss_epoch=2.83e+6] Epoch 425/3000:  14%|█▍        | 424/3000 [03:00&lt;19:36,  2.19it/s, v_num=1, train_loss_step=2.9e+6, train_loss_epoch=2.83e+6]Epoch 425/3000:  14%|█▍        | 425/3000 [03:00&lt;19:23,  2.21it/s, v_num=1, train_loss_step=2.9e+6, train_loss_epoch=2.83e+6]Epoch 425/3000:  14%|█▍        | 425/3000 [03:00&lt;19:23,  2.21it/s, v_num=1, train_loss_step=2.75e+6, train_loss_epoch=2.82e+6]Epoch 426/3000:  14%|█▍        | 425/3000 [03:00&lt;19:23,  2.21it/s, v_num=1, train_loss_step=2.75e+6, train_loss_epoch=2.82e+6]Epoch 426/3000:  14%|█▍        | 426/3000 [03:01&lt;18:13,  2.35it/s, v_num=1, train_loss_step=2.75e+6, train_loss_epoch=2.82e+6]Epoch 426/3000:  14%|█▍        | 426/3000 [03:01&lt;18:13,  2.35it/s, v_num=1, train_loss_step=2.87e+6, train_loss_epoch=2.81e+6]Epoch 427/3000:  14%|█▍        | 426/3000 [03:01&lt;18:13,  2.35it/s, v_num=1, train_loss_step=2.87e+6, train_loss_epoch=2.81e+6]Epoch 427/3000:  14%|█▍        | 427/3000 [03:01&lt;18:56,  2.26it/s, v_num=1, train_loss_step=2.87e+6, train_loss_epoch=2.81e+6]Epoch 427/3000:  14%|█▍        | 427/3000 [03:01&lt;18:56,  2.26it/s, v_num=1, train_loss_step=2.77e+6, train_loss_epoch=2.81e+6]Epoch 428/3000:  14%|█▍        | 427/3000 [03:01&lt;18:56,  2.26it/s, v_num=1, train_loss_step=2.77e+6, train_loss_epoch=2.81e+6]Epoch 428/3000:  14%|█▍        | 428/3000 [03:02&lt;19:52,  2.16it/s, v_num=1, train_loss_step=2.77e+6, train_loss_epoch=2.81e+6]Epoch 428/3000:  14%|█▍        | 428/3000 [03:02&lt;19:52,  2.16it/s, v_num=1, train_loss_step=2.76e+6, train_loss_epoch=2.8e+6] Epoch 429/3000:  14%|█▍        | 428/3000 [03:02&lt;19:52,  2.16it/s, v_num=1, train_loss_step=2.76e+6, train_loss_epoch=2.8e+6]Epoch 429/3000:  14%|█▍        | 429/3000 [03:02&lt;20:05,  2.13it/s, v_num=1, train_loss_step=2.76e+6, train_loss_epoch=2.8e+6]Epoch 429/3000:  14%|█▍        | 429/3000 [03:02&lt;20:05,  2.13it/s, v_num=1, train_loss_step=2.68e+6, train_loss_epoch=2.8e+6]Epoch 430/3000:  14%|█▍        | 429/3000 [03:02&lt;20:05,  2.13it/s, v_num=1, train_loss_step=2.68e+6, train_loss_epoch=2.8e+6]Epoch 430/3000:  14%|█▍        | 430/3000 [03:03&lt;19:07,  2.24it/s, v_num=1, train_loss_step=2.68e+6, train_loss_epoch=2.8e+6]Epoch 430/3000:  14%|█▍        | 430/3000 [03:03&lt;19:07,  2.24it/s, v_num=1, train_loss_step=2.83e+6, train_loss_epoch=2.79e+6]Epoch 431/3000:  14%|█▍        | 430/3000 [03:03&lt;19:07,  2.24it/s, v_num=1, train_loss_step=2.83e+6, train_loss_epoch=2.79e+6]Epoch 431/3000:  14%|█▍        | 431/3000 [03:03&lt;18:55,  2.26it/s, v_num=1, train_loss_step=2.83e+6, train_loss_epoch=2.79e+6]Epoch 431/3000:  14%|█▍        | 431/3000 [03:03&lt;18:55,  2.26it/s, v_num=1, train_loss_step=2.72e+6, train_loss_epoch=2.79e+6]Epoch 432/3000:  14%|█▍        | 431/3000 [03:03&lt;18:55,  2.26it/s, v_num=1, train_loss_step=2.72e+6, train_loss_epoch=2.79e+6]Epoch 432/3000:  14%|█▍        | 432/3000 [03:04&lt;19:58,  2.14it/s, v_num=1, train_loss_step=2.72e+6, train_loss_epoch=2.79e+6]Epoch 432/3000:  14%|█▍        | 432/3000 [03:04&lt;19:58,  2.14it/s, v_num=1, train_loss_step=3.01e+6, train_loss_epoch=2.78e+6]Epoch 433/3000:  14%|█▍        | 432/3000 [03:04&lt;19:58,  2.14it/s, v_num=1, train_loss_step=3.01e+6, train_loss_epoch=2.78e+6]Epoch 433/3000:  14%|█▍        | 433/3000 [03:04&lt;20:42,  2.07it/s, v_num=1, train_loss_step=3.01e+6, train_loss_epoch=2.78e+6]Epoch 433/3000:  14%|█▍        | 433/3000 [03:04&lt;20:42,  2.07it/s, v_num=1, train_loss_step=2.75e+6, train_loss_epoch=2.78e+6]Epoch 434/3000:  14%|█▍        | 433/3000 [03:04&lt;20:42,  2.07it/s, v_num=1, train_loss_step=2.75e+6, train_loss_epoch=2.78e+6]Epoch 434/3000:  14%|█▍        | 434/3000 [03:05&lt;22:32,  1.90it/s, v_num=1, train_loss_step=2.75e+6, train_loss_epoch=2.78e+6]Epoch 434/3000:  14%|█▍        | 434/3000 [03:05&lt;22:32,  1.90it/s, v_num=1, train_loss_step=2.73e+6, train_loss_epoch=2.77e+6]Epoch 435/3000:  14%|█▍        | 434/3000 [03:05&lt;22:32,  1.90it/s, v_num=1, train_loss_step=2.73e+6, train_loss_epoch=2.77e+6]Epoch 435/3000:  14%|█▍        | 435/3000 [03:05&lt;20:37,  2.07it/s, v_num=1, train_loss_step=2.73e+6, train_loss_epoch=2.77e+6]Epoch 435/3000:  14%|█▍        | 435/3000 [03:05&lt;20:37,  2.07it/s, v_num=1, train_loss_step=2.8e+6, train_loss_epoch=2.77e+6] Epoch 436/3000:  14%|█▍        | 435/3000 [03:05&lt;20:37,  2.07it/s, v_num=1, train_loss_step=2.8e+6, train_loss_epoch=2.77e+6]Epoch 436/3000:  15%|█▍        | 436/3000 [03:06&lt;21:24,  2.00it/s, v_num=1, train_loss_step=2.8e+6, train_loss_epoch=2.77e+6]Epoch 436/3000:  15%|█▍        | 436/3000 [03:06&lt;21:24,  2.00it/s, v_num=1, train_loss_step=2.83e+6, train_loss_epoch=2.76e+6]Epoch 437/3000:  15%|█▍        | 436/3000 [03:06&lt;21:24,  2.00it/s, v_num=1, train_loss_step=2.83e+6, train_loss_epoch=2.76e+6]Epoch 437/3000:  15%|█▍        | 437/3000 [03:06&lt;20:51,  2.05it/s, v_num=1, train_loss_step=2.83e+6, train_loss_epoch=2.76e+6]Epoch 437/3000:  15%|█▍        | 437/3000 [03:06&lt;20:51,  2.05it/s, v_num=1, train_loss_step=2.7e+6, train_loss_epoch=2.76e+6] Epoch 438/3000:  15%|█▍        | 437/3000 [03:06&lt;20:51,  2.05it/s, v_num=1, train_loss_step=2.7e+6, train_loss_epoch=2.76e+6]Epoch 438/3000:  15%|█▍        | 438/3000 [03:07&lt;19:46,  2.16it/s, v_num=1, train_loss_step=2.7e+6, train_loss_epoch=2.76e+6]Epoch 438/3000:  15%|█▍        | 438/3000 [03:07&lt;19:46,  2.16it/s, v_num=1, train_loss_step=2.89e+6, train_loss_epoch=2.75e+6]Epoch 439/3000:  15%|█▍        | 438/3000 [03:07&lt;19:46,  2.16it/s, v_num=1, train_loss_step=2.89e+6, train_loss_epoch=2.75e+6]Epoch 439/3000:  15%|█▍        | 439/3000 [03:07&lt;19:59,  2.14it/s, v_num=1, train_loss_step=2.89e+6, train_loss_epoch=2.75e+6]Epoch 439/3000:  15%|█▍        | 439/3000 [03:07&lt;19:59,  2.14it/s, v_num=1, train_loss_step=2.74e+6, train_loss_epoch=2.74e+6]Epoch 440/3000:  15%|█▍        | 439/3000 [03:07&lt;19:59,  2.14it/s, v_num=1, train_loss_step=2.74e+6, train_loss_epoch=2.74e+6]Epoch 440/3000:  15%|█▍        | 440/3000 [03:08&lt;20:47,  2.05it/s, v_num=1, train_loss_step=2.74e+6, train_loss_epoch=2.74e+6]Epoch 440/3000:  15%|█▍        | 440/3000 [03:08&lt;20:47,  2.05it/s, v_num=1, train_loss_step=2.95e+6, train_loss_epoch=2.74e+6]Epoch 441/3000:  15%|█▍        | 440/3000 [03:08&lt;20:47,  2.05it/s, v_num=1, train_loss_step=2.95e+6, train_loss_epoch=2.74e+6]Epoch 441/3000:  15%|█▍        | 441/3000 [03:08&lt;21:12,  2.01it/s, v_num=1, train_loss_step=2.95e+6, train_loss_epoch=2.74e+6]Epoch 441/3000:  15%|█▍        | 441/3000 [03:08&lt;21:12,  2.01it/s, v_num=1, train_loss_step=2.79e+6, train_loss_epoch=2.73e+6]Epoch 442/3000:  15%|█▍        | 441/3000 [03:08&lt;21:12,  2.01it/s, v_num=1, train_loss_step=2.79e+6, train_loss_epoch=2.73e+6]Epoch 442/3000:  15%|█▍        | 442/3000 [03:08&lt;17:09,  2.48it/s, v_num=1, train_loss_step=2.79e+6, train_loss_epoch=2.73e+6]Epoch 442/3000:  15%|█▍        | 442/3000 [03:08&lt;17:09,  2.48it/s, v_num=1, train_loss_step=2.66e+6, train_loss_epoch=2.73e+6]Epoch 443/3000:  15%|█▍        | 442/3000 [03:08&lt;17:09,  2.48it/s, v_num=1, train_loss_step=2.66e+6, train_loss_epoch=2.73e+6]Epoch 443/3000:  15%|█▍        | 443/3000 [03:09&lt;19:11,  2.22it/s, v_num=1, train_loss_step=2.66e+6, train_loss_epoch=2.73e+6]Epoch 443/3000:  15%|█▍        | 443/3000 [03:09&lt;19:11,  2.22it/s, v_num=1, train_loss_step=2.67e+6, train_loss_epoch=2.72e+6]Epoch 444/3000:  15%|█▍        | 443/3000 [03:09&lt;19:11,  2.22it/s, v_num=1, train_loss_step=2.67e+6, train_loss_epoch=2.72e+6]Epoch 444/3000:  15%|█▍        | 444/3000 [03:09&lt;21:10,  2.01it/s, v_num=1, train_loss_step=2.67e+6, train_loss_epoch=2.72e+6]Epoch 444/3000:  15%|█▍        | 444/3000 [03:09&lt;21:10,  2.01it/s, v_num=1, train_loss_step=2.85e+6, train_loss_epoch=2.72e+6]Epoch 445/3000:  15%|█▍        | 444/3000 [03:09&lt;21:10,  2.01it/s, v_num=1, train_loss_step=2.85e+6, train_loss_epoch=2.72e+6]Epoch 445/3000:  15%|█▍        | 445/3000 [03:10&lt;21:00,  2.03it/s, v_num=1, train_loss_step=2.85e+6, train_loss_epoch=2.72e+6]Epoch 445/3000:  15%|█▍        | 445/3000 [03:10&lt;21:00,  2.03it/s, v_num=1, train_loss_step=2.74e+6, train_loss_epoch=2.71e+6]Epoch 446/3000:  15%|█▍        | 445/3000 [03:10&lt;21:00,  2.03it/s, v_num=1, train_loss_step=2.74e+6, train_loss_epoch=2.71e+6]Epoch 446/3000:  15%|█▍        | 446/3000 [03:10&lt;20:19,  2.09it/s, v_num=1, train_loss_step=2.74e+6, train_loss_epoch=2.71e+6]Epoch 446/3000:  15%|█▍        | 446/3000 [03:10&lt;20:19,  2.09it/s, v_num=1, train_loss_step=2.61e+6, train_loss_epoch=2.71e+6]Epoch 447/3000:  15%|█▍        | 446/3000 [03:10&lt;20:19,  2.09it/s, v_num=1, train_loss_step=2.61e+6, train_loss_epoch=2.71e+6]Epoch 447/3000:  15%|█▍        | 447/3000 [03:11&lt;20:04,  2.12it/s, v_num=1, train_loss_step=2.61e+6, train_loss_epoch=2.71e+6]Epoch 447/3000:  15%|█▍        | 447/3000 [03:11&lt;20:04,  2.12it/s, v_num=1, train_loss_step=2.75e+6, train_loss_epoch=2.7e+6] Epoch 448/3000:  15%|█▍        | 447/3000 [03:11&lt;20:04,  2.12it/s, v_num=1, train_loss_step=2.75e+6, train_loss_epoch=2.7e+6]Epoch 448/3000:  15%|█▍        | 448/3000 [03:11&lt;22:09,  1.92it/s, v_num=1, train_loss_step=2.75e+6, train_loss_epoch=2.7e+6]Epoch 448/3000:  15%|█▍        | 448/3000 [03:11&lt;22:09,  1.92it/s, v_num=1, train_loss_step=2.68e+6, train_loss_epoch=2.7e+6]Epoch 449/3000:  15%|█▍        | 448/3000 [03:11&lt;22:09,  1.92it/s, v_num=1, train_loss_step=2.68e+6, train_loss_epoch=2.7e+6]Epoch 449/3000:  15%|█▍        | 449/3000 [03:12&lt;20:32,  2.07it/s, v_num=1, train_loss_step=2.68e+6, train_loss_epoch=2.7e+6]Epoch 449/3000:  15%|█▍        | 449/3000 [03:12&lt;20:32,  2.07it/s, v_num=1, train_loss_step=2.62e+6, train_loss_epoch=2.69e+6]Epoch 450/3000:  15%|█▍        | 449/3000 [03:12&lt;20:32,  2.07it/s, v_num=1, train_loss_step=2.62e+6, train_loss_epoch=2.69e+6]Epoch 450/3000:  15%|█▌        | 450/3000 [03:12&lt;20:56,  2.03it/s, v_num=1, train_loss_step=2.62e+6, train_loss_epoch=2.69e+6]Epoch 450/3000:  15%|█▌        | 450/3000 [03:12&lt;20:56,  2.03it/s, v_num=1, train_loss_step=2.7e+6, train_loss_epoch=2.69e+6] Epoch 451/3000:  15%|█▌        | 450/3000 [03:12&lt;20:56,  2.03it/s, v_num=1, train_loss_step=2.7e+6, train_loss_epoch=2.69e+6]Epoch 451/3000:  15%|█▌        | 451/3000 [03:13&lt;21:32,  1.97it/s, v_num=1, train_loss_step=2.7e+6, train_loss_epoch=2.69e+6]Epoch 451/3000:  15%|█▌        | 451/3000 [03:13&lt;21:32,  1.97it/s, v_num=1, train_loss_step=2.75e+6, train_loss_epoch=2.68e+6]Epoch 452/3000:  15%|█▌        | 451/3000 [03:13&lt;21:32,  1.97it/s, v_num=1, train_loss_step=2.75e+6, train_loss_epoch=2.68e+6]Epoch 452/3000:  15%|█▌        | 452/3000 [03:13&lt;21:01,  2.02it/s, v_num=1, train_loss_step=2.75e+6, train_loss_epoch=2.68e+6]Epoch 452/3000:  15%|█▌        | 452/3000 [03:13&lt;21:01,  2.02it/s, v_num=1, train_loss_step=2.68e+6, train_loss_epoch=2.68e+6]Epoch 453/3000:  15%|█▌        | 452/3000 [03:13&lt;21:01,  2.02it/s, v_num=1, train_loss_step=2.68e+6, train_loss_epoch=2.68e+6]Epoch 453/3000:  15%|█▌        | 453/3000 [03:14&lt;20:24,  2.08it/s, v_num=1, train_loss_step=2.68e+6, train_loss_epoch=2.68e+6]Epoch 453/3000:  15%|█▌        | 453/3000 [03:14&lt;20:24,  2.08it/s, v_num=1, train_loss_step=2.8e+6, train_loss_epoch=2.67e+6] Epoch 454/3000:  15%|█▌        | 453/3000 [03:14&lt;20:24,  2.08it/s, v_num=1, train_loss_step=2.8e+6, train_loss_epoch=2.67e+6]Epoch 454/3000:  15%|█▌        | 454/3000 [03:14&lt;21:16,  1.99it/s, v_num=1, train_loss_step=2.8e+6, train_loss_epoch=2.67e+6]Epoch 454/3000:  15%|█▌        | 454/3000 [03:14&lt;21:16,  1.99it/s, v_num=1, train_loss_step=2.69e+6, train_loss_epoch=2.67e+6]Epoch 455/3000:  15%|█▌        | 454/3000 [03:14&lt;21:16,  1.99it/s, v_num=1, train_loss_step=2.69e+6, train_loss_epoch=2.67e+6]Epoch 455/3000:  15%|█▌        | 455/3000 [03:15&lt;22:17,  1.90it/s, v_num=1, train_loss_step=2.69e+6, train_loss_epoch=2.67e+6]Epoch 455/3000:  15%|█▌        | 455/3000 [03:15&lt;22:17,  1.90it/s, v_num=1, train_loss_step=2.78e+6, train_loss_epoch=2.66e+6]Epoch 456/3000:  15%|█▌        | 455/3000 [03:15&lt;22:17,  1.90it/s, v_num=1, train_loss_step=2.78e+6, train_loss_epoch=2.66e+6]Epoch 456/3000:  15%|█▌        | 456/3000 [03:15&lt;20:03,  2.11it/s, v_num=1, train_loss_step=2.78e+6, train_loss_epoch=2.66e+6]Epoch 456/3000:  15%|█▌        | 456/3000 [03:15&lt;20:03,  2.11it/s, v_num=1, train_loss_step=2.66e+6, train_loss_epoch=2.66e+6]Epoch 457/3000:  15%|█▌        | 456/3000 [03:15&lt;20:03,  2.11it/s, v_num=1, train_loss_step=2.66e+6, train_loss_epoch=2.66e+6]Epoch 457/3000:  15%|█▌        | 457/3000 [03:16&lt;19:37,  2.16it/s, v_num=1, train_loss_step=2.66e+6, train_loss_epoch=2.66e+6]Epoch 457/3000:  15%|█▌        | 457/3000 [03:16&lt;19:37,  2.16it/s, v_num=1, train_loss_step=2.7e+6, train_loss_epoch=2.65e+6] Epoch 458/3000:  15%|█▌        | 457/3000 [03:16&lt;19:37,  2.16it/s, v_num=1, train_loss_step=2.7e+6, train_loss_epoch=2.65e+6]Epoch 458/3000:  15%|█▌        | 458/3000 [03:16&lt;21:16,  1.99it/s, v_num=1, train_loss_step=2.7e+6, train_loss_epoch=2.65e+6]Epoch 458/3000:  15%|█▌        | 458/3000 [03:16&lt;21:16,  1.99it/s, v_num=1, train_loss_step=2.75e+6, train_loss_epoch=2.65e+6]Epoch 459/3000:  15%|█▌        | 458/3000 [03:16&lt;21:16,  1.99it/s, v_num=1, train_loss_step=2.75e+6, train_loss_epoch=2.65e+6]Epoch 459/3000:  15%|█▌        | 459/3000 [03:17&lt;21:54,  1.93it/s, v_num=1, train_loss_step=2.75e+6, train_loss_epoch=2.65e+6]Epoch 459/3000:  15%|█▌        | 459/3000 [03:17&lt;21:54,  1.93it/s, v_num=1, train_loss_step=2.71e+6, train_loss_epoch=2.65e+6]Epoch 460/3000:  15%|█▌        | 459/3000 [03:17&lt;21:54,  1.93it/s, v_num=1, train_loss_step=2.71e+6, train_loss_epoch=2.65e+6]Epoch 460/3000:  15%|█▌        | 460/3000 [03:17&lt;21:19,  1.99it/s, v_num=1, train_loss_step=2.71e+6, train_loss_epoch=2.65e+6]Epoch 460/3000:  15%|█▌        | 460/3000 [03:17&lt;21:19,  1.99it/s, v_num=1, train_loss_step=2.83e+6, train_loss_epoch=2.64e+6]Epoch 461/3000:  15%|█▌        | 460/3000 [03:17&lt;21:19,  1.99it/s, v_num=1, train_loss_step=2.83e+6, train_loss_epoch=2.64e+6]Epoch 461/3000:  15%|█▌        | 461/3000 [03:18&lt;20:37,  2.05it/s, v_num=1, train_loss_step=2.83e+6, train_loss_epoch=2.64e+6]Epoch 461/3000:  15%|█▌        | 461/3000 [03:18&lt;20:37,  2.05it/s, v_num=1, train_loss_step=2.6e+6, train_loss_epoch=2.64e+6] Epoch 462/3000:  15%|█▌        | 461/3000 [03:18&lt;20:37,  2.05it/s, v_num=1, train_loss_step=2.6e+6, train_loss_epoch=2.64e+6]Epoch 462/3000:  15%|█▌        | 462/3000 [03:18&lt;19:36,  2.16it/s, v_num=1, train_loss_step=2.6e+6, train_loss_epoch=2.64e+6]Epoch 462/3000:  15%|█▌        | 462/3000 [03:18&lt;19:36,  2.16it/s, v_num=1, train_loss_step=2.5e+6, train_loss_epoch=2.63e+6]Epoch 463/3000:  15%|█▌        | 462/3000 [03:18&lt;19:36,  2.16it/s, v_num=1, train_loss_step=2.5e+6, train_loss_epoch=2.63e+6]Epoch 463/3000:  15%|█▌        | 463/3000 [03:19&lt;20:43,  2.04it/s, v_num=1, train_loss_step=2.5e+6, train_loss_epoch=2.63e+6]Epoch 463/3000:  15%|█▌        | 463/3000 [03:19&lt;20:43,  2.04it/s, v_num=1, train_loss_step=2.68e+6, train_loss_epoch=2.63e+6]Epoch 464/3000:  15%|█▌        | 463/3000 [03:19&lt;20:43,  2.04it/s, v_num=1, train_loss_step=2.68e+6, train_loss_epoch=2.63e+6]Epoch 464/3000:  15%|█▌        | 464/3000 [03:19&lt;21:06,  2.00it/s, v_num=1, train_loss_step=2.68e+6, train_loss_epoch=2.63e+6]Epoch 464/3000:  15%|█▌        | 464/3000 [03:19&lt;21:06,  2.00it/s, v_num=1, train_loss_step=2.74e+6, train_loss_epoch=2.62e+6]Epoch 465/3000:  15%|█▌        | 464/3000 [03:19&lt;21:06,  2.00it/s, v_num=1, train_loss_step=2.74e+6, train_loss_epoch=2.62e+6]Epoch 465/3000:  16%|█▌        | 465/3000 [03:20&lt;19:27,  2.17it/s, v_num=1, train_loss_step=2.74e+6, train_loss_epoch=2.62e+6]Epoch 465/3000:  16%|█▌        | 465/3000 [03:20&lt;19:27,  2.17it/s, v_num=1, train_loss_step=2.61e+6, train_loss_epoch=2.62e+6]Epoch 466/3000:  16%|█▌        | 465/3000 [03:20&lt;19:27,  2.17it/s, v_num=1, train_loss_step=2.61e+6, train_loss_epoch=2.62e+6]Epoch 466/3000:  16%|█▌        | 466/3000 [03:20&lt;20:41,  2.04it/s, v_num=1, train_loss_step=2.61e+6, train_loss_epoch=2.62e+6]Epoch 466/3000:  16%|█▌        | 466/3000 [03:20&lt;20:41,  2.04it/s, v_num=1, train_loss_step=2.56e+6, train_loss_epoch=2.61e+6]Epoch 467/3000:  16%|█▌        | 466/3000 [03:20&lt;20:41,  2.04it/s, v_num=1, train_loss_step=2.56e+6, train_loss_epoch=2.61e+6]Epoch 467/3000:  16%|█▌        | 467/3000 [03:21&lt;21:26,  1.97it/s, v_num=1, train_loss_step=2.56e+6, train_loss_epoch=2.61e+6]Epoch 467/3000:  16%|█▌        | 467/3000 [03:21&lt;21:26,  1.97it/s, v_num=1, train_loss_step=2.65e+6, train_loss_epoch=2.61e+6]Epoch 468/3000:  16%|█▌        | 467/3000 [03:21&lt;21:26,  1.97it/s, v_num=1, train_loss_step=2.65e+6, train_loss_epoch=2.61e+6]Epoch 468/3000:  16%|█▌        | 468/3000 [03:21&lt;21:49,  1.93it/s, v_num=1, train_loss_step=2.65e+6, train_loss_epoch=2.61e+6]Epoch 468/3000:  16%|█▌        | 468/3000 [03:21&lt;21:49,  1.93it/s, v_num=1, train_loss_step=2.53e+6, train_loss_epoch=2.6e+6] Epoch 469/3000:  16%|█▌        | 468/3000 [03:21&lt;21:49,  1.93it/s, v_num=1, train_loss_step=2.53e+6, train_loss_epoch=2.6e+6]Epoch 469/3000:  16%|█▌        | 469/3000 [03:22&lt;22:08,  1.90it/s, v_num=1, train_loss_step=2.53e+6, train_loss_epoch=2.6e+6]Epoch 469/3000:  16%|█▌        | 469/3000 [03:22&lt;22:08,  1.90it/s, v_num=1, train_loss_step=2.56e+6, train_loss_epoch=2.6e+6]Epoch 470/3000:  16%|█▌        | 469/3000 [03:22&lt;22:08,  1.90it/s, v_num=1, train_loss_step=2.56e+6, train_loss_epoch=2.6e+6]Epoch 470/3000:  16%|█▌        | 470/3000 [03:22&lt;19:46,  2.13it/s, v_num=1, train_loss_step=2.56e+6, train_loss_epoch=2.6e+6]Epoch 470/3000:  16%|█▌        | 470/3000 [03:22&lt;19:46,  2.13it/s, v_num=1, train_loss_step=2.64e+6, train_loss_epoch=2.59e+6]Epoch 471/3000:  16%|█▌        | 470/3000 [03:22&lt;19:46,  2.13it/s, v_num=1, train_loss_step=2.64e+6, train_loss_epoch=2.59e+6]Epoch 471/3000:  16%|█▌        | 471/3000 [03:23&lt;20:21,  2.07it/s, v_num=1, train_loss_step=2.64e+6, train_loss_epoch=2.59e+6]Epoch 471/3000:  16%|█▌        | 471/3000 [03:23&lt;20:21,  2.07it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.59e+6]Epoch 472/3000:  16%|█▌        | 471/3000 [03:23&lt;20:21,  2.07it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.59e+6]Epoch 472/3000:  16%|█▌        | 472/3000 [03:23&lt;19:11,  2.19it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.59e+6]Epoch 472/3000:  16%|█▌        | 472/3000 [03:23&lt;19:11,  2.19it/s, v_num=1, train_loss_step=2.63e+6, train_loss_epoch=2.58e+6]Epoch 473/3000:  16%|█▌        | 472/3000 [03:23&lt;19:11,  2.19it/s, v_num=1, train_loss_step=2.63e+6, train_loss_epoch=2.58e+6]Epoch 473/3000:  16%|█▌        | 473/3000 [03:24&lt;20:22,  2.07it/s, v_num=1, train_loss_step=2.63e+6, train_loss_epoch=2.58e+6]Epoch 473/3000:  16%|█▌        | 473/3000 [03:24&lt;20:22,  2.07it/s, v_num=1, train_loss_step=2.56e+6, train_loss_epoch=2.58e+6]Epoch 474/3000:  16%|█▌        | 473/3000 [03:24&lt;20:22,  2.07it/s, v_num=1, train_loss_step=2.56e+6, train_loss_epoch=2.58e+6]Epoch 474/3000:  16%|█▌        | 474/3000 [03:24&lt;18:23,  2.29it/s, v_num=1, train_loss_step=2.56e+6, train_loss_epoch=2.58e+6]Epoch 474/3000:  16%|█▌        | 474/3000 [03:24&lt;18:23,  2.29it/s, v_num=1, train_loss_step=2.56e+6, train_loss_epoch=2.58e+6]Epoch 475/3000:  16%|█▌        | 474/3000 [03:24&lt;18:23,  2.29it/s, v_num=1, train_loss_step=2.56e+6, train_loss_epoch=2.58e+6]Epoch 475/3000:  16%|█▌        | 475/3000 [03:24&lt;18:09,  2.32it/s, v_num=1, train_loss_step=2.56e+6, train_loss_epoch=2.58e+6]Epoch 475/3000:  16%|█▌        | 475/3000 [03:24&lt;18:09,  2.32it/s, v_num=1, train_loss_step=2.64e+6, train_loss_epoch=2.57e+6]Epoch 476/3000:  16%|█▌        | 475/3000 [03:24&lt;18:09,  2.32it/s, v_num=1, train_loss_step=2.64e+6, train_loss_epoch=2.57e+6]Epoch 476/3000:  16%|█▌        | 476/3000 [03:25&lt;19:42,  2.13it/s, v_num=1, train_loss_step=2.64e+6, train_loss_epoch=2.57e+6]Epoch 476/3000:  16%|█▌        | 476/3000 [03:25&lt;19:42,  2.13it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.57e+6]Epoch 477/3000:  16%|█▌        | 476/3000 [03:25&lt;19:42,  2.13it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.57e+6]Epoch 477/3000:  16%|█▌        | 477/3000 [03:26&lt;22:29,  1.87it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.57e+6]Epoch 477/3000:  16%|█▌        | 477/3000 [03:26&lt;22:29,  1.87it/s, v_num=1, train_loss_step=2.74e+6, train_loss_epoch=2.56e+6]Epoch 478/3000:  16%|█▌        | 477/3000 [03:26&lt;22:29,  1.87it/s, v_num=1, train_loss_step=2.74e+6, train_loss_epoch=2.56e+6]Epoch 478/3000:  16%|█▌        | 478/3000 [03:26&lt;21:53,  1.92it/s, v_num=1, train_loss_step=2.74e+6, train_loss_epoch=2.56e+6]Epoch 478/3000:  16%|█▌        | 478/3000 [03:26&lt;21:53,  1.92it/s, v_num=1, train_loss_step=2.55e+6, train_loss_epoch=2.56e+6]Epoch 479/3000:  16%|█▌        | 478/3000 [03:26&lt;21:53,  1.92it/s, v_num=1, train_loss_step=2.55e+6, train_loss_epoch=2.56e+6]Epoch 479/3000:  16%|█▌        | 479/3000 [03:27&lt;21:48,  1.93it/s, v_num=1, train_loss_step=2.55e+6, train_loss_epoch=2.56e+6]Epoch 479/3000:  16%|█▌        | 479/3000 [03:27&lt;21:48,  1.93it/s, v_num=1, train_loss_step=2.54e+6, train_loss_epoch=2.55e+6]Epoch 480/3000:  16%|█▌        | 479/3000 [03:27&lt;21:48,  1.93it/s, v_num=1, train_loss_step=2.54e+6, train_loss_epoch=2.55e+6]Epoch 480/3000:  16%|█▌        | 480/3000 [03:27&lt;20:38,  2.03it/s, v_num=1, train_loss_step=2.54e+6, train_loss_epoch=2.55e+6]Epoch 480/3000:  16%|█▌        | 480/3000 [03:27&lt;20:38,  2.03it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.55e+6]Epoch 481/3000:  16%|█▌        | 480/3000 [03:27&lt;20:38,  2.03it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.55e+6]Epoch 481/3000:  16%|█▌        | 481/3000 [03:28&lt;21:02,  2.00it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.55e+6]Epoch 481/3000:  16%|█▌        | 481/3000 [03:28&lt;21:02,  2.00it/s, v_num=1, train_loss_step=2.54e+6, train_loss_epoch=2.54e+6]Epoch 482/3000:  16%|█▌        | 481/3000 [03:28&lt;21:02,  2.00it/s, v_num=1, train_loss_step=2.54e+6, train_loss_epoch=2.54e+6]Epoch 482/3000:  16%|█▌        | 482/3000 [03:28&lt;20:16,  2.07it/s, v_num=1, train_loss_step=2.54e+6, train_loss_epoch=2.54e+6]Epoch 482/3000:  16%|█▌        | 482/3000 [03:28&lt;20:16,  2.07it/s, v_num=1, train_loss_step=2.26e+6, train_loss_epoch=2.54e+6]Epoch 483/3000:  16%|█▌        | 482/3000 [03:28&lt;20:16,  2.07it/s, v_num=1, train_loss_step=2.26e+6, train_loss_epoch=2.54e+6]Epoch 483/3000:  16%|█▌        | 483/3000 [03:28&lt;19:12,  2.18it/s, v_num=1, train_loss_step=2.26e+6, train_loss_epoch=2.54e+6]Epoch 483/3000:  16%|█▌        | 483/3000 [03:28&lt;19:12,  2.18it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.54e+6]Epoch 484/3000:  16%|█▌        | 483/3000 [03:28&lt;19:12,  2.18it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.54e+6]Epoch 484/3000:  16%|█▌        | 484/3000 [03:29&lt;18:56,  2.21it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.54e+6]Epoch 484/3000:  16%|█▌        | 484/3000 [03:29&lt;18:56,  2.21it/s, v_num=1, train_loss_step=2.45e+6, train_loss_epoch=2.53e+6]Epoch 485/3000:  16%|█▌        | 484/3000 [03:29&lt;18:56,  2.21it/s, v_num=1, train_loss_step=2.45e+6, train_loss_epoch=2.53e+6]Epoch 485/3000:  16%|█▌        | 485/3000 [03:29&lt;16:47,  2.50it/s, v_num=1, train_loss_step=2.45e+6, train_loss_epoch=2.53e+6]Epoch 485/3000:  16%|█▌        | 485/3000 [03:29&lt;16:47,  2.50it/s, v_num=1, train_loss_step=2.62e+6, train_loss_epoch=2.53e+6]Epoch 486/3000:  16%|█▌        | 485/3000 [03:29&lt;16:47,  2.50it/s, v_num=1, train_loss_step=2.62e+6, train_loss_epoch=2.53e+6]Epoch 486/3000:  16%|█▌        | 486/3000 [03:29&lt;16:21,  2.56it/s, v_num=1, train_loss_step=2.62e+6, train_loss_epoch=2.53e+6]Epoch 486/3000:  16%|█▌        | 486/3000 [03:29&lt;16:21,  2.56it/s, v_num=1, train_loss_step=2.46e+6, train_loss_epoch=2.52e+6]Epoch 487/3000:  16%|█▌        | 486/3000 [03:29&lt;16:21,  2.56it/s, v_num=1, train_loss_step=2.46e+6, train_loss_epoch=2.52e+6]Epoch 487/3000:  16%|█▌        | 487/3000 [03:30&lt;17:39,  2.37it/s, v_num=1, train_loss_step=2.46e+6, train_loss_epoch=2.52e+6]Epoch 487/3000:  16%|█▌        | 487/3000 [03:30&lt;17:39,  2.37it/s, v_num=1, train_loss_step=2.44e+6, train_loss_epoch=2.52e+6]Epoch 488/3000:  16%|█▌        | 487/3000 [03:30&lt;17:39,  2.37it/s, v_num=1, train_loss_step=2.44e+6, train_loss_epoch=2.52e+6]Epoch 488/3000:  16%|█▋        | 488/3000 [03:30&lt;16:38,  2.52it/s, v_num=1, train_loss_step=2.44e+6, train_loss_epoch=2.52e+6]Epoch 488/3000:  16%|█▋        | 488/3000 [03:30&lt;16:38,  2.52it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.51e+6]Epoch 489/3000:  16%|█▋        | 488/3000 [03:30&lt;16:38,  2.52it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.51e+6]Epoch 489/3000:  16%|█▋        | 489/3000 [03:31&lt;19:37,  2.13it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.51e+6]Epoch 489/3000:  16%|█▋        | 489/3000 [03:31&lt;19:37,  2.13it/s, v_num=1, train_loss_step=2.49e+6, train_loss_epoch=2.51e+6]Epoch 490/3000:  16%|█▋        | 489/3000 [03:31&lt;19:37,  2.13it/s, v_num=1, train_loss_step=2.49e+6, train_loss_epoch=2.51e+6]Epoch 490/3000:  16%|█▋        | 490/3000 [03:31&lt;19:02,  2.20it/s, v_num=1, train_loss_step=2.49e+6, train_loss_epoch=2.51e+6]Epoch 490/3000:  16%|█▋        | 490/3000 [03:31&lt;19:02,  2.20it/s, v_num=1, train_loss_step=2.42e+6, train_loss_epoch=2.51e+6]Epoch 491/3000:  16%|█▋        | 490/3000 [03:31&lt;19:02,  2.20it/s, v_num=1, train_loss_step=2.42e+6, train_loss_epoch=2.51e+6]Epoch 491/3000:  16%|█▋        | 491/3000 [03:32&lt;19:30,  2.14it/s, v_num=1, train_loss_step=2.42e+6, train_loss_epoch=2.51e+6]Epoch 491/3000:  16%|█▋        | 491/3000 [03:32&lt;19:30,  2.14it/s, v_num=1, train_loss_step=2.48e+6, train_loss_epoch=2.5e+6] Epoch 492/3000:  16%|█▋        | 491/3000 [03:32&lt;19:30,  2.14it/s, v_num=1, train_loss_step=2.48e+6, train_loss_epoch=2.5e+6]Epoch 492/3000:  16%|█▋        | 492/3000 [03:32&lt;16:33,  2.52it/s, v_num=1, train_loss_step=2.48e+6, train_loss_epoch=2.5e+6]Epoch 492/3000:  16%|█▋        | 492/3000 [03:32&lt;16:33,  2.52it/s, v_num=1, train_loss_step=2.48e+6, train_loss_epoch=2.5e+6]Epoch 493/3000:  16%|█▋        | 492/3000 [03:32&lt;16:33,  2.52it/s, v_num=1, train_loss_step=2.48e+6, train_loss_epoch=2.5e+6]Epoch 493/3000:  16%|█▋        | 493/3000 [03:32&lt;15:47,  2.64it/s, v_num=1, train_loss_step=2.48e+6, train_loss_epoch=2.5e+6]Epoch 493/3000:  16%|█▋        | 493/3000 [03:32&lt;15:47,  2.64it/s, v_num=1, train_loss_step=2.49e+6, train_loss_epoch=2.49e+6]Epoch 494/3000:  16%|█▋        | 493/3000 [03:32&lt;15:47,  2.64it/s, v_num=1, train_loss_step=2.49e+6, train_loss_epoch=2.49e+6]Epoch 494/3000:  16%|█▋        | 494/3000 [03:33&lt;16:50,  2.48it/s, v_num=1, train_loss_step=2.49e+6, train_loss_epoch=2.49e+6]Epoch 494/3000:  16%|█▋        | 494/3000 [03:33&lt;16:50,  2.48it/s, v_num=1, train_loss_step=2.46e+6, train_loss_epoch=2.49e+6]Epoch 495/3000:  16%|█▋        | 494/3000 [03:33&lt;16:50,  2.48it/s, v_num=1, train_loss_step=2.46e+6, train_loss_epoch=2.49e+6]Epoch 495/3000:  16%|█▋        | 495/3000 [03:33&lt;17:05,  2.44it/s, v_num=1, train_loss_step=2.46e+6, train_loss_epoch=2.49e+6]Epoch 495/3000:  16%|█▋        | 495/3000 [03:33&lt;17:05,  2.44it/s, v_num=1, train_loss_step=2.46e+6, train_loss_epoch=2.49e+6]Epoch 496/3000:  16%|█▋        | 495/3000 [03:33&lt;17:05,  2.44it/s, v_num=1, train_loss_step=2.46e+6, train_loss_epoch=2.49e+6]Epoch 496/3000:  17%|█▋        | 496/3000 [03:34&lt;17:01,  2.45it/s, v_num=1, train_loss_step=2.46e+6, train_loss_epoch=2.49e+6]Epoch 496/3000:  17%|█▋        | 496/3000 [03:34&lt;17:01,  2.45it/s, v_num=1, train_loss_step=2.48e+6, train_loss_epoch=2.48e+6]Epoch 497/3000:  17%|█▋        | 496/3000 [03:34&lt;17:01,  2.45it/s, v_num=1, train_loss_step=2.48e+6, train_loss_epoch=2.48e+6]Epoch 497/3000:  17%|█▋        | 497/3000 [03:34&lt;18:18,  2.28it/s, v_num=1, train_loss_step=2.48e+6, train_loss_epoch=2.48e+6]Epoch 497/3000:  17%|█▋        | 497/3000 [03:34&lt;18:18,  2.28it/s, v_num=1, train_loss_step=2.43e+6, train_loss_epoch=2.48e+6]Epoch 498/3000:  17%|█▋        | 497/3000 [03:34&lt;18:18,  2.28it/s, v_num=1, train_loss_step=2.43e+6, train_loss_epoch=2.48e+6]Epoch 498/3000:  17%|█▋        | 498/3000 [03:35&lt;17:09,  2.43it/s, v_num=1, train_loss_step=2.43e+6, train_loss_epoch=2.48e+6]Epoch 498/3000:  17%|█▋        | 498/3000 [03:35&lt;17:09,  2.43it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.47e+6]Epoch 499/3000:  17%|█▋        | 498/3000 [03:35&lt;17:09,  2.43it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.47e+6]Epoch 499/3000:  17%|█▋        | 499/3000 [03:35&lt;16:52,  2.47it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.47e+6]Epoch 499/3000:  17%|█▋        | 499/3000 [03:35&lt;16:52,  2.47it/s, v_num=1, train_loss_step=2.49e+6, train_loss_epoch=2.47e+6]Epoch 500/3000:  17%|█▋        | 499/3000 [03:35&lt;16:52,  2.47it/s, v_num=1, train_loss_step=2.49e+6, train_loss_epoch=2.47e+6]Epoch 500/3000:  17%|█▋        | 500/3000 [03:35&lt;16:07,  2.58it/s, v_num=1, train_loss_step=2.49e+6, train_loss_epoch=2.47e+6]Epoch 500/3000:  17%|█▋        | 500/3000 [03:35&lt;16:07,  2.58it/s, v_num=1, train_loss_step=2.39e+6, train_loss_epoch=2.46e+6]Epoch 501/3000:  17%|█▋        | 500/3000 [03:35&lt;16:07,  2.58it/s, v_num=1, train_loss_step=2.39e+6, train_loss_epoch=2.46e+6]Epoch 501/3000:  17%|█▋        | 501/3000 [03:36&lt;16:22,  2.54it/s, v_num=1, train_loss_step=2.39e+6, train_loss_epoch=2.46e+6]Epoch 501/3000:  17%|█▋        | 501/3000 [03:36&lt;16:22,  2.54it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.46e+6]Epoch 502/3000:  17%|█▋        | 501/3000 [03:36&lt;16:22,  2.54it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.46e+6]Epoch 502/3000:  17%|█▋        | 502/3000 [03:36&lt;19:32,  2.13it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.46e+6]Epoch 502/3000:  17%|█▋        | 502/3000 [03:36&lt;19:32,  2.13it/s, v_num=1, train_loss_step=2.4e+6, train_loss_epoch=2.46e+6] Epoch 503/3000:  17%|█▋        | 502/3000 [03:36&lt;19:32,  2.13it/s, v_num=1, train_loss_step=2.4e+6, train_loss_epoch=2.46e+6]Epoch 503/3000:  17%|█▋        | 503/3000 [03:37&lt;18:10,  2.29it/s, v_num=1, train_loss_step=2.4e+6, train_loss_epoch=2.46e+6]Epoch 503/3000:  17%|█▋        | 503/3000 [03:37&lt;18:10,  2.29it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.45e+6]Epoch 504/3000:  17%|█▋        | 503/3000 [03:37&lt;18:10,  2.29it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.45e+6]Epoch 504/3000:  17%|█▋        | 504/3000 [03:37&lt;17:31,  2.37it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.45e+6]Epoch 504/3000:  17%|█▋        | 504/3000 [03:37&lt;17:31,  2.37it/s, v_num=1, train_loss_step=2.47e+6, train_loss_epoch=2.45e+6]Epoch 505/3000:  17%|█▋        | 504/3000 [03:37&lt;17:31,  2.37it/s, v_num=1, train_loss_step=2.47e+6, train_loss_epoch=2.45e+6]Epoch 505/3000:  17%|█▋        | 505/3000 [03:38&lt;17:25,  2.39it/s, v_num=1, train_loss_step=2.47e+6, train_loss_epoch=2.45e+6]Epoch 505/3000:  17%|█▋        | 505/3000 [03:38&lt;17:25,  2.39it/s, v_num=1, train_loss_step=2.38e+6, train_loss_epoch=2.44e+6]Epoch 506/3000:  17%|█▋        | 505/3000 [03:38&lt;17:25,  2.39it/s, v_num=1, train_loss_step=2.38e+6, train_loss_epoch=2.44e+6]Epoch 506/3000:  17%|█▋        | 506/3000 [03:38&lt;18:14,  2.28it/s, v_num=1, train_loss_step=2.38e+6, train_loss_epoch=2.44e+6]Epoch 506/3000:  17%|█▋        | 506/3000 [03:38&lt;18:14,  2.28it/s, v_num=1, train_loss_step=2.5e+6, train_loss_epoch=2.44e+6] Epoch 507/3000:  17%|█▋        | 506/3000 [03:38&lt;18:14,  2.28it/s, v_num=1, train_loss_step=2.5e+6, train_loss_epoch=2.44e+6]Epoch 507/3000:  17%|█▋        | 507/3000 [03:38&lt;16:21,  2.54it/s, v_num=1, train_loss_step=2.5e+6, train_loss_epoch=2.44e+6]Epoch 507/3000:  17%|█▋        | 507/3000 [03:38&lt;16:21,  2.54it/s, v_num=1, train_loss_step=2.41e+6, train_loss_epoch=2.44e+6]Epoch 508/3000:  17%|█▋        | 507/3000 [03:38&lt;16:21,  2.54it/s, v_num=1, train_loss_step=2.41e+6, train_loss_epoch=2.44e+6]Epoch 508/3000:  17%|█▋        | 508/3000 [03:39&lt;17:13,  2.41it/s, v_num=1, train_loss_step=2.41e+6, train_loss_epoch=2.44e+6]Epoch 508/3000:  17%|█▋        | 508/3000 [03:39&lt;17:13,  2.41it/s, v_num=1, train_loss_step=2.43e+6, train_loss_epoch=2.43e+6]Epoch 509/3000:  17%|█▋        | 508/3000 [03:39&lt;17:13,  2.41it/s, v_num=1, train_loss_step=2.43e+6, train_loss_epoch=2.43e+6]Epoch 509/3000:  17%|█▋        | 509/3000 [03:39&lt;17:06,  2.43it/s, v_num=1, train_loss_step=2.43e+6, train_loss_epoch=2.43e+6]Epoch 509/3000:  17%|█▋        | 509/3000 [03:39&lt;17:06,  2.43it/s, v_num=1, train_loss_step=2.38e+6, train_loss_epoch=2.43e+6]Epoch 510/3000:  17%|█▋        | 509/3000 [03:39&lt;17:06,  2.43it/s, v_num=1, train_loss_step=2.38e+6, train_loss_epoch=2.43e+6]Epoch 510/3000:  17%|█▋        | 510/3000 [03:40&lt;18:24,  2.25it/s, v_num=1, train_loss_step=2.38e+6, train_loss_epoch=2.43e+6]Epoch 510/3000:  17%|█▋        | 510/3000 [03:40&lt;18:24,  2.25it/s, v_num=1, train_loss_step=2.44e+6, train_loss_epoch=2.43e+6]Epoch 511/3000:  17%|█▋        | 510/3000 [03:40&lt;18:24,  2.25it/s, v_num=1, train_loss_step=2.44e+6, train_loss_epoch=2.43e+6]Epoch 511/3000:  17%|█▋        | 511/3000 [03:40&lt;19:13,  2.16it/s, v_num=1, train_loss_step=2.44e+6, train_loss_epoch=2.43e+6]Epoch 511/3000:  17%|█▋        | 511/3000 [03:40&lt;19:13,  2.16it/s, v_num=1, train_loss_step=2.52e+6, train_loss_epoch=2.42e+6]Epoch 512/3000:  17%|█▋        | 511/3000 [03:40&lt;19:13,  2.16it/s, v_num=1, train_loss_step=2.52e+6, train_loss_epoch=2.42e+6]Epoch 512/3000:  17%|█▋        | 512/3000 [03:41&lt;19:56,  2.08it/s, v_num=1, train_loss_step=2.52e+6, train_loss_epoch=2.42e+6]Epoch 512/3000:  17%|█▋        | 512/3000 [03:41&lt;19:56,  2.08it/s, v_num=1, train_loss_step=2.54e+6, train_loss_epoch=2.42e+6]Epoch 513/3000:  17%|█▋        | 512/3000 [03:41&lt;19:56,  2.08it/s, v_num=1, train_loss_step=2.54e+6, train_loss_epoch=2.42e+6]Epoch 513/3000:  17%|█▋        | 513/3000 [03:41&lt;19:52,  2.09it/s, v_num=1, train_loss_step=2.54e+6, train_loss_epoch=2.42e+6]Epoch 513/3000:  17%|█▋        | 513/3000 [03:41&lt;19:52,  2.09it/s, v_num=1, train_loss_step=2.45e+6, train_loss_epoch=2.41e+6]Epoch 514/3000:  17%|█▋        | 513/3000 [03:41&lt;19:52,  2.09it/s, v_num=1, train_loss_step=2.45e+6, train_loss_epoch=2.41e+6]Epoch 514/3000:  17%|█▋        | 514/3000 [03:42&lt;20:26,  2.03it/s, v_num=1, train_loss_step=2.45e+6, train_loss_epoch=2.41e+6]Epoch 514/3000:  17%|█▋        | 514/3000 [03:42&lt;20:26,  2.03it/s, v_num=1, train_loss_step=2.43e+6, train_loss_epoch=2.41e+6]Epoch 515/3000:  17%|█▋        | 514/3000 [03:42&lt;20:26,  2.03it/s, v_num=1, train_loss_step=2.43e+6, train_loss_epoch=2.41e+6]Epoch 515/3000:  17%|█▋        | 515/3000 [03:42&lt;18:08,  2.28it/s, v_num=1, train_loss_step=2.43e+6, train_loss_epoch=2.41e+6]Epoch 515/3000:  17%|█▋        | 515/3000 [03:42&lt;18:08,  2.28it/s, v_num=1, train_loss_step=2.49e+6, train_loss_epoch=2.41e+6]Epoch 516/3000:  17%|█▋        | 515/3000 [03:42&lt;18:08,  2.28it/s, v_num=1, train_loss_step=2.49e+6, train_loss_epoch=2.41e+6]Epoch 516/3000:  17%|█▋        | 516/3000 [03:42&lt;18:17,  2.26it/s, v_num=1, train_loss_step=2.49e+6, train_loss_epoch=2.41e+6]Epoch 516/3000:  17%|█▋        | 516/3000 [03:42&lt;18:17,  2.26it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.4e+6] Epoch 517/3000:  17%|█▋        | 516/3000 [03:42&lt;18:17,  2.26it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.4e+6]Epoch 517/3000:  17%|█▋        | 517/3000 [03:43&lt;19:42,  2.10it/s, v_num=1, train_loss_step=2.59e+6, train_loss_epoch=2.4e+6]Epoch 517/3000:  17%|█▋        | 517/3000 [03:43&lt;19:42,  2.10it/s, v_num=1, train_loss_step=2.39e+6, train_loss_epoch=2.4e+6]Epoch 518/3000:  17%|█▋        | 517/3000 [03:43&lt;19:42,  2.10it/s, v_num=1, train_loss_step=2.39e+6, train_loss_epoch=2.4e+6]Epoch 518/3000:  17%|█▋        | 518/3000 [03:43&lt;19:00,  2.18it/s, v_num=1, train_loss_step=2.39e+6, train_loss_epoch=2.4e+6]Epoch 518/3000:  17%|█▋        | 518/3000 [03:43&lt;19:00,  2.18it/s, v_num=1, train_loss_step=2.27e+6, train_loss_epoch=2.39e+6]Epoch 519/3000:  17%|█▋        | 518/3000 [03:43&lt;19:00,  2.18it/s, v_num=1, train_loss_step=2.27e+6, train_loss_epoch=2.39e+6]Epoch 519/3000:  17%|█▋        | 519/3000 [03:44&lt;20:27,  2.02it/s, v_num=1, train_loss_step=2.27e+6, train_loss_epoch=2.39e+6]Epoch 519/3000:  17%|█▋        | 519/3000 [03:44&lt;20:27,  2.02it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.39e+6]Epoch 520/3000:  17%|█▋        | 519/3000 [03:44&lt;20:27,  2.02it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.39e+6]Epoch 520/3000:  17%|█▋        | 520/3000 [03:45&lt;20:12,  2.05it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.39e+6]Epoch 520/3000:  17%|█▋        | 520/3000 [03:45&lt;20:12,  2.05it/s, v_num=1, train_loss_step=2.43e+6, train_loss_epoch=2.39e+6]Epoch 521/3000:  17%|█▋        | 520/3000 [03:45&lt;20:12,  2.05it/s, v_num=1, train_loss_step=2.43e+6, train_loss_epoch=2.39e+6]Epoch 521/3000:  17%|█▋        | 521/3000 [03:45&lt;20:10,  2.05it/s, v_num=1, train_loss_step=2.43e+6, train_loss_epoch=2.39e+6]Epoch 521/3000:  17%|█▋        | 521/3000 [03:45&lt;20:10,  2.05it/s, v_num=1, train_loss_step=2.45e+6, train_loss_epoch=2.38e+6]Epoch 522/3000:  17%|█▋        | 521/3000 [03:45&lt;20:10,  2.05it/s, v_num=1, train_loss_step=2.45e+6, train_loss_epoch=2.38e+6]Epoch 522/3000:  17%|█▋        | 522/3000 [03:45&lt;18:14,  2.26it/s, v_num=1, train_loss_step=2.45e+6, train_loss_epoch=2.38e+6]Epoch 522/3000:  17%|█▋        | 522/3000 [03:45&lt;18:14,  2.26it/s, v_num=1, train_loss_step=2.35e+6, train_loss_epoch=2.38e+6]Epoch 523/3000:  17%|█▋        | 522/3000 [03:45&lt;18:14,  2.26it/s, v_num=1, train_loss_step=2.35e+6, train_loss_epoch=2.38e+6]Epoch 523/3000:  17%|█▋        | 523/3000 [03:46&lt;18:06,  2.28it/s, v_num=1, train_loss_step=2.35e+6, train_loss_epoch=2.38e+6]Epoch 523/3000:  17%|█▋        | 523/3000 [03:46&lt;18:06,  2.28it/s, v_num=1, train_loss_step=2.28e+6, train_loss_epoch=2.38e+6]Epoch 524/3000:  17%|█▋        | 523/3000 [03:46&lt;18:06,  2.28it/s, v_num=1, train_loss_step=2.28e+6, train_loss_epoch=2.38e+6]Epoch 524/3000:  17%|█▋        | 524/3000 [03:46&lt;16:37,  2.48it/s, v_num=1, train_loss_step=2.28e+6, train_loss_epoch=2.38e+6]Epoch 524/3000:  17%|█▋        | 524/3000 [03:46&lt;16:37,  2.48it/s, v_num=1, train_loss_step=2.32e+6, train_loss_epoch=2.37e+6]Epoch 525/3000:  17%|█▋        | 524/3000 [03:46&lt;16:37,  2.48it/s, v_num=1, train_loss_step=2.32e+6, train_loss_epoch=2.37e+6]Epoch 525/3000:  18%|█▊        | 525/3000 [03:47&lt;18:32,  2.23it/s, v_num=1, train_loss_step=2.32e+6, train_loss_epoch=2.37e+6]Epoch 525/3000:  18%|█▊        | 525/3000 [03:47&lt;18:32,  2.23it/s, v_num=1, train_loss_step=2.45e+6, train_loss_epoch=2.37e+6]Epoch 526/3000:  18%|█▊        | 525/3000 [03:47&lt;18:32,  2.23it/s, v_num=1, train_loss_step=2.45e+6, train_loss_epoch=2.37e+6]Epoch 526/3000:  18%|█▊        | 526/3000 [03:47&lt;20:06,  2.05it/s, v_num=1, train_loss_step=2.45e+6, train_loss_epoch=2.37e+6]Epoch 526/3000:  18%|█▊        | 526/3000 [03:47&lt;20:06,  2.05it/s, v_num=1, train_loss_step=2.35e+6, train_loss_epoch=2.36e+6]Epoch 527/3000:  18%|█▊        | 526/3000 [03:47&lt;20:06,  2.05it/s, v_num=1, train_loss_step=2.35e+6, train_loss_epoch=2.36e+6]Epoch 527/3000:  18%|█▊        | 527/3000 [03:48&lt;19:16,  2.14it/s, v_num=1, train_loss_step=2.35e+6, train_loss_epoch=2.36e+6]Epoch 527/3000:  18%|█▊        | 527/3000 [03:48&lt;19:16,  2.14it/s, v_num=1, train_loss_step=2.39e+6, train_loss_epoch=2.36e+6]Epoch 528/3000:  18%|█▊        | 527/3000 [03:48&lt;19:16,  2.14it/s, v_num=1, train_loss_step=2.39e+6, train_loss_epoch=2.36e+6]Epoch 528/3000:  18%|█▊        | 528/3000 [03:48&lt;18:32,  2.22it/s, v_num=1, train_loss_step=2.39e+6, train_loss_epoch=2.36e+6]Epoch 528/3000:  18%|█▊        | 528/3000 [03:48&lt;18:32,  2.22it/s, v_num=1, train_loss_step=2.26e+6, train_loss_epoch=2.36e+6]Epoch 529/3000:  18%|█▊        | 528/3000 [03:48&lt;18:32,  2.22it/s, v_num=1, train_loss_step=2.26e+6, train_loss_epoch=2.36e+6]Epoch 529/3000:  18%|█▊        | 529/3000 [03:49&lt;19:11,  2.15it/s, v_num=1, train_loss_step=2.26e+6, train_loss_epoch=2.36e+6]Epoch 529/3000:  18%|█▊        | 529/3000 [03:49&lt;19:11,  2.15it/s, v_num=1, train_loss_step=2.39e+6, train_loss_epoch=2.35e+6]Epoch 530/3000:  18%|█▊        | 529/3000 [03:49&lt;19:11,  2.15it/s, v_num=1, train_loss_step=2.39e+6, train_loss_epoch=2.35e+6]Epoch 530/3000:  18%|█▊        | 530/3000 [03:49&lt;16:18,  2.52it/s, v_num=1, train_loss_step=2.39e+6, train_loss_epoch=2.35e+6]Epoch 530/3000:  18%|█▊        | 530/3000 [03:49&lt;16:18,  2.52it/s, v_num=1, train_loss_step=2.37e+6, train_loss_epoch=2.35e+6]Epoch 531/3000:  18%|█▊        | 530/3000 [03:49&lt;16:18,  2.52it/s, v_num=1, train_loss_step=2.37e+6, train_loss_epoch=2.35e+6]Epoch 531/3000:  18%|█▊        | 531/3000 [03:49&lt;16:53,  2.44it/s, v_num=1, train_loss_step=2.37e+6, train_loss_epoch=2.35e+6]Epoch 531/3000:  18%|█▊        | 531/3000 [03:49&lt;16:53,  2.44it/s, v_num=1, train_loss_step=2.34e+6, train_loss_epoch=2.35e+6]Epoch 532/3000:  18%|█▊        | 531/3000 [03:49&lt;16:53,  2.44it/s, v_num=1, train_loss_step=2.34e+6, train_loss_epoch=2.35e+6]Epoch 532/3000:  18%|█▊        | 532/3000 [03:50&lt;18:43,  2.20it/s, v_num=1, train_loss_step=2.34e+6, train_loss_epoch=2.35e+6]Epoch 532/3000:  18%|█▊        | 532/3000 [03:50&lt;18:43,  2.20it/s, v_num=1, train_loss_step=2.39e+6, train_loss_epoch=2.34e+6]Epoch 533/3000:  18%|█▊        | 532/3000 [03:50&lt;18:43,  2.20it/s, v_num=1, train_loss_step=2.39e+6, train_loss_epoch=2.34e+6]Epoch 533/3000:  18%|█▊        | 533/3000 [03:50&lt;19:37,  2.10it/s, v_num=1, train_loss_step=2.39e+6, train_loss_epoch=2.34e+6]Epoch 533/3000:  18%|█▊        | 533/3000 [03:50&lt;19:37,  2.10it/s, v_num=1, train_loss_step=2.3e+6, train_loss_epoch=2.34e+6] Epoch 534/3000:  18%|█▊        | 533/3000 [03:50&lt;19:37,  2.10it/s, v_num=1, train_loss_step=2.3e+6, train_loss_epoch=2.34e+6]Epoch 534/3000:  18%|█▊        | 534/3000 [03:51&lt;19:03,  2.16it/s, v_num=1, train_loss_step=2.3e+6, train_loss_epoch=2.34e+6]Epoch 534/3000:  18%|█▊        | 534/3000 [03:51&lt;19:03,  2.16it/s, v_num=1, train_loss_step=2.35e+6, train_loss_epoch=2.34e+6]Epoch 535/3000:  18%|█▊        | 534/3000 [03:51&lt;19:03,  2.16it/s, v_num=1, train_loss_step=2.35e+6, train_loss_epoch=2.34e+6]Epoch 535/3000:  18%|█▊        | 535/3000 [03:51&lt;20:14,  2.03it/s, v_num=1, train_loss_step=2.35e+6, train_loss_epoch=2.34e+6]Epoch 535/3000:  18%|█▊        | 535/3000 [03:51&lt;20:14,  2.03it/s, v_num=1, train_loss_step=2.38e+6, train_loss_epoch=2.33e+6]Epoch 536/3000:  18%|█▊        | 535/3000 [03:51&lt;20:14,  2.03it/s, v_num=1, train_loss_step=2.38e+6, train_loss_epoch=2.33e+6]Epoch 536/3000:  18%|█▊        | 536/3000 [03:52&lt;19:31,  2.10it/s, v_num=1, train_loss_step=2.38e+6, train_loss_epoch=2.33e+6]Epoch 536/3000:  18%|█▊        | 536/3000 [03:52&lt;19:31,  2.10it/s, v_num=1, train_loss_step=2.46e+6, train_loss_epoch=2.33e+6]Epoch 537/3000:  18%|█▊        | 536/3000 [03:52&lt;19:31,  2.10it/s, v_num=1, train_loss_step=2.46e+6, train_loss_epoch=2.33e+6]Epoch 537/3000:  18%|█▊        | 537/3000 [03:52&lt;21:24,  1.92it/s, v_num=1, train_loss_step=2.46e+6, train_loss_epoch=2.33e+6]Epoch 537/3000:  18%|█▊        | 537/3000 [03:52&lt;21:24,  1.92it/s, v_num=1, train_loss_step=2.34e+6, train_loss_epoch=2.33e+6]Epoch 538/3000:  18%|█▊        | 537/3000 [03:52&lt;21:24,  1.92it/s, v_num=1, train_loss_step=2.34e+6, train_loss_epoch=2.33e+6]Epoch 538/3000:  18%|█▊        | 538/3000 [03:53&lt;21:02,  1.95it/s, v_num=1, train_loss_step=2.34e+6, train_loss_epoch=2.33e+6]Epoch 538/3000:  18%|█▊        | 538/3000 [03:53&lt;21:02,  1.95it/s, v_num=1, train_loss_step=2.34e+6, train_loss_epoch=2.32e+6]Epoch 539/3000:  18%|█▊        | 538/3000 [03:53&lt;21:02,  1.95it/s, v_num=1, train_loss_step=2.34e+6, train_loss_epoch=2.32e+6]Epoch 539/3000:  18%|█▊        | 539/3000 [03:53&lt;20:19,  2.02it/s, v_num=1, train_loss_step=2.34e+6, train_loss_epoch=2.32e+6]Epoch 539/3000:  18%|█▊        | 539/3000 [03:53&lt;20:19,  2.02it/s, v_num=1, train_loss_step=2.31e+6, train_loss_epoch=2.32e+6]Epoch 540/3000:  18%|█▊        | 539/3000 [03:53&lt;20:19,  2.02it/s, v_num=1, train_loss_step=2.31e+6, train_loss_epoch=2.32e+6]Epoch 540/3000:  18%|█▊        | 540/3000 [03:54&lt;18:23,  2.23it/s, v_num=1, train_loss_step=2.31e+6, train_loss_epoch=2.32e+6]Epoch 540/3000:  18%|█▊        | 540/3000 [03:54&lt;18:23,  2.23it/s, v_num=1, train_loss_step=2.22e+6, train_loss_epoch=2.31e+6]Epoch 541/3000:  18%|█▊        | 540/3000 [03:54&lt;18:23,  2.23it/s, v_num=1, train_loss_step=2.22e+6, train_loss_epoch=2.31e+6]Epoch 541/3000:  18%|█▊        | 541/3000 [03:54&lt;17:26,  2.35it/s, v_num=1, train_loss_step=2.22e+6, train_loss_epoch=2.31e+6]Epoch 541/3000:  18%|█▊        | 541/3000 [03:54&lt;17:26,  2.35it/s, v_num=1, train_loss_step=2.28e+6, train_loss_epoch=2.31e+6]Epoch 542/3000:  18%|█▊        | 541/3000 [03:54&lt;17:26,  2.35it/s, v_num=1, train_loss_step=2.28e+6, train_loss_epoch=2.31e+6]Epoch 542/3000:  18%|█▊        | 542/3000 [03:54&lt;15:26,  2.65it/s, v_num=1, train_loss_step=2.28e+6, train_loss_epoch=2.31e+6]Epoch 542/3000:  18%|█▊        | 542/3000 [03:54&lt;15:26,  2.65it/s, v_num=1, train_loss_step=2.37e+6, train_loss_epoch=2.31e+6]Epoch 543/3000:  18%|█▊        | 542/3000 [03:54&lt;15:26,  2.65it/s, v_num=1, train_loss_step=2.37e+6, train_loss_epoch=2.31e+6]Epoch 543/3000:  18%|█▊        | 543/3000 [03:55&lt;13:36,  3.01it/s, v_num=1, train_loss_step=2.37e+6, train_loss_epoch=2.31e+6]Epoch 543/3000:  18%|█▊        | 543/3000 [03:55&lt;13:36,  3.01it/s, v_num=1, train_loss_step=2.27e+6, train_loss_epoch=2.3e+6] Epoch 544/3000:  18%|█▊        | 543/3000 [03:55&lt;13:36,  3.01it/s, v_num=1, train_loss_step=2.27e+6, train_loss_epoch=2.3e+6]Epoch 544/3000:  18%|█▊        | 544/3000 [03:55&lt;12:42,  3.22it/s, v_num=1, train_loss_step=2.27e+6, train_loss_epoch=2.3e+6]Epoch 544/3000:  18%|█▊        | 544/3000 [03:55&lt;12:42,  3.22it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.3e+6]Epoch 545/3000:  18%|█▊        | 544/3000 [03:55&lt;12:42,  3.22it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.3e+6]Epoch 545/3000:  18%|█▊        | 545/3000 [03:55&lt;16:17,  2.51it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.3e+6]Epoch 545/3000:  18%|█▊        | 545/3000 [03:55&lt;16:17,  2.51it/s, v_num=1, train_loss_step=2.21e+6, train_loss_epoch=2.3e+6]Epoch 546/3000:  18%|█▊        | 545/3000 [03:55&lt;16:17,  2.51it/s, v_num=1, train_loss_step=2.21e+6, train_loss_epoch=2.3e+6]Epoch 546/3000:  18%|█▊        | 546/3000 [03:56&lt;18:03,  2.26it/s, v_num=1, train_loss_step=2.21e+6, train_loss_epoch=2.3e+6]Epoch 546/3000:  18%|█▊        | 546/3000 [03:56&lt;18:03,  2.26it/s, v_num=1, train_loss_step=2.23e+6, train_loss_epoch=2.29e+6]Epoch 547/3000:  18%|█▊        | 546/3000 [03:56&lt;18:03,  2.26it/s, v_num=1, train_loss_step=2.23e+6, train_loss_epoch=2.29e+6]Epoch 547/3000:  18%|█▊        | 547/3000 [03:56&lt;16:58,  2.41it/s, v_num=1, train_loss_step=2.23e+6, train_loss_epoch=2.29e+6]Epoch 547/3000:  18%|█▊        | 547/3000 [03:56&lt;16:58,  2.41it/s, v_num=1, train_loss_step=2.37e+6, train_loss_epoch=2.29e+6]Epoch 548/3000:  18%|█▊        | 547/3000 [03:56&lt;16:58,  2.41it/s, v_num=1, train_loss_step=2.37e+6, train_loss_epoch=2.29e+6]Epoch 548/3000:  18%|█▊        | 548/3000 [03:57&lt;17:45,  2.30it/s, v_num=1, train_loss_step=2.37e+6, train_loss_epoch=2.29e+6]Epoch 548/3000:  18%|█▊        | 548/3000 [03:57&lt;17:45,  2.30it/s, v_num=1, train_loss_step=2.24e+6, train_loss_epoch=2.29e+6]Epoch 549/3000:  18%|█▊        | 548/3000 [03:57&lt;17:45,  2.30it/s, v_num=1, train_loss_step=2.24e+6, train_loss_epoch=2.29e+6]Epoch 549/3000:  18%|█▊        | 549/3000 [03:57&lt;18:19,  2.23it/s, v_num=1, train_loss_step=2.24e+6, train_loss_epoch=2.29e+6]Epoch 549/3000:  18%|█▊        | 549/3000 [03:57&lt;18:19,  2.23it/s, v_num=1, train_loss_step=2.2e+6, train_loss_epoch=2.28e+6] Epoch 550/3000:  18%|█▊        | 549/3000 [03:57&lt;18:19,  2.23it/s, v_num=1, train_loss_step=2.2e+6, train_loss_epoch=2.28e+6]Epoch 550/3000:  18%|█▊        | 550/3000 [03:58&lt;17:07,  2.39it/s, v_num=1, train_loss_step=2.2e+6, train_loss_epoch=2.28e+6]Epoch 550/3000:  18%|█▊        | 550/3000 [03:58&lt;17:07,  2.39it/s, v_num=1, train_loss_step=2.18e+6, train_loss_epoch=2.28e+6]Epoch 551/3000:  18%|█▊        | 550/3000 [03:58&lt;17:07,  2.39it/s, v_num=1, train_loss_step=2.18e+6, train_loss_epoch=2.28e+6]Epoch 551/3000:  18%|█▊        | 551/3000 [03:58&lt;17:07,  2.38it/s, v_num=1, train_loss_step=2.18e+6, train_loss_epoch=2.28e+6]Epoch 551/3000:  18%|█▊        | 551/3000 [03:58&lt;17:07,  2.38it/s, v_num=1, train_loss_step=2.32e+6, train_loss_epoch=2.28e+6]Epoch 552/3000:  18%|█▊        | 551/3000 [03:58&lt;17:07,  2.38it/s, v_num=1, train_loss_step=2.32e+6, train_loss_epoch=2.28e+6]Epoch 552/3000:  18%|█▊        | 552/3000 [03:58&lt;17:06,  2.39it/s, v_num=1, train_loss_step=2.32e+6, train_loss_epoch=2.28e+6]Epoch 552/3000:  18%|█▊        | 552/3000 [03:58&lt;17:06,  2.39it/s, v_num=1, train_loss_step=2.42e+6, train_loss_epoch=2.27e+6]Epoch 553/3000:  18%|█▊        | 552/3000 [03:58&lt;17:06,  2.39it/s, v_num=1, train_loss_step=2.42e+6, train_loss_epoch=2.27e+6]Epoch 553/3000:  18%|█▊        | 553/3000 [03:59&lt;17:25,  2.34it/s, v_num=1, train_loss_step=2.42e+6, train_loss_epoch=2.27e+6]Epoch 553/3000:  18%|█▊        | 553/3000 [03:59&lt;17:25,  2.34it/s, v_num=1, train_loss_step=2.28e+6, train_loss_epoch=2.27e+6]Epoch 554/3000:  18%|█▊        | 553/3000 [03:59&lt;17:25,  2.34it/s, v_num=1, train_loss_step=2.28e+6, train_loss_epoch=2.27e+6]Epoch 554/3000:  18%|█▊        | 554/3000 [03:59&lt;16:16,  2.50it/s, v_num=1, train_loss_step=2.28e+6, train_loss_epoch=2.27e+6]Epoch 554/3000:  18%|█▊        | 554/3000 [03:59&lt;16:16,  2.50it/s, v_num=1, train_loss_step=2.21e+6, train_loss_epoch=2.27e+6]Epoch 555/3000:  18%|█▊        | 554/3000 [03:59&lt;16:16,  2.50it/s, v_num=1, train_loss_step=2.21e+6, train_loss_epoch=2.27e+6]Epoch 555/3000:  18%|█▊        | 555/3000 [04:00&lt;15:39,  2.60it/s, v_num=1, train_loss_step=2.21e+6, train_loss_epoch=2.27e+6]Epoch 555/3000:  18%|█▊        | 555/3000 [04:00&lt;15:39,  2.60it/s, v_num=1, train_loss_step=2.18e+6, train_loss_epoch=2.26e+6]Epoch 556/3000:  18%|█▊        | 555/3000 [04:00&lt;15:39,  2.60it/s, v_num=1, train_loss_step=2.18e+6, train_loss_epoch=2.26e+6]Epoch 556/3000:  19%|█▊        | 556/3000 [04:00&lt;14:25,  2.82it/s, v_num=1, train_loss_step=2.18e+6, train_loss_epoch=2.26e+6]Epoch 556/3000:  19%|█▊        | 556/3000 [04:00&lt;14:25,  2.82it/s, v_num=1, train_loss_step=2.3e+6, train_loss_epoch=2.26e+6] Epoch 557/3000:  19%|█▊        | 556/3000 [04:00&lt;14:25,  2.82it/s, v_num=1, train_loss_step=2.3e+6, train_loss_epoch=2.26e+6]Epoch 557/3000:  19%|█▊        | 557/3000 [04:00&lt;13:29,  3.02it/s, v_num=1, train_loss_step=2.3e+6, train_loss_epoch=2.26e+6]Epoch 557/3000:  19%|█▊        | 557/3000 [04:00&lt;13:29,  3.02it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.26e+6]Epoch 558/3000:  19%|█▊        | 557/3000 [04:00&lt;13:29,  3.02it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.26e+6]Epoch 558/3000:  19%|█▊        | 558/3000 [04:00&lt;12:42,  3.20it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.26e+6]Epoch 558/3000:  19%|█▊        | 558/3000 [04:00&lt;12:42,  3.20it/s, v_num=1, train_loss_step=2.23e+6, train_loss_epoch=2.25e+6]Epoch 559/3000:  19%|█▊        | 558/3000 [04:00&lt;12:42,  3.20it/s, v_num=1, train_loss_step=2.23e+6, train_loss_epoch=2.25e+6]Epoch 559/3000:  19%|█▊        | 559/3000 [04:01&lt;13:56,  2.92it/s, v_num=1, train_loss_step=2.23e+6, train_loss_epoch=2.25e+6]Epoch 559/3000:  19%|█▊        | 559/3000 [04:01&lt;13:56,  2.92it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.25e+6]Epoch 560/3000:  19%|█▊        | 559/3000 [04:01&lt;13:56,  2.92it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.25e+6]Epoch 560/3000:  19%|█▊        | 560/3000 [04:01&lt;13:20,  3.05it/s, v_num=1, train_loss_step=2.36e+6, train_loss_epoch=2.25e+6]Epoch 560/3000:  19%|█▊        | 560/3000 [04:01&lt;13:20,  3.05it/s, v_num=1, train_loss_step=2.3e+6, train_loss_epoch=2.25e+6] Epoch 561/3000:  19%|█▊        | 560/3000 [04:01&lt;13:20,  3.05it/s, v_num=1, train_loss_step=2.3e+6, train_loss_epoch=2.25e+6]Epoch 561/3000:  19%|█▊        | 561/3000 [04:02&lt;16:18,  2.49it/s, v_num=1, train_loss_step=2.3e+6, train_loss_epoch=2.25e+6]Epoch 561/3000:  19%|█▊        | 561/3000 [04:02&lt;16:18,  2.49it/s, v_num=1, train_loss_step=2.34e+6, train_loss_epoch=2.24e+6]Epoch 562/3000:  19%|█▊        | 561/3000 [04:02&lt;16:18,  2.49it/s, v_num=1, train_loss_step=2.34e+6, train_loss_epoch=2.24e+6]Epoch 562/3000:  19%|█▊        | 562/3000 [04:02&lt;17:13,  2.36it/s, v_num=1, train_loss_step=2.34e+6, train_loss_epoch=2.24e+6]Epoch 562/3000:  19%|█▊        | 562/3000 [04:02&lt;17:13,  2.36it/s, v_num=1, train_loss_step=2.32e+6, train_loss_epoch=2.24e+6]Epoch 563/3000:  19%|█▊        | 562/3000 [04:02&lt;17:13,  2.36it/s, v_num=1, train_loss_step=2.32e+6, train_loss_epoch=2.24e+6]Epoch 563/3000:  19%|█▉        | 563/3000 [04:03&lt;18:11,  2.23it/s, v_num=1, train_loss_step=2.32e+6, train_loss_epoch=2.24e+6]Epoch 563/3000:  19%|█▉        | 563/3000 [04:03&lt;18:11,  2.23it/s, v_num=1, train_loss_step=2.25e+6, train_loss_epoch=2.24e+6]Epoch 564/3000:  19%|█▉        | 563/3000 [04:03&lt;18:11,  2.23it/s, v_num=1, train_loss_step=2.25e+6, train_loss_epoch=2.24e+6]Epoch 564/3000:  19%|█▉        | 564/3000 [04:03&lt;20:03,  2.02it/s, v_num=1, train_loss_step=2.25e+6, train_loss_epoch=2.24e+6]Epoch 564/3000:  19%|█▉        | 564/3000 [04:03&lt;20:03,  2.02it/s, v_num=1, train_loss_step=2.24e+6, train_loss_epoch=2.23e+6]Epoch 565/3000:  19%|█▉        | 564/3000 [04:03&lt;20:03,  2.02it/s, v_num=1, train_loss_step=2.24e+6, train_loss_epoch=2.23e+6]Epoch 565/3000:  19%|█▉        | 565/3000 [04:04&lt;20:39,  1.96it/s, v_num=1, train_loss_step=2.24e+6, train_loss_epoch=2.23e+6]Epoch 565/3000:  19%|█▉        | 565/3000 [04:04&lt;20:39,  1.96it/s, v_num=1, train_loss_step=2.06e+6, train_loss_epoch=2.23e+6]Epoch 566/3000:  19%|█▉        | 565/3000 [04:04&lt;20:39,  1.96it/s, v_num=1, train_loss_step=2.06e+6, train_loss_epoch=2.23e+6]Epoch 566/3000:  19%|█▉        | 566/3000 [04:04&lt;19:15,  2.11it/s, v_num=1, train_loss_step=2.06e+6, train_loss_epoch=2.23e+6]Epoch 566/3000:  19%|█▉        | 566/3000 [04:04&lt;19:15,  2.11it/s, v_num=1, train_loss_step=2.25e+6, train_loss_epoch=2.23e+6]Epoch 567/3000:  19%|█▉        | 566/3000 [04:04&lt;19:15,  2.11it/s, v_num=1, train_loss_step=2.25e+6, train_loss_epoch=2.23e+6]Epoch 567/3000:  19%|█▉        | 567/3000 [04:05&lt;18:10,  2.23it/s, v_num=1, train_loss_step=2.25e+6, train_loss_epoch=2.23e+6]Epoch 567/3000:  19%|█▉        | 567/3000 [04:05&lt;18:10,  2.23it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.23e+6]Epoch 568/3000:  19%|█▉        | 567/3000 [04:05&lt;18:10,  2.23it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.23e+6]Epoch 568/3000:  19%|█▉        | 568/3000 [04:05&lt;15:44,  2.58it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.23e+6]Epoch 568/3000:  19%|█▉        | 568/3000 [04:05&lt;15:44,  2.58it/s, v_num=1, train_loss_step=2.23e+6, train_loss_epoch=2.22e+6]Epoch 569/3000:  19%|█▉        | 568/3000 [04:05&lt;15:44,  2.58it/s, v_num=1, train_loss_step=2.23e+6, train_loss_epoch=2.22e+6]Epoch 569/3000:  19%|█▉        | 569/3000 [04:05&lt;17:24,  2.33it/s, v_num=1, train_loss_step=2.23e+6, train_loss_epoch=2.22e+6]Epoch 569/3000:  19%|█▉        | 569/3000 [04:05&lt;17:24,  2.33it/s, v_num=1, train_loss_step=2.3e+6, train_loss_epoch=2.22e+6] Epoch 570/3000:  19%|█▉        | 569/3000 [04:05&lt;17:24,  2.33it/s, v_num=1, train_loss_step=2.3e+6, train_loss_epoch=2.22e+6]Epoch 570/3000:  19%|█▉        | 570/3000 [04:06&lt;17:08,  2.36it/s, v_num=1, train_loss_step=2.3e+6, train_loss_epoch=2.22e+6]Epoch 570/3000:  19%|█▉        | 570/3000 [04:06&lt;17:08,  2.36it/s, v_num=1, train_loss_step=2.18e+6, train_loss_epoch=2.22e+6]Epoch 571/3000:  19%|█▉        | 570/3000 [04:06&lt;17:08,  2.36it/s, v_num=1, train_loss_step=2.18e+6, train_loss_epoch=2.22e+6]Epoch 571/3000:  19%|█▉        | 571/3000 [04:06&lt;18:05,  2.24it/s, v_num=1, train_loss_step=2.18e+6, train_loss_epoch=2.22e+6]Epoch 571/3000:  19%|█▉        | 571/3000 [04:06&lt;18:05,  2.24it/s, v_num=1, train_loss_step=2.21e+6, train_loss_epoch=2.21e+6]Epoch 572/3000:  19%|█▉        | 571/3000 [04:06&lt;18:05,  2.24it/s, v_num=1, train_loss_step=2.21e+6, train_loss_epoch=2.21e+6]Epoch 572/3000:  19%|█▉        | 572/3000 [04:07&lt;19:17,  2.10it/s, v_num=1, train_loss_step=2.21e+6, train_loss_epoch=2.21e+6]Epoch 572/3000:  19%|█▉        | 572/3000 [04:07&lt;19:17,  2.10it/s, v_num=1, train_loss_step=2.18e+6, train_loss_epoch=2.21e+6]Epoch 573/3000:  19%|█▉        | 572/3000 [04:07&lt;19:17,  2.10it/s, v_num=1, train_loss_step=2.18e+6, train_loss_epoch=2.21e+6]Epoch 573/3000:  19%|█▉        | 573/3000 [04:07&lt;20:31,  1.97it/s, v_num=1, train_loss_step=2.18e+6, train_loss_epoch=2.21e+6]Epoch 573/3000:  19%|█▉        | 573/3000 [04:07&lt;20:31,  1.97it/s, v_num=1, train_loss_step=2.22e+6, train_loss_epoch=2.21e+6]Epoch 574/3000:  19%|█▉        | 573/3000 [04:07&lt;20:31,  1.97it/s, v_num=1, train_loss_step=2.22e+6, train_loss_epoch=2.21e+6]Epoch 574/3000:  19%|█▉        | 574/3000 [04:08&lt;18:22,  2.20it/s, v_num=1, train_loss_step=2.22e+6, train_loss_epoch=2.21e+6]Epoch 574/3000:  19%|█▉        | 574/3000 [04:08&lt;18:22,  2.20it/s, v_num=1, train_loss_step=2.2e+6, train_loss_epoch=2.2e+6]  Epoch 575/3000:  19%|█▉        | 574/3000 [04:08&lt;18:22,  2.20it/s, v_num=1, train_loss_step=2.2e+6, train_loss_epoch=2.2e+6]Epoch 575/3000:  19%|█▉        | 575/3000 [04:08&lt;21:16,  1.90it/s, v_num=1, train_loss_step=2.2e+6, train_loss_epoch=2.2e+6]Epoch 575/3000:  19%|█▉        | 575/3000 [04:08&lt;21:16,  1.90it/s, v_num=1, train_loss_step=2.24e+6, train_loss_epoch=2.2e+6]Epoch 576/3000:  19%|█▉        | 575/3000 [04:08&lt;21:16,  1.90it/s, v_num=1, train_loss_step=2.24e+6, train_loss_epoch=2.2e+6]Epoch 576/3000:  19%|█▉        | 576/3000 [04:09&lt;21:59,  1.84it/s, v_num=1, train_loss_step=2.24e+6, train_loss_epoch=2.2e+6]Epoch 576/3000:  19%|█▉        | 576/3000 [04:09&lt;21:59,  1.84it/s, v_num=1, train_loss_step=2.19e+6, train_loss_epoch=2.2e+6]Epoch 577/3000:  19%|█▉        | 576/3000 [04:09&lt;21:59,  1.84it/s, v_num=1, train_loss_step=2.19e+6, train_loss_epoch=2.2e+6]Epoch 577/3000:  19%|█▉        | 577/3000 [04:10&lt;22:07,  1.83it/s, v_num=1, train_loss_step=2.19e+6, train_loss_epoch=2.2e+6]Epoch 577/3000:  19%|█▉        | 577/3000 [04:10&lt;22:07,  1.83it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.19e+6]Epoch 578/3000:  19%|█▉        | 577/3000 [04:10&lt;22:07,  1.83it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.19e+6]Epoch 578/3000:  19%|█▉        | 578/3000 [04:10&lt;19:55,  2.03it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.19e+6]Epoch 578/3000:  19%|█▉        | 578/3000 [04:10&lt;19:55,  2.03it/s, v_num=1, train_loss_step=2.19e+6, train_loss_epoch=2.19e+6]Epoch 579/3000:  19%|█▉        | 578/3000 [04:10&lt;19:55,  2.03it/s, v_num=1, train_loss_step=2.19e+6, train_loss_epoch=2.19e+6]Epoch 579/3000:  19%|█▉        | 579/3000 [04:10&lt;20:07,  2.01it/s, v_num=1, train_loss_step=2.19e+6, train_loss_epoch=2.19e+6]Epoch 579/3000:  19%|█▉        | 579/3000 [04:10&lt;20:07,  2.01it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.19e+6]Epoch 580/3000:  19%|█▉        | 579/3000 [04:10&lt;20:07,  2.01it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.19e+6]Epoch 580/3000:  19%|█▉        | 580/3000 [04:11&lt;18:25,  2.19it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.19e+6]Epoch 580/3000:  19%|█▉        | 580/3000 [04:11&lt;18:25,  2.19it/s, v_num=1, train_loss_step=2.16e+6, train_loss_epoch=2.19e+6]Epoch 581/3000:  19%|█▉        | 580/3000 [04:11&lt;18:25,  2.19it/s, v_num=1, train_loss_step=2.16e+6, train_loss_epoch=2.19e+6]Epoch 581/3000:  19%|█▉        | 581/3000 [04:11&lt;17:21,  2.32it/s, v_num=1, train_loss_step=2.16e+6, train_loss_epoch=2.19e+6]Epoch 581/3000:  19%|█▉        | 581/3000 [04:11&lt;17:21,  2.32it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.18e+6]Epoch 582/3000:  19%|█▉        | 581/3000 [04:11&lt;17:21,  2.32it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.18e+6]Epoch 582/3000:  19%|█▉        | 582/3000 [04:12&lt;18:11,  2.22it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.18e+6]Epoch 582/3000:  19%|█▉        | 582/3000 [04:12&lt;18:11,  2.22it/s, v_num=1, train_loss_step=2.26e+6, train_loss_epoch=2.18e+6]Epoch 583/3000:  19%|█▉        | 582/3000 [04:12&lt;18:11,  2.22it/s, v_num=1, train_loss_step=2.26e+6, train_loss_epoch=2.18e+6]Epoch 583/3000:  19%|█▉        | 583/3000 [04:12&lt;16:29,  2.44it/s, v_num=1, train_loss_step=2.26e+6, train_loss_epoch=2.18e+6]Epoch 583/3000:  19%|█▉        | 583/3000 [04:12&lt;16:29,  2.44it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.18e+6]Epoch 584/3000:  19%|█▉        | 583/3000 [04:12&lt;16:29,  2.44it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.18e+6]Epoch 584/3000:  19%|█▉        | 584/3000 [04:13&lt;18:32,  2.17it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.18e+6]Epoch 584/3000:  19%|█▉        | 584/3000 [04:13&lt;18:32,  2.17it/s, v_num=1, train_loss_step=2.19e+6, train_loss_epoch=2.17e+6]Epoch 585/3000:  19%|█▉        | 584/3000 [04:13&lt;18:32,  2.17it/s, v_num=1, train_loss_step=2.19e+6, train_loss_epoch=2.17e+6]Epoch 585/3000:  20%|█▉        | 585/3000 [04:13&lt;15:49,  2.54it/s, v_num=1, train_loss_step=2.19e+6, train_loss_epoch=2.17e+6]Epoch 585/3000:  20%|█▉        | 585/3000 [04:13&lt;15:49,  2.54it/s, v_num=1, train_loss_step=2.11e+6, train_loss_epoch=2.17e+6]Epoch 586/3000:  20%|█▉        | 585/3000 [04:13&lt;15:49,  2.54it/s, v_num=1, train_loss_step=2.11e+6, train_loss_epoch=2.17e+6]Epoch 586/3000:  20%|█▉        | 586/3000 [04:13&lt;18:37,  2.16it/s, v_num=1, train_loss_step=2.11e+6, train_loss_epoch=2.17e+6]Epoch 586/3000:  20%|█▉        | 586/3000 [04:13&lt;18:37,  2.16it/s, v_num=1, train_loss_step=2.13e+6, train_loss_epoch=2.17e+6]Epoch 587/3000:  20%|█▉        | 586/3000 [04:13&lt;18:37,  2.16it/s, v_num=1, train_loss_step=2.13e+6, train_loss_epoch=2.17e+6]Epoch 587/3000:  20%|█▉        | 587/3000 [04:14&lt;19:54,  2.02it/s, v_num=1, train_loss_step=2.13e+6, train_loss_epoch=2.17e+6]Epoch 587/3000:  20%|█▉        | 587/3000 [04:14&lt;19:54,  2.02it/s, v_num=1, train_loss_step=2.15e+6, train_loss_epoch=2.16e+6]Epoch 588/3000:  20%|█▉        | 587/3000 [04:14&lt;19:54,  2.02it/s, v_num=1, train_loss_step=2.15e+6, train_loss_epoch=2.16e+6]Epoch 588/3000:  20%|█▉        | 588/3000 [04:14&lt;20:05,  2.00it/s, v_num=1, train_loss_step=2.15e+6, train_loss_epoch=2.16e+6]Epoch 588/3000:  20%|█▉        | 588/3000 [04:14&lt;20:05,  2.00it/s, v_num=1, train_loss_step=2.06e+6, train_loss_epoch=2.16e+6]Epoch 589/3000:  20%|█▉        | 588/3000 [04:15&lt;20:05,  2.00it/s, v_num=1, train_loss_step=2.06e+6, train_loss_epoch=2.16e+6]Epoch 589/3000:  20%|█▉        | 589/3000 [04:15&lt;21:14,  1.89it/s, v_num=1, train_loss_step=2.06e+6, train_loss_epoch=2.16e+6]Epoch 589/3000:  20%|█▉        | 589/3000 [04:15&lt;21:14,  1.89it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.16e+6]Epoch 590/3000:  20%|█▉        | 589/3000 [04:15&lt;21:14,  1.89it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.16e+6]Epoch 590/3000:  20%|█▉        | 590/3000 [04:16&lt;22:36,  1.78it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.16e+6]Epoch 590/3000:  20%|█▉        | 590/3000 [04:16&lt;22:36,  1.78it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.16e+6]Epoch 591/3000:  20%|█▉        | 590/3000 [04:16&lt;22:36,  1.78it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.16e+6]Epoch 591/3000:  20%|█▉        | 591/3000 [04:16&lt;19:46,  2.03it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.16e+6]Epoch 591/3000:  20%|█▉        | 591/3000 [04:16&lt;19:46,  2.03it/s, v_num=1, train_loss_step=2.13e+6, train_loss_epoch=2.15e+6]Epoch 592/3000:  20%|█▉        | 591/3000 [04:16&lt;19:46,  2.03it/s, v_num=1, train_loss_step=2.13e+6, train_loss_epoch=2.15e+6]Epoch 592/3000:  20%|█▉        | 592/3000 [04:17&lt;20:42,  1.94it/s, v_num=1, train_loss_step=2.13e+6, train_loss_epoch=2.15e+6]Epoch 592/3000:  20%|█▉        | 592/3000 [04:17&lt;20:42,  1.94it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.15e+6]Epoch 593/3000:  20%|█▉        | 592/3000 [04:17&lt;20:42,  1.94it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.15e+6]Epoch 593/3000:  20%|█▉        | 593/3000 [04:17&lt;20:37,  1.94it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.15e+6]Epoch 593/3000:  20%|█▉        | 593/3000 [04:17&lt;20:37,  1.94it/s, v_num=1, train_loss_step=2.18e+6, train_loss_epoch=2.15e+6]Epoch 594/3000:  20%|█▉        | 593/3000 [04:17&lt;20:37,  1.94it/s, v_num=1, train_loss_step=2.18e+6, train_loss_epoch=2.15e+6]Epoch 594/3000:  20%|█▉        | 594/3000 [04:18&lt;19:05,  2.10it/s, v_num=1, train_loss_step=2.18e+6, train_loss_epoch=2.15e+6]Epoch 594/3000:  20%|█▉        | 594/3000 [04:18&lt;19:05,  2.10it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.14e+6]Epoch 595/3000:  20%|█▉        | 594/3000 [04:18&lt;19:05,  2.10it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.14e+6]Epoch 595/3000:  20%|█▉        | 595/3000 [04:18&lt;18:59,  2.11it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.14e+6]Epoch 595/3000:  20%|█▉        | 595/3000 [04:18&lt;18:59,  2.11it/s, v_num=1, train_loss_step=2.19e+6, train_loss_epoch=2.14e+6]Epoch 596/3000:  20%|█▉        | 595/3000 [04:18&lt;18:59,  2.11it/s, v_num=1, train_loss_step=2.19e+6, train_loss_epoch=2.14e+6]Epoch 596/3000:  20%|█▉        | 596/3000 [04:18&lt;18:53,  2.12it/s, v_num=1, train_loss_step=2.19e+6, train_loss_epoch=2.14e+6]Epoch 596/3000:  20%|█▉        | 596/3000 [04:18&lt;18:53,  2.12it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.14e+6]Epoch 597/3000:  20%|█▉        | 596/3000 [04:18&lt;18:53,  2.12it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.14e+6]Epoch 597/3000:  20%|█▉        | 597/3000 [04:19&lt;20:08,  1.99it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.14e+6]Epoch 597/3000:  20%|█▉        | 597/3000 [04:19&lt;20:08,  1.99it/s, v_num=1, train_loss_step=2.15e+6, train_loss_epoch=2.14e+6]Epoch 598/3000:  20%|█▉        | 597/3000 [04:19&lt;20:08,  1.99it/s, v_num=1, train_loss_step=2.15e+6, train_loss_epoch=2.14e+6]Epoch 598/3000:  20%|█▉        | 598/3000 [04:20&lt;20:47,  1.92it/s, v_num=1, train_loss_step=2.15e+6, train_loss_epoch=2.14e+6]Epoch 598/3000:  20%|█▉        | 598/3000 [04:20&lt;20:47,  1.92it/s, v_num=1, train_loss_step=2.09e+6, train_loss_epoch=2.13e+6]Epoch 599/3000:  20%|█▉        | 598/3000 [04:20&lt;20:47,  1.92it/s, v_num=1, train_loss_step=2.09e+6, train_loss_epoch=2.13e+6]Epoch 599/3000:  20%|█▉        | 599/3000 [04:20&lt;20:46,  1.93it/s, v_num=1, train_loss_step=2.09e+6, train_loss_epoch=2.13e+6]Epoch 599/3000:  20%|█▉        | 599/3000 [04:20&lt;20:46,  1.93it/s, v_num=1, train_loss_step=2.09e+6, train_loss_epoch=2.13e+6]Epoch 600/3000:  20%|█▉        | 599/3000 [04:20&lt;20:46,  1.93it/s, v_num=1, train_loss_step=2.09e+6, train_loss_epoch=2.13e+6]Epoch 600/3000:  20%|██        | 600/3000 [04:21&lt;20:25,  1.96it/s, v_num=1, train_loss_step=2.09e+6, train_loss_epoch=2.13e+6]Epoch 600/3000:  20%|██        | 600/3000 [04:21&lt;20:25,  1.96it/s, v_num=1, train_loss_step=2.22e+6, train_loss_epoch=2.13e+6]Epoch 601/3000:  20%|██        | 600/3000 [04:21&lt;20:25,  1.96it/s, v_num=1, train_loss_step=2.22e+6, train_loss_epoch=2.13e+6]Epoch 601/3000:  20%|██        | 601/3000 [04:21&lt;18:33,  2.15it/s, v_num=1, train_loss_step=2.22e+6, train_loss_epoch=2.13e+6]Epoch 601/3000:  20%|██        | 601/3000 [04:21&lt;18:33,  2.15it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.12e+6]Epoch 602/3000:  20%|██        | 601/3000 [04:21&lt;18:33,  2.15it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.12e+6]Epoch 602/3000:  20%|██        | 602/3000 [04:21&lt;17:47,  2.25it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.12e+6]Epoch 602/3000:  20%|██        | 602/3000 [04:21&lt;17:47,  2.25it/s, v_num=1, train_loss_step=2.1e+6, train_loss_epoch=2.12e+6] Epoch 603/3000:  20%|██        | 602/3000 [04:21&lt;17:47,  2.25it/s, v_num=1, train_loss_step=2.1e+6, train_loss_epoch=2.12e+6]Epoch 603/3000:  20%|██        | 603/3000 [04:22&lt;17:14,  2.32it/s, v_num=1, train_loss_step=2.1e+6, train_loss_epoch=2.12e+6]Epoch 603/3000:  20%|██        | 603/3000 [04:22&lt;17:14,  2.32it/s, v_num=1, train_loss_step=2.11e+6, train_loss_epoch=2.12e+6]Epoch 604/3000:  20%|██        | 603/3000 [04:22&lt;17:14,  2.32it/s, v_num=1, train_loss_step=2.11e+6, train_loss_epoch=2.12e+6]Epoch 604/3000:  20%|██        | 604/3000 [04:22&lt;16:24,  2.43it/s, v_num=1, train_loss_step=2.11e+6, train_loss_epoch=2.12e+6]Epoch 604/3000:  20%|██        | 604/3000 [04:22&lt;16:24,  2.43it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2.12e+6]Epoch 605/3000:  20%|██        | 604/3000 [04:22&lt;16:24,  2.43it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2.12e+6]Epoch 605/3000:  20%|██        | 605/3000 [04:23&lt;16:45,  2.38it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2.12e+6]Epoch 605/3000:  20%|██        | 605/3000 [04:23&lt;16:45,  2.38it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.11e+6]Epoch 606/3000:  20%|██        | 605/3000 [04:23&lt;16:45,  2.38it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.11e+6]Epoch 606/3000:  20%|██        | 606/3000 [04:23&lt;13:27,  2.97it/s, v_num=1, train_loss_step=2.14e+6, train_loss_epoch=2.11e+6]Epoch 606/3000:  20%|██        | 606/3000 [04:23&lt;13:27,  2.97it/s, v_num=1, train_loss_step=2.15e+6, train_loss_epoch=2.11e+6]Epoch 607/3000:  20%|██        | 606/3000 [04:23&lt;13:27,  2.97it/s, v_num=1, train_loss_step=2.15e+6, train_loss_epoch=2.11e+6]Epoch 607/3000:  20%|██        | 607/3000 [04:23&lt;12:23,  3.22it/s, v_num=1, train_loss_step=2.15e+6, train_loss_epoch=2.11e+6]Epoch 607/3000:  20%|██        | 607/3000 [04:23&lt;12:23,  3.22it/s, v_num=1, train_loss_step=2.16e+6, train_loss_epoch=2.11e+6]Epoch 608/3000:  20%|██        | 607/3000 [04:23&lt;12:23,  3.22it/s, v_num=1, train_loss_step=2.16e+6, train_loss_epoch=2.11e+6]Epoch 608/3000:  20%|██        | 608/3000 [04:23&lt;12:14,  3.26it/s, v_num=1, train_loss_step=2.16e+6, train_loss_epoch=2.11e+6]Epoch 608/3000:  20%|██        | 608/3000 [04:23&lt;12:14,  3.26it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.11e+6]Epoch 609/3000:  20%|██        | 608/3000 [04:23&lt;12:14,  3.26it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.11e+6]Epoch 609/3000:  20%|██        | 609/3000 [04:23&lt;10:53,  3.66it/s, v_num=1, train_loss_step=2.17e+6, train_loss_epoch=2.11e+6]Epoch 609/3000:  20%|██        | 609/3000 [04:23&lt;10:53,  3.66it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2.1e+6] Epoch 610/3000:  20%|██        | 609/3000 [04:23&lt;10:53,  3.66it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2.1e+6]Epoch 610/3000:  20%|██        | 610/3000 [04:24&lt;09:41,  4.11it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2.1e+6]Epoch 610/3000:  20%|██        | 610/3000 [04:24&lt;09:41,  4.11it/s, v_num=1, train_loss_step=2.07e+6, train_loss_epoch=2.1e+6]Epoch 611/3000:  20%|██        | 610/3000 [04:24&lt;09:41,  4.11it/s, v_num=1, train_loss_step=2.07e+6, train_loss_epoch=2.1e+6]Epoch 611/3000:  20%|██        | 611/3000 [04:24&lt;14:10,  2.81it/s, v_num=1, train_loss_step=2.07e+6, train_loss_epoch=2.1e+6]Epoch 611/3000:  20%|██        | 611/3000 [04:24&lt;14:10,  2.81it/s, v_num=1, train_loss_step=2.19e+6, train_loss_epoch=2.1e+6]Epoch 612/3000:  20%|██        | 611/3000 [04:24&lt;14:10,  2.81it/s, v_num=1, train_loss_step=2.19e+6, train_loss_epoch=2.1e+6]Epoch 612/3000:  20%|██        | 612/3000 [04:25&lt;13:44,  2.90it/s, v_num=1, train_loss_step=2.19e+6, train_loss_epoch=2.1e+6]Epoch 612/3000:  20%|██        | 612/3000 [04:25&lt;13:44,  2.90it/s, v_num=1, train_loss_step=2.12e+6, train_loss_epoch=2.09e+6]Epoch 613/3000:  20%|██        | 612/3000 [04:25&lt;13:44,  2.90it/s, v_num=1, train_loss_step=2.12e+6, train_loss_epoch=2.09e+6]Epoch 613/3000:  20%|██        | 613/3000 [04:25&lt;15:23,  2.58it/s, v_num=1, train_loss_step=2.12e+6, train_loss_epoch=2.09e+6]Epoch 613/3000:  20%|██        | 613/3000 [04:25&lt;15:23,  2.58it/s, v_num=1, train_loss_step=2.15e+6, train_loss_epoch=2.09e+6]Epoch 614/3000:  20%|██        | 613/3000 [04:25&lt;15:23,  2.58it/s, v_num=1, train_loss_step=2.15e+6, train_loss_epoch=2.09e+6]Epoch 614/3000:  20%|██        | 614/3000 [04:25&lt;15:56,  2.50it/s, v_num=1, train_loss_step=2.15e+6, train_loss_epoch=2.09e+6]Epoch 614/3000:  20%|██        | 614/3000 [04:25&lt;15:56,  2.50it/s, v_num=1, train_loss_step=2.05e+6, train_loss_epoch=2.09e+6]Epoch 615/3000:  20%|██        | 614/3000 [04:25&lt;15:56,  2.50it/s, v_num=1, train_loss_step=2.05e+6, train_loss_epoch=2.09e+6]Epoch 615/3000:  20%|██        | 615/3000 [04:26&lt;16:39,  2.39it/s, v_num=1, train_loss_step=2.05e+6, train_loss_epoch=2.09e+6]Epoch 615/3000:  20%|██        | 615/3000 [04:26&lt;16:39,  2.39it/s, v_num=1, train_loss_step=2.09e+6, train_loss_epoch=2.09e+6]Epoch 616/3000:  20%|██        | 615/3000 [04:26&lt;16:39,  2.39it/s, v_num=1, train_loss_step=2.09e+6, train_loss_epoch=2.09e+6]Epoch 616/3000:  21%|██        | 616/3000 [04:26&lt;17:34,  2.26it/s, v_num=1, train_loss_step=2.09e+6, train_loss_epoch=2.09e+6]Epoch 616/3000:  21%|██        | 616/3000 [04:26&lt;17:34,  2.26it/s, v_num=1, train_loss_step=2.16e+6, train_loss_epoch=2.08e+6]Epoch 617/3000:  21%|██        | 616/3000 [04:26&lt;17:34,  2.26it/s, v_num=1, train_loss_step=2.16e+6, train_loss_epoch=2.08e+6]Epoch 617/3000:  21%|██        | 617/3000 [04:27&lt;16:33,  2.40it/s, v_num=1, train_loss_step=2.16e+6, train_loss_epoch=2.08e+6]Epoch 617/3000:  21%|██        | 617/3000 [04:27&lt;16:33,  2.40it/s, v_num=1, train_loss_step=2.11e+6, train_loss_epoch=2.08e+6]Epoch 618/3000:  21%|██        | 617/3000 [04:27&lt;16:33,  2.40it/s, v_num=1, train_loss_step=2.11e+6, train_loss_epoch=2.08e+6]Epoch 618/3000:  21%|██        | 618/3000 [04:27&lt;15:54,  2.50it/s, v_num=1, train_loss_step=2.11e+6, train_loss_epoch=2.08e+6]Epoch 618/3000:  21%|██        | 618/3000 [04:27&lt;15:54,  2.50it/s, v_num=1, train_loss_step=2.08e+6, train_loss_epoch=2.08e+6]Epoch 619/3000:  21%|██        | 618/3000 [04:27&lt;15:54,  2.50it/s, v_num=1, train_loss_step=2.08e+6, train_loss_epoch=2.08e+6]Epoch 619/3000:  21%|██        | 619/3000 [04:28&lt;16:31,  2.40it/s, v_num=1, train_loss_step=2.08e+6, train_loss_epoch=2.08e+6]Epoch 619/3000:  21%|██        | 619/3000 [04:28&lt;16:31,  2.40it/s, v_num=1, train_loss_step=1.96e+6, train_loss_epoch=2.08e+6]Epoch 620/3000:  21%|██        | 619/3000 [04:28&lt;16:31,  2.40it/s, v_num=1, train_loss_step=1.96e+6, train_loss_epoch=2.08e+6]Epoch 620/3000:  21%|██        | 620/3000 [04:28&lt;17:17,  2.29it/s, v_num=1, train_loss_step=1.96e+6, train_loss_epoch=2.08e+6]Epoch 620/3000:  21%|██        | 620/3000 [04:28&lt;17:17,  2.29it/s, v_num=1, train_loss_step=1.96e+6, train_loss_epoch=2.07e+6]Epoch 621/3000:  21%|██        | 620/3000 [04:28&lt;17:17,  2.29it/s, v_num=1, train_loss_step=1.96e+6, train_loss_epoch=2.07e+6]Epoch 621/3000:  21%|██        | 621/3000 [04:28&lt;15:47,  2.51it/s, v_num=1, train_loss_step=1.96e+6, train_loss_epoch=2.07e+6]Epoch 621/3000:  21%|██        | 621/3000 [04:28&lt;15:47,  2.51it/s, v_num=1, train_loss_step=2.02e+6, train_loss_epoch=2.07e+6]Epoch 622/3000:  21%|██        | 621/3000 [04:28&lt;15:47,  2.51it/s, v_num=1, train_loss_step=2.02e+6, train_loss_epoch=2.07e+6]Epoch 622/3000:  21%|██        | 622/3000 [04:29&lt;14:08,  2.80it/s, v_num=1, train_loss_step=2.02e+6, train_loss_epoch=2.07e+6]Epoch 622/3000:  21%|██        | 622/3000 [04:29&lt;14:08,  2.80it/s, v_num=1, train_loss_step=2e+6, train_loss_epoch=2.07e+6]   Epoch 623/3000:  21%|██        | 622/3000 [04:29&lt;14:08,  2.80it/s, v_num=1, train_loss_step=2e+6, train_loss_epoch=2.07e+6]Epoch 623/3000:  21%|██        | 623/3000 [04:29&lt;14:35,  2.71it/s, v_num=1, train_loss_step=2e+6, train_loss_epoch=2.07e+6]Epoch 623/3000:  21%|██        | 623/3000 [04:29&lt;14:35,  2.71it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=2.07e+6]Epoch 624/3000:  21%|██        | 623/3000 [04:29&lt;14:35,  2.71it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=2.07e+6]Epoch 624/3000:  21%|██        | 624/3000 [04:30&lt;16:44,  2.36it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=2.07e+6]Epoch 624/3000:  21%|██        | 624/3000 [04:30&lt;16:44,  2.36it/s, v_num=1, train_loss_step=2.07e+6, train_loss_epoch=2.06e+6]Epoch 625/3000:  21%|██        | 624/3000 [04:30&lt;16:44,  2.36it/s, v_num=1, train_loss_step=2.07e+6, train_loss_epoch=2.06e+6]Epoch 625/3000:  21%|██        | 625/3000 [04:30&lt;19:19,  2.05it/s, v_num=1, train_loss_step=2.07e+6, train_loss_epoch=2.06e+6]Epoch 625/3000:  21%|██        | 625/3000 [04:30&lt;19:19,  2.05it/s, v_num=1, train_loss_step=2.06e+6, train_loss_epoch=2.06e+6]Epoch 626/3000:  21%|██        | 625/3000 [04:30&lt;19:19,  2.05it/s, v_num=1, train_loss_step=2.06e+6, train_loss_epoch=2.06e+6]Epoch 626/3000:  21%|██        | 626/3000 [04:31&lt;20:05,  1.97it/s, v_num=1, train_loss_step=2.06e+6, train_loss_epoch=2.06e+6]Epoch 626/3000:  21%|██        | 626/3000 [04:31&lt;20:05,  1.97it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=2.06e+6]Epoch 627/3000:  21%|██        | 626/3000 [04:31&lt;20:05,  1.97it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=2.06e+6]Epoch 627/3000:  21%|██        | 627/3000 [04:31&lt;18:41,  2.12it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=2.06e+6]Epoch 627/3000:  21%|██        | 627/3000 [04:31&lt;18:41,  2.12it/s, v_num=1, train_loss_step=2.04e+6, train_loss_epoch=2.06e+6]Epoch 628/3000:  21%|██        | 627/3000 [04:31&lt;18:41,  2.12it/s, v_num=1, train_loss_step=2.04e+6, train_loss_epoch=2.06e+6]Epoch 628/3000:  21%|██        | 628/3000 [04:32&lt;19:00,  2.08it/s, v_num=1, train_loss_step=2.04e+6, train_loss_epoch=2.06e+6]Epoch 628/3000:  21%|██        | 628/3000 [04:32&lt;19:00,  2.08it/s, v_num=1, train_loss_step=2.13e+6, train_loss_epoch=2.05e+6]Epoch 629/3000:  21%|██        | 628/3000 [04:32&lt;19:00,  2.08it/s, v_num=1, train_loss_step=2.13e+6, train_loss_epoch=2.05e+6]Epoch 629/3000:  21%|██        | 629/3000 [04:32&lt;20:21,  1.94it/s, v_num=1, train_loss_step=2.13e+6, train_loss_epoch=2.05e+6]Epoch 629/3000:  21%|██        | 629/3000 [04:32&lt;20:21,  1.94it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=2.05e+6]Epoch 630/3000:  21%|██        | 629/3000 [04:32&lt;20:21,  1.94it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=2.05e+6]Epoch 630/3000:  21%|██        | 630/3000 [04:33&lt;19:52,  1.99it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=2.05e+6]Epoch 630/3000:  21%|██        | 630/3000 [04:33&lt;19:52,  1.99it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=2.05e+6]Epoch 631/3000:  21%|██        | 630/3000 [04:33&lt;19:52,  1.99it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=2.05e+6]Epoch 631/3000:  21%|██        | 631/3000 [04:33&lt;18:30,  2.13it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=2.05e+6]Epoch 631/3000:  21%|██        | 631/3000 [04:33&lt;18:30,  2.13it/s, v_num=1, train_loss_step=2.08e+6, train_loss_epoch=2.05e+6]Epoch 632/3000:  21%|██        | 631/3000 [04:33&lt;18:30,  2.13it/s, v_num=1, train_loss_step=2.08e+6, train_loss_epoch=2.05e+6]Epoch 632/3000:  21%|██        | 632/3000 [04:34&lt;17:21,  2.27it/s, v_num=1, train_loss_step=2.08e+6, train_loss_epoch=2.05e+6]Epoch 632/3000:  21%|██        | 632/3000 [04:34&lt;17:21,  2.27it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2.04e+6]Epoch 633/3000:  21%|██        | 632/3000 [04:34&lt;17:21,  2.27it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2.04e+6]Epoch 633/3000:  21%|██        | 633/3000 [04:34&lt;18:49,  2.10it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2.04e+6]Epoch 633/3000:  21%|██        | 633/3000 [04:34&lt;18:49,  2.10it/s, v_num=1, train_loss_step=1.97e+6, train_loss_epoch=2.04e+6]Epoch 634/3000:  21%|██        | 633/3000 [04:34&lt;18:49,  2.10it/s, v_num=1, train_loss_step=1.97e+6, train_loss_epoch=2.04e+6]Epoch 634/3000:  21%|██        | 634/3000 [04:35&lt;19:53,  1.98it/s, v_num=1, train_loss_step=1.97e+6, train_loss_epoch=2.04e+6]Epoch 634/3000:  21%|██        | 634/3000 [04:35&lt;19:53,  1.98it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=2.04e+6]Epoch 635/3000:  21%|██        | 634/3000 [04:35&lt;19:53,  1.98it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=2.04e+6]Epoch 635/3000:  21%|██        | 635/3000 [04:35&lt;19:13,  2.05it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=2.04e+6]Epoch 635/3000:  21%|██        | 635/3000 [04:35&lt;19:13,  2.05it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2.04e+6]Epoch 636/3000:  21%|██        | 635/3000 [04:35&lt;19:13,  2.05it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2.04e+6]Epoch 636/3000:  21%|██        | 636/3000 [04:36&lt;20:15,  1.95it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2.04e+6]Epoch 636/3000:  21%|██        | 636/3000 [04:36&lt;20:15,  1.95it/s, v_num=1, train_loss_step=2.02e+6, train_loss_epoch=2.03e+6]Epoch 637/3000:  21%|██        | 636/3000 [04:36&lt;20:15,  1.95it/s, v_num=1, train_loss_step=2.02e+6, train_loss_epoch=2.03e+6]Epoch 637/3000:  21%|██        | 637/3000 [04:36&lt;20:07,  1.96it/s, v_num=1, train_loss_step=2.02e+6, train_loss_epoch=2.03e+6]Epoch 637/3000:  21%|██        | 637/3000 [04:36&lt;20:07,  1.96it/s, v_num=1, train_loss_step=2.08e+6, train_loss_epoch=2.03e+6]Epoch 638/3000:  21%|██        | 637/3000 [04:36&lt;20:07,  1.96it/s, v_num=1, train_loss_step=2.08e+6, train_loss_epoch=2.03e+6]Epoch 638/3000:  21%|██▏       | 638/3000 [04:37&lt;20:04,  1.96it/s, v_num=1, train_loss_step=2.08e+6, train_loss_epoch=2.03e+6]Epoch 638/3000:  21%|██▏       | 638/3000 [04:37&lt;20:04,  1.96it/s, v_num=1, train_loss_step=1.97e+6, train_loss_epoch=2.03e+6]Epoch 639/3000:  21%|██▏       | 638/3000 [04:37&lt;20:04,  1.96it/s, v_num=1, train_loss_step=1.97e+6, train_loss_epoch=2.03e+6]Epoch 639/3000:  21%|██▏       | 639/3000 [04:37&lt;19:03,  2.06it/s, v_num=1, train_loss_step=1.97e+6, train_loss_epoch=2.03e+6]Epoch 639/3000:  21%|██▏       | 639/3000 [04:37&lt;19:03,  2.06it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2.03e+6]Epoch 640/3000:  21%|██▏       | 639/3000 [04:37&lt;19:03,  2.06it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2.03e+6]Epoch 640/3000:  21%|██▏       | 640/3000 [04:37&lt;17:21,  2.27it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2.03e+6]Epoch 640/3000:  21%|██▏       | 640/3000 [04:37&lt;17:21,  2.27it/s, v_num=1, train_loss_step=1.98e+6, train_loss_epoch=2.02e+6]Epoch 641/3000:  21%|██▏       | 640/3000 [04:37&lt;17:21,  2.27it/s, v_num=1, train_loss_step=1.98e+6, train_loss_epoch=2.02e+6]Epoch 641/3000:  21%|██▏       | 641/3000 [04:38&lt;15:21,  2.56it/s, v_num=1, train_loss_step=1.98e+6, train_loss_epoch=2.02e+6]Epoch 641/3000:  21%|██▏       | 641/3000 [04:38&lt;15:21,  2.56it/s, v_num=1, train_loss_step=2.16e+6, train_loss_epoch=2.02e+6]Epoch 642/3000:  21%|██▏       | 641/3000 [04:38&lt;15:21,  2.56it/s, v_num=1, train_loss_step=2.16e+6, train_loss_epoch=2.02e+6]Epoch 642/3000:  21%|██▏       | 642/3000 [04:38&lt;15:20,  2.56it/s, v_num=1, train_loss_step=2.16e+6, train_loss_epoch=2.02e+6]Epoch 642/3000:  21%|██▏       | 642/3000 [04:38&lt;15:20,  2.56it/s, v_num=1, train_loss_step=2.08e+6, train_loss_epoch=2.02e+6]Epoch 643/3000:  21%|██▏       | 642/3000 [04:38&lt;15:20,  2.56it/s, v_num=1, train_loss_step=2.08e+6, train_loss_epoch=2.02e+6]Epoch 643/3000:  21%|██▏       | 643/3000 [04:38&lt;13:27,  2.92it/s, v_num=1, train_loss_step=2.08e+6, train_loss_epoch=2.02e+6]Epoch 643/3000:  21%|██▏       | 643/3000 [04:38&lt;13:27,  2.92it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=2.02e+6]Epoch 644/3000:  21%|██▏       | 643/3000 [04:38&lt;13:27,  2.92it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=2.02e+6]Epoch 644/3000:  21%|██▏       | 644/3000 [04:39&lt;15:19,  2.56it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=2.02e+6]Epoch 644/3000:  21%|██▏       | 644/3000 [04:39&lt;15:19,  2.56it/s, v_num=1, train_loss_step=2.07e+6, train_loss_epoch=2.01e+6]Epoch 645/3000:  21%|██▏       | 644/3000 [04:39&lt;15:19,  2.56it/s, v_num=1, train_loss_step=2.07e+6, train_loss_epoch=2.01e+6]Epoch 645/3000:  22%|██▏       | 645/3000 [04:39&lt;16:15,  2.41it/s, v_num=1, train_loss_step=2.07e+6, train_loss_epoch=2.01e+6]Epoch 645/3000:  22%|██▏       | 645/3000 [04:39&lt;16:15,  2.41it/s, v_num=1, train_loss_step=2.07e+6, train_loss_epoch=2.01e+6]Epoch 646/3000:  22%|██▏       | 645/3000 [04:39&lt;16:15,  2.41it/s, v_num=1, train_loss_step=2.07e+6, train_loss_epoch=2.01e+6]Epoch 646/3000:  22%|██▏       | 646/3000 [04:40&lt;17:37,  2.23it/s, v_num=1, train_loss_step=2.07e+6, train_loss_epoch=2.01e+6]Epoch 646/3000:  22%|██▏       | 646/3000 [04:40&lt;17:37,  2.23it/s, v_num=1, train_loss_step=1.95e+6, train_loss_epoch=2.01e+6]Epoch 647/3000:  22%|██▏       | 646/3000 [04:40&lt;17:37,  2.23it/s, v_num=1, train_loss_step=1.95e+6, train_loss_epoch=2.01e+6]Epoch 647/3000:  22%|██▏       | 647/3000 [04:40&lt;19:08,  2.05it/s, v_num=1, train_loss_step=1.95e+6, train_loss_epoch=2.01e+6]Epoch 647/3000:  22%|██▏       | 647/3000 [04:40&lt;19:08,  2.05it/s, v_num=1, train_loss_step=2.02e+6, train_loss_epoch=2.01e+6]Epoch 648/3000:  22%|██▏       | 647/3000 [04:40&lt;19:08,  2.05it/s, v_num=1, train_loss_step=2.02e+6, train_loss_epoch=2.01e+6]Epoch 648/3000:  22%|██▏       | 648/3000 [04:41&lt;20:03,  1.95it/s, v_num=1, train_loss_step=2.02e+6, train_loss_epoch=2.01e+6]Epoch 648/3000:  22%|██▏       | 648/3000 [04:41&lt;20:03,  1.95it/s, v_num=1, train_loss_step=2e+6, train_loss_epoch=2e+6]      Epoch 649/3000:  22%|██▏       | 648/3000 [04:41&lt;20:03,  1.95it/s, v_num=1, train_loss_step=2e+6, train_loss_epoch=2e+6]Epoch 649/3000:  22%|██▏       | 649/3000 [04:42&lt;20:00,  1.96it/s, v_num=1, train_loss_step=2e+6, train_loss_epoch=2e+6]Epoch 649/3000:  22%|██▏       | 649/3000 [04:42&lt;20:00,  1.96it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2e+6]Epoch 650/3000:  22%|██▏       | 649/3000 [04:42&lt;20:00,  1.96it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2e+6]Epoch 650/3000:  22%|██▏       | 650/3000 [04:42&lt;20:15,  1.93it/s, v_num=1, train_loss_step=2.03e+6, train_loss_epoch=2e+6]Epoch 650/3000:  22%|██▏       | 650/3000 [04:42&lt;20:15,  1.93it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=2e+6]Epoch 651/3000:  22%|██▏       | 650/3000 [04:42&lt;20:15,  1.93it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=2e+6]Epoch 651/3000:  22%|██▏       | 651/3000 [04:43&lt;19:38,  1.99it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=2e+6]Epoch 651/3000:  22%|██▏       | 651/3000 [04:43&lt;19:38,  1.99it/s, v_num=1, train_loss_step=1.98e+6, train_loss_epoch=2e+6]Epoch 652/3000:  22%|██▏       | 651/3000 [04:43&lt;19:38,  1.99it/s, v_num=1, train_loss_step=1.98e+6, train_loss_epoch=2e+6]Epoch 652/3000:  22%|██▏       | 652/3000 [04:43&lt;17:43,  2.21it/s, v_num=1, train_loss_step=1.98e+6, train_loss_epoch=2e+6]Epoch 652/3000:  22%|██▏       | 652/3000 [04:43&lt;17:43,  2.21it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=2e+6]Epoch 653/3000:  22%|██▏       | 652/3000 [04:43&lt;17:43,  2.21it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=2e+6]Epoch 653/3000:  22%|██▏       | 653/3000 [04:43&lt;15:56,  2.45it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=2e+6]Epoch 653/3000:  22%|██▏       | 653/3000 [04:43&lt;15:56,  2.45it/s, v_num=1, train_loss_step=1.95e+6, train_loss_epoch=1.99e+6]Epoch 654/3000:  22%|██▏       | 653/3000 [04:43&lt;15:56,  2.45it/s, v_num=1, train_loss_step=1.95e+6, train_loss_epoch=1.99e+6]Epoch 654/3000:  22%|██▏       | 654/3000 [04:43&lt;14:44,  2.65it/s, v_num=1, train_loss_step=1.95e+6, train_loss_epoch=1.99e+6]Epoch 654/3000:  22%|██▏       | 654/3000 [04:43&lt;14:44,  2.65it/s, v_num=1, train_loss_step=1.95e+6, train_loss_epoch=1.99e+6]Epoch 655/3000:  22%|██▏       | 654/3000 [04:43&lt;14:44,  2.65it/s, v_num=1, train_loss_step=1.95e+6, train_loss_epoch=1.99e+6]Epoch 655/3000:  22%|██▏       | 655/3000 [04:44&lt;14:49,  2.64it/s, v_num=1, train_loss_step=1.95e+6, train_loss_epoch=1.99e+6]Epoch 655/3000:  22%|██▏       | 655/3000 [04:44&lt;14:49,  2.64it/s, v_num=1, train_loss_step=1.88e+6, train_loss_epoch=1.99e+6]Epoch 656/3000:  22%|██▏       | 655/3000 [04:44&lt;14:49,  2.64it/s, v_num=1, train_loss_step=1.88e+6, train_loss_epoch=1.99e+6]Epoch 656/3000:  22%|██▏       | 656/3000 [04:44&lt;15:17,  2.55it/s, v_num=1, train_loss_step=1.88e+6, train_loss_epoch=1.99e+6]Epoch 656/3000:  22%|██▏       | 656/3000 [04:44&lt;15:17,  2.55it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=1.99e+6]Epoch 657/3000:  22%|██▏       | 656/3000 [04:44&lt;15:17,  2.55it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=1.99e+6]Epoch 657/3000:  22%|██▏       | 657/3000 [04:45&lt;17:33,  2.23it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=1.99e+6]Epoch 657/3000:  22%|██▏       | 657/3000 [04:45&lt;17:33,  2.23it/s, v_num=1, train_loss_step=2.12e+6, train_loss_epoch=1.98e+6]Epoch 658/3000:  22%|██▏       | 657/3000 [04:45&lt;17:33,  2.23it/s, v_num=1, train_loss_step=2.12e+6, train_loss_epoch=1.98e+6]Epoch 658/3000:  22%|██▏       | 658/3000 [04:45&lt;18:25,  2.12it/s, v_num=1, train_loss_step=2.12e+6, train_loss_epoch=1.98e+6]Epoch 658/3000:  22%|██▏       | 658/3000 [04:45&lt;18:25,  2.12it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=1.98e+6]Epoch 659/3000:  22%|██▏       | 658/3000 [04:45&lt;18:25,  2.12it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=1.98e+6]Epoch 659/3000:  22%|██▏       | 659/3000 [04:46&lt;16:28,  2.37it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=1.98e+6]Epoch 659/3000:  22%|██▏       | 659/3000 [04:46&lt;16:28,  2.37it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.98e+6]Epoch 660/3000:  22%|██▏       | 659/3000 [04:46&lt;16:28,  2.37it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.98e+6]Epoch 660/3000:  22%|██▏       | 660/3000 [04:46&lt;17:51,  2.18it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.98e+6]Epoch 660/3000:  22%|██▏       | 660/3000 [04:46&lt;17:51,  2.18it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=1.98e+6]Epoch 661/3000:  22%|██▏       | 660/3000 [04:46&lt;17:51,  2.18it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=1.98e+6]Epoch 661/3000:  22%|██▏       | 661/3000 [04:47&lt;17:37,  2.21it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=1.98e+6]Epoch 661/3000:  22%|██▏       | 661/3000 [04:47&lt;17:37,  2.21it/s, v_num=1, train_loss_step=2.07e+6, train_loss_epoch=1.97e+6]Epoch 662/3000:  22%|██▏       | 661/3000 [04:47&lt;17:37,  2.21it/s, v_num=1, train_loss_step=2.07e+6, train_loss_epoch=1.97e+6]Epoch 662/3000:  22%|██▏       | 662/3000 [04:47&lt;17:37,  2.21it/s, v_num=1, train_loss_step=2.07e+6, train_loss_epoch=1.97e+6]Epoch 662/3000:  22%|██▏       | 662/3000 [04:47&lt;17:37,  2.21it/s, v_num=1, train_loss_step=2e+6, train_loss_epoch=1.97e+6]   Epoch 663/3000:  22%|██▏       | 662/3000 [04:47&lt;17:37,  2.21it/s, v_num=1, train_loss_step=2e+6, train_loss_epoch=1.97e+6]Epoch 663/3000:  22%|██▏       | 663/3000 [04:47&lt;16:27,  2.37it/s, v_num=1, train_loss_step=2e+6, train_loss_epoch=1.97e+6]Epoch 663/3000:  22%|██▏       | 663/3000 [04:47&lt;16:27,  2.37it/s, v_num=1, train_loss_step=1.89e+6, train_loss_epoch=1.97e+6]Epoch 664/3000:  22%|██▏       | 663/3000 [04:47&lt;16:27,  2.37it/s, v_num=1, train_loss_step=1.89e+6, train_loss_epoch=1.97e+6]Epoch 664/3000:  22%|██▏       | 664/3000 [04:48&lt;17:52,  2.18it/s, v_num=1, train_loss_step=1.89e+6, train_loss_epoch=1.97e+6]Epoch 664/3000:  22%|██▏       | 664/3000 [04:48&lt;17:52,  2.18it/s, v_num=1, train_loss_step=2e+6, train_loss_epoch=1.97e+6]   Epoch 665/3000:  22%|██▏       | 664/3000 [04:48&lt;17:52,  2.18it/s, v_num=1, train_loss_step=2e+6, train_loss_epoch=1.97e+6]Epoch 665/3000:  22%|██▏       | 665/3000 [04:48&lt;17:22,  2.24it/s, v_num=1, train_loss_step=2e+6, train_loss_epoch=1.97e+6]Epoch 665/3000:  22%|██▏       | 665/3000 [04:48&lt;17:22,  2.24it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=1.97e+6]Epoch 666/3000:  22%|██▏       | 665/3000 [04:48&lt;17:22,  2.24it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=1.97e+6]Epoch 666/3000:  22%|██▏       | 666/3000 [04:49&lt;18:27,  2.11it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=1.97e+6]Epoch 666/3000:  22%|██▏       | 666/3000 [04:49&lt;18:27,  2.11it/s, v_num=1, train_loss_step=2e+6, train_loss_epoch=1.96e+6]   Epoch 667/3000:  22%|██▏       | 666/3000 [04:49&lt;18:27,  2.11it/s, v_num=1, train_loss_step=2e+6, train_loss_epoch=1.96e+6]Epoch 667/3000:  22%|██▏       | 667/3000 [04:49&lt;17:17,  2.25it/s, v_num=1, train_loss_step=2e+6, train_loss_epoch=1.96e+6]Epoch 667/3000:  22%|██▏       | 667/3000 [04:49&lt;17:17,  2.25it/s, v_num=1, train_loss_step=1.98e+6, train_loss_epoch=1.96e+6]Epoch 668/3000:  22%|██▏       | 667/3000 [04:49&lt;17:17,  2.25it/s, v_num=1, train_loss_step=1.98e+6, train_loss_epoch=1.96e+6]Epoch 668/3000:  22%|██▏       | 668/3000 [04:50&lt;17:56,  2.17it/s, v_num=1, train_loss_step=1.98e+6, train_loss_epoch=1.96e+6]Epoch 668/3000:  22%|██▏       | 668/3000 [04:50&lt;17:56,  2.17it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=1.96e+6]Epoch 669/3000:  22%|██▏       | 668/3000 [04:50&lt;17:56,  2.17it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=1.96e+6]Epoch 669/3000:  22%|██▏       | 669/3000 [04:50&lt;17:34,  2.21it/s, v_num=1, train_loss_step=1.99e+6, train_loss_epoch=1.96e+6]Epoch 669/3000:  22%|██▏       | 669/3000 [04:50&lt;17:34,  2.21it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=1.96e+6]Epoch 670/3000:  22%|██▏       | 669/3000 [04:50&lt;17:34,  2.21it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=1.96e+6]Epoch 670/3000:  22%|██▏       | 670/3000 [04:51&lt;18:23,  2.11it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=1.96e+6]Epoch 670/3000:  22%|██▏       | 670/3000 [04:51&lt;18:23,  2.11it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=1.95e+6]Epoch 671/3000:  22%|██▏       | 670/3000 [04:51&lt;18:23,  2.11it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=1.95e+6]Epoch 671/3000:  22%|██▏       | 671/3000 [04:51&lt;19:07,  2.03it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=1.95e+6]Epoch 671/3000:  22%|██▏       | 671/3000 [04:51&lt;19:07,  2.03it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.95e+6]Epoch 672/3000:  22%|██▏       | 671/3000 [04:51&lt;19:07,  2.03it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.95e+6]Epoch 672/3000:  22%|██▏       | 672/3000 [04:52&lt;21:12,  1.83it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.95e+6]Epoch 672/3000:  22%|██▏       | 672/3000 [04:52&lt;21:12,  1.83it/s, v_num=1, train_loss_step=1.86e+6, train_loss_epoch=1.95e+6]Epoch 673/3000:  22%|██▏       | 672/3000 [04:52&lt;21:12,  1.83it/s, v_num=1, train_loss_step=1.86e+6, train_loss_epoch=1.95e+6]Epoch 673/3000:  22%|██▏       | 673/3000 [04:53&lt;21:31,  1.80it/s, v_num=1, train_loss_step=1.86e+6, train_loss_epoch=1.95e+6]Epoch 673/3000:  22%|██▏       | 673/3000 [04:53&lt;21:31,  1.80it/s, v_num=1, train_loss_step=1.9e+6, train_loss_epoch=1.95e+6] Epoch 674/3000:  22%|██▏       | 673/3000 [04:53&lt;21:31,  1.80it/s, v_num=1, train_loss_step=1.9e+6, train_loss_epoch=1.95e+6]Epoch 674/3000:  22%|██▏       | 674/3000 [04:53&lt;20:35,  1.88it/s, v_num=1, train_loss_step=1.9e+6, train_loss_epoch=1.95e+6]Epoch 674/3000:  22%|██▏       | 674/3000 [04:53&lt;20:35,  1.88it/s, v_num=1, train_loss_step=2.04e+6, train_loss_epoch=1.95e+6]Epoch 675/3000:  22%|██▏       | 674/3000 [04:53&lt;20:35,  1.88it/s, v_num=1, train_loss_step=2.04e+6, train_loss_epoch=1.95e+6]Epoch 675/3000:  22%|██▎       | 675/3000 [04:54&lt;20:04,  1.93it/s, v_num=1, train_loss_step=2.04e+6, train_loss_epoch=1.95e+6]Epoch 675/3000:  22%|██▎       | 675/3000 [04:54&lt;20:04,  1.93it/s, v_num=1, train_loss_step=1.98e+6, train_loss_epoch=1.94e+6]Epoch 676/3000:  22%|██▎       | 675/3000 [04:54&lt;20:04,  1.93it/s, v_num=1, train_loss_step=1.98e+6, train_loss_epoch=1.94e+6]Epoch 676/3000:  23%|██▎       | 676/3000 [04:54&lt;18:14,  2.12it/s, v_num=1, train_loss_step=1.98e+6, train_loss_epoch=1.94e+6]Epoch 676/3000:  23%|██▎       | 676/3000 [04:54&lt;18:14,  2.12it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.94e+6]Epoch 677/3000:  23%|██▎       | 676/3000 [04:54&lt;18:14,  2.12it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.94e+6]Epoch 677/3000:  23%|██▎       | 677/3000 [04:54&lt;19:08,  2.02it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.94e+6]Epoch 677/3000:  23%|██▎       | 677/3000 [04:54&lt;19:08,  2.02it/s, v_num=1, train_loss_step=1.96e+6, train_loss_epoch=1.94e+6]Epoch 678/3000:  23%|██▎       | 677/3000 [04:54&lt;19:08,  2.02it/s, v_num=1, train_loss_step=1.96e+6, train_loss_epoch=1.94e+6]Epoch 678/3000:  23%|██▎       | 678/3000 [04:55&lt;19:58,  1.94it/s, v_num=1, train_loss_step=1.96e+6, train_loss_epoch=1.94e+6]Epoch 678/3000:  23%|██▎       | 678/3000 [04:55&lt;19:58,  1.94it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.94e+6]Epoch 679/3000:  23%|██▎       | 678/3000 [04:55&lt;19:58,  1.94it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.94e+6]Epoch 679/3000:  23%|██▎       | 679/3000 [04:56&lt;20:30,  1.89it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.94e+6]Epoch 679/3000:  23%|██▎       | 679/3000 [04:56&lt;20:30,  1.89it/s, v_num=1, train_loss_step=1.97e+6, train_loss_epoch=1.93e+6]Epoch 680/3000:  23%|██▎       | 679/3000 [04:56&lt;20:30,  1.89it/s, v_num=1, train_loss_step=1.97e+6, train_loss_epoch=1.93e+6]Epoch 680/3000:  23%|██▎       | 680/3000 [04:56&lt;21:36,  1.79it/s, v_num=1, train_loss_step=1.97e+6, train_loss_epoch=1.93e+6]Epoch 680/3000:  23%|██▎       | 680/3000 [04:56&lt;21:36,  1.79it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.93e+6]Epoch 681/3000:  23%|██▎       | 680/3000 [04:56&lt;21:36,  1.79it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.93e+6]Epoch 681/3000:  23%|██▎       | 681/3000 [04:57&lt;19:32,  1.98it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.93e+6]Epoch 681/3000:  23%|██▎       | 681/3000 [04:57&lt;19:32,  1.98it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.93e+6]Epoch 682/3000:  23%|██▎       | 681/3000 [04:57&lt;19:32,  1.98it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.93e+6]Epoch 682/3000:  23%|██▎       | 682/3000 [04:57&lt;20:59,  1.84it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.93e+6]Epoch 682/3000:  23%|██▎       | 682/3000 [04:57&lt;20:59,  1.84it/s, v_num=1, train_loss_step=1.84e+6, train_loss_epoch=1.93e+6]Epoch 683/3000:  23%|██▎       | 682/3000 [04:57&lt;20:59,  1.84it/s, v_num=1, train_loss_step=1.84e+6, train_loss_epoch=1.93e+6]Epoch 683/3000:  23%|██▎       | 683/3000 [04:58&lt;19:32,  1.98it/s, v_num=1, train_loss_step=1.84e+6, train_loss_epoch=1.93e+6]Epoch 683/3000:  23%|██▎       | 683/3000 [04:58&lt;19:32,  1.98it/s, v_num=1, train_loss_step=1.96e+6, train_loss_epoch=1.93e+6]Epoch 684/3000:  23%|██▎       | 683/3000 [04:58&lt;19:32,  1.98it/s, v_num=1, train_loss_step=1.96e+6, train_loss_epoch=1.93e+6]Epoch 684/3000:  23%|██▎       | 684/3000 [04:58&lt;16:59,  2.27it/s, v_num=1, train_loss_step=1.96e+6, train_loss_epoch=1.93e+6]Epoch 684/3000:  23%|██▎       | 684/3000 [04:58&lt;16:59,  2.27it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.92e+6]Epoch 685/3000:  23%|██▎       | 684/3000 [04:58&lt;16:59,  2.27it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.92e+6]Epoch 685/3000:  23%|██▎       | 685/3000 [04:58&lt;14:18,  2.70it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.92e+6]Epoch 685/3000:  23%|██▎       | 685/3000 [04:58&lt;14:18,  2.70it/s, v_num=1, train_loss_step=1.9e+6, train_loss_epoch=1.92e+6] Epoch 686/3000:  23%|██▎       | 685/3000 [04:58&lt;14:18,  2.70it/s, v_num=1, train_loss_step=1.9e+6, train_loss_epoch=1.92e+6]Epoch 686/3000:  23%|██▎       | 686/3000 [04:59&lt;14:23,  2.68it/s, v_num=1, train_loss_step=1.9e+6, train_loss_epoch=1.92e+6]Epoch 686/3000:  23%|██▎       | 686/3000 [04:59&lt;14:23,  2.68it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.92e+6]Epoch 687/3000:  23%|██▎       | 686/3000 [04:59&lt;14:23,  2.68it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.92e+6]Epoch 687/3000:  23%|██▎       | 687/3000 [04:59&lt;16:20,  2.36it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.92e+6]Epoch 687/3000:  23%|██▎       | 687/3000 [04:59&lt;16:20,  2.36it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.92e+6]Epoch 688/3000:  23%|██▎       | 687/3000 [04:59&lt;16:20,  2.36it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.92e+6]Epoch 688/3000:  23%|██▎       | 688/3000 [04:59&lt;15:28,  2.49it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.92e+6]Epoch 688/3000:  23%|██▎       | 688/3000 [04:59&lt;15:28,  2.49it/s, v_num=1, train_loss_step=1.95e+6, train_loss_epoch=1.92e+6]Epoch 689/3000:  23%|██▎       | 688/3000 [04:59&lt;15:28,  2.49it/s, v_num=1, train_loss_step=1.95e+6, train_loss_epoch=1.92e+6]Epoch 689/3000:  23%|██▎       | 689/3000 [05:00&lt;17:14,  2.23it/s, v_num=1, train_loss_step=1.95e+6, train_loss_epoch=1.92e+6]Epoch 689/3000:  23%|██▎       | 689/3000 [05:00&lt;17:14,  2.23it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.91e+6]Epoch 690/3000:  23%|██▎       | 689/3000 [05:00&lt;17:14,  2.23it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.91e+6]Epoch 690/3000:  23%|██▎       | 690/3000 [05:00&lt;16:51,  2.28it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.91e+6]Epoch 690/3000:  23%|██▎       | 690/3000 [05:00&lt;16:51,  2.28it/s, v_num=1, train_loss_step=1.92e+6, train_loss_epoch=1.91e+6]Epoch 691/3000:  23%|██▎       | 690/3000 [05:00&lt;16:51,  2.28it/s, v_num=1, train_loss_step=1.92e+6, train_loss_epoch=1.91e+6]Epoch 691/3000:  23%|██▎       | 691/3000 [05:01&lt;16:30,  2.33it/s, v_num=1, train_loss_step=1.92e+6, train_loss_epoch=1.91e+6]Epoch 691/3000:  23%|██▎       | 691/3000 [05:01&lt;16:30,  2.33it/s, v_num=1, train_loss_step=1.97e+6, train_loss_epoch=1.91e+6]Epoch 692/3000:  23%|██▎       | 691/3000 [05:01&lt;16:30,  2.33it/s, v_num=1, train_loss_step=1.97e+6, train_loss_epoch=1.91e+6]Epoch 692/3000:  23%|██▎       | 692/3000 [05:01&lt;17:53,  2.15it/s, v_num=1, train_loss_step=1.97e+6, train_loss_epoch=1.91e+6]Epoch 692/3000:  23%|██▎       | 692/3000 [05:01&lt;17:53,  2.15it/s, v_num=1, train_loss_step=1.87e+6, train_loss_epoch=1.91e+6]Epoch 693/3000:  23%|██▎       | 692/3000 [05:01&lt;17:53,  2.15it/s, v_num=1, train_loss_step=1.87e+6, train_loss_epoch=1.91e+6]Epoch 693/3000:  23%|██▎       | 693/3000 [05:02&lt;17:38,  2.18it/s, v_num=1, train_loss_step=1.87e+6, train_loss_epoch=1.91e+6]Epoch 693/3000:  23%|██▎       | 693/3000 [05:02&lt;17:38,  2.18it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.91e+6]Epoch 694/3000:  23%|██▎       | 693/3000 [05:02&lt;17:38,  2.18it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.91e+6]Epoch 694/3000:  23%|██▎       | 694/3000 [05:02&lt;19:31,  1.97it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.91e+6]Epoch 694/3000:  23%|██▎       | 694/3000 [05:02&lt;19:31,  1.97it/s, v_num=1, train_loss_step=2.08e+6, train_loss_epoch=1.9e+6] Epoch 695/3000:  23%|██▎       | 694/3000 [05:02&lt;19:31,  1.97it/s, v_num=1, train_loss_step=2.08e+6, train_loss_epoch=1.9e+6]Epoch 695/3000:  23%|██▎       | 695/3000 [05:03&lt;20:04,  1.91it/s, v_num=1, train_loss_step=2.08e+6, train_loss_epoch=1.9e+6]Epoch 695/3000:  23%|██▎       | 695/3000 [05:03&lt;20:04,  1.91it/s, v_num=1, train_loss_step=1.87e+6, train_loss_epoch=1.9e+6]Epoch 696/3000:  23%|██▎       | 695/3000 [05:03&lt;20:04,  1.91it/s, v_num=1, train_loss_step=1.87e+6, train_loss_epoch=1.9e+6]Epoch 696/3000:  23%|██▎       | 696/3000 [05:04&lt;20:23,  1.88it/s, v_num=1, train_loss_step=1.87e+6, train_loss_epoch=1.9e+6]Epoch 696/3000:  23%|██▎       | 696/3000 [05:04&lt;20:23,  1.88it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=1.9e+6]Epoch 697/3000:  23%|██▎       | 696/3000 [05:04&lt;20:23,  1.88it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=1.9e+6]Epoch 697/3000:  23%|██▎       | 697/3000 [05:04&lt;20:12,  1.90it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=1.9e+6]Epoch 697/3000:  23%|██▎       | 697/3000 [05:04&lt;20:12,  1.90it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=1.9e+6]Epoch 698/3000:  23%|██▎       | 697/3000 [05:04&lt;20:12,  1.90it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=1.9e+6]Epoch 698/3000:  23%|██▎       | 698/3000 [05:04&lt;19:18,  1.99it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=1.9e+6]Epoch 698/3000:  23%|██▎       | 698/3000 [05:04&lt;19:18,  1.99it/s, v_num=1, train_loss_step=1.86e+6, train_loss_epoch=1.9e+6]Epoch 699/3000:  23%|██▎       | 698/3000 [05:04&lt;19:18,  1.99it/s, v_num=1, train_loss_step=1.86e+6, train_loss_epoch=1.9e+6]Epoch 699/3000:  23%|██▎       | 699/3000 [05:05&lt;18:52,  2.03it/s, v_num=1, train_loss_step=1.86e+6, train_loss_epoch=1.9e+6]Epoch 699/3000:  23%|██▎       | 699/3000 [05:05&lt;18:52,  2.03it/s, v_num=1, train_loss_step=1.87e+6, train_loss_epoch=1.89e+6]Epoch 700/3000:  23%|██▎       | 699/3000 [05:05&lt;18:52,  2.03it/s, v_num=1, train_loss_step=1.87e+6, train_loss_epoch=1.89e+6]Epoch 700/3000:  23%|██▎       | 700/3000 [05:06&lt;20:12,  1.90it/s, v_num=1, train_loss_step=1.87e+6, train_loss_epoch=1.89e+6]Epoch 700/3000:  23%|██▎       | 700/3000 [05:06&lt;20:12,  1.90it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.89e+6]Epoch 701/3000:  23%|██▎       | 700/3000 [05:06&lt;20:12,  1.90it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.89e+6]Epoch 701/3000:  23%|██▎       | 701/3000 [05:06&lt;19:13,  1.99it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.89e+6]Epoch 701/3000:  23%|██▎       | 701/3000 [05:06&lt;19:13,  1.99it/s, v_num=1, train_loss_step=1.92e+6, train_loss_epoch=1.89e+6]Epoch 702/3000:  23%|██▎       | 701/3000 [05:06&lt;19:13,  1.99it/s, v_num=1, train_loss_step=1.92e+6, train_loss_epoch=1.89e+6]Epoch 702/3000:  23%|██▎       | 702/3000 [05:06&lt;18:50,  2.03it/s, v_num=1, train_loss_step=1.92e+6, train_loss_epoch=1.89e+6]Epoch 702/3000:  23%|██▎       | 702/3000 [05:06&lt;18:50,  2.03it/s, v_num=1, train_loss_step=1.9e+6, train_loss_epoch=1.89e+6] Epoch 703/3000:  23%|██▎       | 702/3000 [05:06&lt;18:50,  2.03it/s, v_num=1, train_loss_step=1.9e+6, train_loss_epoch=1.89e+6]Epoch 703/3000:  23%|██▎       | 703/3000 [05:07&lt;19:02,  2.01it/s, v_num=1, train_loss_step=1.9e+6, train_loss_epoch=1.89e+6]Epoch 703/3000:  23%|██▎       | 703/3000 [05:07&lt;19:02,  2.01it/s, v_num=1, train_loss_step=1.89e+6, train_loss_epoch=1.89e+6]Epoch 704/3000:  23%|██▎       | 703/3000 [05:07&lt;19:02,  2.01it/s, v_num=1, train_loss_step=1.89e+6, train_loss_epoch=1.89e+6]Epoch 704/3000:  23%|██▎       | 704/3000 [05:08&lt;20:30,  1.87it/s, v_num=1, train_loss_step=1.89e+6, train_loss_epoch=1.89e+6]Epoch 704/3000:  23%|██▎       | 704/3000 [05:08&lt;20:30,  1.87it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=1.88e+6]Epoch 705/3000:  23%|██▎       | 704/3000 [05:08&lt;20:30,  1.87it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=1.88e+6]Epoch 705/3000:  24%|██▎       | 705/3000 [05:08&lt;21:39,  1.77it/s, v_num=1, train_loss_step=2.01e+6, train_loss_epoch=1.88e+6]Epoch 705/3000:  24%|██▎       | 705/3000 [05:08&lt;21:39,  1.77it/s, v_num=1, train_loss_step=1.86e+6, train_loss_epoch=1.88e+6]Epoch 706/3000:  24%|██▎       | 705/3000 [05:08&lt;21:39,  1.77it/s, v_num=1, train_loss_step=1.86e+6, train_loss_epoch=1.88e+6]Epoch 706/3000:  24%|██▎       | 706/3000 [05:09&lt;21:44,  1.76it/s, v_num=1, train_loss_step=1.86e+6, train_loss_epoch=1.88e+6]Epoch 706/3000:  24%|██▎       | 706/3000 [05:09&lt;21:44,  1.76it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.88e+6]Epoch 707/3000:  24%|██▎       | 706/3000 [05:09&lt;21:44,  1.76it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.88e+6]Epoch 707/3000:  24%|██▎       | 707/3000 [05:09&lt;20:38,  1.85it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.88e+6]Epoch 707/3000:  24%|██▎       | 707/3000 [05:09&lt;20:38,  1.85it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.88e+6]Epoch 708/3000:  24%|██▎       | 707/3000 [05:09&lt;20:38,  1.85it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.88e+6]Epoch 708/3000:  24%|██▎       | 708/3000 [05:10&lt;20:30,  1.86it/s, v_num=1, train_loss_step=1.94e+6, train_loss_epoch=1.88e+6]Epoch 708/3000:  24%|██▎       | 708/3000 [05:10&lt;20:30,  1.86it/s, v_num=1, train_loss_step=1.87e+6, train_loss_epoch=1.88e+6]Epoch 709/3000:  24%|██▎       | 708/3000 [05:10&lt;20:30,  1.86it/s, v_num=1, train_loss_step=1.87e+6, train_loss_epoch=1.88e+6]Epoch 709/3000:  24%|██▎       | 709/3000 [05:10&lt;19:57,  1.91it/s, v_num=1, train_loss_step=1.87e+6, train_loss_epoch=1.88e+6]Epoch 709/3000:  24%|██▎       | 709/3000 [05:10&lt;19:57,  1.91it/s, v_num=1, train_loss_step=1.81e+6, train_loss_epoch=1.87e+6]Epoch 710/3000:  24%|██▎       | 709/3000 [05:10&lt;19:57,  1.91it/s, v_num=1, train_loss_step=1.81e+6, train_loss_epoch=1.87e+6]Epoch 710/3000:  24%|██▎       | 710/3000 [05:10&lt;15:10,  2.51it/s, v_num=1, train_loss_step=1.81e+6, train_loss_epoch=1.87e+6]Epoch 710/3000:  24%|██▎       | 710/3000 [05:10&lt;15:10,  2.51it/s, v_num=1, train_loss_step=1.96e+6, train_loss_epoch=1.87e+6]Epoch 711/3000:  24%|██▎       | 710/3000 [05:10&lt;15:10,  2.51it/s, v_num=1, train_loss_step=1.96e+6, train_loss_epoch=1.87e+6]Epoch 711/3000:  24%|██▎       | 711/3000 [05:11&lt;15:55,  2.39it/s, v_num=1, train_loss_step=1.96e+6, train_loss_epoch=1.87e+6]Epoch 711/3000:  24%|██▎       | 711/3000 [05:11&lt;15:55,  2.39it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.87e+6]Epoch 712/3000:  24%|██▎       | 711/3000 [05:11&lt;15:55,  2.39it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.87e+6]Epoch 712/3000:  24%|██▎       | 712/3000 [05:11&lt;16:19,  2.34it/s, v_num=1, train_loss_step=1.91e+6, train_loss_epoch=1.87e+6]Epoch 712/3000:  24%|██▎       | 712/3000 [05:11&lt;16:19,  2.34it/s, v_num=1, train_loss_step=1.81e+6, train_loss_epoch=1.87e+6]Epoch 713/3000:  24%|██▎       | 712/3000 [05:11&lt;16:19,  2.34it/s, v_num=1, train_loss_step=1.81e+6, train_loss_epoch=1.87e+6]Epoch 713/3000:  24%|██▍       | 713/3000 [05:12&lt;17:47,  2.14it/s, v_num=1, train_loss_step=1.81e+6, train_loss_epoch=1.87e+6]Epoch 713/3000:  24%|██▍       | 713/3000 [05:12&lt;17:47,  2.14it/s, v_num=1, train_loss_step=1.87e+6, train_loss_epoch=1.87e+6]Epoch 714/3000:  24%|██▍       | 713/3000 [05:12&lt;17:47,  2.14it/s, v_num=1, train_loss_step=1.87e+6, train_loss_epoch=1.87e+6]Epoch 714/3000:  24%|██▍       | 714/3000 [05:12&lt;17:01,  2.24it/s, v_num=1, train_loss_step=1.87e+6, train_loss_epoch=1.87e+6]Epoch 714/3000:  24%|██▍       | 714/3000 [05:12&lt;17:01,  2.24it/s, v_num=1, train_loss_step=1.86e+6, train_loss_epoch=1.86e+6]Epoch 715/3000:  24%|██▍       | 714/3000 [05:12&lt;17:01,  2.24it/s, v_num=1, train_loss_step=1.86e+6, train_loss_epoch=1.86e+6]Epoch 715/3000:  24%|██▍       | 715/3000 [05:13&lt;18:33,  2.05it/s, v_num=1, train_loss_step=1.86e+6, train_loss_epoch=1.86e+6]Epoch 715/3000:  24%|██▍       | 715/3000 [05:13&lt;18:33,  2.05it/s, v_num=1, train_loss_step=1.88e+6, train_loss_epoch=1.86e+6]Epoch 716/3000:  24%|██▍       | 715/3000 [05:13&lt;18:33,  2.05it/s, v_num=1, train_loss_step=1.88e+6, train_loss_epoch=1.86e+6]Epoch 716/3000:  24%|██▍       | 716/3000 [05:13&lt;16:52,  2.26it/s, v_num=1, train_loss_step=1.88e+6, train_loss_epoch=1.86e+6]Epoch 716/3000:  24%|██▍       | 716/3000 [05:13&lt;16:52,  2.26it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.86e+6]Epoch 717/3000:  24%|██▍       | 716/3000 [05:13&lt;16:52,  2.26it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.86e+6]Epoch 717/3000:  24%|██▍       | 717/3000 [05:14&lt;18:08,  2.10it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.86e+6]Epoch 717/3000:  24%|██▍       | 717/3000 [05:14&lt;18:08,  2.10it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.86e+6]Epoch 718/3000:  24%|██▍       | 717/3000 [05:14&lt;18:08,  2.10it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.86e+6]Epoch 718/3000:  24%|██▍       | 718/3000 [05:14&lt;17:36,  2.16it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.86e+6]Epoch 718/3000:  24%|██▍       | 718/3000 [05:14&lt;17:36,  2.16it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.86e+6]Epoch 719/3000:  24%|██▍       | 718/3000 [05:14&lt;17:36,  2.16it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.86e+6]Epoch 719/3000:  24%|██▍       | 719/3000 [05:15&lt;16:48,  2.26it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.86e+6]Epoch 719/3000:  24%|██▍       | 719/3000 [05:15&lt;16:48,  2.26it/s, v_num=1, train_loss_step=1.9e+6, train_loss_epoch=1.85e+6] Epoch 720/3000:  24%|██▍       | 719/3000 [05:15&lt;16:48,  2.26it/s, v_num=1, train_loss_step=1.9e+6, train_loss_epoch=1.85e+6]Epoch 720/3000:  24%|██▍       | 720/3000 [05:15&lt;16:43,  2.27it/s, v_num=1, train_loss_step=1.9e+6, train_loss_epoch=1.85e+6]Epoch 720/3000:  24%|██▍       | 720/3000 [05:15&lt;16:43,  2.27it/s, v_num=1, train_loss_step=1.97e+6, train_loss_epoch=1.85e+6]Epoch 721/3000:  24%|██▍       | 720/3000 [05:15&lt;16:43,  2.27it/s, v_num=1, train_loss_step=1.97e+6, train_loss_epoch=1.85e+6]Epoch 721/3000:  24%|██▍       | 721/3000 [05:16&lt;17:40,  2.15it/s, v_num=1, train_loss_step=1.97e+6, train_loss_epoch=1.85e+6]Epoch 721/3000:  24%|██▍       | 721/3000 [05:16&lt;17:40,  2.15it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.85e+6]Epoch 722/3000:  24%|██▍       | 721/3000 [05:16&lt;17:40,  2.15it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.85e+6]Epoch 722/3000:  24%|██▍       | 722/3000 [05:16&lt;18:01,  2.11it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.85e+6]Epoch 722/3000:  24%|██▍       | 722/3000 [05:16&lt;18:01,  2.11it/s, v_num=1, train_loss_step=1.82e+6, train_loss_epoch=1.85e+6]Epoch 723/3000:  24%|██▍       | 722/3000 [05:16&lt;18:01,  2.11it/s, v_num=1, train_loss_step=1.82e+6, train_loss_epoch=1.85e+6]Epoch 723/3000:  24%|██▍       | 723/3000 [05:17&lt;17:59,  2.11it/s, v_num=1, train_loss_step=1.82e+6, train_loss_epoch=1.85e+6]Epoch 723/3000:  24%|██▍       | 723/3000 [05:17&lt;17:59,  2.11it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=1.85e+6]Epoch 724/3000:  24%|██▍       | 723/3000 [05:17&lt;17:59,  2.11it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=1.85e+6]Epoch 724/3000:  24%|██▍       | 724/3000 [05:17&lt;17:06,  2.22it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=1.85e+6]Epoch 724/3000:  24%|██▍       | 724/3000 [05:17&lt;17:06,  2.22it/s, v_num=1, train_loss_step=1.81e+6, train_loss_epoch=1.85e+6]Epoch 725/3000:  24%|██▍       | 724/3000 [05:17&lt;17:06,  2.22it/s, v_num=1, train_loss_step=1.81e+6, train_loss_epoch=1.85e+6]Epoch 725/3000:  24%|██▍       | 725/3000 [05:17&lt;17:25,  2.18it/s, v_num=1, train_loss_step=1.81e+6, train_loss_epoch=1.85e+6]Epoch 725/3000:  24%|██▍       | 725/3000 [05:17&lt;17:25,  2.18it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.84e+6]Epoch 726/3000:  24%|██▍       | 725/3000 [05:17&lt;17:25,  2.18it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.84e+6]Epoch 726/3000:  24%|██▍       | 726/3000 [05:18&lt;19:09,  1.98it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.84e+6]Epoch 726/3000:  24%|██▍       | 726/3000 [05:18&lt;19:09,  1.98it/s, v_num=1, train_loss_step=1.84e+6, train_loss_epoch=1.84e+6]Epoch 727/3000:  24%|██▍       | 726/3000 [05:18&lt;19:09,  1.98it/s, v_num=1, train_loss_step=1.84e+6, train_loss_epoch=1.84e+6]Epoch 727/3000:  24%|██▍       | 727/3000 [05:19&lt;19:24,  1.95it/s, v_num=1, train_loss_step=1.84e+6, train_loss_epoch=1.84e+6]Epoch 727/3000:  24%|██▍       | 727/3000 [05:19&lt;19:24,  1.95it/s, v_num=1, train_loss_step=1.82e+6, train_loss_epoch=1.84e+6]Epoch 728/3000:  24%|██▍       | 727/3000 [05:19&lt;19:24,  1.95it/s, v_num=1, train_loss_step=1.82e+6, train_loss_epoch=1.84e+6]Epoch 728/3000:  24%|██▍       | 728/3000 [05:19&lt;20:14,  1.87it/s, v_num=1, train_loss_step=1.82e+6, train_loss_epoch=1.84e+6]Epoch 728/3000:  24%|██▍       | 728/3000 [05:19&lt;20:14,  1.87it/s, v_num=1, train_loss_step=1.88e+6, train_loss_epoch=1.84e+6]Epoch 729/3000:  24%|██▍       | 728/3000 [05:19&lt;20:14,  1.87it/s, v_num=1, train_loss_step=1.88e+6, train_loss_epoch=1.84e+6]Epoch 729/3000:  24%|██▍       | 729/3000 [05:19&lt;18:14,  2.08it/s, v_num=1, train_loss_step=1.88e+6, train_loss_epoch=1.84e+6]Epoch 729/3000:  24%|██▍       | 729/3000 [05:19&lt;18:14,  2.08it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.84e+6]Epoch 730/3000:  24%|██▍       | 729/3000 [05:19&lt;18:14,  2.08it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.84e+6]Epoch 730/3000:  24%|██▍       | 730/3000 [05:20&lt;16:38,  2.27it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.84e+6]Epoch 730/3000:  24%|██▍       | 730/3000 [05:20&lt;16:38,  2.27it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.83e+6]Epoch 731/3000:  24%|██▍       | 730/3000 [05:20&lt;16:38,  2.27it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.83e+6]Epoch 731/3000:  24%|██▍       | 731/3000 [05:20&lt;18:13,  2.07it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.83e+6]Epoch 731/3000:  24%|██▍       | 731/3000 [05:20&lt;18:13,  2.07it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.83e+6]Epoch 732/3000:  24%|██▍       | 731/3000 [05:20&lt;18:13,  2.07it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.83e+6]Epoch 732/3000:  24%|██▍       | 732/3000 [05:21&lt;16:55,  2.23it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.83e+6]Epoch 732/3000:  24%|██▍       | 732/3000 [05:21&lt;16:55,  2.23it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=1.83e+6]Epoch 733/3000:  24%|██▍       | 732/3000 [05:21&lt;16:55,  2.23it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=1.83e+6]Epoch 733/3000:  24%|██▍       | 733/3000 [05:21&lt;14:39,  2.58it/s, v_num=1, train_loss_step=1.93e+6, train_loss_epoch=1.83e+6]Epoch 733/3000:  24%|██▍       | 733/3000 [05:21&lt;14:39,  2.58it/s, v_num=1, train_loss_step=1.82e+6, train_loss_epoch=1.83e+6]Epoch 734/3000:  24%|██▍       | 733/3000 [05:21&lt;14:39,  2.58it/s, v_num=1, train_loss_step=1.82e+6, train_loss_epoch=1.83e+6]Epoch 734/3000:  24%|██▍       | 734/3000 [05:21&lt;13:29,  2.80it/s, v_num=1, train_loss_step=1.82e+6, train_loss_epoch=1.83e+6]Epoch 734/3000:  24%|██▍       | 734/3000 [05:21&lt;13:29,  2.80it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.83e+6]Epoch 735/3000:  24%|██▍       | 734/3000 [05:21&lt;13:29,  2.80it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.83e+6]Epoch 735/3000:  24%|██▍       | 735/3000 [05:22&lt;13:01,  2.90it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.83e+6]Epoch 735/3000:  24%|██▍       | 735/3000 [05:22&lt;13:01,  2.90it/s, v_num=1, train_loss_step=1.8e+6, train_loss_epoch=1.83e+6] Epoch 736/3000:  24%|██▍       | 735/3000 [05:22&lt;13:01,  2.90it/s, v_num=1, train_loss_step=1.8e+6, train_loss_epoch=1.83e+6]Epoch 736/3000:  25%|██▍       | 736/3000 [05:22&lt;13:36,  2.77it/s, v_num=1, train_loss_step=1.8e+6, train_loss_epoch=1.83e+6]Epoch 736/3000:  25%|██▍       | 736/3000 [05:22&lt;13:36,  2.77it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.82e+6]Epoch 737/3000:  25%|██▍       | 736/3000 [05:22&lt;13:36,  2.77it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.82e+6]Epoch 737/3000:  25%|██▍       | 737/3000 [05:22&lt;14:33,  2.59it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.82e+6]Epoch 737/3000:  25%|██▍       | 737/3000 [05:22&lt;14:33,  2.59it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.82e+6]Epoch 738/3000:  25%|██▍       | 737/3000 [05:22&lt;14:33,  2.59it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.82e+6]Epoch 738/3000:  25%|██▍       | 738/3000 [05:23&lt;12:27,  3.03it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.82e+6]Epoch 738/3000:  25%|██▍       | 738/3000 [05:23&lt;12:27,  3.03it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.82e+6]Epoch 739/3000:  25%|██▍       | 738/3000 [05:23&lt;12:27,  3.03it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.82e+6]Epoch 739/3000:  25%|██▍       | 739/3000 [05:23&lt;10:59,  3.43it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.82e+6]Epoch 739/3000:  25%|██▍       | 739/3000 [05:23&lt;10:59,  3.43it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.82e+6]Epoch 740/3000:  25%|██▍       | 739/3000 [05:23&lt;10:59,  3.43it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.82e+6]Epoch 740/3000:  25%|██▍       | 740/3000 [05:23&lt;13:08,  2.87it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.82e+6]Epoch 740/3000:  25%|██▍       | 740/3000 [05:23&lt;13:08,  2.87it/s, v_num=1, train_loss_step=1.84e+6, train_loss_epoch=1.82e+6]Epoch 741/3000:  25%|██▍       | 740/3000 [05:23&lt;13:08,  2.87it/s, v_num=1, train_loss_step=1.84e+6, train_loss_epoch=1.82e+6]Epoch 741/3000:  25%|██▍       | 741/3000 [05:24&lt;15:11,  2.48it/s, v_num=1, train_loss_step=1.84e+6, train_loss_epoch=1.82e+6]Epoch 741/3000:  25%|██▍       | 741/3000 [05:24&lt;15:11,  2.48it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.81e+6]Epoch 742/3000:  25%|██▍       | 741/3000 [05:24&lt;15:11,  2.48it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.81e+6]Epoch 742/3000:  25%|██▍       | 742/3000 [05:25&lt;18:19,  2.05it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.81e+6]Epoch 742/3000:  25%|██▍       | 742/3000 [05:25&lt;18:19,  2.05it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.81e+6]Epoch 743/3000:  25%|██▍       | 742/3000 [05:25&lt;18:19,  2.05it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.81e+6]Epoch 743/3000:  25%|██▍       | 743/3000 [05:25&lt;19:39,  1.91it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.81e+6]Epoch 743/3000:  25%|██▍       | 743/3000 [05:25&lt;19:39,  1.91it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.81e+6]Epoch 744/3000:  25%|██▍       | 743/3000 [05:25&lt;19:39,  1.91it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.81e+6]Epoch 744/3000:  25%|██▍       | 744/3000 [05:26&lt;19:25,  1.94it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.81e+6]Epoch 744/3000:  25%|██▍       | 744/3000 [05:26&lt;19:25,  1.94it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.81e+6]Epoch 745/3000:  25%|██▍       | 744/3000 [05:26&lt;19:25,  1.94it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.81e+6]Epoch 745/3000:  25%|██▍       | 745/3000 [05:26&lt;19:18,  1.95it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.81e+6]Epoch 745/3000:  25%|██▍       | 745/3000 [05:26&lt;19:18,  1.95it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.81e+6]Epoch 746/3000:  25%|██▍       | 745/3000 [05:26&lt;19:18,  1.95it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.81e+6]Epoch 746/3000:  25%|██▍       | 746/3000 [05:27&lt;18:58,  1.98it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.81e+6]Epoch 746/3000:  25%|██▍       | 746/3000 [05:27&lt;18:58,  1.98it/s, v_num=1, train_loss_step=1.82e+6, train_loss_epoch=1.81e+6]Epoch 747/3000:  25%|██▍       | 746/3000 [05:27&lt;18:58,  1.98it/s, v_num=1, train_loss_step=1.82e+6, train_loss_epoch=1.81e+6]Epoch 747/3000:  25%|██▍       | 747/3000 [05:27&lt;16:19,  2.30it/s, v_num=1, train_loss_step=1.82e+6, train_loss_epoch=1.81e+6]Epoch 747/3000:  25%|██▍       | 747/3000 [05:27&lt;16:19,  2.30it/s, v_num=1, train_loss_step=1.73e+6, train_loss_epoch=1.8e+6] Epoch 748/3000:  25%|██▍       | 747/3000 [05:27&lt;16:19,  2.30it/s, v_num=1, train_loss_step=1.73e+6, train_loss_epoch=1.8e+6]Epoch 748/3000:  25%|██▍       | 748/3000 [05:27&lt;14:42,  2.55it/s, v_num=1, train_loss_step=1.73e+6, train_loss_epoch=1.8e+6]Epoch 748/3000:  25%|██▍       | 748/3000 [05:27&lt;14:42,  2.55it/s, v_num=1, train_loss_step=1.84e+6, train_loss_epoch=1.8e+6]Epoch 749/3000:  25%|██▍       | 748/3000 [05:27&lt;14:42,  2.55it/s, v_num=1, train_loss_step=1.84e+6, train_loss_epoch=1.8e+6]Epoch 749/3000:  25%|██▍       | 749/3000 [05:28&lt;14:08,  2.65it/s, v_num=1, train_loss_step=1.84e+6, train_loss_epoch=1.8e+6]Epoch 749/3000:  25%|██▍       | 749/3000 [05:28&lt;14:08,  2.65it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.8e+6]Epoch 750/3000:  25%|██▍       | 749/3000 [05:28&lt;14:08,  2.65it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.8e+6]Epoch 750/3000:  25%|██▌       | 750/3000 [05:28&lt;15:43,  2.38it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.8e+6]Epoch 750/3000:  25%|██▌       | 750/3000 [05:28&lt;15:43,  2.38it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.8e+6]Epoch 751/3000:  25%|██▌       | 750/3000 [05:28&lt;15:43,  2.38it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.8e+6]Epoch 751/3000:  25%|██▌       | 751/3000 [05:29&lt;16:14,  2.31it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.8e+6]Epoch 751/3000:  25%|██▌       | 751/3000 [05:29&lt;16:14,  2.31it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.8e+6]Epoch 752/3000:  25%|██▌       | 751/3000 [05:29&lt;16:14,  2.31it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.8e+6]Epoch 752/3000:  25%|██▌       | 752/3000 [05:29&lt;16:16,  2.30it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.8e+6]Epoch 752/3000:  25%|██▌       | 752/3000 [05:29&lt;16:16,  2.30it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.8e+6]Epoch 753/3000:  25%|██▌       | 752/3000 [05:29&lt;16:16,  2.30it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.8e+6]Epoch 753/3000:  25%|██▌       | 753/3000 [05:29&lt;16:41,  2.24it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.8e+6]Epoch 753/3000:  25%|██▌       | 753/3000 [05:29&lt;16:41,  2.24it/s, v_num=1, train_loss_step=1.8e+6, train_loss_epoch=1.79e+6]Epoch 754/3000:  25%|██▌       | 753/3000 [05:29&lt;16:41,  2.24it/s, v_num=1, train_loss_step=1.8e+6, train_loss_epoch=1.79e+6]Epoch 754/3000:  25%|██▌       | 754/3000 [05:30&lt;19:13,  1.95it/s, v_num=1, train_loss_step=1.8e+6, train_loss_epoch=1.79e+6]Epoch 754/3000:  25%|██▌       | 754/3000 [05:30&lt;19:13,  1.95it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.79e+6]Epoch 755/3000:  25%|██▌       | 754/3000 [05:30&lt;19:13,  1.95it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.79e+6]Epoch 755/3000:  25%|██▌       | 755/3000 [05:30&lt;17:08,  2.18it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.79e+6]Epoch 755/3000:  25%|██▌       | 755/3000 [05:30&lt;17:08,  2.18it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.79e+6]Epoch 756/3000:  25%|██▌       | 755/3000 [05:30&lt;17:08,  2.18it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.79e+6]Epoch 756/3000:  25%|██▌       | 756/3000 [05:31&lt;17:46,  2.10it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.79e+6]Epoch 756/3000:  25%|██▌       | 756/3000 [05:31&lt;17:46,  2.10it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.79e+6]Epoch 757/3000:  25%|██▌       | 756/3000 [05:31&lt;17:46,  2.10it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.79e+6]Epoch 757/3000:  25%|██▌       | 757/3000 [05:31&lt;17:42,  2.11it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.79e+6]Epoch 757/3000:  25%|██▌       | 757/3000 [05:31&lt;17:42,  2.11it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.79e+6]Epoch 758/3000:  25%|██▌       | 757/3000 [05:31&lt;17:42,  2.11it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.79e+6]Epoch 758/3000:  25%|██▌       | 758/3000 [05:32&lt;16:31,  2.26it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.79e+6]Epoch 758/3000:  25%|██▌       | 758/3000 [05:32&lt;16:31,  2.26it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.79e+6]Epoch 759/3000:  25%|██▌       | 758/3000 [05:32&lt;16:31,  2.26it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.79e+6]Epoch 759/3000:  25%|██▌       | 759/3000 [05:32&lt;16:01,  2.33it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.79e+6]Epoch 759/3000:  25%|██▌       | 759/3000 [05:32&lt;16:01,  2.33it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.78e+6]Epoch 760/3000:  25%|██▌       | 759/3000 [05:32&lt;16:01,  2.33it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.78e+6]Epoch 760/3000:  25%|██▌       | 760/3000 [05:33&lt;15:14,  2.45it/s, v_num=1, train_loss_step=1.83e+6, train_loss_epoch=1.78e+6]Epoch 760/3000:  25%|██▌       | 760/3000 [05:33&lt;15:14,  2.45it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.78e+6]Epoch 761/3000:  25%|██▌       | 760/3000 [05:33&lt;15:14,  2.45it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.78e+6]Epoch 761/3000:  25%|██▌       | 761/3000 [05:33&lt;14:03,  2.65it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.78e+6]Epoch 761/3000:  25%|██▌       | 761/3000 [05:33&lt;14:03,  2.65it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.78e+6]Epoch 762/3000:  25%|██▌       | 761/3000 [05:33&lt;14:03,  2.65it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.78e+6]Epoch 762/3000:  25%|██▌       | 762/3000 [05:33&lt;13:26,  2.78it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.78e+6]Epoch 762/3000:  25%|██▌       | 762/3000 [05:33&lt;13:26,  2.78it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.78e+6]Epoch 763/3000:  25%|██▌       | 762/3000 [05:33&lt;13:26,  2.78it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.78e+6]Epoch 763/3000:  25%|██▌       | 763/3000 [05:34&lt;13:13,  2.82it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.78e+6]Epoch 763/3000:  25%|██▌       | 763/3000 [05:34&lt;13:13,  2.82it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.78e+6]Epoch 764/3000:  25%|██▌       | 763/3000 [05:34&lt;13:13,  2.82it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.78e+6]Epoch 764/3000:  25%|██▌       | 764/3000 [05:34&lt;11:23,  3.27it/s, v_num=1, train_loss_step=1.85e+6, train_loss_epoch=1.78e+6]Epoch 764/3000:  25%|██▌       | 764/3000 [05:34&lt;11:23,  3.27it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.78e+6]Epoch 765/3000:  25%|██▌       | 764/3000 [05:34&lt;11:23,  3.27it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.78e+6]Epoch 765/3000:  26%|██▌       | 765/3000 [05:34&lt;09:42,  3.83it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.78e+6]Epoch 765/3000:  26%|██▌       | 765/3000 [05:34&lt;09:42,  3.83it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.77e+6]Epoch 766/3000:  26%|██▌       | 765/3000 [05:34&lt;09:42,  3.83it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.77e+6]Epoch 766/3000:  26%|██▌       | 766/3000 [05:34&lt;12:13,  3.05it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.77e+6]Epoch 766/3000:  26%|██▌       | 766/3000 [05:34&lt;12:13,  3.05it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.77e+6]Epoch 767/3000:  26%|██▌       | 766/3000 [05:34&lt;12:13,  3.05it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.77e+6]Epoch 767/3000:  26%|██▌       | 767/3000 [05:35&lt;13:54,  2.68it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.77e+6]Epoch 767/3000:  26%|██▌       | 767/3000 [05:35&lt;13:54,  2.68it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.77e+6]Epoch 768/3000:  26%|██▌       | 767/3000 [05:35&lt;13:54,  2.68it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.77e+6]Epoch 768/3000:  26%|██▌       | 768/3000 [05:35&lt;15:07,  2.46it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.77e+6]Epoch 768/3000:  26%|██▌       | 768/3000 [05:35&lt;15:07,  2.46it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.77e+6]Epoch 769/3000:  26%|██▌       | 768/3000 [05:35&lt;15:07,  2.46it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.77e+6]Epoch 769/3000:  26%|██▌       | 769/3000 [05:36&lt;15:51,  2.35it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.77e+6]Epoch 769/3000:  26%|██▌       | 769/3000 [05:36&lt;15:51,  2.35it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.77e+6]Epoch 770/3000:  26%|██▌       | 769/3000 [05:36&lt;15:51,  2.35it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.77e+6]Epoch 770/3000:  26%|██▌       | 770/3000 [05:36&lt;15:09,  2.45it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.77e+6]Epoch 770/3000:  26%|██▌       | 770/3000 [05:36&lt;15:09,  2.45it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.77e+6]Epoch 771/3000:  26%|██▌       | 770/3000 [05:36&lt;15:09,  2.45it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.77e+6]Epoch 771/3000:  26%|██▌       | 771/3000 [05:36&lt;13:37,  2.73it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.77e+6]Epoch 771/3000:  26%|██▌       | 771/3000 [05:36&lt;13:37,  2.73it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.76e+6]Epoch 772/3000:  26%|██▌       | 771/3000 [05:36&lt;13:37,  2.73it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.76e+6]Epoch 772/3000:  26%|██▌       | 772/3000 [05:37&lt;14:03,  2.64it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.76e+6]Epoch 772/3000:  26%|██▌       | 772/3000 [05:37&lt;14:03,  2.64it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.76e+6]Epoch 773/3000:  26%|██▌       | 772/3000 [05:37&lt;14:03,  2.64it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.76e+6]Epoch 773/3000:  26%|██▌       | 773/3000 [05:37&lt;14:10,  2.62it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.76e+6]Epoch 773/3000:  26%|██▌       | 773/3000 [05:37&lt;14:10,  2.62it/s, v_num=1, train_loss_step=1.8e+6, train_loss_epoch=1.76e+6] Epoch 774/3000:  26%|██▌       | 773/3000 [05:37&lt;14:10,  2.62it/s, v_num=1, train_loss_step=1.8e+6, train_loss_epoch=1.76e+6]Epoch 774/3000:  26%|██▌       | 774/3000 [05:38&lt;14:45,  2.51it/s, v_num=1, train_loss_step=1.8e+6, train_loss_epoch=1.76e+6]Epoch 774/3000:  26%|██▌       | 774/3000 [05:38&lt;14:45,  2.51it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.76e+6]Epoch 775/3000:  26%|██▌       | 774/3000 [05:38&lt;14:45,  2.51it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.76e+6]Epoch 775/3000:  26%|██▌       | 775/3000 [05:38&lt;16:56,  2.19it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.76e+6]Epoch 775/3000:  26%|██▌       | 775/3000 [05:38&lt;16:56,  2.19it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.76e+6]Epoch 776/3000:  26%|██▌       | 775/3000 [05:38&lt;16:56,  2.19it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.76e+6]Epoch 776/3000:  26%|██▌       | 776/3000 [05:39&lt;16:42,  2.22it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.76e+6]Epoch 776/3000:  26%|██▌       | 776/3000 [05:39&lt;16:42,  2.22it/s, v_num=1, train_loss_step=1.71e+6, train_loss_epoch=1.76e+6]Epoch 777/3000:  26%|██▌       | 776/3000 [05:39&lt;16:42,  2.22it/s, v_num=1, train_loss_step=1.71e+6, train_loss_epoch=1.76e+6]Epoch 777/3000:  26%|██▌       | 777/3000 [05:39&lt;15:40,  2.36it/s, v_num=1, train_loss_step=1.71e+6, train_loss_epoch=1.76e+6]Epoch 777/3000:  26%|██▌       | 777/3000 [05:39&lt;15:40,  2.36it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.75e+6]Epoch 778/3000:  26%|██▌       | 777/3000 [05:39&lt;15:40,  2.36it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.75e+6]Epoch 778/3000:  26%|██▌       | 778/3000 [05:40&lt;16:00,  2.31it/s, v_num=1, train_loss_step=1.79e+6, train_loss_epoch=1.75e+6]Epoch 778/3000:  26%|██▌       | 778/3000 [05:40&lt;16:00,  2.31it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.75e+6]Epoch 779/3000:  26%|██▌       | 778/3000 [05:40&lt;16:00,  2.31it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.75e+6]Epoch 779/3000:  26%|██▌       | 779/3000 [05:40&lt;18:27,  2.00it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.75e+6]Epoch 779/3000:  26%|██▌       | 779/3000 [05:40&lt;18:27,  2.00it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.75e+6]Epoch 780/3000:  26%|██▌       | 779/3000 [05:40&lt;18:27,  2.00it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.75e+6]Epoch 780/3000:  26%|██▌       | 780/3000 [05:41&lt;16:58,  2.18it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.75e+6]Epoch 780/3000:  26%|██▌       | 780/3000 [05:41&lt;16:58,  2.18it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.75e+6]Epoch 781/3000:  26%|██▌       | 780/3000 [05:41&lt;16:58,  2.18it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.75e+6]Epoch 781/3000:  26%|██▌       | 781/3000 [05:41&lt;18:36,  1.99it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.75e+6]Epoch 781/3000:  26%|██▌       | 781/3000 [05:41&lt;18:36,  1.99it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.75e+6]Epoch 782/3000:  26%|██▌       | 781/3000 [05:41&lt;18:36,  1.99it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.75e+6]Epoch 782/3000:  26%|██▌       | 782/3000 [05:42&lt;17:12,  2.15it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.75e+6]Epoch 782/3000:  26%|██▌       | 782/3000 [05:42&lt;17:12,  2.15it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.75e+6]Epoch 783/3000:  26%|██▌       | 782/3000 [05:42&lt;17:12,  2.15it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.75e+6]Epoch 783/3000:  26%|██▌       | 783/3000 [05:42&lt;18:28,  2.00it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.75e+6]Epoch 783/3000:  26%|██▌       | 783/3000 [05:42&lt;18:28,  2.00it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.75e+6]Epoch 784/3000:  26%|██▌       | 783/3000 [05:42&lt;18:28,  2.00it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.75e+6]Epoch 784/3000:  26%|██▌       | 784/3000 [05:43&lt;19:32,  1.89it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.75e+6]Epoch 784/3000:  26%|██▌       | 784/3000 [05:43&lt;19:32,  1.89it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.74e+6]Epoch 785/3000:  26%|██▌       | 784/3000 [05:43&lt;19:32,  1.89it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.74e+6]Epoch 785/3000:  26%|██▌       | 785/3000 [05:43&lt;15:11,  2.43it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.74e+6]Epoch 785/3000:  26%|██▌       | 785/3000 [05:43&lt;15:11,  2.43it/s, v_num=1, train_loss_step=1.68e+6, train_loss_epoch=1.74e+6]Epoch 786/3000:  26%|██▌       | 785/3000 [05:43&lt;15:11,  2.43it/s, v_num=1, train_loss_step=1.68e+6, train_loss_epoch=1.74e+6]Epoch 786/3000:  26%|██▌       | 786/3000 [05:43&lt;13:59,  2.64it/s, v_num=1, train_loss_step=1.68e+6, train_loss_epoch=1.74e+6]Epoch 786/3000:  26%|██▌       | 786/3000 [05:43&lt;13:59,  2.64it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.74e+6]Epoch 787/3000:  26%|██▌       | 786/3000 [05:43&lt;13:59,  2.64it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.74e+6]Epoch 787/3000:  26%|██▌       | 787/3000 [05:44&lt;14:59,  2.46it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.74e+6]Epoch 787/3000:  26%|██▌       | 787/3000 [05:44&lt;14:59,  2.46it/s, v_num=1, train_loss_step=1.71e+6, train_loss_epoch=1.74e+6]Epoch 788/3000:  26%|██▌       | 787/3000 [05:44&lt;14:59,  2.46it/s, v_num=1, train_loss_step=1.71e+6, train_loss_epoch=1.74e+6]Epoch 788/3000:  26%|██▋       | 788/3000 [05:44&lt;15:50,  2.33it/s, v_num=1, train_loss_step=1.71e+6, train_loss_epoch=1.74e+6]Epoch 788/3000:  26%|██▋       | 788/3000 [05:44&lt;15:50,  2.33it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.74e+6]Epoch 789/3000:  26%|██▋       | 788/3000 [05:44&lt;15:50,  2.33it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.74e+6]Epoch 789/3000:  26%|██▋       | 789/3000 [05:45&lt;16:10,  2.28it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.74e+6]Epoch 789/3000:  26%|██▋       | 789/3000 [05:45&lt;16:10,  2.28it/s, v_num=1, train_loss_step=1.8e+6, train_loss_epoch=1.74e+6] Epoch 790/3000:  26%|██▋       | 789/3000 [05:45&lt;16:10,  2.28it/s, v_num=1, train_loss_step=1.8e+6, train_loss_epoch=1.74e+6]Epoch 790/3000:  26%|██▋       | 790/3000 [05:45&lt;14:02,  2.62it/s, v_num=1, train_loss_step=1.8e+6, train_loss_epoch=1.74e+6]Epoch 790/3000:  26%|██▋       | 790/3000 [05:45&lt;14:02,  2.62it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.73e+6]Epoch 791/3000:  26%|██▋       | 790/3000 [05:45&lt;14:02,  2.62it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.73e+6]Epoch 791/3000:  26%|██▋       | 791/3000 [05:45&lt;14:34,  2.53it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.73e+6]Epoch 791/3000:  26%|██▋       | 791/3000 [05:45&lt;14:34,  2.53it/s, v_num=1, train_loss_step=1.71e+6, train_loss_epoch=1.73e+6]Epoch 792/3000:  26%|██▋       | 791/3000 [05:45&lt;14:34,  2.53it/s, v_num=1, train_loss_step=1.71e+6, train_loss_epoch=1.73e+6]Epoch 792/3000:  26%|██▋       | 792/3000 [05:46&lt;16:12,  2.27it/s, v_num=1, train_loss_step=1.71e+6, train_loss_epoch=1.73e+6]Epoch 792/3000:  26%|██▋       | 792/3000 [05:46&lt;16:12,  2.27it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.73e+6]Epoch 793/3000:  26%|██▋       | 792/3000 [05:46&lt;16:12,  2.27it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.73e+6]Epoch 793/3000:  26%|██▋       | 793/3000 [05:46&lt;18:36,  1.98it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.73e+6]Epoch 793/3000:  26%|██▋       | 793/3000 [05:46&lt;18:36,  1.98it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.73e+6]Epoch 794/3000:  26%|██▋       | 793/3000 [05:46&lt;18:36,  1.98it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.73e+6]Epoch 794/3000:  26%|██▋       | 794/3000 [05:47&lt;18:18,  2.01it/s, v_num=1, train_loss_step=1.76e+6, train_loss_epoch=1.73e+6]Epoch 794/3000:  26%|██▋       | 794/3000 [05:47&lt;18:18,  2.01it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.73e+6]Epoch 795/3000:  26%|██▋       | 794/3000 [05:47&lt;18:18,  2.01it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.73e+6]Epoch 795/3000:  26%|██▋       | 795/3000 [05:47&lt;18:16,  2.01it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.73e+6]Epoch 795/3000:  26%|██▋       | 795/3000 [05:47&lt;18:16,  2.01it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.73e+6]Epoch 796/3000:  26%|██▋       | 795/3000 [05:47&lt;18:16,  2.01it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.73e+6]Epoch 796/3000:  27%|██▋       | 796/3000 [05:48&lt;17:09,  2.14it/s, v_num=1, train_loss_step=1.77e+6, train_loss_epoch=1.73e+6]Epoch 796/3000:  27%|██▋       | 796/3000 [05:48&lt;17:09,  2.14it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.73e+6]Epoch 797/3000:  27%|██▋       | 796/3000 [05:48&lt;17:09,  2.14it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.73e+6]Epoch 797/3000:  27%|██▋       | 797/3000 [05:48&lt;16:56,  2.17it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.73e+6]Epoch 797/3000:  27%|██▋       | 797/3000 [05:48&lt;16:56,  2.17it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.72e+6]Epoch 798/3000:  27%|██▋       | 797/3000 [05:48&lt;16:56,  2.17it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.72e+6]Epoch 798/3000:  27%|██▋       | 798/3000 [05:49&lt;16:32,  2.22it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.72e+6]Epoch 798/3000:  27%|██▋       | 798/3000 [05:49&lt;16:32,  2.22it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.72e+6] Epoch 799/3000:  27%|██▋       | 798/3000 [05:49&lt;16:32,  2.22it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.72e+6]Epoch 799/3000:  27%|██▋       | 799/3000 [05:49&lt;17:17,  2.12it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.72e+6]Epoch 799/3000:  27%|██▋       | 799/3000 [05:49&lt;17:17,  2.12it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.72e+6]Epoch 800/3000:  27%|██▋       | 799/3000 [05:49&lt;17:17,  2.12it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.72e+6]Epoch 800/3000:  27%|██▋       | 800/3000 [05:50&lt;17:49,  2.06it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.72e+6]Epoch 800/3000:  27%|██▋       | 800/3000 [05:50&lt;17:49,  2.06it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.72e+6]Epoch 801/3000:  27%|██▋       | 800/3000 [05:50&lt;17:49,  2.06it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.72e+6]Epoch 801/3000:  27%|██▋       | 801/3000 [05:50&lt;17:52,  2.05it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.72e+6]Epoch 801/3000:  27%|██▋       | 801/3000 [05:50&lt;17:52,  2.05it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.72e+6]Epoch 802/3000:  27%|██▋       | 801/3000 [05:50&lt;17:52,  2.05it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.72e+6]Epoch 802/3000:  27%|██▋       | 802/3000 [05:51&lt;18:15,  2.01it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.72e+6]Epoch 802/3000:  27%|██▋       | 802/3000 [05:51&lt;18:15,  2.01it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.72e+6]Epoch 803/3000:  27%|██▋       | 802/3000 [05:51&lt;18:15,  2.01it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.72e+6]Epoch 803/3000:  27%|██▋       | 803/3000 [05:51&lt;17:14,  2.12it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.72e+6]Epoch 803/3000:  27%|██▋       | 803/3000 [05:51&lt;17:14,  2.12it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.72e+6] Epoch 804/3000:  27%|██▋       | 803/3000 [05:51&lt;17:14,  2.12it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.72e+6]Epoch 804/3000:  27%|██▋       | 804/3000 [05:52&lt;18:38,  1.96it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.72e+6]Epoch 804/3000:  27%|██▋       | 804/3000 [05:52&lt;18:38,  1.96it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.71e+6]Epoch 805/3000:  27%|██▋       | 804/3000 [05:52&lt;18:38,  1.96it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.71e+6]Epoch 805/3000:  27%|██▋       | 805/3000 [05:52&lt;17:01,  2.15it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.71e+6]Epoch 805/3000:  27%|██▋       | 805/3000 [05:52&lt;17:01,  2.15it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.71e+6] Epoch 806/3000:  27%|██▋       | 805/3000 [05:52&lt;17:01,  2.15it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.71e+6]Epoch 806/3000:  27%|██▋       | 806/3000 [05:52&lt;15:08,  2.41it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.71e+6]Epoch 806/3000:  27%|██▋       | 806/3000 [05:52&lt;15:08,  2.41it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.71e+6]Epoch 807/3000:  27%|██▋       | 806/3000 [05:52&lt;15:08,  2.41it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.71e+6]Epoch 807/3000:  27%|██▋       | 807/3000 [05:53&lt;15:21,  2.38it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.71e+6]Epoch 807/3000:  27%|██▋       | 807/3000 [05:53&lt;15:21,  2.38it/s, v_num=1, train_loss_step=1.63e+6, train_loss_epoch=1.71e+6]Epoch 808/3000:  27%|██▋       | 807/3000 [05:53&lt;15:21,  2.38it/s, v_num=1, train_loss_step=1.63e+6, train_loss_epoch=1.71e+6]Epoch 808/3000:  27%|██▋       | 808/3000 [05:53&lt;16:21,  2.23it/s, v_num=1, train_loss_step=1.63e+6, train_loss_epoch=1.71e+6]Epoch 808/3000:  27%|██▋       | 808/3000 [05:53&lt;16:21,  2.23it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.71e+6]Epoch 809/3000:  27%|██▋       | 808/3000 [05:53&lt;16:21,  2.23it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.71e+6]Epoch 809/3000:  27%|██▋       | 809/3000 [05:54&lt;15:46,  2.32it/s, v_num=1, train_loss_step=1.78e+6, train_loss_epoch=1.71e+6]Epoch 809/3000:  27%|██▋       | 809/3000 [05:54&lt;15:46,  2.32it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.71e+6]Epoch 810/3000:  27%|██▋       | 809/3000 [05:54&lt;15:46,  2.32it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.71e+6]Epoch 810/3000:  27%|██▋       | 810/3000 [05:54&lt;16:16,  2.24it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.71e+6]Epoch 810/3000:  27%|██▋       | 810/3000 [05:54&lt;16:16,  2.24it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.71e+6] Epoch 811/3000:  27%|██▋       | 810/3000 [05:54&lt;16:16,  2.24it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.71e+6]Epoch 811/3000:  27%|██▋       | 811/3000 [05:55&lt;17:34,  2.08it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.71e+6]Epoch 811/3000:  27%|██▋       | 811/3000 [05:55&lt;17:34,  2.08it/s, v_num=1, train_loss_step=1.71e+6, train_loss_epoch=1.7e+6]Epoch 812/3000:  27%|██▋       | 811/3000 [05:55&lt;17:34,  2.08it/s, v_num=1, train_loss_step=1.71e+6, train_loss_epoch=1.7e+6]Epoch 812/3000:  27%|██▋       | 812/3000 [05:55&lt;18:30,  1.97it/s, v_num=1, train_loss_step=1.71e+6, train_loss_epoch=1.7e+6]Epoch 812/3000:  27%|██▋       | 812/3000 [05:55&lt;18:30,  1.97it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.7e+6]Epoch 813/3000:  27%|██▋       | 812/3000 [05:55&lt;18:30,  1.97it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.7e+6]Epoch 813/3000:  27%|██▋       | 813/3000 [05:56&lt;19:47,  1.84it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.7e+6]Epoch 813/3000:  27%|██▋       | 813/3000 [05:56&lt;19:47,  1.84it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.7e+6]Epoch 814/3000:  27%|██▋       | 813/3000 [05:56&lt;19:47,  1.84it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.7e+6]Epoch 814/3000:  27%|██▋       | 814/3000 [05:56&lt;18:34,  1.96it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.7e+6]Epoch 814/3000:  27%|██▋       | 814/3000 [05:56&lt;18:34,  1.96it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.7e+6]Epoch 815/3000:  27%|██▋       | 814/3000 [05:56&lt;18:34,  1.96it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.7e+6]Epoch 815/3000:  27%|██▋       | 815/3000 [05:57&lt;18:36,  1.96it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.7e+6]Epoch 815/3000:  27%|██▋       | 815/3000 [05:57&lt;18:36,  1.96it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.7e+6] Epoch 816/3000:  27%|██▋       | 815/3000 [05:57&lt;18:36,  1.96it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.7e+6]Epoch 816/3000:  27%|██▋       | 816/3000 [05:57&lt;18:11,  2.00it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.7e+6]Epoch 816/3000:  27%|██▋       | 816/3000 [05:57&lt;18:11,  2.00it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.7e+6]Epoch 817/3000:  27%|██▋       | 816/3000 [05:57&lt;18:11,  2.00it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.7e+6]Epoch 817/3000:  27%|██▋       | 817/3000 [05:58&lt;18:57,  1.92it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.7e+6]Epoch 817/3000:  27%|██▋       | 817/3000 [05:58&lt;18:57,  1.92it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.7e+6]Epoch 818/3000:  27%|██▋       | 817/3000 [05:58&lt;18:57,  1.92it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.7e+6]Epoch 818/3000:  27%|██▋       | 818/3000 [05:59&lt;19:49,  1.83it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.7e+6]Epoch 818/3000:  27%|██▋       | 818/3000 [05:59&lt;19:49,  1.83it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.69e+6]Epoch 819/3000:  27%|██▋       | 818/3000 [05:59&lt;19:49,  1.83it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.69e+6]Epoch 819/3000:  27%|██▋       | 819/3000 [05:59&lt;17:32,  2.07it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.69e+6]Epoch 819/3000:  27%|██▋       | 819/3000 [05:59&lt;17:32,  2.07it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.69e+6]Epoch 820/3000:  27%|██▋       | 819/3000 [05:59&lt;17:32,  2.07it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.69e+6]Epoch 820/3000:  27%|██▋       | 820/3000 [05:59&lt;17:31,  2.07it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.69e+6]Epoch 821/3000:  27%|██▋       | 820/3000 [05:59&lt;17:31,  2.07it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.69e+6]Epoch 821/3000:  27%|██▋       | 821/3000 [05:59&lt;10:50,  3.35it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.69e+6]Epoch 821/3000:  27%|██▋       | 821/3000 [05:59&lt;10:50,  3.35it/s, v_num=1, train_loss_step=1.73e+6, train_loss_epoch=1.69e+6]Epoch 822/3000:  27%|██▋       | 821/3000 [05:59&lt;10:50,  3.35it/s, v_num=1, train_loss_step=1.73e+6, train_loss_epoch=1.69e+6]Epoch 822/3000:  27%|██▋       | 822/3000 [05:59&lt;09:28,  3.83it/s, v_num=1, train_loss_step=1.73e+6, train_loss_epoch=1.69e+6]Epoch 822/3000:  27%|██▋       | 822/3000 [05:59&lt;09:28,  3.83it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.69e+6]Epoch 823/3000:  27%|██▋       | 822/3000 [05:59&lt;09:28,  3.83it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.69e+6]Epoch 823/3000:  27%|██▋       | 823/3000 [05:59&lt;08:02,  4.52it/s, v_num=1, train_loss_step=1.74e+6, train_loss_epoch=1.69e+6]Epoch 823/3000:  27%|██▋       | 823/3000 [05:59&lt;08:02,  4.52it/s, v_num=1, train_loss_step=1.71e+6, train_loss_epoch=1.69e+6]Epoch 824/3000:  27%|██▋       | 823/3000 [05:59&lt;08:02,  4.52it/s, v_num=1, train_loss_step=1.71e+6, train_loss_epoch=1.69e+6]Epoch 824/3000:  27%|██▋       | 824/3000 [06:00&lt;09:17,  3.90it/s, v_num=1, train_loss_step=1.71e+6, train_loss_epoch=1.69e+6]Epoch 824/3000:  27%|██▋       | 824/3000 [06:00&lt;09:17,  3.90it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.69e+6]Epoch 825/3000:  27%|██▋       | 824/3000 [06:00&lt;09:17,  3.90it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.69e+6]Epoch 825/3000:  28%|██▊       | 825/3000 [06:00&lt;11:14,  3.23it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.69e+6]Epoch 825/3000:  28%|██▊       | 825/3000 [06:00&lt;11:14,  3.23it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.68e+6]Epoch 826/3000:  28%|██▊       | 825/3000 [06:00&lt;11:14,  3.23it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.68e+6]Epoch 826/3000:  28%|██▊       | 826/3000 [06:01&lt;12:12,  2.97it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.68e+6]Epoch 826/3000:  28%|██▊       | 826/3000 [06:01&lt;12:12,  2.97it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.68e+6]Epoch 827/3000:  28%|██▊       | 826/3000 [06:01&lt;12:12,  2.97it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.68e+6]Epoch 827/3000:  28%|██▊       | 827/3000 [06:01&lt;14:07,  2.56it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.68e+6]Epoch 827/3000:  28%|██▊       | 827/3000 [06:01&lt;14:07,  2.56it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.68e+6]Epoch 828/3000:  28%|██▊       | 827/3000 [06:01&lt;14:07,  2.56it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.68e+6]Epoch 828/3000:  28%|██▊       | 828/3000 [06:01&lt;14:20,  2.53it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.68e+6]Epoch 828/3000:  28%|██▊       | 828/3000 [06:01&lt;14:20,  2.53it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.68e+6]Epoch 829/3000:  28%|██▊       | 828/3000 [06:01&lt;14:20,  2.53it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.68e+6]Epoch 829/3000:  28%|██▊       | 829/3000 [06:02&lt;14:58,  2.42it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.68e+6]Epoch 829/3000:  28%|██▊       | 829/3000 [06:02&lt;14:58,  2.42it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.68e+6]Epoch 830/3000:  28%|██▊       | 829/3000 [06:02&lt;14:58,  2.42it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.68e+6]Epoch 830/3000:  28%|██▊       | 830/3000 [06:02&lt;15:11,  2.38it/s, v_num=1, train_loss_step=1.72e+6, train_loss_epoch=1.68e+6]Epoch 830/3000:  28%|██▊       | 830/3000 [06:02&lt;15:11,  2.38it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.68e+6]Epoch 831/3000:  28%|██▊       | 830/3000 [06:02&lt;15:11,  2.38it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.68e+6]Epoch 831/3000:  28%|██▊       | 831/3000 [06:03&lt;17:00,  2.13it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.68e+6]Epoch 831/3000:  28%|██▊       | 831/3000 [06:03&lt;17:00,  2.13it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.68e+6]Epoch 832/3000:  28%|██▊       | 831/3000 [06:03&lt;17:00,  2.13it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.68e+6]Epoch 832/3000:  28%|██▊       | 832/3000 [06:03&lt;17:33,  2.06it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.68e+6]Epoch 832/3000:  28%|██▊       | 832/3000 [06:03&lt;17:33,  2.06it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.68e+6] Epoch 833/3000:  28%|██▊       | 832/3000 [06:03&lt;17:33,  2.06it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.68e+6]Epoch 833/3000:  28%|██▊       | 833/3000 [06:04&lt;18:53,  1.91it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.68e+6]Epoch 833/3000:  28%|██▊       | 833/3000 [06:04&lt;18:53,  1.91it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.67e+6]Epoch 834/3000:  28%|██▊       | 833/3000 [06:04&lt;18:53,  1.91it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.67e+6]Epoch 834/3000:  28%|██▊       | 834/3000 [06:04&lt;17:40,  2.04it/s, v_num=1, train_loss_step=1.75e+6, train_loss_epoch=1.67e+6]Epoch 834/3000:  28%|██▊       | 834/3000 [06:04&lt;17:40,  2.04it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.67e+6] Epoch 835/3000:  28%|██▊       | 834/3000 [06:04&lt;17:40,  2.04it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.67e+6]Epoch 835/3000:  28%|██▊       | 835/3000 [06:05&lt;17:05,  2.11it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.67e+6]Epoch 835/3000:  28%|██▊       | 835/3000 [06:05&lt;17:05,  2.11it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.67e+6]Epoch 836/3000:  28%|██▊       | 835/3000 [06:05&lt;17:05,  2.11it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.67e+6]Epoch 836/3000:  28%|██▊       | 836/3000 [06:05&lt;16:57,  2.13it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.67e+6]Epoch 836/3000:  28%|██▊       | 836/3000 [06:05&lt;16:57,  2.13it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.67e+6]Epoch 837/3000:  28%|██▊       | 836/3000 [06:05&lt;16:57,  2.13it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.67e+6]Epoch 837/3000:  28%|██▊       | 837/3000 [06:06&lt;15:57,  2.26it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.67e+6]Epoch 837/3000:  28%|██▊       | 837/3000 [06:06&lt;15:57,  2.26it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.67e+6]Epoch 838/3000:  28%|██▊       | 837/3000 [06:06&lt;15:57,  2.26it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.67e+6]Epoch 838/3000:  28%|██▊       | 838/3000 [06:06&lt;14:58,  2.40it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.67e+6]Epoch 838/3000:  28%|██▊       | 838/3000 [06:06&lt;14:58,  2.40it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.67e+6]Epoch 839/3000:  28%|██▊       | 838/3000 [06:06&lt;14:58,  2.40it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.67e+6]Epoch 839/3000:  28%|██▊       | 839/3000 [06:06&lt;14:34,  2.47it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.67e+6]Epoch 839/3000:  28%|██▊       | 839/3000 [06:06&lt;14:34,  2.47it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.67e+6]Epoch 840/3000:  28%|██▊       | 839/3000 [06:06&lt;14:34,  2.47it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.67e+6]Epoch 840/3000:  28%|██▊       | 840/3000 [06:07&lt;15:17,  2.36it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.67e+6]Epoch 840/3000:  28%|██▊       | 840/3000 [06:07&lt;15:17,  2.36it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.66e+6]Epoch 841/3000:  28%|██▊       | 840/3000 [06:07&lt;15:17,  2.36it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.66e+6]Epoch 841/3000:  28%|██▊       | 841/3000 [06:07&lt;14:37,  2.46it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.66e+6]Epoch 841/3000:  28%|██▊       | 841/3000 [06:07&lt;14:37,  2.46it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.66e+6]Epoch 842/3000:  28%|██▊       | 841/3000 [06:07&lt;14:37,  2.46it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.66e+6]Epoch 842/3000:  28%|██▊       | 842/3000 [06:08&lt;13:39,  2.63it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.66e+6]Epoch 842/3000:  28%|██▊       | 842/3000 [06:08&lt;13:39,  2.63it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.66e+6]Epoch 843/3000:  28%|██▊       | 842/3000 [06:08&lt;13:39,  2.63it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.66e+6]Epoch 843/3000:  28%|██▊       | 843/3000 [06:08&lt;16:19,  2.20it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.66e+6]Epoch 843/3000:  28%|██▊       | 843/3000 [06:08&lt;16:19,  2.20it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.66e+6]Epoch 844/3000:  28%|██▊       | 843/3000 [06:08&lt;16:19,  2.20it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.66e+6]Epoch 844/3000:  28%|██▊       | 844/3000 [06:09&lt;17:51,  2.01it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.66e+6]Epoch 844/3000:  28%|██▊       | 844/3000 [06:09&lt;17:51,  2.01it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.66e+6]Epoch 845/3000:  28%|██▊       | 844/3000 [06:09&lt;17:51,  2.01it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.66e+6]Epoch 845/3000:  28%|██▊       | 845/3000 [06:09&lt;15:48,  2.27it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.66e+6]Epoch 845/3000:  28%|██▊       | 845/3000 [06:09&lt;15:48,  2.27it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.66e+6] Epoch 846/3000:  28%|██▊       | 845/3000 [06:09&lt;15:48,  2.27it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.66e+6]Epoch 846/3000:  28%|██▊       | 846/3000 [06:09&lt;14:28,  2.48it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.66e+6]Epoch 846/3000:  28%|██▊       | 846/3000 [06:09&lt;14:28,  2.48it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.66e+6]Epoch 847/3000:  28%|██▊       | 846/3000 [06:09&lt;14:28,  2.48it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.66e+6]Epoch 847/3000:  28%|██▊       | 847/3000 [06:10&lt;13:01,  2.75it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.66e+6]Epoch 847/3000:  28%|██▊       | 847/3000 [06:10&lt;13:01,  2.75it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.66e+6] Epoch 848/3000:  28%|██▊       | 847/3000 [06:10&lt;13:01,  2.75it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.66e+6]Epoch 848/3000:  28%|██▊       | 848/3000 [06:10&lt;14:12,  2.52it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.66e+6]Epoch 848/3000:  28%|██▊       | 848/3000 [06:10&lt;14:12,  2.52it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.65e+6]Epoch 849/3000:  28%|██▊       | 848/3000 [06:10&lt;14:12,  2.52it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.65e+6]Epoch 849/3000:  28%|██▊       | 849/3000 [06:11&lt;14:42,  2.44it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.65e+6]Epoch 849/3000:  28%|██▊       | 849/3000 [06:11&lt;14:42,  2.44it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.65e+6]Epoch 850/3000:  28%|██▊       | 849/3000 [06:11&lt;14:42,  2.44it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.65e+6]Epoch 850/3000:  28%|██▊       | 850/3000 [06:11&lt;16:09,  2.22it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.65e+6]Epoch 850/3000:  28%|██▊       | 850/3000 [06:11&lt;16:09,  2.22it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.65e+6]Epoch 851/3000:  28%|██▊       | 850/3000 [06:11&lt;16:09,  2.22it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.65e+6]Epoch 851/3000:  28%|██▊       | 851/3000 [06:12&lt;16:30,  2.17it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.65e+6]Epoch 851/3000:  28%|██▊       | 851/3000 [06:12&lt;16:30,  2.17it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.65e+6] Epoch 852/3000:  28%|██▊       | 851/3000 [06:12&lt;16:30,  2.17it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.65e+6]Epoch 852/3000:  28%|██▊       | 852/3000 [06:12&lt;16:04,  2.23it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.65e+6]Epoch 852/3000:  28%|██▊       | 852/3000 [06:12&lt;16:04,  2.23it/s, v_num=1, train_loss_step=1.68e+6, train_loss_epoch=1.65e+6]Epoch 853/3000:  28%|██▊       | 852/3000 [06:12&lt;16:04,  2.23it/s, v_num=1, train_loss_step=1.68e+6, train_loss_epoch=1.65e+6]Epoch 853/3000:  28%|██▊       | 853/3000 [06:13&lt;17:00,  2.10it/s, v_num=1, train_loss_step=1.68e+6, train_loss_epoch=1.65e+6]Epoch 853/3000:  28%|██▊       | 853/3000 [06:13&lt;17:00,  2.10it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.65e+6]Epoch 854/3000:  28%|██▊       | 853/3000 [06:13&lt;17:00,  2.10it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.65e+6]Epoch 854/3000:  28%|██▊       | 854/3000 [06:13&lt;16:10,  2.21it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.65e+6]Epoch 854/3000:  28%|██▊       | 854/3000 [06:13&lt;16:10,  2.21it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.65e+6] Epoch 855/3000:  28%|██▊       | 854/3000 [06:13&lt;16:10,  2.21it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.65e+6]Epoch 855/3000:  28%|██▊       | 855/3000 [06:14&lt;18:24,  1.94it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.65e+6]Epoch 855/3000:  28%|██▊       | 855/3000 [06:14&lt;18:24,  1.94it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.65e+6]Epoch 856/3000:  28%|██▊       | 855/3000 [06:14&lt;18:24,  1.94it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.65e+6]Epoch 856/3000:  29%|██▊       | 856/3000 [06:14&lt;18:04,  1.98it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.65e+6]Epoch 856/3000:  29%|██▊       | 856/3000 [06:14&lt;18:04,  1.98it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.64e+6]Epoch 857/3000:  29%|██▊       | 856/3000 [06:14&lt;18:04,  1.98it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.64e+6]Epoch 857/3000:  29%|██▊       | 857/3000 [06:15&lt;16:03,  2.22it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.64e+6]Epoch 857/3000:  29%|██▊       | 857/3000 [06:15&lt;16:03,  2.22it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.64e+6]Epoch 858/3000:  29%|██▊       | 857/3000 [06:15&lt;16:03,  2.22it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.64e+6]Epoch 858/3000:  29%|██▊       | 858/3000 [06:15&lt;14:52,  2.40it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.64e+6]Epoch 858/3000:  29%|██▊       | 858/3000 [06:15&lt;14:52,  2.40it/s, v_num=1, train_loss_step=1.63e+6, train_loss_epoch=1.64e+6]Epoch 859/3000:  29%|██▊       | 858/3000 [06:15&lt;14:52,  2.40it/s, v_num=1, train_loss_step=1.63e+6, train_loss_epoch=1.64e+6]Epoch 859/3000:  29%|██▊       | 859/3000 [06:15&lt;16:47,  2.12it/s, v_num=1, train_loss_step=1.63e+6, train_loss_epoch=1.64e+6]Epoch 859/3000:  29%|██▊       | 859/3000 [06:15&lt;16:47,  2.12it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.64e+6]Epoch 860/3000:  29%|██▊       | 859/3000 [06:15&lt;16:47,  2.12it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.64e+6]Epoch 860/3000:  29%|██▊       | 860/3000 [06:16&lt;16:23,  2.18it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.64e+6]Epoch 860/3000:  29%|██▊       | 860/3000 [06:16&lt;16:23,  2.18it/s, v_num=1, train_loss_step=1.58e+6, train_loss_epoch=1.64e+6]Epoch 861/3000:  29%|██▊       | 860/3000 [06:16&lt;16:23,  2.18it/s, v_num=1, train_loss_step=1.58e+6, train_loss_epoch=1.64e+6]Epoch 861/3000:  29%|██▊       | 861/3000 [06:16&lt;16:46,  2.13it/s, v_num=1, train_loss_step=1.58e+6, train_loss_epoch=1.64e+6]Epoch 861/3000:  29%|██▊       | 861/3000 [06:16&lt;16:46,  2.13it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.64e+6]Epoch 862/3000:  29%|██▊       | 861/3000 [06:16&lt;16:46,  2.13it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.64e+6]Epoch 862/3000:  29%|██▊       | 862/3000 [06:17&lt;14:32,  2.45it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.64e+6]Epoch 862/3000:  29%|██▊       | 862/3000 [06:17&lt;14:32,  2.45it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.64e+6]Epoch 863/3000:  29%|██▊       | 862/3000 [06:17&lt;14:32,  2.45it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.64e+6]Epoch 863/3000:  29%|██▉       | 863/3000 [06:17&lt;14:40,  2.43it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.64e+6]Epoch 863/3000:  29%|██▉       | 863/3000 [06:17&lt;14:40,  2.43it/s, v_num=1, train_loss_step=1.68e+6, train_loss_epoch=1.64e+6]Epoch 864/3000:  29%|██▉       | 863/3000 [06:17&lt;14:40,  2.43it/s, v_num=1, train_loss_step=1.68e+6, train_loss_epoch=1.64e+6]Epoch 864/3000:  29%|██▉       | 864/3000 [06:17&lt;14:26,  2.47it/s, v_num=1, train_loss_step=1.68e+6, train_loss_epoch=1.64e+6]Epoch 864/3000:  29%|██▉       | 864/3000 [06:17&lt;14:26,  2.47it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.63e+6]Epoch 865/3000:  29%|██▉       | 864/3000 [06:17&lt;14:26,  2.47it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.63e+6]Epoch 865/3000:  29%|██▉       | 865/3000 [06:18&lt;14:47,  2.41it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.63e+6]Epoch 865/3000:  29%|██▉       | 865/3000 [06:18&lt;14:47,  2.41it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.63e+6] Epoch 866/3000:  29%|██▉       | 865/3000 [06:18&lt;14:47,  2.41it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.63e+6]Epoch 866/3000:  29%|██▉       | 866/3000 [06:19&lt;17:02,  2.09it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.63e+6]Epoch 866/3000:  29%|██▉       | 866/3000 [06:19&lt;17:02,  2.09it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.63e+6]Epoch 867/3000:  29%|██▉       | 866/3000 [06:19&lt;17:02,  2.09it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.63e+6]Epoch 867/3000:  29%|██▉       | 867/3000 [06:19&lt;18:21,  1.94it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.63e+6]Epoch 867/3000:  29%|██▉       | 867/3000 [06:19&lt;18:21,  1.94it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.63e+6]Epoch 868/3000:  29%|██▉       | 867/3000 [06:19&lt;18:21,  1.94it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.63e+6]Epoch 868/3000:  29%|██▉       | 868/3000 [06:20&lt;18:58,  1.87it/s, v_num=1, train_loss_step=1.69e+6, train_loss_epoch=1.63e+6]Epoch 868/3000:  29%|██▉       | 868/3000 [06:20&lt;18:58,  1.87it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.63e+6]Epoch 869/3000:  29%|██▉       | 868/3000 [06:20&lt;18:58,  1.87it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.63e+6]Epoch 869/3000:  29%|██▉       | 869/3000 [06:20&lt;17:50,  1.99it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.63e+6]Epoch 869/3000:  29%|██▉       | 869/3000 [06:20&lt;17:50,  1.99it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.63e+6]Epoch 870/3000:  29%|██▉       | 869/3000 [06:20&lt;17:50,  1.99it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.63e+6]Epoch 870/3000:  29%|██▉       | 870/3000 [06:21&lt;17:58,  1.98it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.63e+6]Epoch 870/3000:  29%|██▉       | 870/3000 [06:21&lt;17:58,  1.98it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.63e+6]Epoch 871/3000:  29%|██▉       | 870/3000 [06:21&lt;17:58,  1.98it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.63e+6]Epoch 871/3000:  29%|██▉       | 871/3000 [06:21&lt;19:36,  1.81it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.63e+6]Epoch 871/3000:  29%|██▉       | 871/3000 [06:21&lt;19:36,  1.81it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.63e+6]Epoch 872/3000:  29%|██▉       | 871/3000 [06:21&lt;19:36,  1.81it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.63e+6]Epoch 872/3000:  29%|██▉       | 872/3000 [06:22&lt;19:34,  1.81it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.63e+6]Epoch 872/3000:  29%|██▉       | 872/3000 [06:22&lt;19:34,  1.81it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.62e+6]Epoch 873/3000:  29%|██▉       | 872/3000 [06:22&lt;19:34,  1.81it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.62e+6]Epoch 873/3000:  29%|██▉       | 873/3000 [06:22&lt;17:40,  2.01it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.62e+6]Epoch 873/3000:  29%|██▉       | 873/3000 [06:22&lt;17:40,  2.01it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.62e+6]Epoch 874/3000:  29%|██▉       | 873/3000 [06:22&lt;17:40,  2.01it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.62e+6]Epoch 874/3000:  29%|██▉       | 874/3000 [06:22&lt;14:40,  2.41it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.62e+6]Epoch 874/3000:  29%|██▉       | 874/3000 [06:22&lt;14:40,  2.41it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.62e+6]Epoch 875/3000:  29%|██▉       | 874/3000 [06:22&lt;14:40,  2.41it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.62e+6]Epoch 875/3000:  29%|██▉       | 875/3000 [06:23&lt;13:36,  2.60it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.62e+6]Epoch 875/3000:  29%|██▉       | 875/3000 [06:23&lt;13:36,  2.60it/s, v_num=1, train_loss_step=1.63e+6, train_loss_epoch=1.62e+6]Epoch 876/3000:  29%|██▉       | 875/3000 [06:23&lt;13:36,  2.60it/s, v_num=1, train_loss_step=1.63e+6, train_loss_epoch=1.62e+6]Epoch 876/3000:  29%|██▉       | 876/3000 [06:23&lt;15:17,  2.31it/s, v_num=1, train_loss_step=1.63e+6, train_loss_epoch=1.62e+6]Epoch 876/3000:  29%|██▉       | 876/3000 [06:23&lt;15:17,  2.31it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.62e+6] Epoch 877/3000:  29%|██▉       | 876/3000 [06:23&lt;15:17,  2.31it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.62e+6]Epoch 877/3000:  29%|██▉       | 877/3000 [06:24&lt;14:19,  2.47it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.62e+6]Epoch 877/3000:  29%|██▉       | 877/3000 [06:24&lt;14:19,  2.47it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.62e+6]Epoch 878/3000:  29%|██▉       | 877/3000 [06:24&lt;14:19,  2.47it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.62e+6]Epoch 878/3000:  29%|██▉       | 878/3000 [06:24&lt;15:36,  2.27it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.62e+6]Epoch 878/3000:  29%|██▉       | 878/3000 [06:24&lt;15:36,  2.27it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.62e+6]Epoch 879/3000:  29%|██▉       | 878/3000 [06:24&lt;15:36,  2.27it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.62e+6]Epoch 879/3000:  29%|██▉       | 879/3000 [06:25&lt;16:26,  2.15it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.62e+6]Epoch 879/3000:  29%|██▉       | 879/3000 [06:25&lt;16:26,  2.15it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.62e+6]Epoch 880/3000:  29%|██▉       | 879/3000 [06:25&lt;16:26,  2.15it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.62e+6]Epoch 880/3000:  29%|██▉       | 880/3000 [06:25&lt;13:19,  2.65it/s, v_num=1, train_loss_step=1.66e+6, train_loss_epoch=1.62e+6]Epoch 880/3000:  29%|██▉       | 880/3000 [06:25&lt;13:19,  2.65it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.61e+6] Epoch 881/3000:  29%|██▉       | 880/3000 [06:25&lt;13:19,  2.65it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.61e+6]Epoch 881/3000:  29%|██▉       | 881/3000 [06:25&lt;14:05,  2.50it/s, v_num=1, train_loss_step=1.7e+6, train_loss_epoch=1.61e+6]Epoch 881/3000:  29%|██▉       | 881/3000 [06:25&lt;14:05,  2.50it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.61e+6]Epoch 882/3000:  29%|██▉       | 881/3000 [06:25&lt;14:05,  2.50it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.61e+6]Epoch 882/3000:  29%|██▉       | 882/3000 [06:26&lt;15:06,  2.34it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.61e+6]Epoch 882/3000:  29%|██▉       | 882/3000 [06:26&lt;15:06,  2.34it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.61e+6]Epoch 883/3000:  29%|██▉       | 882/3000 [06:26&lt;15:06,  2.34it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.61e+6]Epoch 883/3000:  29%|██▉       | 883/3000 [06:26&lt;15:45,  2.24it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.61e+6]Epoch 883/3000:  29%|██▉       | 883/3000 [06:26&lt;15:45,  2.24it/s, v_num=1, train_loss_step=1.63e+6, train_loss_epoch=1.61e+6]Epoch 884/3000:  29%|██▉       | 883/3000 [06:26&lt;15:45,  2.24it/s, v_num=1, train_loss_step=1.63e+6, train_loss_epoch=1.61e+6]Epoch 884/3000:  29%|██▉       | 884/3000 [06:27&lt;17:29,  2.02it/s, v_num=1, train_loss_step=1.63e+6, train_loss_epoch=1.61e+6]Epoch 884/3000:  29%|██▉       | 884/3000 [06:27&lt;17:29,  2.02it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.61e+6]Epoch 885/3000:  29%|██▉       | 884/3000 [06:27&lt;17:29,  2.02it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.61e+6]Epoch 885/3000:  30%|██▉       | 885/3000 [06:27&lt;16:03,  2.20it/s, v_num=1, train_loss_step=1.65e+6, train_loss_epoch=1.61e+6]Epoch 885/3000:  30%|██▉       | 885/3000 [06:27&lt;16:03,  2.20it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.61e+6]Epoch 886/3000:  30%|██▉       | 885/3000 [06:27&lt;16:03,  2.20it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.61e+6]Epoch 886/3000:  30%|██▉       | 886/3000 [06:28&lt;16:26,  2.14it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.61e+6]Epoch 886/3000:  30%|██▉       | 886/3000 [06:28&lt;16:26,  2.14it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.61e+6]Epoch 887/3000:  30%|██▉       | 886/3000 [06:28&lt;16:26,  2.14it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.61e+6]Epoch 887/3000:  30%|██▉       | 887/3000 [06:28&lt;15:48,  2.23it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.61e+6]Epoch 887/3000:  30%|██▉       | 887/3000 [06:28&lt;15:48,  2.23it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.61e+6]Epoch 888/3000:  30%|██▉       | 887/3000 [06:28&lt;15:48,  2.23it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.61e+6]Epoch 888/3000:  30%|██▉       | 888/3000 [06:29&lt;15:26,  2.28it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.61e+6]Epoch 888/3000:  30%|██▉       | 888/3000 [06:29&lt;15:26,  2.28it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.61e+6]Epoch 889/3000:  30%|██▉       | 888/3000 [06:29&lt;15:26,  2.28it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.61e+6]Epoch 889/3000:  30%|██▉       | 889/3000 [06:29&lt;14:46,  2.38it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.61e+6]Epoch 889/3000:  30%|██▉       | 889/3000 [06:29&lt;14:46,  2.38it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.6e+6] Epoch 890/3000:  30%|██▉       | 889/3000 [06:29&lt;14:46,  2.38it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.6e+6]Epoch 890/3000:  30%|██▉       | 890/3000 [06:29&lt;14:11,  2.48it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.6e+6]Epoch 890/3000:  30%|██▉       | 890/3000 [06:29&lt;14:11,  2.48it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.6e+6]Epoch 891/3000:  30%|██▉       | 890/3000 [06:29&lt;14:11,  2.48it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.6e+6]Epoch 891/3000:  30%|██▉       | 891/3000 [06:30&lt;16:57,  2.07it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.6e+6]Epoch 891/3000:  30%|██▉       | 891/3000 [06:30&lt;16:57,  2.07it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.6e+6]Epoch 892/3000:  30%|██▉       | 891/3000 [06:30&lt;16:57,  2.07it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.6e+6]Epoch 892/3000:  30%|██▉       | 892/3000 [06:30&lt;16:32,  2.12it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.6e+6]Epoch 892/3000:  30%|██▉       | 892/3000 [06:30&lt;16:32,  2.12it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.6e+6]Epoch 893/3000:  30%|██▉       | 892/3000 [06:30&lt;16:32,  2.12it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.6e+6]Epoch 893/3000:  30%|██▉       | 893/3000 [06:31&lt;16:01,  2.19it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.6e+6]Epoch 893/3000:  30%|██▉       | 893/3000 [06:31&lt;16:01,  2.19it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.6e+6]Epoch 894/3000:  30%|██▉       | 893/3000 [06:31&lt;16:01,  2.19it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.6e+6]Epoch 894/3000:  30%|██▉       | 894/3000 [06:31&lt;17:19,  2.03it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.6e+6]Epoch 894/3000:  30%|██▉       | 894/3000 [06:31&lt;17:19,  2.03it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.6e+6] Epoch 895/3000:  30%|██▉       | 894/3000 [06:31&lt;17:19,  2.03it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.6e+6]Epoch 895/3000:  30%|██▉       | 895/3000 [06:32&lt;17:56,  1.96it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.6e+6]Epoch 895/3000:  30%|██▉       | 895/3000 [06:32&lt;17:56,  1.96it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.6e+6]Epoch 896/3000:  30%|██▉       | 895/3000 [06:32&lt;17:56,  1.96it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.6e+6]Epoch 896/3000:  30%|██▉       | 896/3000 [06:32&lt;16:48,  2.09it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.6e+6]Epoch 896/3000:  30%|██▉       | 896/3000 [06:32&lt;16:48,  2.09it/s, v_num=1, train_loss_step=1.58e+6, train_loss_epoch=1.6e+6]Epoch 897/3000:  30%|██▉       | 896/3000 [06:32&lt;16:48,  2.09it/s, v_num=1, train_loss_step=1.58e+6, train_loss_epoch=1.6e+6]Epoch 897/3000:  30%|██▉       | 897/3000 [06:33&lt;15:27,  2.27it/s, v_num=1, train_loss_step=1.58e+6, train_loss_epoch=1.6e+6]Epoch 897/3000:  30%|██▉       | 897/3000 [06:33&lt;15:27,  2.27it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.6e+6]Epoch 898/3000:  30%|██▉       | 897/3000 [06:33&lt;15:27,  2.27it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.6e+6]Epoch 898/3000:  30%|██▉       | 898/3000 [06:33&lt;16:20,  2.14it/s, v_num=1, train_loss_step=1.67e+6, train_loss_epoch=1.6e+6]Epoch 898/3000:  30%|██▉       | 898/3000 [06:33&lt;16:20,  2.14it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.59e+6]Epoch 899/3000:  30%|██▉       | 898/3000 [06:33&lt;16:20,  2.14it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.59e+6]Epoch 899/3000:  30%|██▉       | 899/3000 [06:34&lt;17:23,  2.01it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.59e+6]Epoch 899/3000:  30%|██▉       | 899/3000 [06:34&lt;17:23,  2.01it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.59e+6]Epoch 900/3000:  30%|██▉       | 899/3000 [06:34&lt;17:23,  2.01it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.59e+6]Epoch 900/3000:  30%|███       | 900/3000 [06:34&lt;17:29,  2.00it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.59e+6]Epoch 900/3000:  30%|███       | 900/3000 [06:34&lt;17:29,  2.00it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.59e+6]Epoch 901/3000:  30%|███       | 900/3000 [06:34&lt;17:29,  2.00it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.59e+6]Epoch 901/3000:  30%|███       | 901/3000 [06:35&lt;16:44,  2.09it/s, v_num=1, train_loss_step=1.62e+6, train_loss_epoch=1.59e+6]Epoch 901/3000:  30%|███       | 901/3000 [06:35&lt;16:44,  2.09it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.59e+6]Epoch 902/3000:  30%|███       | 901/3000 [06:35&lt;16:44,  2.09it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.59e+6]Epoch 902/3000:  30%|███       | 902/3000 [06:35&lt;16:38,  2.10it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.59e+6]Epoch 902/3000:  30%|███       | 902/3000 [06:35&lt;16:38,  2.10it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.59e+6]Epoch 903/3000:  30%|███       | 902/3000 [06:35&lt;16:38,  2.10it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.59e+6]Epoch 903/3000:  30%|███       | 903/3000 [06:36&lt;15:51,  2.20it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.59e+6]Epoch 903/3000:  30%|███       | 903/3000 [06:36&lt;15:51,  2.20it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.59e+6] Epoch 904/3000:  30%|███       | 903/3000 [06:36&lt;15:51,  2.20it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.59e+6]Epoch 904/3000:  30%|███       | 904/3000 [06:36&lt;16:54,  2.07it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.59e+6]Epoch 904/3000:  30%|███       | 904/3000 [06:36&lt;16:54,  2.07it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.59e+6]Epoch 905/3000:  30%|███       | 904/3000 [06:36&lt;16:54,  2.07it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.59e+6]Epoch 905/3000:  30%|███       | 905/3000 [06:37&lt;16:18,  2.14it/s, v_num=1, train_loss_step=1.64e+6, train_loss_epoch=1.59e+6]Epoch 905/3000:  30%|███       | 905/3000 [06:37&lt;16:18,  2.14it/s, v_num=1, train_loss_step=1.63e+6, train_loss_epoch=1.59e+6]Epoch 906/3000:  30%|███       | 905/3000 [06:37&lt;16:18,  2.14it/s, v_num=1, train_loss_step=1.63e+6, train_loss_epoch=1.59e+6]Epoch 906/3000:  30%|███       | 906/3000 [06:37&lt;16:23,  2.13it/s, v_num=1, train_loss_step=1.63e+6, train_loss_epoch=1.59e+6]Epoch 906/3000:  30%|███       | 906/3000 [06:37&lt;16:23,  2.13it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.59e+6]Epoch 907/3000:  30%|███       | 906/3000 [06:37&lt;16:23,  2.13it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.59e+6]Epoch 907/3000:  30%|███       | 907/3000 [06:38&lt;17:45,  1.96it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.59e+6]Epoch 907/3000:  30%|███       | 907/3000 [06:38&lt;17:45,  1.96it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.58e+6] Epoch 908/3000:  30%|███       | 907/3000 [06:38&lt;17:45,  1.96it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.58e+6]Epoch 908/3000:  30%|███       | 908/3000 [06:38&lt;18:26,  1.89it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.58e+6]Epoch 908/3000:  30%|███       | 908/3000 [06:38&lt;18:26,  1.89it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.58e+6]Epoch 909/3000:  30%|███       | 908/3000 [06:38&lt;18:26,  1.89it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.58e+6]Epoch 909/3000:  30%|███       | 909/3000 [06:39&lt;16:08,  2.16it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.58e+6]Epoch 909/3000:  30%|███       | 909/3000 [06:39&lt;16:08,  2.16it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.58e+6]Epoch 910/3000:  30%|███       | 909/3000 [06:39&lt;16:08,  2.16it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.58e+6]Epoch 910/3000:  30%|███       | 910/3000 [06:39&lt;16:31,  2.11it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.58e+6]Epoch 910/3000:  30%|███       | 910/3000 [06:39&lt;16:31,  2.11it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.58e+6]Epoch 911/3000:  30%|███       | 910/3000 [06:39&lt;16:31,  2.11it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.58e+6]Epoch 911/3000:  30%|███       | 911/3000 [06:39&lt;13:06,  2.65it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.58e+6]Epoch 911/3000:  30%|███       | 911/3000 [06:39&lt;13:06,  2.65it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.58e+6]Epoch 912/3000:  30%|███       | 911/3000 [06:39&lt;13:06,  2.65it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.58e+6]Epoch 912/3000:  30%|███       | 912/3000 [06:39&lt;11:02,  3.15it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.58e+6]Epoch 912/3000:  30%|███       | 912/3000 [06:39&lt;11:02,  3.15it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.58e+6]Epoch 913/3000:  30%|███       | 912/3000 [06:39&lt;11:02,  3.15it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.58e+6]Epoch 913/3000:  30%|███       | 913/3000 [06:40&lt;09:28,  3.67it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.58e+6]Epoch 913/3000:  30%|███       | 913/3000 [06:40&lt;09:28,  3.67it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.58e+6]Epoch 914/3000:  30%|███       | 913/3000 [06:40&lt;09:28,  3.67it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.58e+6]Epoch 914/3000:  30%|███       | 914/3000 [06:40&lt;08:52,  3.92it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.58e+6]Epoch 914/3000:  30%|███       | 914/3000 [06:40&lt;08:52,  3.92it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.58e+6] Epoch 915/3000:  30%|███       | 914/3000 [06:40&lt;08:52,  3.92it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.58e+6]Epoch 915/3000:  30%|███       | 915/3000 [06:40&lt;10:02,  3.46it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.58e+6]Epoch 915/3000:  30%|███       | 915/3000 [06:40&lt;10:02,  3.46it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.58e+6]Epoch 916/3000:  30%|███       | 915/3000 [06:40&lt;10:02,  3.46it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.58e+6]Epoch 916/3000:  31%|███       | 916/3000 [06:41&lt;13:19,  2.61it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.58e+6]Epoch 916/3000:  31%|███       | 916/3000 [06:41&lt;13:19,  2.61it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.57e+6] Epoch 917/3000:  31%|███       | 916/3000 [06:41&lt;13:19,  2.61it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.57e+6]Epoch 917/3000:  31%|███       | 917/3000 [06:41&lt;15:55,  2.18it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.57e+6]Epoch 917/3000:  31%|███       | 917/3000 [06:41&lt;15:55,  2.18it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.57e+6]Epoch 918/3000:  31%|███       | 917/3000 [06:41&lt;15:55,  2.18it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.57e+6]Epoch 918/3000:  31%|███       | 918/3000 [06:42&lt;15:11,  2.28it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.57e+6]Epoch 918/3000:  31%|███       | 918/3000 [06:42&lt;15:11,  2.28it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.57e+6] Epoch 919/3000:  31%|███       | 918/3000 [06:42&lt;15:11,  2.28it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.57e+6]Epoch 919/3000:  31%|███       | 919/3000 [06:42&lt;15:04,  2.30it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.57e+6]Epoch 919/3000:  31%|███       | 919/3000 [06:42&lt;15:04,  2.30it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.57e+6]Epoch 920/3000:  31%|███       | 919/3000 [06:42&lt;15:04,  2.30it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.57e+6]Epoch 920/3000:  31%|███       | 920/3000 [06:43&lt;15:10,  2.28it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.57e+6]Epoch 920/3000:  31%|███       | 920/3000 [06:43&lt;15:10,  2.28it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.57e+6] Epoch 921/3000:  31%|███       | 920/3000 [06:43&lt;15:10,  2.28it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.57e+6]Epoch 921/3000:  31%|███       | 921/3000 [06:43&lt;16:10,  2.14it/s, v_num=1, train_loss_step=1.6e+6, train_loss_epoch=1.57e+6]Epoch 921/3000:  31%|███       | 921/3000 [06:43&lt;16:10,  2.14it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.57e+6]Epoch 922/3000:  31%|███       | 921/3000 [06:43&lt;16:10,  2.14it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.57e+6]Epoch 922/3000:  31%|███       | 922/3000 [06:44&lt;16:03,  2.16it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.57e+6]Epoch 922/3000:  31%|███       | 922/3000 [06:44&lt;16:03,  2.16it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.57e+6]Epoch 923/3000:  31%|███       | 922/3000 [06:44&lt;16:03,  2.16it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.57e+6]Epoch 923/3000:  31%|███       | 923/3000 [06:44&lt;14:33,  2.38it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.57e+6]Epoch 923/3000:  31%|███       | 923/3000 [06:44&lt;14:33,  2.38it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.57e+6]Epoch 924/3000:  31%|███       | 923/3000 [06:44&lt;14:33,  2.38it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.57e+6]Epoch 924/3000:  31%|███       | 924/3000 [06:44&lt;14:17,  2.42it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.57e+6]Epoch 924/3000:  31%|███       | 924/3000 [06:44&lt;14:17,  2.42it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.57e+6]Epoch 925/3000:  31%|███       | 924/3000 [06:44&lt;14:17,  2.42it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.57e+6]Epoch 925/3000:  31%|███       | 925/3000 [06:45&lt;15:44,  2.20it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.57e+6]Epoch 925/3000:  31%|███       | 925/3000 [06:45&lt;15:44,  2.20it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.57e+6]Epoch 926/3000:  31%|███       | 925/3000 [06:45&lt;15:44,  2.20it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.57e+6]Epoch 926/3000:  31%|███       | 926/3000 [06:45&lt;12:30,  2.76it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.57e+6]Epoch 926/3000:  31%|███       | 926/3000 [06:45&lt;12:30,  2.76it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.56e+6]Epoch 927/3000:  31%|███       | 926/3000 [06:45&lt;12:30,  2.76it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.56e+6]Epoch 927/3000:  31%|███       | 927/3000 [06:45&lt;12:34,  2.75it/s, v_num=1, train_loss_step=1.59e+6, train_loss_epoch=1.56e+6]Epoch 927/3000:  31%|███       | 927/3000 [06:45&lt;12:34,  2.75it/s, v_num=1, train_loss_step=1.58e+6, train_loss_epoch=1.56e+6]Epoch 928/3000:  31%|███       | 927/3000 [06:45&lt;12:34,  2.75it/s, v_num=1, train_loss_step=1.58e+6, train_loss_epoch=1.56e+6]Epoch 928/3000:  31%|███       | 928/3000 [06:46&lt;16:00,  2.16it/s, v_num=1, train_loss_step=1.58e+6, train_loss_epoch=1.56e+6]Epoch 928/3000:  31%|███       | 928/3000 [06:46&lt;16:00,  2.16it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.56e+6]Epoch 929/3000:  31%|███       | 928/3000 [06:46&lt;16:00,  2.16it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.56e+6]Epoch 929/3000:  31%|███       | 929/3000 [06:47&lt;16:53,  2.04it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.56e+6]Epoch 929/3000:  31%|███       | 929/3000 [06:47&lt;16:53,  2.04it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.56e+6]Epoch 930/3000:  31%|███       | 929/3000 [06:47&lt;16:53,  2.04it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.56e+6]Epoch 930/3000:  31%|███       | 930/3000 [06:47&lt;15:44,  2.19it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.56e+6]Epoch 930/3000:  31%|███       | 930/3000 [06:47&lt;15:44,  2.19it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.56e+6]Epoch 931/3000:  31%|███       | 930/3000 [06:47&lt;15:44,  2.19it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.56e+6]Epoch 931/3000:  31%|███       | 931/3000 [06:48&lt;15:45,  2.19it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.56e+6]Epoch 931/3000:  31%|███       | 931/3000 [06:48&lt;15:45,  2.19it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.56e+6]Epoch 932/3000:  31%|███       | 931/3000 [06:48&lt;15:45,  2.19it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.56e+6]Epoch 932/3000:  31%|███       | 932/3000 [06:48&lt;15:51,  2.17it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.56e+6]Epoch 932/3000:  31%|███       | 932/3000 [06:48&lt;15:51,  2.17it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.56e+6]Epoch 933/3000:  31%|███       | 932/3000 [06:48&lt;15:51,  2.17it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.56e+6]Epoch 933/3000:  31%|███       | 933/3000 [06:48&lt;16:06,  2.14it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.56e+6]Epoch 933/3000:  31%|███       | 933/3000 [06:48&lt;16:06,  2.14it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.56e+6]Epoch 934/3000:  31%|███       | 933/3000 [06:49&lt;16:06,  2.14it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.56e+6]Epoch 934/3000:  31%|███       | 934/3000 [06:49&lt;15:11,  2.27it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.56e+6]Epoch 934/3000:  31%|███       | 934/3000 [06:49&lt;15:11,  2.27it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.56e+6]Epoch 935/3000:  31%|███       | 934/3000 [06:49&lt;15:11,  2.27it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.56e+6]Epoch 935/3000:  31%|███       | 935/3000 [06:49&lt;15:31,  2.22it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.56e+6]Epoch 935/3000:  31%|███       | 935/3000 [06:49&lt;15:31,  2.22it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.55e+6]Epoch 936/3000:  31%|███       | 935/3000 [06:49&lt;15:31,  2.22it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.55e+6]Epoch 936/3000:  31%|███       | 936/3000 [06:50&lt;15:48,  2.18it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.55e+6]Epoch 936/3000:  31%|███       | 936/3000 [06:50&lt;15:48,  2.18it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.55e+6]Epoch 937/3000:  31%|███       | 936/3000 [06:50&lt;15:48,  2.18it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.55e+6]Epoch 937/3000:  31%|███       | 937/3000 [06:50&lt;16:36,  2.07it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.55e+6]Epoch 937/3000:  31%|███       | 937/3000 [06:50&lt;16:36,  2.07it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.55e+6]Epoch 938/3000:  31%|███       | 937/3000 [06:50&lt;16:36,  2.07it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.55e+6]Epoch 938/3000:  31%|███▏      | 938/3000 [06:51&lt;16:42,  2.06it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.55e+6]Epoch 938/3000:  31%|███▏      | 938/3000 [06:51&lt;16:42,  2.06it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.55e+6]Epoch 939/3000:  31%|███▏      | 938/3000 [06:51&lt;16:42,  2.06it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.55e+6]Epoch 939/3000:  31%|███▏      | 939/3000 [06:51&lt;16:32,  2.08it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.55e+6]Epoch 939/3000:  31%|███▏      | 939/3000 [06:51&lt;16:32,  2.08it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.55e+6]Epoch 940/3000:  31%|███▏      | 939/3000 [06:51&lt;16:32,  2.08it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.55e+6]Epoch 940/3000:  31%|███▏      | 940/3000 [06:52&lt;18:45,  1.83it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.55e+6]Epoch 940/3000:  31%|███▏      | 940/3000 [06:52&lt;18:45,  1.83it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.55e+6]Epoch 941/3000:  31%|███▏      | 940/3000 [06:52&lt;18:45,  1.83it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.55e+6]Epoch 941/3000:  31%|███▏      | 941/3000 [06:53&lt;18:17,  1.88it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.55e+6]Epoch 941/3000:  31%|███▏      | 941/3000 [06:53&lt;18:17,  1.88it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.55e+6]Epoch 942/3000:  31%|███▏      | 941/3000 [06:53&lt;18:17,  1.88it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.55e+6]Epoch 942/3000:  31%|███▏      | 942/3000 [06:53&lt;18:06,  1.89it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.55e+6]Epoch 942/3000:  31%|███▏      | 942/3000 [06:53&lt;18:06,  1.89it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.55e+6]Epoch 943/3000:  31%|███▏      | 942/3000 [06:53&lt;18:06,  1.89it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.55e+6]Epoch 943/3000:  31%|███▏      | 943/3000 [06:53&lt;15:51,  2.16it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.55e+6]Epoch 943/3000:  31%|███▏      | 943/3000 [06:53&lt;15:51,  2.16it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.55e+6]Epoch 944/3000:  31%|███▏      | 943/3000 [06:53&lt;15:51,  2.16it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.55e+6]Epoch 944/3000:  31%|███▏      | 944/3000 [06:54&lt;15:06,  2.27it/s, v_num=1, train_loss_step=1.61e+6, train_loss_epoch=1.55e+6]Epoch 944/3000:  31%|███▏      | 944/3000 [06:54&lt;15:06,  2.27it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.55e+6]Epoch 945/3000:  31%|███▏      | 944/3000 [06:54&lt;15:06,  2.27it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.55e+6]Epoch 945/3000:  32%|███▏      | 945/3000 [06:54&lt;15:17,  2.24it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.55e+6]Epoch 945/3000:  32%|███▏      | 945/3000 [06:54&lt;15:17,  2.24it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.54e+6]Epoch 946/3000:  32%|███▏      | 945/3000 [06:54&lt;15:17,  2.24it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.54e+6]Epoch 946/3000:  32%|███▏      | 946/3000 [06:55&lt;15:28,  2.21it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.54e+6]Epoch 946/3000:  32%|███▏      | 946/3000 [06:55&lt;15:28,  2.21it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.54e+6]Epoch 947/3000:  32%|███▏      | 946/3000 [06:55&lt;15:28,  2.21it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.54e+6]Epoch 947/3000:  32%|███▏      | 947/3000 [06:55&lt;15:28,  2.21it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.54e+6]Epoch 947/3000:  32%|███▏      | 947/3000 [06:55&lt;15:28,  2.21it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.54e+6]Epoch 948/3000:  32%|███▏      | 947/3000 [06:55&lt;15:28,  2.21it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.54e+6]Epoch 948/3000:  32%|███▏      | 948/3000 [06:55&lt;13:16,  2.58it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.54e+6]Epoch 948/3000:  32%|███▏      | 948/3000 [06:55&lt;13:16,  2.58it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.54e+6]Epoch 949/3000:  32%|███▏      | 948/3000 [06:55&lt;13:16,  2.58it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.54e+6]Epoch 949/3000:  32%|███▏      | 949/3000 [06:56&lt;13:44,  2.49it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.54e+6]Epoch 949/3000:  32%|███▏      | 949/3000 [06:56&lt;13:44,  2.49it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.54e+6]Epoch 950/3000:  32%|███▏      | 949/3000 [06:56&lt;13:44,  2.49it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.54e+6]Epoch 950/3000:  32%|███▏      | 950/3000 [06:56&lt;15:10,  2.25it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.54e+6]Epoch 950/3000:  32%|███▏      | 950/3000 [06:56&lt;15:10,  2.25it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.54e+6]Epoch 951/3000:  32%|███▏      | 950/3000 [06:56&lt;15:10,  2.25it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.54e+6]Epoch 951/3000:  32%|███▏      | 951/3000 [06:57&lt;15:07,  2.26it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.54e+6]Epoch 951/3000:  32%|███▏      | 951/3000 [06:57&lt;15:07,  2.26it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.54e+6]Epoch 952/3000:  32%|███▏      | 951/3000 [06:57&lt;15:07,  2.26it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.54e+6]Epoch 952/3000:  32%|███▏      | 952/3000 [06:57&lt;16:30,  2.07it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.54e+6]Epoch 952/3000:  32%|███▏      | 952/3000 [06:57&lt;16:30,  2.07it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.54e+6]Epoch 953/3000:  32%|███▏      | 952/3000 [06:57&lt;16:30,  2.07it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.54e+6]Epoch 953/3000:  32%|███▏      | 953/3000 [06:58&lt;16:08,  2.11it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.54e+6]Epoch 953/3000:  32%|███▏      | 953/3000 [06:58&lt;16:08,  2.11it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.54e+6]Epoch 954/3000:  32%|███▏      | 953/3000 [06:58&lt;16:08,  2.11it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.54e+6]Epoch 954/3000:  32%|███▏      | 954/3000 [06:58&lt;17:07,  1.99it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.54e+6]Epoch 954/3000:  32%|███▏      | 954/3000 [06:58&lt;17:07,  1.99it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.54e+6]Epoch 955/3000:  32%|███▏      | 954/3000 [06:58&lt;17:07,  1.99it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.54e+6]Epoch 955/3000:  32%|███▏      | 955/3000 [06:59&lt;15:14,  2.24it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.54e+6]Epoch 955/3000:  32%|███▏      | 955/3000 [06:59&lt;15:14,  2.24it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.54e+6]Epoch 956/3000:  32%|███▏      | 955/3000 [06:59&lt;15:14,  2.24it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.54e+6]Epoch 956/3000:  32%|███▏      | 956/3000 [06:59&lt;15:01,  2.27it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.54e+6]Epoch 956/3000:  32%|███▏      | 956/3000 [06:59&lt;15:01,  2.27it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.53e+6]Epoch 957/3000:  32%|███▏      | 956/3000 [06:59&lt;15:01,  2.27it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.53e+6]Epoch 957/3000:  32%|███▏      | 957/3000 [07:00&lt;15:59,  2.13it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.53e+6]Epoch 957/3000:  32%|███▏      | 957/3000 [07:00&lt;15:59,  2.13it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.53e+6]Epoch 958/3000:  32%|███▏      | 957/3000 [07:00&lt;15:59,  2.13it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.53e+6]Epoch 958/3000:  32%|███▏      | 958/3000 [07:00&lt;14:56,  2.28it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.53e+6]Epoch 958/3000:  32%|███▏      | 958/3000 [07:00&lt;14:56,  2.28it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.53e+6]Epoch 959/3000:  32%|███▏      | 958/3000 [07:00&lt;14:56,  2.28it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.53e+6]Epoch 959/3000:  32%|███▏      | 959/3000 [07:00&lt;13:58,  2.43it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.53e+6]Epoch 959/3000:  32%|███▏      | 959/3000 [07:00&lt;13:58,  2.43it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.53e+6]Epoch 960/3000:  32%|███▏      | 959/3000 [07:00&lt;13:58,  2.43it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.53e+6]Epoch 960/3000:  32%|███▏      | 960/3000 [07:01&lt;12:45,  2.67it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.53e+6]Epoch 960/3000:  32%|███▏      | 960/3000 [07:01&lt;12:45,  2.67it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.53e+6]Epoch 961/3000:  32%|███▏      | 960/3000 [07:01&lt;12:45,  2.67it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.53e+6]Epoch 961/3000:  32%|███▏      | 961/3000 [07:01&lt;12:56,  2.63it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.53e+6]Epoch 961/3000:  32%|███▏      | 961/3000 [07:01&lt;12:56,  2.63it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.53e+6]Epoch 962/3000:  32%|███▏      | 961/3000 [07:01&lt;12:56,  2.63it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.53e+6]Epoch 962/3000:  32%|███▏      | 962/3000 [07:01&lt;13:35,  2.50it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.53e+6]Epoch 962/3000:  32%|███▏      | 962/3000 [07:01&lt;13:35,  2.50it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.53e+6]Epoch 963/3000:  32%|███▏      | 962/3000 [07:02&lt;13:35,  2.50it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.53e+6]Epoch 963/3000:  32%|███▏      | 963/3000 [07:02&lt;14:00,  2.42it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.53e+6]Epoch 963/3000:  32%|███▏      | 963/3000 [07:02&lt;14:00,  2.42it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.53e+6]Epoch 964/3000:  32%|███▏      | 963/3000 [07:02&lt;14:00,  2.42it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.53e+6]Epoch 964/3000:  32%|███▏      | 964/3000 [07:03&lt;17:07,  1.98it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.53e+6]Epoch 964/3000:  32%|███▏      | 964/3000 [07:03&lt;17:07,  1.98it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.53e+6]Epoch 965/3000:  32%|███▏      | 964/3000 [07:03&lt;17:07,  1.98it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.53e+6]Epoch 965/3000:  32%|███▏      | 965/3000 [07:03&lt;16:10,  2.10it/s, v_num=1, train_loss_step=1.57e+6, train_loss_epoch=1.53e+6]Epoch 965/3000:  32%|███▏      | 965/3000 [07:03&lt;16:10,  2.10it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.53e+6]Epoch 966/3000:  32%|███▏      | 965/3000 [07:03&lt;16:10,  2.10it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.53e+6]Epoch 966/3000:  32%|███▏      | 966/3000 [07:04&lt;17:05,  1.98it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.53e+6]Epoch 966/3000:  32%|███▏      | 966/3000 [07:04&lt;17:05,  1.98it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.52e+6]Epoch 967/3000:  32%|███▏      | 966/3000 [07:04&lt;17:05,  1.98it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.52e+6]Epoch 967/3000:  32%|███▏      | 967/3000 [07:04&lt;16:07,  2.10it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.52e+6]Epoch 967/3000:  32%|███▏      | 967/3000 [07:04&lt;16:07,  2.10it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.52e+6]Epoch 968/3000:  32%|███▏      | 967/3000 [07:04&lt;16:07,  2.10it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.52e+6]Epoch 968/3000:  32%|███▏      | 968/3000 [07:04&lt;14:58,  2.26it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.52e+6]Epoch 968/3000:  32%|███▏      | 968/3000 [07:04&lt;14:58,  2.26it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.52e+6]Epoch 969/3000:  32%|███▏      | 968/3000 [07:04&lt;14:58,  2.26it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.52e+6]Epoch 969/3000:  32%|███▏      | 969/3000 [07:05&lt;15:27,  2.19it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.52e+6]Epoch 969/3000:  32%|███▏      | 969/3000 [07:05&lt;15:27,  2.19it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.52e+6]Epoch 970/3000:  32%|███▏      | 969/3000 [07:05&lt;15:27,  2.19it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.52e+6]Epoch 970/3000:  32%|███▏      | 970/3000 [07:05&lt;16:15,  2.08it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.52e+6]Epoch 970/3000:  32%|███▏      | 970/3000 [07:05&lt;16:15,  2.08it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.52e+6]Epoch 971/3000:  32%|███▏      | 970/3000 [07:05&lt;16:15,  2.08it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.52e+6]Epoch 971/3000:  32%|███▏      | 971/3000 [07:06&lt;15:30,  2.18it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.52e+6]Epoch 971/3000:  32%|███▏      | 971/3000 [07:06&lt;15:30,  2.18it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.52e+6]Epoch 972/3000:  32%|███▏      | 971/3000 [07:06&lt;15:30,  2.18it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.52e+6]Epoch 972/3000:  32%|███▏      | 972/3000 [07:06&lt;14:39,  2.30it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.52e+6]Epoch 972/3000:  32%|███▏      | 972/3000 [07:06&lt;14:39,  2.30it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.52e+6]Epoch 973/3000:  32%|███▏      | 972/3000 [07:06&lt;14:39,  2.30it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.52e+6]Epoch 973/3000:  32%|███▏      | 973/3000 [07:07&lt;13:25,  2.52it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.52e+6]Epoch 973/3000:  32%|███▏      | 973/3000 [07:07&lt;13:25,  2.52it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.52e+6]Epoch 974/3000:  32%|███▏      | 973/3000 [07:07&lt;13:25,  2.52it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.52e+6]Epoch 974/3000:  32%|███▏      | 974/3000 [07:07&lt;15:43,  2.15it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.52e+6]Epoch 974/3000:  32%|███▏      | 974/3000 [07:07&lt;15:43,  2.15it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.52e+6]Epoch 975/3000:  32%|███▏      | 974/3000 [07:07&lt;15:43,  2.15it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.52e+6]Epoch 975/3000:  32%|███▎      | 975/3000 [07:07&lt;14:25,  2.34it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.52e+6]Epoch 975/3000:  32%|███▎      | 975/3000 [07:07&lt;14:25,  2.34it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.52e+6]Epoch 976/3000:  32%|███▎      | 975/3000 [07:08&lt;14:25,  2.34it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.52e+6]Epoch 976/3000:  33%|███▎      | 976/3000 [07:08&lt;17:06,  1.97it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.52e+6]Epoch 976/3000:  33%|███▎      | 976/3000 [07:08&lt;17:06,  1.97it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.52e+6]Epoch 977/3000:  33%|███▎      | 976/3000 [07:08&lt;17:06,  1.97it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.52e+6]Epoch 977/3000:  33%|███▎      | 977/3000 [07:09&lt;17:13,  1.96it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.52e+6]Epoch 977/3000:  33%|███▎      | 977/3000 [07:09&lt;17:13,  1.96it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.51e+6]Epoch 978/3000:  33%|███▎      | 977/3000 [07:09&lt;17:13,  1.96it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.51e+6]Epoch 978/3000:  33%|███▎      | 978/3000 [07:09&lt;18:23,  1.83it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.51e+6]Epoch 978/3000:  33%|███▎      | 978/3000 [07:09&lt;18:23,  1.83it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.51e+6] Epoch 979/3000:  33%|███▎      | 978/3000 [07:09&lt;18:23,  1.83it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.51e+6]Epoch 979/3000:  33%|███▎      | 979/3000 [07:10&lt;16:58,  1.98it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.51e+6]Epoch 979/3000:  33%|███▎      | 979/3000 [07:10&lt;16:58,  1.98it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.51e+6]Epoch 980/3000:  33%|███▎      | 979/3000 [07:10&lt;16:58,  1.98it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.51e+6]Epoch 980/3000:  33%|███▎      | 980/3000 [07:10&lt;17:17,  1.95it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.51e+6]Epoch 980/3000:  33%|███▎      | 980/3000 [07:10&lt;17:17,  1.95it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.51e+6]Epoch 981/3000:  33%|███▎      | 980/3000 [07:10&lt;17:17,  1.95it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.51e+6]Epoch 981/3000:  33%|███▎      | 981/3000 [07:11&lt;15:15,  2.21it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.51e+6]Epoch 981/3000:  33%|███▎      | 981/3000 [07:11&lt;15:15,  2.21it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.51e+6]Epoch 982/3000:  33%|███▎      | 981/3000 [07:11&lt;15:15,  2.21it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.51e+6]Epoch 982/3000:  33%|███▎      | 982/3000 [07:11&lt;15:03,  2.23it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.51e+6]Epoch 982/3000:  33%|███▎      | 982/3000 [07:11&lt;15:03,  2.23it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.51e+6]Epoch 983/3000:  33%|███▎      | 982/3000 [07:11&lt;15:03,  2.23it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.51e+6]Epoch 983/3000:  33%|███▎      | 983/3000 [07:11&lt;13:36,  2.47it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.51e+6]Epoch 983/3000:  33%|███▎      | 983/3000 [07:11&lt;13:36,  2.47it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.51e+6]Epoch 984/3000:  33%|███▎      | 983/3000 [07:11&lt;13:36,  2.47it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.51e+6]Epoch 984/3000:  33%|███▎      | 984/3000 [07:12&lt;12:28,  2.69it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.51e+6]Epoch 984/3000:  33%|███▎      | 984/3000 [07:12&lt;12:28,  2.69it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.51e+6] Epoch 985/3000:  33%|███▎      | 984/3000 [07:12&lt;12:28,  2.69it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.51e+6]Epoch 985/3000:  33%|███▎      | 985/3000 [07:12&lt;12:46,  2.63it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.51e+6]Epoch 985/3000:  33%|███▎      | 985/3000 [07:12&lt;12:46,  2.63it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.51e+6]Epoch 986/3000:  33%|███▎      | 985/3000 [07:12&lt;12:46,  2.63it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.51e+6]Epoch 986/3000:  33%|███▎      | 986/3000 [07:13&lt;14:43,  2.28it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.51e+6]Epoch 986/3000:  33%|███▎      | 986/3000 [07:13&lt;14:43,  2.28it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.51e+6]Epoch 987/3000:  33%|███▎      | 986/3000 [07:13&lt;14:43,  2.28it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.51e+6]Epoch 987/3000:  33%|███▎      | 987/3000 [07:13&lt;16:50,  1.99it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.51e+6]Epoch 987/3000:  33%|███▎      | 987/3000 [07:13&lt;16:50,  1.99it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.51e+6]Epoch 988/3000:  33%|███▎      | 987/3000 [07:13&lt;16:50,  1.99it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.51e+6]Epoch 988/3000:  33%|███▎      | 988/3000 [07:14&lt;17:39,  1.90it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.51e+6]Epoch 988/3000:  33%|███▎      | 988/3000 [07:14&lt;17:39,  1.90it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.5e+6]  Epoch 989/3000:  33%|███▎      | 988/3000 [07:14&lt;17:39,  1.90it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.5e+6]Epoch 989/3000:  33%|███▎      | 989/3000 [07:14&lt;16:20,  2.05it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.5e+6]Epoch 989/3000:  33%|███▎      | 989/3000 [07:14&lt;16:20,  2.05it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.5e+6]Epoch 990/3000:  33%|███▎      | 989/3000 [07:14&lt;16:20,  2.05it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.5e+6]Epoch 990/3000:  33%|███▎      | 990/3000 [07:15&lt;16:57,  1.98it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.5e+6]Epoch 990/3000:  33%|███▎      | 990/3000 [07:15&lt;16:57,  1.98it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.5e+6]Epoch 991/3000:  33%|███▎      | 990/3000 [07:15&lt;16:57,  1.98it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.5e+6]Epoch 991/3000:  33%|███▎      | 991/3000 [07:15&lt;17:20,  1.93it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.5e+6]Epoch 991/3000:  33%|███▎      | 991/3000 [07:15&lt;17:20,  1.93it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.5e+6]Epoch 992/3000:  33%|███▎      | 991/3000 [07:15&lt;17:20,  1.93it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.5e+6]Epoch 992/3000:  33%|███▎      | 992/3000 [07:16&lt;17:29,  1.91it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.5e+6]Epoch 992/3000:  33%|███▎      | 992/3000 [07:16&lt;17:29,  1.91it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.5e+6]Epoch 993/3000:  33%|███▎      | 992/3000 [07:16&lt;17:29,  1.91it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.5e+6]Epoch 993/3000:  33%|███▎      | 993/3000 [07:16&lt;16:51,  1.98it/s, v_num=1, train_loss_step=1.56e+6, train_loss_epoch=1.5e+6]Epoch 993/3000:  33%|███▎      | 993/3000 [07:16&lt;16:51,  1.98it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.5e+6]Epoch 994/3000:  33%|███▎      | 993/3000 [07:16&lt;16:51,  1.98it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.5e+6]Epoch 994/3000:  33%|███▎      | 994/3000 [07:17&lt;15:59,  2.09it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.5e+6]Epoch 994/3000:  33%|███▎      | 994/3000 [07:17&lt;15:59,  2.09it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.5e+6]Epoch 995/3000:  33%|███▎      | 994/3000 [07:17&lt;15:59,  2.09it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.5e+6]Epoch 995/3000:  33%|███▎      | 995/3000 [07:17&lt;16:02,  2.08it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.5e+6]Epoch 995/3000:  33%|███▎      | 995/3000 [07:17&lt;16:02,  2.08it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.5e+6]Epoch 996/3000:  33%|███▎      | 995/3000 [07:17&lt;16:02,  2.08it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.5e+6]Epoch 996/3000:  33%|███▎      | 996/3000 [07:18&lt;17:31,  1.91it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.5e+6]Epoch 996/3000:  33%|███▎      | 996/3000 [07:18&lt;17:31,  1.91it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.5e+6]Epoch 997/3000:  33%|███▎      | 996/3000 [07:18&lt;17:31,  1.91it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.5e+6]Epoch 997/3000:  33%|███▎      | 997/3000 [07:18&lt;18:07,  1.84it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.5e+6]Epoch 997/3000:  33%|███▎      | 997/3000 [07:18&lt;18:07,  1.84it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.5e+6]Epoch 998/3000:  33%|███▎      | 997/3000 [07:18&lt;18:07,  1.84it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.5e+6]Epoch 998/3000:  33%|███▎      | 998/3000 [07:19&lt;17:59,  1.85it/s, v_num=1, train_loss_step=1.55e+6, train_loss_epoch=1.5e+6]Epoch 998/3000:  33%|███▎      | 998/3000 [07:19&lt;17:59,  1.85it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.5e+6]Epoch 999/3000:  33%|███▎      | 998/3000 [07:19&lt;17:59,  1.85it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.5e+6]Epoch 999/3000:  33%|███▎      | 999/3000 [07:20&lt;19:03,  1.75it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.5e+6]Epoch 999/3000:  33%|███▎      | 999/3000 [07:20&lt;19:03,  1.75it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.5e+6]Epoch 1000/3000:  33%|███▎      | 999/3000 [07:20&lt;19:03,  1.75it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.5e+6]Epoch 1000/3000:  33%|███▎      | 1000/3000 [07:20&lt;16:37,  2.00it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.5e+6]Epoch 1000/3000:  33%|███▎      | 1000/3000 [07:20&lt;16:37,  2.00it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.49e+6]Epoch 1001/3000:  33%|███▎      | 1000/3000 [07:20&lt;16:37,  2.00it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.49e+6]Epoch 1001/3000:  33%|███▎      | 1001/3000 [07:20&lt;16:38,  2.00it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.49e+6]Epoch 1001/3000:  33%|███▎      | 1001/3000 [07:20&lt;16:38,  2.00it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.49e+6]Epoch 1002/3000:  33%|███▎      | 1001/3000 [07:20&lt;16:38,  2.00it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.49e+6]Epoch 1002/3000:  33%|███▎      | 1002/3000 [07:21&lt;14:27,  2.30it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.49e+6]Epoch 1002/3000:  33%|███▎      | 1002/3000 [07:21&lt;14:27,  2.30it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.49e+6] Epoch 1003/3000:  33%|███▎      | 1002/3000 [07:21&lt;14:27,  2.30it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.49e+6]Epoch 1003/3000:  33%|███▎      | 1003/3000 [07:21&lt;11:34,  2.88it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.49e+6]Epoch 1003/3000:  33%|███▎      | 1003/3000 [07:21&lt;11:34,  2.88it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.49e+6]Epoch 1004/3000:  33%|███▎      | 1003/3000 [07:21&lt;11:34,  2.88it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.49e+6]Epoch 1004/3000:  33%|███▎      | 1004/3000 [07:21&lt;11:59,  2.78it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.49e+6]Epoch 1004/3000:  33%|███▎      | 1004/3000 [07:21&lt;11:59,  2.78it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.49e+6]Epoch 1005/3000:  33%|███▎      | 1004/3000 [07:21&lt;11:59,  2.78it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.49e+6]Epoch 1005/3000:  34%|███▎      | 1005/3000 [07:22&lt;12:27,  2.67it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.49e+6]Epoch 1005/3000:  34%|███▎      | 1005/3000 [07:22&lt;12:27,  2.67it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.49e+6]Epoch 1006/3000:  34%|███▎      | 1005/3000 [07:22&lt;12:27,  2.67it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.49e+6]Epoch 1006/3000:  34%|███▎      | 1006/3000 [07:22&lt;12:45,  2.60it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.49e+6]Epoch 1006/3000:  34%|███▎      | 1006/3000 [07:22&lt;12:45,  2.60it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.49e+6]Epoch 1007/3000:  34%|███▎      | 1006/3000 [07:22&lt;12:45,  2.60it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.49e+6]Epoch 1007/3000:  34%|███▎      | 1007/3000 [07:23&lt;13:12,  2.51it/s, v_num=1, train_loss_step=1.52e+6, train_loss_epoch=1.49e+6]Epoch 1007/3000:  34%|███▎      | 1007/3000 [07:23&lt;13:12,  2.51it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.49e+6]Epoch 1008/3000:  34%|███▎      | 1007/3000 [07:23&lt;13:12,  2.51it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.49e+6]Epoch 1008/3000:  34%|███▎      | 1008/3000 [07:23&lt;13:49,  2.40it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.49e+6]Epoch 1008/3000:  34%|███▎      | 1008/3000 [07:23&lt;13:49,  2.40it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.49e+6]Epoch 1009/3000:  34%|███▎      | 1008/3000 [07:23&lt;13:49,  2.40it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.49e+6]Epoch 1009/3000:  34%|███▎      | 1009/3000 [07:24&lt;16:09,  2.05it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.49e+6]Epoch 1009/3000:  34%|███▎      | 1009/3000 [07:24&lt;16:09,  2.05it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.49e+6] Epoch 1010/3000:  34%|███▎      | 1009/3000 [07:24&lt;16:09,  2.05it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.49e+6]Epoch 1010/3000:  34%|███▎      | 1010/3000 [07:24&lt;17:41,  1.88it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.49e+6]Epoch 1010/3000:  34%|███▎      | 1010/3000 [07:24&lt;17:41,  1.88it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.49e+6]Epoch 1011/3000:  34%|███▎      | 1010/3000 [07:24&lt;17:41,  1.88it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.49e+6]Epoch 1011/3000:  34%|███▎      | 1011/3000 [07:25&lt;16:38,  1.99it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.49e+6]Epoch 1011/3000:  34%|███▎      | 1011/3000 [07:25&lt;16:38,  1.99it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.48e+6]Epoch 1012/3000:  34%|███▎      | 1011/3000 [07:25&lt;16:38,  1.99it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.48e+6]Epoch 1012/3000:  34%|███▎      | 1012/3000 [07:25&lt;16:26,  2.02it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.48e+6]Epoch 1012/3000:  34%|███▎      | 1012/3000 [07:25&lt;16:26,  2.02it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.48e+6]Epoch 1013/3000:  34%|███▎      | 1012/3000 [07:25&lt;16:26,  2.02it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.48e+6]Epoch 1013/3000:  34%|███▍      | 1013/3000 [07:26&lt;15:42,  2.11it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.48e+6]Epoch 1013/3000:  34%|███▍      | 1013/3000 [07:26&lt;15:42,  2.11it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.48e+6]Epoch 1014/3000:  34%|███▍      | 1013/3000 [07:26&lt;15:42,  2.11it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.48e+6]Epoch 1014/3000:  34%|███▍      | 1014/3000 [07:26&lt;15:27,  2.14it/s, v_num=1, train_loss_step=1.54e+6, train_loss_epoch=1.48e+6]Epoch 1014/3000:  34%|███▍      | 1014/3000 [07:26&lt;15:27,  2.14it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.48e+6]Epoch 1015/3000:  34%|███▍      | 1014/3000 [07:26&lt;15:27,  2.14it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.48e+6]Epoch 1015/3000:  34%|███▍      | 1015/3000 [07:27&lt;16:30,  2.00it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.48e+6]Epoch 1015/3000:  34%|███▍      | 1015/3000 [07:27&lt;16:30,  2.00it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.48e+6]Epoch 1016/3000:  34%|███▍      | 1015/3000 [07:27&lt;16:30,  2.00it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.48e+6]Epoch 1016/3000:  34%|███▍      | 1016/3000 [07:27&lt;16:00,  2.07it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.48e+6]Epoch 1016/3000:  34%|███▍      | 1016/3000 [07:27&lt;16:00,  2.07it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.48e+6]Epoch 1017/3000:  34%|███▍      | 1016/3000 [07:27&lt;16:00,  2.07it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.48e+6]Epoch 1017/3000:  34%|███▍      | 1017/3000 [07:28&lt;15:32,  2.13it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.48e+6]Epoch 1017/3000:  34%|███▍      | 1017/3000 [07:28&lt;15:32,  2.13it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.48e+6]Epoch 1018/3000:  34%|███▍      | 1017/3000 [07:28&lt;15:32,  2.13it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.48e+6]Epoch 1018/3000:  34%|███▍      | 1018/3000 [07:28&lt;14:43,  2.24it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.48e+6]Epoch 1018/3000:  34%|███▍      | 1018/3000 [07:28&lt;14:43,  2.24it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.48e+6]Epoch 1019/3000:  34%|███▍      | 1018/3000 [07:28&lt;14:43,  2.24it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.48e+6]Epoch 1019/3000:  34%|███▍      | 1019/3000 [07:28&lt;16:01,  2.06it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.48e+6]Epoch 1019/3000:  34%|███▍      | 1019/3000 [07:28&lt;16:01,  2.06it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.48e+6]Epoch 1020/3000:  34%|███▍      | 1019/3000 [07:28&lt;16:01,  2.06it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.48e+6]Epoch 1020/3000:  34%|███▍      | 1020/3000 [07:29&lt;17:00,  1.94it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.48e+6]Epoch 1020/3000:  34%|███▍      | 1020/3000 [07:29&lt;17:00,  1.94it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.48e+6]Epoch 1021/3000:  34%|███▍      | 1020/3000 [07:29&lt;17:00,  1.94it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.48e+6]Epoch 1021/3000:  34%|███▍      | 1021/3000 [07:30&lt;19:10,  1.72it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.48e+6]Epoch 1021/3000:  34%|███▍      | 1021/3000 [07:30&lt;19:10,  1.72it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.48e+6]Epoch 1022/3000:  34%|███▍      | 1021/3000 [07:30&lt;19:10,  1.72it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.48e+6]Epoch 1022/3000:  34%|███▍      | 1022/3000 [07:30&lt;18:56,  1.74it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.48e+6]Epoch 1022/3000:  34%|███▍      | 1022/3000 [07:30&lt;18:56,  1.74it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.48e+6]Epoch 1023/3000:  34%|███▍      | 1022/3000 [07:30&lt;18:56,  1.74it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.48e+6]Epoch 1023/3000:  34%|███▍      | 1023/3000 [07:31&lt;18:42,  1.76it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.48e+6]Epoch 1023/3000:  34%|███▍      | 1023/3000 [07:31&lt;18:42,  1.76it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.47e+6] Epoch 1024/3000:  34%|███▍      | 1023/3000 [07:31&lt;18:42,  1.76it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.47e+6]Epoch 1024/3000:  34%|███▍      | 1024/3000 [07:31&lt;16:27,  2.00it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.47e+6]Epoch 1024/3000:  34%|███▍      | 1024/3000 [07:31&lt;16:27,  2.00it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.47e+6]Epoch 1025/3000:  34%|███▍      | 1024/3000 [07:31&lt;16:27,  2.00it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.47e+6]Epoch 1025/3000:  34%|███▍      | 1025/3000 [07:32&lt;16:01,  2.06it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.47e+6]Epoch 1025/3000:  34%|███▍      | 1025/3000 [07:32&lt;16:01,  2.06it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.47e+6]Epoch 1026/3000:  34%|███▍      | 1025/3000 [07:32&lt;16:01,  2.06it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.47e+6]Epoch 1026/3000:  34%|███▍      | 1026/3000 [07:32&lt;15:57,  2.06it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.47e+6]Epoch 1026/3000:  34%|███▍      | 1026/3000 [07:32&lt;15:57,  2.06it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.47e+6]Epoch 1027/3000:  34%|███▍      | 1026/3000 [07:32&lt;15:57,  2.06it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.47e+6]Epoch 1027/3000:  34%|███▍      | 1027/3000 [07:33&lt;16:02,  2.05it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.47e+6]Epoch 1027/3000:  34%|███▍      | 1027/3000 [07:33&lt;16:02,  2.05it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.47e+6]Epoch 1028/3000:  34%|███▍      | 1027/3000 [07:33&lt;16:02,  2.05it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.47e+6]Epoch 1028/3000:  34%|███▍      | 1028/3000 [07:33&lt;15:24,  2.13it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.47e+6]Epoch 1028/3000:  34%|███▍      | 1028/3000 [07:33&lt;15:24,  2.13it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.47e+6]Epoch 1029/3000:  34%|███▍      | 1028/3000 [07:33&lt;15:24,  2.13it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.47e+6]Epoch 1029/3000:  34%|███▍      | 1029/3000 [07:34&lt;15:04,  2.18it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.47e+6]Epoch 1029/3000:  34%|███▍      | 1029/3000 [07:34&lt;15:04,  2.18it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.47e+6]Epoch 1030/3000:  34%|███▍      | 1029/3000 [07:34&lt;15:04,  2.18it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.47e+6]Epoch 1030/3000:  34%|███▍      | 1030/3000 [07:34&lt;15:09,  2.17it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.47e+6]Epoch 1030/3000:  34%|███▍      | 1030/3000 [07:34&lt;15:09,  2.17it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.47e+6]Epoch 1031/3000:  34%|███▍      | 1030/3000 [07:34&lt;15:09,  2.17it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.47e+6]Epoch 1031/3000:  34%|███▍      | 1031/3000 [07:34&lt;14:16,  2.30it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.47e+6]Epoch 1031/3000:  34%|███▍      | 1031/3000 [07:34&lt;14:16,  2.30it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.47e+6]Epoch 1032/3000:  34%|███▍      | 1031/3000 [07:34&lt;14:16,  2.30it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.47e+6]Epoch 1032/3000:  34%|███▍      | 1032/3000 [07:35&lt;15:39,  2.10it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.47e+6]Epoch 1032/3000:  34%|███▍      | 1032/3000 [07:35&lt;15:39,  2.10it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.47e+6]Epoch 1033/3000:  34%|███▍      | 1032/3000 [07:35&lt;15:39,  2.10it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.47e+6]Epoch 1033/3000:  34%|███▍      | 1033/3000 [07:35&lt;15:14,  2.15it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.47e+6]Epoch 1033/3000:  34%|███▍      | 1033/3000 [07:35&lt;15:14,  2.15it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.47e+6]Epoch 1034/3000:  34%|███▍      | 1033/3000 [07:35&lt;15:14,  2.15it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.47e+6]Epoch 1034/3000:  34%|███▍      | 1034/3000 [07:36&lt;13:30,  2.43it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.47e+6]Epoch 1034/3000:  34%|███▍      | 1034/3000 [07:36&lt;13:30,  2.43it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.47e+6]Epoch 1035/3000:  34%|███▍      | 1034/3000 [07:36&lt;13:30,  2.43it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.47e+6]Epoch 1035/3000:  34%|███▍      | 1035/3000 [07:36&lt;11:23,  2.87it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.47e+6]Epoch 1035/3000:  34%|███▍      | 1035/3000 [07:36&lt;11:23,  2.87it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.47e+6]Epoch 1036/3000:  34%|███▍      | 1035/3000 [07:36&lt;11:23,  2.87it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.47e+6]Epoch 1036/3000:  35%|███▍      | 1036/3000 [07:36&lt;11:42,  2.80it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.47e+6]Epoch 1036/3000:  35%|███▍      | 1036/3000 [07:36&lt;11:42,  2.80it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.46e+6]Epoch 1037/3000:  35%|███▍      | 1036/3000 [07:36&lt;11:42,  2.80it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.46e+6]Epoch 1037/3000:  35%|███▍      | 1037/3000 [07:37&lt;13:28,  2.43it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.46e+6]Epoch 1037/3000:  35%|███▍      | 1037/3000 [07:37&lt;13:28,  2.43it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.46e+6]Epoch 1038/3000:  35%|███▍      | 1037/3000 [07:37&lt;13:28,  2.43it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.46e+6]Epoch 1038/3000:  35%|███▍      | 1038/3000 [07:37&lt;14:20,  2.28it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.46e+6]Epoch 1038/3000:  35%|███▍      | 1038/3000 [07:37&lt;14:20,  2.28it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.46e+6] Epoch 1039/3000:  35%|███▍      | 1038/3000 [07:37&lt;14:20,  2.28it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.46e+6]Epoch 1039/3000:  35%|███▍      | 1039/3000 [07:38&lt;13:33,  2.41it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.46e+6]Epoch 1039/3000:  35%|███▍      | 1039/3000 [07:38&lt;13:33,  2.41it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.46e+6]Epoch 1040/3000:  35%|███▍      | 1039/3000 [07:38&lt;13:33,  2.41it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.46e+6]Epoch 1040/3000:  35%|███▍      | 1040/3000 [07:38&lt;11:38,  2.81it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.46e+6]Epoch 1040/3000:  35%|███▍      | 1040/3000 [07:38&lt;11:38,  2.81it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.46e+6]Epoch 1041/3000:  35%|███▍      | 1040/3000 [07:38&lt;11:38,  2.81it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.46e+6]Epoch 1041/3000:  35%|███▍      | 1041/3000 [07:38&lt;13:23,  2.44it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.46e+6]Epoch 1041/3000:  35%|███▍      | 1041/3000 [07:38&lt;13:23,  2.44it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.46e+6]Epoch 1042/3000:  35%|███▍      | 1041/3000 [07:38&lt;13:23,  2.44it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.46e+6]Epoch 1042/3000:  35%|███▍      | 1042/3000 [07:39&lt;14:23,  2.27it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.46e+6]Epoch 1042/3000:  35%|███▍      | 1042/3000 [07:39&lt;14:23,  2.27it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.46e+6]Epoch 1043/3000:  35%|███▍      | 1042/3000 [07:39&lt;14:23,  2.27it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.46e+6]Epoch 1043/3000:  35%|███▍      | 1043/3000 [07:40&lt;16:28,  1.98it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.46e+6]Epoch 1043/3000:  35%|███▍      | 1043/3000 [07:40&lt;16:28,  1.98it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.46e+6] Epoch 1044/3000:  35%|███▍      | 1043/3000 [07:40&lt;16:28,  1.98it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.46e+6]Epoch 1044/3000:  35%|███▍      | 1044/3000 [07:40&lt;15:51,  2.06it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.46e+6]Epoch 1044/3000:  35%|███▍      | 1044/3000 [07:40&lt;15:51,  2.06it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.46e+6]Epoch 1045/3000:  35%|███▍      | 1044/3000 [07:40&lt;15:51,  2.06it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.46e+6]Epoch 1045/3000:  35%|███▍      | 1045/3000 [07:41&lt;16:51,  1.93it/s, v_num=1, train_loss_step=1.51e+6, train_loss_epoch=1.46e+6]Epoch 1045/3000:  35%|███▍      | 1045/3000 [07:41&lt;16:51,  1.93it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.46e+6]Epoch 1046/3000:  35%|███▍      | 1045/3000 [07:41&lt;16:51,  1.93it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.46e+6]Epoch 1046/3000:  35%|███▍      | 1046/3000 [07:41&lt;17:07,  1.90it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.46e+6]Epoch 1046/3000:  35%|███▍      | 1046/3000 [07:41&lt;17:07,  1.90it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.46e+6]Epoch 1047/3000:  35%|███▍      | 1046/3000 [07:41&lt;17:07,  1.90it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.46e+6]Epoch 1047/3000:  35%|███▍      | 1047/3000 [07:42&lt;16:46,  1.94it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.46e+6]Epoch 1047/3000:  35%|███▍      | 1047/3000 [07:42&lt;16:46,  1.94it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.46e+6]Epoch 1048/3000:  35%|███▍      | 1047/3000 [07:42&lt;16:46,  1.94it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.46e+6]Epoch 1048/3000:  35%|███▍      | 1048/3000 [07:42&lt;15:26,  2.11it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.46e+6]Epoch 1048/3000:  35%|███▍      | 1048/3000 [07:42&lt;15:26,  2.11it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.46e+6] Epoch 1049/3000:  35%|███▍      | 1048/3000 [07:42&lt;15:26,  2.11it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.46e+6]Epoch 1049/3000:  35%|███▍      | 1049/3000 [07:43&lt;15:31,  2.10it/s, v_num=1, train_loss_step=1.5e+6, train_loss_epoch=1.46e+6]Epoch 1049/3000:  35%|███▍      | 1049/3000 [07:43&lt;15:31,  2.10it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.45e+6]Epoch 1050/3000:  35%|███▍      | 1049/3000 [07:43&lt;15:31,  2.10it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.45e+6]Epoch 1050/3000:  35%|███▌      | 1050/3000 [07:43&lt;14:29,  2.24it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.45e+6]Epoch 1050/3000:  35%|███▌      | 1050/3000 [07:43&lt;14:29,  2.24it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.45e+6]Epoch 1051/3000:  35%|███▌      | 1050/3000 [07:43&lt;14:29,  2.24it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.45e+6]Epoch 1051/3000:  35%|███▌      | 1051/3000 [07:43&lt;15:59,  2.03it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.45e+6]Epoch 1051/3000:  35%|███▌      | 1051/3000 [07:44&lt;15:59,  2.03it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.45e+6]Epoch 1052/3000:  35%|███▌      | 1051/3000 [07:44&lt;15:59,  2.03it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.45e+6]Epoch 1052/3000:  35%|███▌      | 1052/3000 [07:44&lt;16:30,  1.97it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.45e+6]Epoch 1052/3000:  35%|███▌      | 1052/3000 [07:44&lt;16:30,  1.97it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.45e+6]Epoch 1053/3000:  35%|███▌      | 1052/3000 [07:44&lt;16:30,  1.97it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.45e+6]Epoch 1053/3000:  35%|███▌      | 1053/3000 [07:45&lt;17:39,  1.84it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.45e+6]Epoch 1053/3000:  35%|███▌      | 1053/3000 [07:45&lt;17:39,  1.84it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.45e+6]Epoch 1054/3000:  35%|███▌      | 1053/3000 [07:45&lt;17:39,  1.84it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.45e+6]Epoch 1054/3000:  35%|███▌      | 1054/3000 [07:45&lt;18:03,  1.80it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.45e+6]Epoch 1054/3000:  35%|███▌      | 1054/3000 [07:45&lt;18:03,  1.80it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.45e+6]Epoch 1055/3000:  35%|███▌      | 1054/3000 [07:45&lt;18:03,  1.80it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.45e+6]Epoch 1055/3000:  35%|███▌      | 1055/3000 [07:46&lt;17:28,  1.85it/s, v_num=1, train_loss_step=1.53e+6, train_loss_epoch=1.45e+6]Epoch 1055/3000:  35%|███▌      | 1055/3000 [07:46&lt;17:28,  1.85it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.45e+6]Epoch 1056/3000:  35%|███▌      | 1055/3000 [07:46&lt;17:28,  1.85it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.45e+6]Epoch 1056/3000:  35%|███▌      | 1056/3000 [07:46&lt;17:18,  1.87it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.45e+6]Epoch 1056/3000:  35%|███▌      | 1056/3000 [07:46&lt;17:18,  1.87it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.45e+6]Epoch 1057/3000:  35%|███▌      | 1056/3000 [07:46&lt;17:18,  1.87it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.45e+6]Epoch 1057/3000:  35%|███▌      | 1057/3000 [07:47&lt;16:19,  1.98it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.45e+6]Epoch 1057/3000:  35%|███▌      | 1057/3000 [07:47&lt;16:19,  1.98it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.45e+6]Epoch 1058/3000:  35%|███▌      | 1057/3000 [07:47&lt;16:19,  1.98it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.45e+6]Epoch 1058/3000:  35%|███▌      | 1058/3000 [07:47&lt;16:43,  1.94it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.45e+6]Epoch 1058/3000:  35%|███▌      | 1058/3000 [07:47&lt;16:43,  1.94it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.45e+6]Epoch 1059/3000:  35%|███▌      | 1058/3000 [07:47&lt;16:43,  1.94it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.45e+6]Epoch 1059/3000:  35%|███▌      | 1059/3000 [07:48&lt;15:13,  2.13it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.45e+6]Epoch 1059/3000:  35%|███▌      | 1059/3000 [07:48&lt;15:13,  2.13it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.45e+6]Epoch 1060/3000:  35%|███▌      | 1059/3000 [07:48&lt;15:13,  2.13it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.45e+6]Epoch 1060/3000:  35%|███▌      | 1060/3000 [07:48&lt;13:50,  2.34it/s, v_num=1, train_loss_step=1.49e+6, train_loss_epoch=1.45e+6]Epoch 1060/3000:  35%|███▌      | 1060/3000 [07:48&lt;13:50,  2.34it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.45e+6]Epoch 1061/3000:  35%|███▌      | 1060/3000 [07:48&lt;13:50,  2.34it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.45e+6]Epoch 1061/3000:  35%|███▌      | 1061/3000 [07:48&lt;13:39,  2.37it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.45e+6]Epoch 1061/3000:  35%|███▌      | 1061/3000 [07:48&lt;13:39,  2.37it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.45e+6]Epoch 1062/3000:  35%|███▌      | 1061/3000 [07:48&lt;13:39,  2.37it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.45e+6]Epoch 1062/3000:  35%|███▌      | 1062/3000 [07:49&lt;12:41,  2.54it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.45e+6]Epoch 1062/3000:  35%|███▌      | 1062/3000 [07:49&lt;12:41,  2.54it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.44e+6]Epoch 1063/3000:  35%|███▌      | 1062/3000 [07:49&lt;12:41,  2.54it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.44e+6]Epoch 1063/3000:  35%|███▌      | 1063/3000 [07:49&lt;12:19,  2.62it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.44e+6]Epoch 1063/3000:  35%|███▌      | 1063/3000 [07:49&lt;12:19,  2.62it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.44e+6]Epoch 1064/3000:  35%|███▌      | 1063/3000 [07:49&lt;12:19,  2.62it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.44e+6]Epoch 1064/3000:  35%|███▌      | 1064/3000 [07:49&lt;11:20,  2.85it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.44e+6]Epoch 1064/3000:  35%|███▌      | 1064/3000 [07:49&lt;11:20,  2.85it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.44e+6]Epoch 1065/3000:  35%|███▌      | 1064/3000 [07:49&lt;11:20,  2.85it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.44e+6]Epoch 1065/3000:  36%|███▌      | 1065/3000 [07:50&lt;11:45,  2.74it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.44e+6]Epoch 1065/3000:  36%|███▌      | 1065/3000 [07:50&lt;11:45,  2.74it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.44e+6] Epoch 1066/3000:  36%|███▌      | 1065/3000 [07:50&lt;11:45,  2.74it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.44e+6]Epoch 1066/3000:  36%|███▌      | 1066/3000 [07:50&lt;11:05,  2.90it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.44e+6]Epoch 1066/3000:  36%|███▌      | 1066/3000 [07:50&lt;11:05,  2.90it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.44e+6]Epoch 1067/3000:  36%|███▌      | 1066/3000 [07:50&lt;11:05,  2.90it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.44e+6]Epoch 1067/3000:  36%|███▌      | 1067/3000 [07:50&lt;09:30,  3.39it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.44e+6]Epoch 1067/3000:  36%|███▌      | 1067/3000 [07:50&lt;09:30,  3.39it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.44e+6]Epoch 1068/3000:  36%|███▌      | 1067/3000 [07:50&lt;09:30,  3.39it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.44e+6]Epoch 1068/3000:  36%|███▌      | 1068/3000 [07:50&lt;08:35,  3.75it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.44e+6]Epoch 1068/3000:  36%|███▌      | 1068/3000 [07:50&lt;08:35,  3.75it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.44e+6]Epoch 1069/3000:  36%|███▌      | 1068/3000 [07:50&lt;08:35,  3.75it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.44e+6]Epoch 1069/3000:  36%|███▌      | 1069/3000 [07:51&lt;07:15,  4.43it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.44e+6]Epoch 1069/3000:  36%|███▌      | 1069/3000 [07:51&lt;07:15,  4.43it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.44e+6]Epoch 1070/3000:  36%|███▌      | 1069/3000 [07:51&lt;07:15,  4.43it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.44e+6]Epoch 1070/3000:  36%|███▌      | 1070/3000 [07:51&lt;06:29,  4.95it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.44e+6]Epoch 1070/3000:  36%|███▌      | 1070/3000 [07:51&lt;06:29,  4.95it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.44e+6]Epoch 1071/3000:  36%|███▌      | 1070/3000 [07:51&lt;06:29,  4.95it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.44e+6]Epoch 1071/3000:  36%|███▌      | 1071/3000 [07:51&lt;09:10,  3.50it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.44e+6]Epoch 1071/3000:  36%|███▌      | 1071/3000 [07:51&lt;09:10,  3.50it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.44e+6]Epoch 1072/3000:  36%|███▌      | 1071/3000 [07:51&lt;09:10,  3.50it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.44e+6]Epoch 1072/3000:  36%|███▌      | 1072/3000 [07:52&lt;10:50,  2.96it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.44e+6]Epoch 1072/3000:  36%|███▌      | 1072/3000 [07:52&lt;10:50,  2.96it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.44e+6]Epoch 1073/3000:  36%|███▌      | 1072/3000 [07:52&lt;10:50,  2.96it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.44e+6]Epoch 1073/3000:  36%|███▌      | 1073/3000 [07:52&lt;13:12,  2.43it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.44e+6]Epoch 1073/3000:  36%|███▌      | 1073/3000 [07:52&lt;13:12,  2.43it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.44e+6] Epoch 1074/3000:  36%|███▌      | 1073/3000 [07:52&lt;13:12,  2.43it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.44e+6]Epoch 1074/3000:  36%|███▌      | 1074/3000 [07:53&lt;12:33,  2.56it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.44e+6]Epoch 1074/3000:  36%|███▌      | 1074/3000 [07:53&lt;12:33,  2.56it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.44e+6]Epoch 1075/3000:  36%|███▌      | 1074/3000 [07:53&lt;12:33,  2.56it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.44e+6]Epoch 1075/3000:  36%|███▌      | 1075/3000 [07:53&lt;12:22,  2.59it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.44e+6]Epoch 1075/3000:  36%|███▌      | 1075/3000 [07:53&lt;12:22,  2.59it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.44e+6]Epoch 1076/3000:  36%|███▌      | 1075/3000 [07:53&lt;12:22,  2.59it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.44e+6]Epoch 1076/3000:  36%|███▌      | 1076/3000 [07:53&lt;12:47,  2.51it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.44e+6]Epoch 1076/3000:  36%|███▌      | 1076/3000 [07:53&lt;12:47,  2.51it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.43e+6]Epoch 1077/3000:  36%|███▌      | 1076/3000 [07:53&lt;12:47,  2.51it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.43e+6]Epoch 1077/3000:  36%|███▌      | 1077/3000 [07:54&lt;14:29,  2.21it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.43e+6]Epoch 1077/3000:  36%|███▌      | 1077/3000 [07:54&lt;14:29,  2.21it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.43e+6]Epoch 1078/3000:  36%|███▌      | 1077/3000 [07:54&lt;14:29,  2.21it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.43e+6]Epoch 1078/3000:  36%|███▌      | 1078/3000 [07:54&lt;13:14,  2.42it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.43e+6]Epoch 1078/3000:  36%|███▌      | 1078/3000 [07:54&lt;13:14,  2.42it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.43e+6]Epoch 1079/3000:  36%|███▌      | 1078/3000 [07:54&lt;13:14,  2.42it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.43e+6]Epoch 1079/3000:  36%|███▌      | 1079/3000 [07:55&lt;12:57,  2.47it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.43e+6]Epoch 1079/3000:  36%|███▌      | 1079/3000 [07:55&lt;12:57,  2.47it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.43e+6]Epoch 1080/3000:  36%|███▌      | 1079/3000 [07:55&lt;12:57,  2.47it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.43e+6]Epoch 1080/3000:  36%|███▌      | 1080/3000 [07:55&lt;11:57,  2.67it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.43e+6]Epoch 1080/3000:  36%|███▌      | 1080/3000 [07:55&lt;11:57,  2.67it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.43e+6]Epoch 1081/3000:  36%|███▌      | 1080/3000 [07:55&lt;11:57,  2.67it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.43e+6]Epoch 1081/3000:  36%|███▌      | 1081/3000 [07:55&lt;12:01,  2.66it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.43e+6]Epoch 1081/3000:  36%|███▌      | 1081/3000 [07:55&lt;12:01,  2.66it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.43e+6]Epoch 1082/3000:  36%|███▌      | 1081/3000 [07:55&lt;12:01,  2.66it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.43e+6]Epoch 1082/3000:  36%|███▌      | 1082/3000 [07:56&lt;12:29,  2.56it/s, v_num=1, train_loss_step=1.46e+6, train_loss_epoch=1.43e+6]Epoch 1082/3000:  36%|███▌      | 1082/3000 [07:56&lt;12:29,  2.56it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.43e+6]Epoch 1083/3000:  36%|███▌      | 1082/3000 [07:56&lt;12:29,  2.56it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.43e+6]Epoch 1083/3000:  36%|███▌      | 1083/3000 [07:56&lt;12:11,  2.62it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.43e+6]Epoch 1083/3000:  36%|███▌      | 1083/3000 [07:56&lt;12:11,  2.62it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.43e+6]Epoch 1084/3000:  36%|███▌      | 1083/3000 [07:56&lt;12:11,  2.62it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.43e+6]Epoch 1084/3000:  36%|███▌      | 1084/3000 [07:57&lt;14:35,  2.19it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.43e+6]Epoch 1084/3000:  36%|███▌      | 1084/3000 [07:57&lt;14:35,  2.19it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.43e+6]Epoch 1085/3000:  36%|███▌      | 1084/3000 [07:57&lt;14:35,  2.19it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.43e+6]Epoch 1085/3000:  36%|███▌      | 1085/3000 [07:57&lt;14:12,  2.25it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.43e+6]Epoch 1085/3000:  36%|███▌      | 1085/3000 [07:57&lt;14:12,  2.25it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.43e+6]Epoch 1086/3000:  36%|███▌      | 1085/3000 [07:57&lt;14:12,  2.25it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.43e+6]Epoch 1086/3000:  36%|███▌      | 1086/3000 [07:58&lt;14:11,  2.25it/s, v_num=1, train_loss_step=1.48e+6, train_loss_epoch=1.43e+6]Epoch 1086/3000:  36%|███▌      | 1086/3000 [07:58&lt;14:11,  2.25it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.43e+6]Epoch 1087/3000:  36%|███▌      | 1086/3000 [07:58&lt;14:11,  2.25it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.43e+6]Epoch 1087/3000:  36%|███▌      | 1087/3000 [07:58&lt;14:44,  2.16it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.43e+6]Epoch 1087/3000:  36%|███▌      | 1087/3000 [07:58&lt;14:44,  2.16it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.43e+6]Epoch 1088/3000:  36%|███▌      | 1087/3000 [07:58&lt;14:44,  2.16it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.43e+6]Epoch 1088/3000:  36%|███▋      | 1088/3000 [07:59&lt;15:05,  2.11it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.43e+6]Epoch 1088/3000:  36%|███▋      | 1088/3000 [07:59&lt;15:05,  2.11it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.43e+6]Epoch 1089/3000:  36%|███▋      | 1088/3000 [07:59&lt;15:05,  2.11it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.43e+6]Epoch 1089/3000:  36%|███▋      | 1089/3000 [07:59&lt;14:19,  2.22it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.43e+6]Epoch 1089/3000:  36%|███▋      | 1089/3000 [07:59&lt;14:19,  2.22it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.43e+6]Epoch 1090/3000:  36%|███▋      | 1089/3000 [07:59&lt;14:19,  2.22it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.43e+6]Epoch 1090/3000:  36%|███▋      | 1090/3000 [08:00&lt;15:20,  2.08it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.43e+6]Epoch 1090/3000:  36%|███▋      | 1090/3000 [08:00&lt;15:20,  2.08it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.42e+6]Epoch 1091/3000:  36%|███▋      | 1090/3000 [08:00&lt;15:20,  2.08it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.42e+6]Epoch 1091/3000:  36%|███▋      | 1091/3000 [08:00&lt;15:14,  2.09it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.42e+6]Epoch 1091/3000:  36%|███▋      | 1091/3000 [08:00&lt;15:14,  2.09it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.42e+6]Epoch 1092/3000:  36%|███▋      | 1091/3000 [08:00&lt;15:14,  2.09it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.42e+6]Epoch 1092/3000:  36%|███▋      | 1092/3000 [08:00&lt;15:16,  2.08it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.42e+6]Epoch 1092/3000:  36%|███▋      | 1092/3000 [08:00&lt;15:16,  2.08it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.42e+6]Epoch 1093/3000:  36%|███▋      | 1092/3000 [08:00&lt;15:16,  2.08it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.42e+6]Epoch 1093/3000:  36%|███▋      | 1093/3000 [08:01&lt;16:04,  1.98it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.42e+6]Epoch 1093/3000:  36%|███▋      | 1093/3000 [08:01&lt;16:04,  1.98it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.42e+6] Epoch 1094/3000:  36%|███▋      | 1093/3000 [08:01&lt;16:04,  1.98it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.42e+6]Epoch 1094/3000:  36%|███▋      | 1094/3000 [08:02&lt;16:14,  1.96it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.42e+6]Epoch 1094/3000:  36%|███▋      | 1094/3000 [08:02&lt;16:14,  1.96it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.42e+6]Epoch 1095/3000:  36%|███▋      | 1094/3000 [08:02&lt;16:14,  1.96it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.42e+6]Epoch 1095/3000:  36%|███▋      | 1095/3000 [08:02&lt;16:43,  1.90it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.42e+6]Epoch 1095/3000:  36%|███▋      | 1095/3000 [08:02&lt;16:43,  1.90it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.42e+6]Epoch 1096/3000:  36%|███▋      | 1095/3000 [08:02&lt;16:43,  1.90it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.42e+6]Epoch 1096/3000:  37%|███▋      | 1096/3000 [08:03&lt;15:59,  1.98it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.42e+6]Epoch 1096/3000:  37%|███▋      | 1096/3000 [08:03&lt;15:59,  1.98it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.42e+6]Epoch 1097/3000:  37%|███▋      | 1096/3000 [08:03&lt;15:59,  1.98it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.42e+6]Epoch 1097/3000:  37%|███▋      | 1097/3000 [08:03&lt;16:11,  1.96it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.42e+6]Epoch 1097/3000:  37%|███▋      | 1097/3000 [08:03&lt;16:11,  1.96it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.42e+6]Epoch 1098/3000:  37%|███▋      | 1097/3000 [08:03&lt;16:11,  1.96it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.42e+6]Epoch 1098/3000:  37%|███▋      | 1098/3000 [08:04&lt;16:22,  1.94it/s, v_num=1, train_loss_step=1.47e+6, train_loss_epoch=1.42e+6]Epoch 1098/3000:  37%|███▋      | 1098/3000 [08:04&lt;16:22,  1.94it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.42e+6]Epoch 1099/3000:  37%|███▋      | 1098/3000 [08:04&lt;16:22,  1.94it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.42e+6]Epoch 1099/3000:  37%|███▋      | 1099/3000 [08:04&lt;15:24,  2.06it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.42e+6]Epoch 1099/3000:  37%|███▋      | 1099/3000 [08:04&lt;15:24,  2.06it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.42e+6]Epoch 1100/3000:  37%|███▋      | 1099/3000 [08:04&lt;15:24,  2.06it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.42e+6]Epoch 1100/3000:  37%|███▋      | 1100/3000 [08:05&lt;16:13,  1.95it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.42e+6]Epoch 1100/3000:  37%|███▋      | 1100/3000 [08:05&lt;16:13,  1.95it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.42e+6]Epoch 1101/3000:  37%|███▋      | 1100/3000 [08:05&lt;16:13,  1.95it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.42e+6]Epoch 1101/3000:  37%|███▋      | 1101/3000 [08:05&lt;14:49,  2.14it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.42e+6]Epoch 1101/3000:  37%|███▋      | 1101/3000 [08:05&lt;14:49,  2.14it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.42e+6]Epoch 1102/3000:  37%|███▋      | 1101/3000 [08:05&lt;14:49,  2.14it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.42e+6]Epoch 1102/3000:  37%|███▋      | 1102/3000 [08:05&lt;13:19,  2.37it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.42e+6]Epoch 1102/3000:  37%|███▋      | 1102/3000 [08:05&lt;13:19,  2.37it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.42e+6]Epoch 1103/3000:  37%|███▋      | 1102/3000 [08:05&lt;13:19,  2.37it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.42e+6]Epoch 1103/3000:  37%|███▋      | 1103/3000 [08:05&lt;11:06,  2.85it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.42e+6]Epoch 1103/3000:  37%|███▋      | 1103/3000 [08:05&lt;11:06,  2.85it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.42e+6]Epoch 1104/3000:  37%|███▋      | 1103/3000 [08:06&lt;11:06,  2.85it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.42e+6]Epoch 1104/3000:  37%|███▋      | 1104/3000 [08:06&lt;13:01,  2.43it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.42e+6]Epoch 1104/3000:  37%|███▋      | 1104/3000 [08:06&lt;13:01,  2.43it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.41e+6]Epoch 1105/3000:  37%|███▋      | 1104/3000 [08:06&lt;13:01,  2.43it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.41e+6]Epoch 1105/3000:  37%|███▋      | 1105/3000 [08:06&lt;12:33,  2.52it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.41e+6]Epoch 1105/3000:  37%|███▋      | 1105/3000 [08:06&lt;12:33,  2.52it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.41e+6]Epoch 1106/3000:  37%|███▋      | 1105/3000 [08:06&lt;12:33,  2.52it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.41e+6]Epoch 1106/3000:  37%|███▋      | 1106/3000 [08:07&lt;11:37,  2.72it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.41e+6]Epoch 1106/3000:  37%|███▋      | 1106/3000 [08:07&lt;11:37,  2.72it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.41e+6]Epoch 1107/3000:  37%|███▋      | 1106/3000 [08:07&lt;11:37,  2.72it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.41e+6]Epoch 1107/3000:  37%|███▋      | 1107/3000 [08:07&lt;13:46,  2.29it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.41e+6]Epoch 1107/3000:  37%|███▋      | 1107/3000 [08:07&lt;13:46,  2.29it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.41e+6] Epoch 1108/3000:  37%|███▋      | 1107/3000 [08:07&lt;13:46,  2.29it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.41e+6]Epoch 1108/3000:  37%|███▋      | 1108/3000 [08:08&lt;14:34,  2.16it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.41e+6]Epoch 1108/3000:  37%|███▋      | 1108/3000 [08:08&lt;14:34,  2.16it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.41e+6]Epoch 1109/3000:  37%|███▋      | 1108/3000 [08:08&lt;14:34,  2.16it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.41e+6]Epoch 1109/3000:  37%|███▋      | 1109/3000 [08:08&lt;13:48,  2.28it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.41e+6]Epoch 1109/3000:  37%|███▋      | 1109/3000 [08:08&lt;13:48,  2.28it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.41e+6]Epoch 1110/3000:  37%|███▋      | 1109/3000 [08:08&lt;13:48,  2.28it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.41e+6]Epoch 1110/3000:  37%|███▋      | 1110/3000 [08:09&lt;14:39,  2.15it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.41e+6]Epoch 1110/3000:  37%|███▋      | 1110/3000 [08:09&lt;14:39,  2.15it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.41e+6]Epoch 1111/3000:  37%|███▋      | 1110/3000 [08:09&lt;14:39,  2.15it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.41e+6]Epoch 1111/3000:  37%|███▋      | 1111/3000 [08:09&lt;14:31,  2.17it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.41e+6]Epoch 1111/3000:  37%|███▋      | 1111/3000 [08:09&lt;14:31,  2.17it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.41e+6]Epoch 1112/3000:  37%|███▋      | 1111/3000 [08:09&lt;14:31,  2.17it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.41e+6]Epoch 1112/3000:  37%|███▋      | 1112/3000 [08:10&lt;14:51,  2.12it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.41e+6]Epoch 1112/3000:  37%|███▋      | 1112/3000 [08:10&lt;14:51,  2.12it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.41e+6]Epoch 1113/3000:  37%|███▋      | 1112/3000 [08:10&lt;14:51,  2.12it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.41e+6]Epoch 1113/3000:  37%|███▋      | 1113/3000 [08:10&lt;14:44,  2.13it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.41e+6]Epoch 1113/3000:  37%|███▋      | 1113/3000 [08:10&lt;14:44,  2.13it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.41e+6]Epoch 1114/3000:  37%|███▋      | 1113/3000 [08:10&lt;14:44,  2.13it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.41e+6]Epoch 1114/3000:  37%|███▋      | 1114/3000 [08:11&lt;13:51,  2.27it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.41e+6]Epoch 1114/3000:  37%|███▋      | 1114/3000 [08:11&lt;13:51,  2.27it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.41e+6]Epoch 1115/3000:  37%|███▋      | 1114/3000 [08:11&lt;13:51,  2.27it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.41e+6]Epoch 1115/3000:  37%|███▋      | 1115/3000 [08:11&lt;14:31,  2.16it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.41e+6]Epoch 1115/3000:  37%|███▋      | 1115/3000 [08:11&lt;14:31,  2.16it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.41e+6]Epoch 1116/3000:  37%|███▋      | 1115/3000 [08:11&lt;14:31,  2.16it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.41e+6]Epoch 1116/3000:  37%|███▋      | 1116/3000 [08:12&lt;14:44,  2.13it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.41e+6]Epoch 1116/3000:  37%|███▋      | 1116/3000 [08:12&lt;14:44,  2.13it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.41e+6]Epoch 1117/3000:  37%|███▋      | 1116/3000 [08:12&lt;14:44,  2.13it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.41e+6]Epoch 1117/3000:  37%|███▋      | 1117/3000 [08:12&lt;14:24,  2.18it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.41e+6]Epoch 1117/3000:  37%|███▋      | 1117/3000 [08:12&lt;14:24,  2.18it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.41e+6]Epoch 1118/3000:  37%|███▋      | 1117/3000 [08:12&lt;14:24,  2.18it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.41e+6]Epoch 1118/3000:  37%|███▋      | 1118/3000 [08:13&lt;16:38,  1.88it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.41e+6]Epoch 1118/3000:  37%|███▋      | 1118/3000 [08:13&lt;16:38,  1.88it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.41e+6]Epoch 1119/3000:  37%|███▋      | 1118/3000 [08:13&lt;16:38,  1.88it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.41e+6]Epoch 1119/3000:  37%|███▋      | 1119/3000 [08:13&lt;16:52,  1.86it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.41e+6]Epoch 1119/3000:  37%|███▋      | 1119/3000 [08:13&lt;16:52,  1.86it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.4e+6] Epoch 1120/3000:  37%|███▋      | 1119/3000 [08:13&lt;16:52,  1.86it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.4e+6]Epoch 1120/3000:  37%|███▋      | 1120/3000 [08:14&lt;15:56,  1.97it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.4e+6]Epoch 1120/3000:  37%|███▋      | 1120/3000 [08:14&lt;15:56,  1.97it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.4e+6]Epoch 1121/3000:  37%|███▋      | 1120/3000 [08:14&lt;15:56,  1.97it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.4e+6]Epoch 1121/3000:  37%|███▋      | 1121/3000 [08:14&lt;15:49,  1.98it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.4e+6]Epoch 1121/3000:  37%|███▋      | 1121/3000 [08:14&lt;15:49,  1.98it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.4e+6]Epoch 1122/3000:  37%|███▋      | 1121/3000 [08:14&lt;15:49,  1.98it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.4e+6]Epoch 1122/3000:  37%|███▋      | 1122/3000 [08:15&lt;14:36,  2.14it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.4e+6]Epoch 1122/3000:  37%|███▋      | 1122/3000 [08:15&lt;14:36,  2.14it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.4e+6]Epoch 1123/3000:  37%|███▋      | 1122/3000 [08:15&lt;14:36,  2.14it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.4e+6]Epoch 1123/3000:  37%|███▋      | 1123/3000 [08:15&lt;14:04,  2.22it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.4e+6]Epoch 1123/3000:  37%|███▋      | 1123/3000 [08:15&lt;14:04,  2.22it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.4e+6]Epoch 1124/3000:  37%|███▋      | 1123/3000 [08:15&lt;14:04,  2.22it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.4e+6]Epoch 1124/3000:  37%|███▋      | 1124/3000 [08:15&lt;13:40,  2.29it/s, v_num=1, train_loss_step=1.43e+6, train_loss_epoch=1.4e+6]Epoch 1124/3000:  37%|███▋      | 1124/3000 [08:15&lt;13:40,  2.29it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.4e+6]Epoch 1125/3000:  37%|███▋      | 1124/3000 [08:15&lt;13:40,  2.29it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.4e+6]Epoch 1125/3000:  38%|███▊      | 1125/3000 [08:16&lt;14:02,  2.22it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.4e+6]Epoch 1125/3000:  38%|███▊      | 1125/3000 [08:16&lt;14:02,  2.22it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.4e+6]Epoch 1126/3000:  38%|███▊      | 1125/3000 [08:16&lt;14:02,  2.22it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.4e+6]Epoch 1126/3000:  38%|███▊      | 1126/3000 [08:16&lt;12:51,  2.43it/s, v_num=1, train_loss_step=1.44e+6, train_loss_epoch=1.4e+6]Epoch 1126/3000:  38%|███▊      | 1126/3000 [08:16&lt;12:51,  2.43it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.4e+6]Epoch 1127/3000:  38%|███▊      | 1126/3000 [08:16&lt;12:51,  2.43it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.4e+6]Epoch 1127/3000:  38%|███▊      | 1127/3000 [08:17&lt;13:38,  2.29it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.4e+6]Epoch 1127/3000:  38%|███▊      | 1127/3000 [08:17&lt;13:38,  2.29it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.4e+6]Epoch 1128/3000:  38%|███▊      | 1127/3000 [08:17&lt;13:38,  2.29it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.4e+6]Epoch 1128/3000:  38%|███▊      | 1128/3000 [08:17&lt;15:28,  2.02it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.4e+6]Epoch 1128/3000:  38%|███▊      | 1128/3000 [08:17&lt;15:28,  2.02it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.4e+6]Epoch 1129/3000:  38%|███▊      | 1128/3000 [08:17&lt;15:28,  2.02it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.4e+6]Epoch 1129/3000:  38%|███▊      | 1129/3000 [08:18&lt;14:32,  2.14it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.4e+6]Epoch 1129/3000:  38%|███▊      | 1129/3000 [08:18&lt;14:32,  2.14it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.4e+6]Epoch 1130/3000:  38%|███▊      | 1129/3000 [08:18&lt;14:32,  2.14it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.4e+6]Epoch 1130/3000:  38%|███▊      | 1130/3000 [08:18&lt;15:22,  2.03it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.4e+6]Epoch 1130/3000:  38%|███▊      | 1130/3000 [08:18&lt;15:22,  2.03it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.4e+6]Epoch 1131/3000:  38%|███▊      | 1130/3000 [08:18&lt;15:22,  2.03it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.4e+6]Epoch 1131/3000:  38%|███▊      | 1131/3000 [08:19&lt;15:38,  1.99it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.4e+6]Epoch 1131/3000:  38%|███▊      | 1131/3000 [08:19&lt;15:38,  1.99it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.4e+6]Epoch 1132/3000:  38%|███▊      | 1131/3000 [08:19&lt;15:38,  1.99it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.4e+6]Epoch 1132/3000:  38%|███▊      | 1132/3000 [08:19&lt;13:02,  2.39it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.4e+6]Epoch 1132/3000:  38%|███▊      | 1132/3000 [08:19&lt;13:02,  2.39it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.4e+6]Epoch 1133/3000:  38%|███▊      | 1132/3000 [08:19&lt;13:02,  2.39it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.4e+6]Epoch 1133/3000:  38%|███▊      | 1133/3000 [08:20&lt;14:09,  2.20it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.4e+6]Epoch 1133/3000:  38%|███▊      | 1133/3000 [08:20&lt;14:09,  2.20it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.4e+6]Epoch 1134/3000:  38%|███▊      | 1133/3000 [08:20&lt;14:09,  2.20it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.4e+6]Epoch 1134/3000:  38%|███▊      | 1134/3000 [08:20&lt;15:00,  2.07it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.4e+6]Epoch 1134/3000:  38%|███▊      | 1134/3000 [08:20&lt;15:00,  2.07it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.4e+6]Epoch 1135/3000:  38%|███▊      | 1134/3000 [08:20&lt;15:00,  2.07it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.4e+6]Epoch 1135/3000:  38%|███▊      | 1135/3000 [08:21&lt;14:41,  2.12it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.4e+6]Epoch 1135/3000:  38%|███▊      | 1135/3000 [08:21&lt;14:41,  2.12it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.39e+6]Epoch 1136/3000:  38%|███▊      | 1135/3000 [08:21&lt;14:41,  2.12it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.39e+6]Epoch 1136/3000:  38%|███▊      | 1136/3000 [08:21&lt;15:10,  2.05it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.39e+6]Epoch 1136/3000:  38%|███▊      | 1136/3000 [08:21&lt;15:10,  2.05it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.39e+6]Epoch 1137/3000:  38%|███▊      | 1136/3000 [08:21&lt;15:10,  2.05it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.39e+6]Epoch 1137/3000:  38%|███▊      | 1137/3000 [08:22&lt;15:37,  1.99it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.39e+6]Epoch 1137/3000:  38%|███▊      | 1137/3000 [08:22&lt;15:37,  1.99it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.39e+6] Epoch 1138/3000:  38%|███▊      | 1137/3000 [08:22&lt;15:37,  1.99it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.39e+6]Epoch 1138/3000:  38%|███▊      | 1138/3000 [08:22&lt;15:10,  2.05it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.39e+6]Epoch 1138/3000:  38%|███▊      | 1138/3000 [08:22&lt;15:10,  2.05it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.39e+6]Epoch 1139/3000:  38%|███▊      | 1138/3000 [08:22&lt;15:10,  2.05it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.39e+6]Epoch 1139/3000:  38%|███▊      | 1139/3000 [08:22&lt;14:27,  2.15it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.39e+6]Epoch 1139/3000:  38%|███▊      | 1139/3000 [08:22&lt;14:27,  2.15it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.39e+6]Epoch 1140/3000:  38%|███▊      | 1139/3000 [08:22&lt;14:27,  2.15it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.39e+6]Epoch 1140/3000:  38%|███▊      | 1140/3000 [08:23&lt;13:59,  2.22it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.39e+6]Epoch 1140/3000:  38%|███▊      | 1140/3000 [08:23&lt;13:59,  2.22it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.39e+6] Epoch 1141/3000:  38%|███▊      | 1140/3000 [08:23&lt;13:59,  2.22it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.39e+6]Epoch 1141/3000:  38%|███▊      | 1141/3000 [08:23&lt;15:22,  2.02it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.39e+6]Epoch 1141/3000:  38%|███▊      | 1141/3000 [08:23&lt;15:22,  2.02it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.39e+6]Epoch 1142/3000:  38%|███▊      | 1141/3000 [08:23&lt;15:22,  2.02it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.39e+6]Epoch 1142/3000:  38%|███▊      | 1142/3000 [08:24&lt;14:08,  2.19it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.39e+6]Epoch 1142/3000:  38%|███▊      | 1142/3000 [08:24&lt;14:08,  2.19it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.39e+6]Epoch 1143/3000:  38%|███▊      | 1142/3000 [08:24&lt;14:08,  2.19it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.39e+6]Epoch 1143/3000:  38%|███▊      | 1143/3000 [08:24&lt;15:07,  2.05it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.39e+6]Epoch 1143/3000:  38%|███▊      | 1143/3000 [08:24&lt;15:07,  2.05it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.39e+6]Epoch 1144/3000:  38%|███▊      | 1143/3000 [08:24&lt;15:07,  2.05it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.39e+6]Epoch 1144/3000:  38%|███▊      | 1144/3000 [08:25&lt;15:07,  2.05it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.39e+6]Epoch 1144/3000:  38%|███▊      | 1144/3000 [08:25&lt;15:07,  2.05it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.39e+6]Epoch 1145/3000:  38%|███▊      | 1144/3000 [08:25&lt;15:07,  2.05it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.39e+6]Epoch 1145/3000:  38%|███▊      | 1145/3000 [08:25&lt;15:48,  1.96it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.39e+6]Epoch 1145/3000:  38%|███▊      | 1145/3000 [08:25&lt;15:48,  1.96it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.39e+6]Epoch 1146/3000:  38%|███▊      | 1145/3000 [08:25&lt;15:48,  1.96it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.39e+6]Epoch 1146/3000:  38%|███▊      | 1146/3000 [08:26&lt;15:59,  1.93it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.39e+6]Epoch 1146/3000:  38%|███▊      | 1146/3000 [08:26&lt;15:59,  1.93it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.39e+6]Epoch 1147/3000:  38%|███▊      | 1146/3000 [08:26&lt;15:59,  1.93it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.39e+6]Epoch 1147/3000:  38%|███▊      | 1147/3000 [08:26&lt;15:53,  1.94it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.39e+6]Epoch 1147/3000:  38%|███▊      | 1147/3000 [08:26&lt;15:53,  1.94it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.39e+6] Epoch 1148/3000:  38%|███▊      | 1147/3000 [08:26&lt;15:53,  1.94it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.39e+6]Epoch 1148/3000:  38%|███▊      | 1148/3000 [08:27&lt;16:46,  1.84it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.39e+6]Epoch 1148/3000:  38%|███▊      | 1148/3000 [08:27&lt;16:46,  1.84it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.39e+6]Epoch 1149/3000:  38%|███▊      | 1148/3000 [08:27&lt;16:46,  1.84it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.39e+6]Epoch 1149/3000:  38%|███▊      | 1149/3000 [08:28&lt;16:22,  1.88it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.39e+6]Epoch 1149/3000:  38%|███▊      | 1149/3000 [08:28&lt;16:22,  1.88it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.39e+6]Epoch 1150/3000:  38%|███▊      | 1149/3000 [08:28&lt;16:22,  1.88it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.39e+6]Epoch 1150/3000:  38%|███▊      | 1150/3000 [08:28&lt;15:03,  2.05it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.39e+6]Epoch 1150/3000:  38%|███▊      | 1150/3000 [08:28&lt;15:03,  2.05it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.39e+6]Epoch 1151/3000:  38%|███▊      | 1150/3000 [08:28&lt;15:03,  2.05it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.39e+6]Epoch 1151/3000:  38%|███▊      | 1151/3000 [08:29&lt;16:34,  1.86it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.39e+6]Epoch 1151/3000:  38%|███▊      | 1151/3000 [08:29&lt;16:34,  1.86it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.38e+6]Epoch 1152/3000:  38%|███▊      | 1151/3000 [08:29&lt;16:34,  1.86it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.38e+6]Epoch 1152/3000:  38%|███▊      | 1152/3000 [08:29&lt;16:39,  1.85it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.38e+6]Epoch 1152/3000:  38%|███▊      | 1152/3000 [08:29&lt;16:39,  1.85it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.38e+6]Epoch 1153/3000:  38%|███▊      | 1152/3000 [08:29&lt;16:39,  1.85it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.38e+6]Epoch 1153/3000:  38%|███▊      | 1153/3000 [08:30&lt;16:28,  1.87it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.38e+6]Epoch 1153/3000:  38%|███▊      | 1153/3000 [08:30&lt;16:28,  1.87it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.38e+6]Epoch 1154/3000:  38%|███▊      | 1153/3000 [08:30&lt;16:28,  1.87it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.38e+6]Epoch 1154/3000:  38%|███▊      | 1154/3000 [08:30&lt;15:58,  1.93it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.38e+6]Epoch 1154/3000:  38%|███▊      | 1154/3000 [08:30&lt;15:58,  1.93it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.38e+6] Epoch 1155/3000:  38%|███▊      | 1154/3000 [08:30&lt;15:58,  1.93it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.38e+6]Epoch 1155/3000:  38%|███▊      | 1155/3000 [08:31&lt;15:19,  2.01it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.38e+6]Epoch 1155/3000:  38%|███▊      | 1155/3000 [08:31&lt;15:19,  2.01it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.38e+6]Epoch 1156/3000:  38%|███▊      | 1155/3000 [08:31&lt;15:19,  2.01it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.38e+6]Epoch 1156/3000:  39%|███▊      | 1156/3000 [08:31&lt;14:44,  2.08it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.38e+6]Epoch 1156/3000:  39%|███▊      | 1156/3000 [08:31&lt;14:44,  2.08it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.38e+6]Epoch 1157/3000:  39%|███▊      | 1156/3000 [08:31&lt;14:44,  2.08it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.38e+6]Epoch 1157/3000:  39%|███▊      | 1157/3000 [08:32&lt;16:09,  1.90it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.38e+6]Epoch 1157/3000:  39%|███▊      | 1157/3000 [08:32&lt;16:09,  1.90it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.38e+6] Epoch 1158/3000:  39%|███▊      | 1157/3000 [08:32&lt;16:09,  1.90it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.38e+6]Epoch 1158/3000:  39%|███▊      | 1158/3000 [08:32&lt;15:36,  1.97it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.38e+6]Epoch 1158/3000:  39%|███▊      | 1158/3000 [08:32&lt;15:36,  1.97it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.38e+6]Epoch 1159/3000:  39%|███▊      | 1158/3000 [08:32&lt;15:36,  1.97it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.38e+6]Epoch 1159/3000:  39%|███▊      | 1159/3000 [08:33&lt;14:50,  2.07it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.38e+6]Epoch 1159/3000:  39%|███▊      | 1159/3000 [08:33&lt;14:50,  2.07it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.38e+6]Epoch 1160/3000:  39%|███▊      | 1159/3000 [08:33&lt;14:50,  2.07it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.38e+6]Epoch 1160/3000:  39%|███▊      | 1160/3000 [08:33&lt;14:42,  2.08it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.38e+6]Epoch 1160/3000:  39%|███▊      | 1160/3000 [08:33&lt;14:42,  2.08it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.38e+6]Epoch 1161/3000:  39%|███▊      | 1160/3000 [08:33&lt;14:42,  2.08it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.38e+6]Epoch 1161/3000:  39%|███▊      | 1161/3000 [08:34&lt;14:29,  2.12it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.38e+6]Epoch 1161/3000:  39%|███▊      | 1161/3000 [08:34&lt;14:29,  2.12it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.38e+6]Epoch 1162/3000:  39%|███▊      | 1161/3000 [08:34&lt;14:29,  2.12it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.38e+6]Epoch 1162/3000:  39%|███▊      | 1162/3000 [08:34&lt;15:54,  1.93it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.38e+6]Epoch 1162/3000:  39%|███▊      | 1162/3000 [08:34&lt;15:54,  1.93it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.38e+6]Epoch 1163/3000:  39%|███▊      | 1162/3000 [08:34&lt;15:54,  1.93it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.38e+6]Epoch 1163/3000:  39%|███▉      | 1163/3000 [08:35&lt;15:34,  1.97it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.38e+6]Epoch 1163/3000:  39%|███▉      | 1163/3000 [08:35&lt;15:34,  1.97it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.38e+6] Epoch 1164/3000:  39%|███▉      | 1163/3000 [08:35&lt;15:34,  1.97it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.38e+6]Epoch 1164/3000:  39%|███▉      | 1164/3000 [08:35&lt;14:18,  2.14it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.38e+6]Epoch 1164/3000:  39%|███▉      | 1164/3000 [08:35&lt;14:18,  2.14it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.38e+6]Epoch 1165/3000:  39%|███▉      | 1164/3000 [08:35&lt;14:18,  2.14it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.38e+6]Epoch 1165/3000:  39%|███▉      | 1165/3000 [08:36&lt;14:38,  2.09it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.38e+6]Epoch 1165/3000:  39%|███▉      | 1165/3000 [08:36&lt;14:38,  2.09it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.38e+6]Epoch 1166/3000:  39%|███▉      | 1165/3000 [08:36&lt;14:38,  2.09it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.38e+6]Epoch 1166/3000:  39%|███▉      | 1166/3000 [08:36&lt;14:35,  2.10it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.38e+6]Epoch 1166/3000:  39%|███▉      | 1166/3000 [08:36&lt;14:35,  2.10it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.38e+6]Epoch 1167/3000:  39%|███▉      | 1166/3000 [08:36&lt;14:35,  2.10it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.38e+6]Epoch 1167/3000:  39%|███▉      | 1167/3000 [08:37&lt;15:16,  2.00it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.38e+6]Epoch 1167/3000:  39%|███▉      | 1167/3000 [08:37&lt;15:16,  2.00it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.37e+6]Epoch 1168/3000:  39%|███▉      | 1167/3000 [08:37&lt;15:16,  2.00it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.37e+6]Epoch 1168/3000:  39%|███▉      | 1168/3000 [08:37&lt;15:05,  2.02it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.37e+6]Epoch 1168/3000:  39%|███▉      | 1168/3000 [08:37&lt;15:05,  2.02it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.37e+6]Epoch 1169/3000:  39%|███▉      | 1168/3000 [08:37&lt;15:05,  2.02it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.37e+6]Epoch 1169/3000:  39%|███▉      | 1169/3000 [08:38&lt;15:55,  1.92it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.37e+6]Epoch 1169/3000:  39%|███▉      | 1169/3000 [08:38&lt;15:55,  1.92it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.37e+6]Epoch 1170/3000:  39%|███▉      | 1169/3000 [08:38&lt;15:55,  1.92it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.37e+6]Epoch 1170/3000:  39%|███▉      | 1170/3000 [08:38&lt;15:37,  1.95it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.37e+6]Epoch 1170/3000:  39%|███▉      | 1170/3000 [08:38&lt;15:37,  1.95it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.37e+6]Epoch 1171/3000:  39%|███▉      | 1170/3000 [08:38&lt;15:37,  1.95it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.37e+6]Epoch 1171/3000:  39%|███▉      | 1171/3000 [08:39&lt;16:32,  1.84it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.37e+6]Epoch 1171/3000:  39%|███▉      | 1171/3000 [08:39&lt;16:32,  1.84it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.37e+6]Epoch 1172/3000:  39%|███▉      | 1171/3000 [08:39&lt;16:32,  1.84it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.37e+6]Epoch 1172/3000:  39%|███▉      | 1172/3000 [08:39&lt;16:10,  1.88it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.37e+6]Epoch 1172/3000:  39%|███▉      | 1172/3000 [08:39&lt;16:10,  1.88it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.37e+6]Epoch 1173/3000:  39%|███▉      | 1172/3000 [08:39&lt;16:10,  1.88it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.37e+6]Epoch 1173/3000:  39%|███▉      | 1173/3000 [08:40&lt;15:44,  1.93it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.37e+6]Epoch 1173/3000:  39%|███▉      | 1173/3000 [08:40&lt;15:44,  1.93it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.37e+6]Epoch 1174/3000:  39%|███▉      | 1173/3000 [08:40&lt;15:44,  1.93it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.37e+6]Epoch 1174/3000:  39%|███▉      | 1174/3000 [08:40&lt;13:34,  2.24it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.37e+6]Epoch 1174/3000:  39%|███▉      | 1174/3000 [08:40&lt;13:34,  2.24it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.37e+6]Epoch 1175/3000:  39%|███▉      | 1174/3000 [08:40&lt;13:34,  2.24it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.37e+6]Epoch 1175/3000:  39%|███▉      | 1175/3000 [08:41&lt;14:41,  2.07it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.37e+6]Epoch 1175/3000:  39%|███▉      | 1175/3000 [08:41&lt;14:41,  2.07it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.37e+6]Epoch 1176/3000:  39%|███▉      | 1175/3000 [08:41&lt;14:41,  2.07it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.37e+6]Epoch 1176/3000:  39%|███▉      | 1176/3000 [08:41&lt;13:23,  2.27it/s, v_num=1, train_loss_step=1.45e+6, train_loss_epoch=1.37e+6]Epoch 1176/3000:  39%|███▉      | 1176/3000 [08:41&lt;13:23,  2.27it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.37e+6]Epoch 1177/3000:  39%|███▉      | 1176/3000 [08:41&lt;13:23,  2.27it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.37e+6]Epoch 1177/3000:  39%|███▉      | 1177/3000 [08:41&lt;13:18,  2.28it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.37e+6]Epoch 1177/3000:  39%|███▉      | 1177/3000 [08:41&lt;13:18,  2.28it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.37e+6]Epoch 1178/3000:  39%|███▉      | 1177/3000 [08:41&lt;13:18,  2.28it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.37e+6]Epoch 1178/3000:  39%|███▉      | 1178/3000 [08:42&lt;12:53,  2.35it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.37e+6]Epoch 1178/3000:  39%|███▉      | 1178/3000 [08:42&lt;12:53,  2.35it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.37e+6]Epoch 1179/3000:  39%|███▉      | 1178/3000 [08:42&lt;12:53,  2.35it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.37e+6]Epoch 1179/3000:  39%|███▉      | 1179/3000 [08:42&lt;13:41,  2.22it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.37e+6]Epoch 1179/3000:  39%|███▉      | 1179/3000 [08:42&lt;13:41,  2.22it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.37e+6]Epoch 1180/3000:  39%|███▉      | 1179/3000 [08:42&lt;13:41,  2.22it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.37e+6]Epoch 1180/3000:  39%|███▉      | 1180/3000 [08:43&lt;14:34,  2.08it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.37e+6]Epoch 1180/3000:  39%|███▉      | 1180/3000 [08:43&lt;14:34,  2.08it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.37e+6]Epoch 1181/3000:  39%|███▉      | 1180/3000 [08:43&lt;14:34,  2.08it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.37e+6]Epoch 1181/3000:  39%|███▉      | 1181/3000 [08:43&lt;12:45,  2.38it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.37e+6]Epoch 1181/3000:  39%|███▉      | 1181/3000 [08:43&lt;12:45,  2.38it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.37e+6]Epoch 1182/3000:  39%|███▉      | 1181/3000 [08:43&lt;12:45,  2.38it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.37e+6]Epoch 1182/3000:  39%|███▉      | 1182/3000 [08:44&lt;13:16,  2.28it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.37e+6]Epoch 1182/3000:  39%|███▉      | 1182/3000 [08:44&lt;13:16,  2.28it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.37e+6]Epoch 1183/3000:  39%|███▉      | 1182/3000 [08:44&lt;13:16,  2.28it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.37e+6]Epoch 1183/3000:  39%|███▉      | 1183/3000 [08:44&lt;13:33,  2.23it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.37e+6]Epoch 1183/3000:  39%|███▉      | 1183/3000 [08:44&lt;13:33,  2.23it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.37e+6]Epoch 1184/3000:  39%|███▉      | 1183/3000 [08:44&lt;13:33,  2.23it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.37e+6]Epoch 1184/3000:  39%|███▉      | 1184/3000 [08:44&lt;12:17,  2.46it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.37e+6]Epoch 1184/3000:  39%|███▉      | 1184/3000 [08:44&lt;12:17,  2.46it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.36e+6]Epoch 1185/3000:  39%|███▉      | 1184/3000 [08:44&lt;12:17,  2.46it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.36e+6]Epoch 1185/3000:  40%|███▉      | 1185/3000 [08:45&lt;14:16,  2.12it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.36e+6]Epoch 1185/3000:  40%|███▉      | 1185/3000 [08:45&lt;14:16,  2.12it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.36e+6]Epoch 1186/3000:  40%|███▉      | 1185/3000 [08:45&lt;14:16,  2.12it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.36e+6]Epoch 1186/3000:  40%|███▉      | 1186/3000 [08:45&lt;13:59,  2.16it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.36e+6]Epoch 1186/3000:  40%|███▉      | 1186/3000 [08:45&lt;13:59,  2.16it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.36e+6]Epoch 1187/3000:  40%|███▉      | 1186/3000 [08:45&lt;13:59,  2.16it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.36e+6]Epoch 1187/3000:  40%|███▉      | 1187/3000 [08:46&lt;14:21,  2.10it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.36e+6]Epoch 1187/3000:  40%|███▉      | 1187/3000 [08:46&lt;14:21,  2.10it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.36e+6]Epoch 1188/3000:  40%|███▉      | 1187/3000 [08:46&lt;14:21,  2.10it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.36e+6]Epoch 1188/3000:  40%|███▉      | 1188/3000 [08:46&lt;14:02,  2.15it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.36e+6]Epoch 1188/3000:  40%|███▉      | 1188/3000 [08:46&lt;14:02,  2.15it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.36e+6]Epoch 1189/3000:  40%|███▉      | 1188/3000 [08:46&lt;14:02,  2.15it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.36e+6]Epoch 1189/3000:  40%|███▉      | 1189/3000 [08:47&lt;13:09,  2.29it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.36e+6]Epoch 1189/3000:  40%|███▉      | 1189/3000 [08:47&lt;13:09,  2.29it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.36e+6]Epoch 1190/3000:  40%|███▉      | 1189/3000 [08:47&lt;13:09,  2.29it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.36e+6]Epoch 1190/3000:  40%|███▉      | 1190/3000 [08:47&lt;12:25,  2.43it/s, v_num=1, train_loss_step=1.41e+6, train_loss_epoch=1.36e+6]Epoch 1190/3000:  40%|███▉      | 1190/3000 [08:47&lt;12:25,  2.43it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.36e+6]Epoch 1191/3000:  40%|███▉      | 1190/3000 [08:47&lt;12:25,  2.43it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.36e+6]Epoch 1191/3000:  40%|███▉      | 1191/3000 [08:47&lt;10:44,  2.81it/s, v_num=1, train_loss_step=1.42e+6, train_loss_epoch=1.36e+6]Epoch 1191/3000:  40%|███▉      | 1191/3000 [08:47&lt;10:44,  2.81it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.36e+6]Epoch 1192/3000:  40%|███▉      | 1191/3000 [08:47&lt;10:44,  2.81it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.36e+6]Epoch 1192/3000:  40%|███▉      | 1192/3000 [08:48&lt;12:10,  2.48it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.36e+6]Epoch 1192/3000:  40%|███▉      | 1192/3000 [08:48&lt;12:10,  2.48it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.36e+6]Epoch 1193/3000:  40%|███▉      | 1192/3000 [08:48&lt;12:10,  2.48it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.36e+6]Epoch 1193/3000:  40%|███▉      | 1193/3000 [08:48&lt;13:00,  2.31it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.36e+6]Epoch 1193/3000:  40%|███▉      | 1193/3000 [08:48&lt;13:00,  2.31it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.36e+6]Epoch 1194/3000:  40%|███▉      | 1193/3000 [08:48&lt;13:00,  2.31it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.36e+6]Epoch 1194/3000:  40%|███▉      | 1194/3000 [08:49&lt;13:38,  2.21it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.36e+6]Epoch 1194/3000:  40%|███▉      | 1194/3000 [08:49&lt;13:38,  2.21it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.36e+6]Epoch 1195/3000:  40%|███▉      | 1194/3000 [08:49&lt;13:38,  2.21it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.36e+6]Epoch 1195/3000:  40%|███▉      | 1195/3000 [08:49&lt;13:33,  2.22it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.36e+6]Epoch 1195/3000:  40%|███▉      | 1195/3000 [08:49&lt;13:33,  2.22it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.36e+6]Epoch 1196/3000:  40%|███▉      | 1195/3000 [08:49&lt;13:33,  2.22it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.36e+6]Epoch 1196/3000:  40%|███▉      | 1196/3000 [08:50&lt;13:01,  2.31it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.36e+6]Epoch 1196/3000:  40%|███▉      | 1196/3000 [08:50&lt;13:01,  2.31it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.36e+6]Epoch 1197/3000:  40%|███▉      | 1196/3000 [08:50&lt;13:01,  2.31it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.36e+6]Epoch 1197/3000:  40%|███▉      | 1197/3000 [08:50&lt;12:28,  2.41it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.36e+6]Epoch 1197/3000:  40%|███▉      | 1197/3000 [08:50&lt;12:28,  2.41it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.36e+6]Epoch 1198/3000:  40%|███▉      | 1197/3000 [08:50&lt;12:28,  2.41it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.36e+6]Epoch 1198/3000:  40%|███▉      | 1198/3000 [08:51&lt;13:57,  2.15it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.36e+6]Epoch 1198/3000:  40%|███▉      | 1198/3000 [08:51&lt;13:57,  2.15it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.36e+6]Epoch 1199/3000:  40%|███▉      | 1198/3000 [08:51&lt;13:57,  2.15it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.36e+6]Epoch 1199/3000:  40%|███▉      | 1199/3000 [08:51&lt;13:49,  2.17it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.36e+6]Epoch 1199/3000:  40%|███▉      | 1199/3000 [08:51&lt;13:49,  2.17it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.36e+6] Epoch 1200/3000:  40%|███▉      | 1199/3000 [08:51&lt;13:49,  2.17it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.36e+6]Epoch 1200/3000:  40%|████      | 1200/3000 [08:51&lt;12:42,  2.36it/s, v_num=1, train_loss_step=1.4e+6, train_loss_epoch=1.36e+6]Epoch 1200/3000:  40%|████      | 1200/3000 [08:51&lt;12:42,  2.36it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.36e+6]Epoch 1201/3000:  40%|████      | 1200/3000 [08:51&lt;12:42,  2.36it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.36e+6]Epoch 1201/3000:  40%|████      | 1201/3000 [08:52&lt;14:23,  2.08it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.36e+6]Epoch 1201/3000:  40%|████      | 1201/3000 [08:52&lt;14:23,  2.08it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.36e+6]Epoch 1202/3000:  40%|████      | 1201/3000 [08:52&lt;14:23,  2.08it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.36e+6]Epoch 1202/3000:  40%|████      | 1202/3000 [08:52&lt;13:04,  2.29it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.36e+6]Epoch 1202/3000:  40%|████      | 1202/3000 [08:52&lt;13:04,  2.29it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1203/3000:  40%|████      | 1202/3000 [08:52&lt;13:04,  2.29it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1203/3000:  40%|████      | 1203/3000 [08:53&lt;13:45,  2.18it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1203/3000:  40%|████      | 1203/3000 [08:53&lt;13:45,  2.18it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1204/3000:  40%|████      | 1203/3000 [08:53&lt;13:45,  2.18it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1204/3000:  40%|████      | 1204/3000 [08:53&lt;14:49,  2.02it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1204/3000:  40%|████      | 1204/3000 [08:53&lt;14:49,  2.02it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.35e+6]Epoch 1205/3000:  40%|████      | 1204/3000 [08:53&lt;14:49,  2.02it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.35e+6]Epoch 1205/3000:  40%|████      | 1205/3000 [08:54&lt;14:05,  2.12it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.35e+6]Epoch 1205/3000:  40%|████      | 1205/3000 [08:54&lt;14:05,  2.12it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1206/3000:  40%|████      | 1205/3000 [08:54&lt;14:05,  2.12it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1206/3000:  40%|████      | 1206/3000 [08:54&lt;14:21,  2.08it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1206/3000:  40%|████      | 1206/3000 [08:54&lt;14:21,  2.08it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1207/3000:  40%|████      | 1206/3000 [08:54&lt;14:21,  2.08it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1207/3000:  40%|████      | 1207/3000 [08:55&lt;13:57,  2.14it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1207/3000:  40%|████      | 1207/3000 [08:55&lt;13:57,  2.14it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.35e+6]Epoch 1208/3000:  40%|████      | 1207/3000 [08:55&lt;13:57,  2.14it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.35e+6]Epoch 1208/3000:  40%|████      | 1208/3000 [08:55&lt;13:52,  2.15it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.35e+6]Epoch 1208/3000:  40%|████      | 1208/3000 [08:55&lt;13:52,  2.15it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.35e+6]Epoch 1209/3000:  40%|████      | 1208/3000 [08:55&lt;13:52,  2.15it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.35e+6]Epoch 1209/3000:  40%|████      | 1209/3000 [08:56&lt;15:40,  1.90it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.35e+6]Epoch 1209/3000:  40%|████      | 1209/3000 [08:56&lt;15:40,  1.90it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.35e+6]Epoch 1210/3000:  40%|████      | 1209/3000 [08:56&lt;15:40,  1.90it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.35e+6]Epoch 1210/3000:  40%|████      | 1210/3000 [08:56&lt;15:07,  1.97it/s, v_num=1, train_loss_step=1.38e+6, train_loss_epoch=1.35e+6]Epoch 1210/3000:  40%|████      | 1210/3000 [08:56&lt;15:07,  1.97it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.35e+6]Epoch 1211/3000:  40%|████      | 1210/3000 [08:56&lt;15:07,  1.97it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.35e+6]Epoch 1211/3000:  40%|████      | 1211/3000 [08:57&lt;13:43,  2.17it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.35e+6]Epoch 1211/3000:  40%|████      | 1211/3000 [08:57&lt;13:43,  2.17it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.35e+6]Epoch 1212/3000:  40%|████      | 1211/3000 [08:57&lt;13:43,  2.17it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.35e+6]Epoch 1212/3000:  40%|████      | 1212/3000 [08:57&lt;13:54,  2.14it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.35e+6]Epoch 1212/3000:  40%|████      | 1212/3000 [08:57&lt;13:54,  2.14it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1213/3000:  40%|████      | 1212/3000 [08:57&lt;13:54,  2.14it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1213/3000:  40%|████      | 1213/3000 [08:58&lt;13:27,  2.21it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1213/3000:  40%|████      | 1213/3000 [08:58&lt;13:27,  2.21it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.35e+6]Epoch 1214/3000:  40%|████      | 1213/3000 [08:58&lt;13:27,  2.21it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.35e+6]Epoch 1214/3000:  40%|████      | 1214/3000 [08:58&lt;13:01,  2.29it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.35e+6]Epoch 1214/3000:  40%|████      | 1214/3000 [08:58&lt;13:01,  2.29it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.35e+6]Epoch 1215/3000:  40%|████      | 1214/3000 [08:58&lt;13:01,  2.29it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.35e+6]Epoch 1215/3000:  40%|████      | 1215/3000 [08:59&lt;13:34,  2.19it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.35e+6]Epoch 1215/3000:  40%|████      | 1215/3000 [08:59&lt;13:34,  2.19it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1216/3000:  40%|████      | 1215/3000 [08:59&lt;13:34,  2.19it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1216/3000:  41%|████      | 1216/3000 [08:59&lt;14:01,  2.12it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1216/3000:  41%|████      | 1216/3000 [08:59&lt;14:01,  2.12it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.35e+6]Epoch 1217/3000:  41%|████      | 1216/3000 [08:59&lt;14:01,  2.12it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.35e+6]Epoch 1217/3000:  41%|████      | 1217/3000 [09:00&lt;14:38,  2.03it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.35e+6]Epoch 1217/3000:  41%|████      | 1217/3000 [09:00&lt;14:38,  2.03it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1218/3000:  41%|████      | 1217/3000 [09:00&lt;14:38,  2.03it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1218/3000:  41%|████      | 1218/3000 [09:00&lt;14:39,  2.03it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.35e+6]Epoch 1218/3000:  41%|████      | 1218/3000 [09:00&lt;14:39,  2.03it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.35e+6]Epoch 1219/3000:  41%|████      | 1218/3000 [09:00&lt;14:39,  2.03it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.35e+6]Epoch 1219/3000:  41%|████      | 1219/3000 [09:01&lt;15:37,  1.90it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.35e+6]Epoch 1219/3000:  41%|████      | 1219/3000 [09:01&lt;15:37,  1.90it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.35e+6]Epoch 1220/3000:  41%|████      | 1219/3000 [09:01&lt;15:37,  1.90it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.35e+6]Epoch 1220/3000:  41%|████      | 1220/3000 [09:01&lt;15:07,  1.96it/s, v_num=1, train_loss_step=1.39e+6, train_loss_epoch=1.35e+6]Epoch 1220/3000:  41%|████      | 1220/3000 [09:01&lt;15:07,  1.96it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.34e+6]Epoch 1221/3000:  41%|████      | 1220/3000 [09:01&lt;15:07,  1.96it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.34e+6]Epoch 1221/3000:  41%|████      | 1221/3000 [09:02&lt;14:32,  2.04it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.34e+6]Epoch 1221/3000:  41%|████      | 1221/3000 [09:02&lt;14:32,  2.04it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.34e+6]Epoch 1222/3000:  41%|████      | 1221/3000 [09:02&lt;14:32,  2.04it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.34e+6]Epoch 1222/3000:  41%|████      | 1222/3000 [09:02&lt;14:39,  2.02it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.34e+6]Epoch 1222/3000:  41%|████      | 1222/3000 [09:02&lt;14:39,  2.02it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.34e+6]Epoch 1223/3000:  41%|████      | 1222/3000 [09:02&lt;14:39,  2.02it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.34e+6]Epoch 1223/3000:  41%|████      | 1223/3000 [09:03&lt;14:18,  2.07it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.34e+6]Epoch 1223/3000:  41%|████      | 1223/3000 [09:03&lt;14:18,  2.07it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.34e+6]Epoch 1224/3000:  41%|████      | 1223/3000 [09:03&lt;14:18,  2.07it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.34e+6]Epoch 1224/3000:  41%|████      | 1224/3000 [09:03&lt;14:25,  2.05it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.34e+6]Epoch 1224/3000:  41%|████      | 1224/3000 [09:03&lt;14:25,  2.05it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1225/3000:  41%|████      | 1224/3000 [09:03&lt;14:25,  2.05it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1225/3000:  41%|████      | 1225/3000 [09:03&lt;13:48,  2.14it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1225/3000:  41%|████      | 1225/3000 [09:03&lt;13:48,  2.14it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.34e+6]Epoch 1226/3000:  41%|████      | 1225/3000 [09:03&lt;13:48,  2.14it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.34e+6]Epoch 1226/3000:  41%|████      | 1226/3000 [09:04&lt;14:31,  2.03it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.34e+6]Epoch 1226/3000:  41%|████      | 1226/3000 [09:04&lt;14:31,  2.03it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1227/3000:  41%|████      | 1226/3000 [09:04&lt;14:31,  2.03it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1227/3000:  41%|████      | 1227/3000 [09:05&lt;15:31,  1.90it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1227/3000:  41%|████      | 1227/3000 [09:05&lt;15:31,  1.90it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.34e+6]Epoch 1228/3000:  41%|████      | 1227/3000 [09:05&lt;15:31,  1.90it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.34e+6]Epoch 1228/3000:  41%|████      | 1228/3000 [09:05&lt;15:38,  1.89it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.34e+6]Epoch 1228/3000:  41%|████      | 1228/3000 [09:05&lt;15:38,  1.89it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.34e+6] Epoch 1229/3000:  41%|████      | 1228/3000 [09:05&lt;15:38,  1.89it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.34e+6]Epoch 1229/3000:  41%|████      | 1229/3000 [09:06&lt;14:33,  2.03it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.34e+6]Epoch 1229/3000:  41%|████      | 1229/3000 [09:06&lt;14:33,  2.03it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1230/3000:  41%|████      | 1229/3000 [09:06&lt;14:33,  2.03it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1230/3000:  41%|████      | 1230/3000 [09:06&lt;15:30,  1.90it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1230/3000:  41%|████      | 1230/3000 [09:06&lt;15:30,  1.90it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1231/3000:  41%|████      | 1230/3000 [09:06&lt;15:30,  1.90it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1231/3000:  41%|████      | 1231/3000 [09:07&lt;16:18,  1.81it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1231/3000:  41%|████      | 1231/3000 [09:07&lt;16:18,  1.81it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.34e+6] Epoch 1232/3000:  41%|████      | 1231/3000 [09:07&lt;16:18,  1.81it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.34e+6]Epoch 1232/3000:  41%|████      | 1232/3000 [09:07&lt;16:27,  1.79it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.34e+6]Epoch 1232/3000:  41%|████      | 1232/3000 [09:07&lt;16:27,  1.79it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1233/3000:  41%|████      | 1232/3000 [09:07&lt;16:27,  1.79it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1233/3000:  41%|████      | 1233/3000 [09:08&lt;15:50,  1.86it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1233/3000:  41%|████      | 1233/3000 [09:08&lt;15:50,  1.86it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.34e+6]Epoch 1234/3000:  41%|████      | 1233/3000 [09:08&lt;15:50,  1.86it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.34e+6]Epoch 1234/3000:  41%|████      | 1234/3000 [09:08&lt;14:26,  2.04it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.34e+6]Epoch 1234/3000:  41%|████      | 1234/3000 [09:08&lt;14:26,  2.04it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.34e+6]Epoch 1235/3000:  41%|████      | 1234/3000 [09:08&lt;14:26,  2.04it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.34e+6]Epoch 1235/3000:  41%|████      | 1235/3000 [09:09&lt;14:48,  1.99it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.34e+6]Epoch 1235/3000:  41%|████      | 1235/3000 [09:09&lt;14:48,  1.99it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.34e+6]Epoch 1236/3000:  41%|████      | 1235/3000 [09:09&lt;14:48,  1.99it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.34e+6]Epoch 1236/3000:  41%|████      | 1236/3000 [09:09&lt;14:20,  2.05it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.34e+6]Epoch 1236/3000:  41%|████      | 1236/3000 [09:09&lt;14:20,  2.05it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.34e+6]Epoch 1237/3000:  41%|████      | 1236/3000 [09:09&lt;14:20,  2.05it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.34e+6]Epoch 1237/3000:  41%|████      | 1237/3000 [09:10&lt;14:05,  2.08it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.34e+6]Epoch 1237/3000:  41%|████      | 1237/3000 [09:10&lt;14:05,  2.08it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1238/3000:  41%|████      | 1237/3000 [09:10&lt;14:05,  2.08it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1238/3000:  41%|████▏     | 1238/3000 [09:10&lt;15:09,  1.94it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.34e+6]Epoch 1238/3000:  41%|████▏     | 1238/3000 [09:10&lt;15:09,  1.94it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.34e+6]Epoch 1239/3000:  41%|████▏     | 1238/3000 [09:10&lt;15:09,  1.94it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.34e+6]Epoch 1239/3000:  41%|████▏     | 1239/3000 [09:11&lt;15:12,  1.93it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.34e+6]Epoch 1239/3000:  41%|████▏     | 1239/3000 [09:11&lt;15:12,  1.93it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.34e+6]Epoch 1240/3000:  41%|████▏     | 1239/3000 [09:11&lt;15:12,  1.93it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.34e+6]Epoch 1240/3000:  41%|████▏     | 1240/3000 [09:11&lt;14:09,  2.07it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.34e+6]Epoch 1240/3000:  41%|████▏     | 1240/3000 [09:11&lt;14:09,  2.07it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1241/3000:  41%|████▏     | 1240/3000 [09:11&lt;14:09,  2.07it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1241/3000:  41%|████▏     | 1241/3000 [09:12&lt;14:05,  2.08it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1241/3000:  41%|████▏     | 1241/3000 [09:12&lt;14:05,  2.08it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1242/3000:  41%|████▏     | 1241/3000 [09:12&lt;14:05,  2.08it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1242/3000:  41%|████▏     | 1242/3000 [09:12&lt;15:07,  1.94it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1242/3000:  41%|████▏     | 1242/3000 [09:12&lt;15:07,  1.94it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.33e+6]Epoch 1243/3000:  41%|████▏     | 1242/3000 [09:12&lt;15:07,  1.94it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.33e+6]Epoch 1243/3000:  41%|████▏     | 1243/3000 [09:13&lt;14:03,  2.08it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.33e+6]Epoch 1243/3000:  41%|████▏     | 1243/3000 [09:13&lt;14:03,  2.08it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.33e+6]Epoch 1244/3000:  41%|████▏     | 1243/3000 [09:13&lt;14:03,  2.08it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.33e+6]Epoch 1244/3000:  41%|████▏     | 1244/3000 [09:13&lt;13:11,  2.22it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.33e+6]Epoch 1244/3000:  41%|████▏     | 1244/3000 [09:13&lt;13:11,  2.22it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1245/3000:  41%|████▏     | 1244/3000 [09:13&lt;13:11,  2.22it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1245/3000:  42%|████▏     | 1245/3000 [09:13&lt;12:05,  2.42it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1245/3000:  42%|████▏     | 1245/3000 [09:13&lt;12:05,  2.42it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1246/3000:  42%|████▏     | 1245/3000 [09:13&lt;12:05,  2.42it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1246/3000:  42%|████▏     | 1246/3000 [09:14&lt;11:53,  2.46it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1246/3000:  42%|████▏     | 1246/3000 [09:14&lt;11:53,  2.46it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.33e+6]Epoch 1247/3000:  42%|████▏     | 1246/3000 [09:14&lt;11:53,  2.46it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.33e+6]Epoch 1247/3000:  42%|████▏     | 1247/3000 [09:14&lt;10:57,  2.67it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.33e+6]Epoch 1247/3000:  42%|████▏     | 1247/3000 [09:14&lt;10:57,  2.67it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1248/3000:  42%|████▏     | 1247/3000 [09:14&lt;10:57,  2.67it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1248/3000:  42%|████▏     | 1248/3000 [09:14&lt;10:17,  2.84it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1248/3000:  42%|████▏     | 1248/3000 [09:14&lt;10:17,  2.84it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.33e+6]Epoch 1249/3000:  42%|████▏     | 1248/3000 [09:14&lt;10:17,  2.84it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.33e+6]Epoch 1249/3000:  42%|████▏     | 1249/3000 [09:15&lt;11:18,  2.58it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.33e+6]Epoch 1249/3000:  42%|████▏     | 1249/3000 [09:15&lt;11:18,  2.58it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.33e+6]Epoch 1250/3000:  42%|████▏     | 1249/3000 [09:15&lt;11:18,  2.58it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.33e+6]Epoch 1250/3000:  42%|████▏     | 1250/3000 [09:15&lt;09:58,  2.92it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.33e+6]Epoch 1250/3000:  42%|████▏     | 1250/3000 [09:15&lt;09:58,  2.92it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1251/3000:  42%|████▏     | 1250/3000 [09:15&lt;09:58,  2.92it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1251/3000:  42%|████▏     | 1251/3000 [09:15&lt;09:39,  3.02it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1251/3000:  42%|████▏     | 1251/3000 [09:15&lt;09:39,  3.02it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1252/3000:  42%|████▏     | 1251/3000 [09:15&lt;09:39,  3.02it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1252/3000:  42%|████▏     | 1252/3000 [09:16&lt;11:23,  2.56it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1252/3000:  42%|████▏     | 1252/3000 [09:16&lt;11:23,  2.56it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1253/3000:  42%|████▏     | 1252/3000 [09:16&lt;11:23,  2.56it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1253/3000:  42%|████▏     | 1253/3000 [09:16&lt;12:13,  2.38it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1253/3000:  42%|████▏     | 1253/3000 [09:16&lt;12:13,  2.38it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.33e+6]Epoch 1254/3000:  42%|████▏     | 1253/3000 [09:16&lt;12:13,  2.38it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.33e+6]Epoch 1254/3000:  42%|████▏     | 1254/3000 [09:17&lt;11:07,  2.62it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.33e+6]Epoch 1254/3000:  42%|████▏     | 1254/3000 [09:17&lt;11:07,  2.62it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1255/3000:  42%|████▏     | 1254/3000 [09:17&lt;11:07,  2.62it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1255/3000:  42%|████▏     | 1255/3000 [09:17&lt;13:09,  2.21it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1255/3000:  42%|████▏     | 1255/3000 [09:17&lt;13:09,  2.21it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1256/3000:  42%|████▏     | 1255/3000 [09:17&lt;13:09,  2.21it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1256/3000:  42%|████▏     | 1256/3000 [09:18&lt;13:36,  2.14it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1256/3000:  42%|████▏     | 1256/3000 [09:18&lt;13:36,  2.14it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.33e+6]Epoch 1257/3000:  42%|████▏     | 1256/3000 [09:18&lt;13:36,  2.14it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.33e+6]Epoch 1257/3000:  42%|████▏     | 1257/3000 [09:18&lt;14:16,  2.04it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.33e+6]Epoch 1257/3000:  42%|████▏     | 1257/3000 [09:18&lt;14:16,  2.04it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1258/3000:  42%|████▏     | 1257/3000 [09:18&lt;14:16,  2.04it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1258/3000:  42%|████▏     | 1258/3000 [09:19&lt;14:17,  2.03it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.33e+6]Epoch 1258/3000:  42%|████▏     | 1258/3000 [09:19&lt;14:17,  2.03it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1259/3000:  42%|████▏     | 1258/3000 [09:19&lt;14:17,  2.03it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1259/3000:  42%|████▏     | 1259/3000 [09:19&lt;14:34,  1.99it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.33e+6]Epoch 1259/3000:  42%|████▏     | 1259/3000 [09:19&lt;14:34,  1.99it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.32e+6]Epoch 1260/3000:  42%|████▏     | 1259/3000 [09:19&lt;14:34,  1.99it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.32e+6]Epoch 1260/3000:  42%|████▏     | 1260/3000 [09:20&lt;14:02,  2.07it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.32e+6]Epoch 1260/3000:  42%|████▏     | 1260/3000 [09:20&lt;14:02,  2.07it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1261/3000:  42%|████▏     | 1260/3000 [09:20&lt;14:02,  2.07it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1261/3000:  42%|████▏     | 1261/3000 [09:20&lt;13:55,  2.08it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1261/3000:  42%|████▏     | 1261/3000 [09:20&lt;13:55,  2.08it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.32e+6]Epoch 1262/3000:  42%|████▏     | 1261/3000 [09:20&lt;13:55,  2.08it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.32e+6]Epoch 1262/3000:  42%|████▏     | 1262/3000 [09:21&lt;14:20,  2.02it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.32e+6]Epoch 1262/3000:  42%|████▏     | 1262/3000 [09:21&lt;14:20,  2.02it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.32e+6]Epoch 1263/3000:  42%|████▏     | 1262/3000 [09:21&lt;14:20,  2.02it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.32e+6]Epoch 1263/3000:  42%|████▏     | 1263/3000 [09:21&lt;14:16,  2.03it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.32e+6]Epoch 1263/3000:  42%|████▏     | 1263/3000 [09:21&lt;14:16,  2.03it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.32e+6] Epoch 1264/3000:  42%|████▏     | 1263/3000 [09:21&lt;14:16,  2.03it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.32e+6]Epoch 1264/3000:  42%|████▏     | 1264/3000 [09:22&lt;15:19,  1.89it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.32e+6]Epoch 1264/3000:  42%|████▏     | 1264/3000 [09:22&lt;15:19,  1.89it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.32e+6]Epoch 1265/3000:  42%|████▏     | 1264/3000 [09:22&lt;15:19,  1.89it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.32e+6]Epoch 1265/3000:  42%|████▏     | 1265/3000 [09:22&lt;15:05,  1.92it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.32e+6]Epoch 1265/3000:  42%|████▏     | 1265/3000 [09:22&lt;15:05,  1.92it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1266/3000:  42%|████▏     | 1265/3000 [09:22&lt;15:05,  1.92it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1266/3000:  42%|████▏     | 1266/3000 [09:23&lt;16:04,  1.80it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1266/3000:  42%|████▏     | 1266/3000 [09:23&lt;16:04,  1.80it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1267/3000:  42%|████▏     | 1266/3000 [09:23&lt;16:04,  1.80it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1267/3000:  42%|████▏     | 1267/3000 [09:24&lt;16:20,  1.77it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1267/3000:  42%|████▏     | 1267/3000 [09:24&lt;16:20,  1.77it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.32e+6]Epoch 1268/3000:  42%|████▏     | 1267/3000 [09:24&lt;16:20,  1.77it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.32e+6]Epoch 1268/3000:  42%|████▏     | 1268/3000 [09:24&lt;16:52,  1.71it/s, v_num=1, train_loss_step=1.36e+6, train_loss_epoch=1.32e+6]Epoch 1268/3000:  42%|████▏     | 1268/3000 [09:24&lt;16:52,  1.71it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1269/3000:  42%|████▏     | 1268/3000 [09:24&lt;16:52,  1.71it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1269/3000:  42%|████▏     | 1269/3000 [09:25&lt;15:04,  1.91it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1269/3000:  42%|████▏     | 1269/3000 [09:25&lt;15:04,  1.91it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.32e+6]Epoch 1270/3000:  42%|████▏     | 1269/3000 [09:25&lt;15:04,  1.91it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.32e+6]Epoch 1270/3000:  42%|████▏     | 1270/3000 [09:25&lt;14:10,  2.03it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.32e+6]Epoch 1270/3000:  42%|████▏     | 1270/3000 [09:25&lt;14:10,  2.03it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.32e+6]Epoch 1271/3000:  42%|████▏     | 1270/3000 [09:25&lt;14:10,  2.03it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.32e+6]Epoch 1271/3000:  42%|████▏     | 1271/3000 [09:26&lt;13:47,  2.09it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.32e+6]Epoch 1271/3000:  42%|████▏     | 1271/3000 [09:26&lt;13:47,  2.09it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.32e+6]Epoch 1272/3000:  42%|████▏     | 1271/3000 [09:26&lt;13:47,  2.09it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.32e+6]Epoch 1272/3000:  42%|████▏     | 1272/3000 [09:26&lt;13:23,  2.15it/s, v_num=1, train_loss_step=1.35e+6, train_loss_epoch=1.32e+6]Epoch 1272/3000:  42%|████▏     | 1272/3000 [09:26&lt;13:23,  2.15it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.32e+6]Epoch 1273/3000:  42%|████▏     | 1272/3000 [09:26&lt;13:23,  2.15it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.32e+6]Epoch 1273/3000:  42%|████▏     | 1273/3000 [09:27&lt;14:19,  2.01it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.32e+6]Epoch 1273/3000:  42%|████▏     | 1273/3000 [09:27&lt;14:19,  2.01it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.32e+6]Epoch 1274/3000:  42%|████▏     | 1273/3000 [09:27&lt;14:19,  2.01it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.32e+6]Epoch 1274/3000:  42%|████▏     | 1274/3000 [09:27&lt;14:14,  2.02it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.32e+6]Epoch 1274/3000:  42%|████▏     | 1274/3000 [09:27&lt;14:14,  2.02it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.32e+6]Epoch 1275/3000:  42%|████▏     | 1274/3000 [09:27&lt;14:14,  2.02it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.32e+6]Epoch 1275/3000:  42%|████▎     | 1275/3000 [09:27&lt;13:25,  2.14it/s, v_num=1, train_loss_step=1.37e+6, train_loss_epoch=1.32e+6]Epoch 1275/3000:  42%|████▎     | 1275/3000 [09:27&lt;13:25,  2.14it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1276/3000:  42%|████▎     | 1275/3000 [09:27&lt;13:25,  2.14it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1276/3000:  43%|████▎     | 1276/3000 [09:28&lt;13:46,  2.09it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1276/3000:  43%|████▎     | 1276/3000 [09:28&lt;13:46,  2.09it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.32e+6]Epoch 1277/3000:  43%|████▎     | 1276/3000 [09:28&lt;13:46,  2.09it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.32e+6]Epoch 1277/3000:  43%|████▎     | 1277/3000 [09:29&lt;14:57,  1.92it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.32e+6]Epoch 1277/3000:  43%|████▎     | 1277/3000 [09:29&lt;14:57,  1.92it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1278/3000:  43%|████▎     | 1277/3000 [09:29&lt;14:57,  1.92it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1278/3000:  43%|████▎     | 1278/3000 [09:29&lt;13:52,  2.07it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.32e+6]Epoch 1278/3000:  43%|████▎     | 1278/3000 [09:29&lt;13:52,  2.07it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.32e+6] Epoch 1279/3000:  43%|████▎     | 1278/3000 [09:29&lt;13:52,  2.07it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.32e+6]Epoch 1279/3000:  43%|████▎     | 1279/3000 [09:29&lt;13:15,  2.16it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.32e+6]Epoch 1279/3000:  43%|████▎     | 1279/3000 [09:29&lt;13:15,  2.16it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.32e+6]Epoch 1280/3000:  43%|████▎     | 1279/3000 [09:29&lt;13:15,  2.16it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.32e+6]Epoch 1280/3000:  43%|████▎     | 1280/3000 [09:30&lt;13:07,  2.19it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.32e+6]Epoch 1280/3000:  43%|████▎     | 1280/3000 [09:30&lt;13:07,  2.19it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.31e+6] Epoch 1281/3000:  43%|████▎     | 1280/3000 [09:30&lt;13:07,  2.19it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.31e+6]Epoch 1281/3000:  43%|████▎     | 1281/3000 [09:30&lt;13:06,  2.18it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.31e+6]Epoch 1281/3000:  43%|████▎     | 1281/3000 [09:30&lt;13:06,  2.18it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.31e+6]Epoch 1282/3000:  43%|████▎     | 1281/3000 [09:30&lt;13:06,  2.18it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.31e+6]Epoch 1282/3000:  43%|████▎     | 1282/3000 [09:31&lt;11:31,  2.48it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.31e+6]Epoch 1282/3000:  43%|████▎     | 1282/3000 [09:31&lt;11:31,  2.48it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.31e+6]Epoch 1283/3000:  43%|████▎     | 1282/3000 [09:31&lt;11:31,  2.48it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.31e+6]Epoch 1283/3000:  43%|████▎     | 1283/3000 [09:31&lt;12:40,  2.26it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.31e+6]Epoch 1283/3000:  43%|████▎     | 1283/3000 [09:31&lt;12:40,  2.26it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.31e+6]Epoch 1284/3000:  43%|████▎     | 1283/3000 [09:31&lt;12:40,  2.26it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.31e+6]Epoch 1284/3000:  43%|████▎     | 1284/3000 [09:31&lt;12:21,  2.31it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.31e+6]Epoch 1284/3000:  43%|████▎     | 1284/3000 [09:31&lt;12:21,  2.31it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.31e+6]Epoch 1285/3000:  43%|████▎     | 1284/3000 [09:31&lt;12:21,  2.31it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.31e+6]Epoch 1285/3000:  43%|████▎     | 1285/3000 [09:32&lt;13:10,  2.17it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.31e+6]Epoch 1285/3000:  43%|████▎     | 1285/3000 [09:32&lt;13:10,  2.17it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1286/3000:  43%|████▎     | 1285/3000 [09:32&lt;13:10,  2.17it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1286/3000:  43%|████▎     | 1286/3000 [09:32&lt;13:19,  2.14it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1286/3000:  43%|████▎     | 1286/3000 [09:32&lt;13:19,  2.14it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.31e+6]Epoch 1287/3000:  43%|████▎     | 1286/3000 [09:32&lt;13:19,  2.14it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.31e+6]Epoch 1287/3000:  43%|████▎     | 1287/3000 [09:33&lt;13:01,  2.19it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.31e+6]Epoch 1287/3000:  43%|████▎     | 1287/3000 [09:33&lt;13:01,  2.19it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1288/3000:  43%|████▎     | 1287/3000 [09:33&lt;13:01,  2.19it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1288/3000:  43%|████▎     | 1288/3000 [09:33&lt;13:37,  2.09it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1288/3000:  43%|████▎     | 1288/3000 [09:33&lt;13:37,  2.09it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.31e+6]Epoch 1289/3000:  43%|████▎     | 1288/3000 [09:33&lt;13:37,  2.09it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.31e+6]Epoch 1289/3000:  43%|████▎     | 1289/3000 [09:34&lt;14:46,  1.93it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.31e+6]Epoch 1289/3000:  43%|████▎     | 1289/3000 [09:34&lt;14:46,  1.93it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1290/3000:  43%|████▎     | 1289/3000 [09:34&lt;14:46,  1.93it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1290/3000:  43%|████▎     | 1290/3000 [09:35&lt;15:41,  1.82it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1290/3000:  43%|████▎     | 1290/3000 [09:35&lt;15:41,  1.82it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.31e+6]Epoch 1291/3000:  43%|████▎     | 1290/3000 [09:35&lt;15:41,  1.82it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.31e+6]Epoch 1291/3000:  43%|████▎     | 1291/3000 [09:35&lt;14:30,  1.96it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.31e+6]Epoch 1291/3000:  43%|████▎     | 1291/3000 [09:35&lt;14:30,  1.96it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.31e+6]Epoch 1292/3000:  43%|████▎     | 1291/3000 [09:35&lt;14:30,  1.96it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.31e+6]Epoch 1292/3000:  43%|████▎     | 1292/3000 [09:35&lt;13:03,  2.18it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.31e+6]Epoch 1292/3000:  43%|████▎     | 1292/3000 [09:35&lt;13:03,  2.18it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.31e+6] Epoch 1293/3000:  43%|████▎     | 1292/3000 [09:35&lt;13:03,  2.18it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.31e+6]Epoch 1293/3000:  43%|████▎     | 1293/3000 [09:36&lt;12:05,  2.35it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.31e+6]Epoch 1293/3000:  43%|████▎     | 1293/3000 [09:36&lt;12:05,  2.35it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.31e+6]Epoch 1294/3000:  43%|████▎     | 1293/3000 [09:36&lt;12:05,  2.35it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.31e+6]Epoch 1294/3000:  43%|████▎     | 1294/3000 [09:36&lt;10:50,  2.62it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.31e+6]Epoch 1294/3000:  43%|████▎     | 1294/3000 [09:36&lt;10:50,  2.62it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1295/3000:  43%|████▎     | 1294/3000 [09:36&lt;10:50,  2.62it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1295/3000:  43%|████▎     | 1295/3000 [09:36&lt;10:38,  2.67it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1295/3000:  43%|████▎     | 1295/3000 [09:36&lt;10:38,  2.67it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1296/3000:  43%|████▎     | 1295/3000 [09:36&lt;10:38,  2.67it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1296/3000:  43%|████▎     | 1296/3000 [09:37&lt;10:13,  2.78it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1296/3000:  43%|████▎     | 1296/3000 [09:37&lt;10:13,  2.78it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.31e+6] Epoch 1297/3000:  43%|████▎     | 1296/3000 [09:37&lt;10:13,  2.78it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.31e+6]Epoch 1297/3000:  43%|████▎     | 1297/3000 [09:37&lt;11:09,  2.54it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.31e+6]Epoch 1297/3000:  43%|████▎     | 1297/3000 [09:37&lt;11:09,  2.54it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1298/3000:  43%|████▎     | 1297/3000 [09:37&lt;11:09,  2.54it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1298/3000:  43%|████▎     | 1298/3000 [09:38&lt;12:23,  2.29it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.31e+6]Epoch 1298/3000:  43%|████▎     | 1298/3000 [09:38&lt;12:23,  2.29it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.31e+6]Epoch 1299/3000:  43%|████▎     | 1298/3000 [09:38&lt;12:23,  2.29it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.31e+6]Epoch 1299/3000:  43%|████▎     | 1299/3000 [09:38&lt;11:41,  2.42it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.31e+6]Epoch 1299/3000:  43%|████▎     | 1299/3000 [09:38&lt;11:41,  2.42it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.31e+6] Epoch 1300/3000:  43%|████▎     | 1299/3000 [09:38&lt;11:41,  2.42it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.31e+6]Epoch 1300/3000:  43%|████▎     | 1300/3000 [09:39&lt;11:45,  2.41it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.31e+6]Epoch 1300/3000:  43%|████▎     | 1300/3000 [09:39&lt;11:45,  2.41it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.31e+6]Epoch 1301/3000:  43%|████▎     | 1300/3000 [09:39&lt;11:45,  2.41it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.31e+6]Epoch 1301/3000:  43%|████▎     | 1301/3000 [09:39&lt;13:08,  2.15it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.31e+6]Epoch 1301/3000:  43%|████▎     | 1301/3000 [09:39&lt;13:08,  2.15it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.3e+6] Epoch 1302/3000:  43%|████▎     | 1301/3000 [09:39&lt;13:08,  2.15it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.3e+6]Epoch 1302/3000:  43%|████▎     | 1302/3000 [09:40&lt;14:19,  1.98it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.3e+6]Epoch 1302/3000:  43%|████▎     | 1302/3000 [09:40&lt;14:19,  1.98it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6] Epoch 1303/3000:  43%|████▎     | 1302/3000 [09:40&lt;14:19,  1.98it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6]Epoch 1303/3000:  43%|████▎     | 1303/3000 [09:40&lt;15:13,  1.86it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6]Epoch 1303/3000:  43%|████▎     | 1303/3000 [09:40&lt;15:13,  1.86it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6]Epoch 1304/3000:  43%|████▎     | 1303/3000 [09:40&lt;15:13,  1.86it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6]Epoch 1304/3000:  43%|████▎     | 1304/3000 [09:41&lt;14:33,  1.94it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6]Epoch 1304/3000:  43%|████▎     | 1304/3000 [09:41&lt;14:33,  1.94it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6]Epoch 1305/3000:  43%|████▎     | 1304/3000 [09:41&lt;14:33,  1.94it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6]Epoch 1305/3000:  44%|████▎     | 1305/3000 [09:41&lt;13:35,  2.08it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6]Epoch 1305/3000:  44%|████▎     | 1305/3000 [09:41&lt;13:35,  2.08it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.3e+6]Epoch 1306/3000:  44%|████▎     | 1305/3000 [09:41&lt;13:35,  2.08it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.3e+6]Epoch 1306/3000:  44%|████▎     | 1306/3000 [09:42&lt;13:03,  2.16it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.3e+6]Epoch 1306/3000:  44%|████▎     | 1306/3000 [09:42&lt;13:03,  2.16it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.3e+6]Epoch 1307/3000:  44%|████▎     | 1306/3000 [09:42&lt;13:03,  2.16it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.3e+6]Epoch 1307/3000:  44%|████▎     | 1307/3000 [09:42&lt;13:02,  2.16it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6] Epoch 1308/3000:  44%|████▎     | 1307/3000 [09:42&lt;13:02,  2.16it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6]Epoch 1308/3000:  44%|████▎     | 1308/3000 [09:42&lt;09:08,  3.08it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6]Epoch 1308/3000:  44%|████▎     | 1308/3000 [09:42&lt;09:08,  3.08it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.3e+6]Epoch 1309/3000:  44%|████▎     | 1308/3000 [09:42&lt;09:08,  3.08it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.3e+6]Epoch 1309/3000:  44%|████▎     | 1309/3000 [09:42&lt;08:25,  3.34it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.3e+6]Epoch 1309/3000:  44%|████▎     | 1309/3000 [09:42&lt;08:25,  3.34it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6] Epoch 1310/3000:  44%|████▎     | 1309/3000 [09:42&lt;08:25,  3.34it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6]Epoch 1310/3000:  44%|████▎     | 1310/3000 [09:42&lt;08:25,  3.34it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.3e+6]Epoch 1311/3000:  44%|████▎     | 1310/3000 [09:42&lt;08:25,  3.34it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.3e+6]Epoch 1311/3000:  44%|████▎     | 1311/3000 [09:42&lt;05:57,  4.72it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.3e+6]Epoch 1311/3000:  44%|████▎     | 1311/3000 [09:42&lt;05:57,  4.72it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6] Epoch 1312/3000:  44%|████▎     | 1311/3000 [09:42&lt;05:57,  4.72it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6]Epoch 1312/3000:  44%|████▎     | 1312/3000 [09:42&lt;05:41,  4.94it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6]Epoch 1312/3000:  44%|████▎     | 1312/3000 [09:42&lt;05:41,  4.94it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6]Epoch 1313/3000:  44%|████▎     | 1312/3000 [09:43&lt;05:41,  4.94it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6]Epoch 1313/3000:  44%|████▍     | 1313/3000 [09:43&lt;05:05,  5.52it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.3e+6]Epoch 1313/3000:  44%|████▍     | 1313/3000 [09:43&lt;05:05,  5.52it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.3e+6]Epoch 1314/3000:  44%|████▍     | 1313/3000 [09:43&lt;05:05,  5.52it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.3e+6]Epoch 1314/3000:  44%|████▍     | 1314/3000 [09:43&lt;05:05,  5.52it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.3e+6]Epoch 1315/3000:  44%|████▍     | 1314/3000 [09:43&lt;05:05,  5.52it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.3e+6]Epoch 1315/3000:  44%|████▍     | 1315/3000 [09:43&lt;05:22,  5.22it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.3e+6]Epoch 1315/3000:  44%|████▍     | 1315/3000 [09:43&lt;05:22,  5.22it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.3e+6]Epoch 1316/3000:  44%|████▍     | 1315/3000 [09:43&lt;05:22,  5.22it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.3e+6]Epoch 1316/3000:  44%|████▍     | 1316/3000 [09:44&lt;07:16,  3.86it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.3e+6]Epoch 1316/3000:  44%|████▍     | 1316/3000 [09:44&lt;07:16,  3.86it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.3e+6]Epoch 1317/3000:  44%|████▍     | 1316/3000 [09:44&lt;07:16,  3.86it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.3e+6]Epoch 1317/3000:  44%|████▍     | 1317/3000 [09:44&lt;08:48,  3.18it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.3e+6]Epoch 1317/3000:  44%|████▍     | 1317/3000 [09:44&lt;08:48,  3.18it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.3e+6]Epoch 1318/3000:  44%|████▍     | 1317/3000 [09:44&lt;08:48,  3.18it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.3e+6]Epoch 1318/3000:  44%|████▍     | 1318/3000 [09:44&lt;09:19,  3.01it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.3e+6]Epoch 1318/3000:  44%|████▍     | 1318/3000 [09:44&lt;09:19,  3.01it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.3e+6]Epoch 1319/3000:  44%|████▍     | 1318/3000 [09:44&lt;09:19,  3.01it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.3e+6]Epoch 1319/3000:  44%|████▍     | 1319/3000 [09:45&lt;11:21,  2.47it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.3e+6]Epoch 1319/3000:  44%|████▍     | 1319/3000 [09:45&lt;11:21,  2.47it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.3e+6]Epoch 1320/3000:  44%|████▍     | 1319/3000 [09:45&lt;11:21,  2.47it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.3e+6]Epoch 1320/3000:  44%|████▍     | 1320/3000 [09:46&lt;12:26,  2.25it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.3e+6]Epoch 1320/3000:  44%|████▍     | 1320/3000 [09:46&lt;12:26,  2.25it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.3e+6]Epoch 1321/3000:  44%|████▍     | 1320/3000 [09:46&lt;12:26,  2.25it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.3e+6]Epoch 1321/3000:  44%|████▍     | 1321/3000 [09:46&lt;13:21,  2.10it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.3e+6]Epoch 1321/3000:  44%|████▍     | 1321/3000 [09:46&lt;13:21,  2.10it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.3e+6]Epoch 1322/3000:  44%|████▍     | 1321/3000 [09:46&lt;13:21,  2.10it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.3e+6]Epoch 1322/3000:  44%|████▍     | 1322/3000 [09:47&lt;14:28,  1.93it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.3e+6]Epoch 1322/3000:  44%|████▍     | 1322/3000 [09:47&lt;14:28,  1.93it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.3e+6]Epoch 1323/3000:  44%|████▍     | 1322/3000 [09:47&lt;14:28,  1.93it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.3e+6]Epoch 1323/3000:  44%|████▍     | 1323/3000 [09:47&lt;12:00,  2.33it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.3e+6]Epoch 1323/3000:  44%|████▍     | 1323/3000 [09:47&lt;12:00,  2.33it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.3e+6]Epoch 1324/3000:  44%|████▍     | 1323/3000 [09:47&lt;12:00,  2.33it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.3e+6]Epoch 1324/3000:  44%|████▍     | 1324/3000 [09:47&lt;12:18,  2.27it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.3e+6]Epoch 1324/3000:  44%|████▍     | 1324/3000 [09:47&lt;12:18,  2.27it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.29e+6]Epoch 1325/3000:  44%|████▍     | 1324/3000 [09:47&lt;12:18,  2.27it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.29e+6]Epoch 1325/3000:  44%|████▍     | 1325/3000 [09:48&lt;12:15,  2.28it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.29e+6]Epoch 1325/3000:  44%|████▍     | 1325/3000 [09:48&lt;12:15,  2.28it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.29e+6]Epoch 1326/3000:  44%|████▍     | 1325/3000 [09:48&lt;12:15,  2.28it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.29e+6]Epoch 1326/3000:  44%|████▍     | 1326/3000 [09:48&lt;11:47,  2.37it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.29e+6]Epoch 1326/3000:  44%|████▍     | 1326/3000 [09:48&lt;11:47,  2.37it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6] Epoch 1327/3000:  44%|████▍     | 1326/3000 [09:48&lt;11:47,  2.37it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1327/3000:  44%|████▍     | 1327/3000 [09:49&lt;12:25,  2.24it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1327/3000:  44%|████▍     | 1327/3000 [09:49&lt;12:25,  2.24it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1328/3000:  44%|████▍     | 1327/3000 [09:49&lt;12:25,  2.24it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1328/3000:  44%|████▍     | 1328/3000 [09:49&lt;12:10,  2.29it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1328/3000:  44%|████▍     | 1328/3000 [09:49&lt;12:10,  2.29it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.29e+6]Epoch 1329/3000:  44%|████▍     | 1328/3000 [09:49&lt;12:10,  2.29it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.29e+6]Epoch 1329/3000:  44%|████▍     | 1329/3000 [09:50&lt;12:20,  2.26it/s, v_num=1, train_loss_step=1.33e+6, train_loss_epoch=1.29e+6]Epoch 1329/3000:  44%|████▍     | 1329/3000 [09:50&lt;12:20,  2.26it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.29e+6]Epoch 1330/3000:  44%|████▍     | 1329/3000 [09:50&lt;12:20,  2.26it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.29e+6]Epoch 1330/3000:  44%|████▍     | 1330/3000 [09:50&lt;12:45,  2.18it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.29e+6]Epoch 1330/3000:  44%|████▍     | 1330/3000 [09:50&lt;12:45,  2.18it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.29e+6]Epoch 1331/3000:  44%|████▍     | 1330/3000 [09:50&lt;12:45,  2.18it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.29e+6]Epoch 1331/3000:  44%|████▍     | 1331/3000 [09:51&lt;14:44,  1.89it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.29e+6]Epoch 1331/3000:  44%|████▍     | 1331/3000 [09:51&lt;14:44,  1.89it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6] Epoch 1332/3000:  44%|████▍     | 1331/3000 [09:51&lt;14:44,  1.89it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1332/3000:  44%|████▍     | 1332/3000 [09:51&lt;14:49,  1.87it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1332/3000:  44%|████▍     | 1332/3000 [09:51&lt;14:49,  1.87it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1333/3000:  44%|████▍     | 1332/3000 [09:51&lt;14:49,  1.87it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1333/3000:  44%|████▍     | 1333/3000 [09:52&lt;13:45,  2.02it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1333/3000:  44%|████▍     | 1333/3000 [09:52&lt;13:45,  2.02it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.29e+6]Epoch 1334/3000:  44%|████▍     | 1333/3000 [09:52&lt;13:45,  2.02it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.29e+6]Epoch 1334/3000:  44%|████▍     | 1334/3000 [09:52&lt;12:36,  2.20it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.29e+6]Epoch 1334/3000:  44%|████▍     | 1334/3000 [09:52&lt;12:36,  2.20it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.29e+6]Epoch 1335/3000:  44%|████▍     | 1334/3000 [09:52&lt;12:36,  2.20it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.29e+6]Epoch 1335/3000:  44%|████▍     | 1335/3000 [09:53&lt;13:08,  2.11it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.29e+6]Epoch 1335/3000:  44%|████▍     | 1335/3000 [09:53&lt;13:08,  2.11it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.29e+6]Epoch 1336/3000:  44%|████▍     | 1335/3000 [09:53&lt;13:08,  2.11it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.29e+6]Epoch 1336/3000:  45%|████▍     | 1336/3000 [09:53&lt;14:03,  1.97it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.29e+6]Epoch 1336/3000:  45%|████▍     | 1336/3000 [09:53&lt;14:03,  1.97it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.29e+6]Epoch 1337/3000:  45%|████▍     | 1336/3000 [09:53&lt;14:03,  1.97it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.29e+6]Epoch 1337/3000:  45%|████▍     | 1337/3000 [09:54&lt;14:37,  1.89it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.29e+6]Epoch 1337/3000:  45%|████▍     | 1337/3000 [09:54&lt;14:37,  1.89it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6] Epoch 1338/3000:  45%|████▍     | 1337/3000 [09:54&lt;14:37,  1.89it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1338/3000:  45%|████▍     | 1338/3000 [09:54&lt;13:25,  2.06it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1338/3000:  45%|████▍     | 1338/3000 [09:54&lt;13:25,  2.06it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.29e+6]Epoch 1339/3000:  45%|████▍     | 1338/3000 [09:54&lt;13:25,  2.06it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.29e+6]Epoch 1339/3000:  45%|████▍     | 1339/3000 [09:54&lt;12:16,  2.26it/s, v_num=1, train_loss_step=1.31e+6, train_loss_epoch=1.29e+6]Epoch 1339/3000:  45%|████▍     | 1339/3000 [09:54&lt;12:16,  2.26it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.29e+6]Epoch 1340/3000:  45%|████▍     | 1339/3000 [09:55&lt;12:16,  2.26it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.29e+6]Epoch 1340/3000:  45%|████▍     | 1340/3000 [09:55&lt;11:02,  2.50it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.29e+6]Epoch 1340/3000:  45%|████▍     | 1340/3000 [09:55&lt;11:02,  2.50it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.29e+6]Epoch 1341/3000:  45%|████▍     | 1340/3000 [09:55&lt;11:02,  2.50it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.29e+6]Epoch 1341/3000:  45%|████▍     | 1341/3000 [09:55&lt;10:03,  2.75it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.29e+6]Epoch 1341/3000:  45%|████▍     | 1341/3000 [09:55&lt;10:03,  2.75it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.29e+6]Epoch 1342/3000:  45%|████▍     | 1341/3000 [09:55&lt;10:03,  2.75it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.29e+6]Epoch 1342/3000:  45%|████▍     | 1342/3000 [09:56&lt;12:12,  2.26it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.29e+6]Epoch 1342/3000:  45%|████▍     | 1342/3000 [09:56&lt;12:12,  2.26it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6] Epoch 1343/3000:  45%|████▍     | 1342/3000 [09:56&lt;12:12,  2.26it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1343/3000:  45%|████▍     | 1343/3000 [09:56&lt;13:30,  2.04it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1343/3000:  45%|████▍     | 1343/3000 [09:56&lt;13:30,  2.04it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1344/3000:  45%|████▍     | 1343/3000 [09:56&lt;13:30,  2.04it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1344/3000:  45%|████▍     | 1344/3000 [09:57&lt;14:29,  1.91it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1344/3000:  45%|████▍     | 1344/3000 [09:57&lt;14:29,  1.91it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1345/3000:  45%|████▍     | 1344/3000 [09:57&lt;14:29,  1.91it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1345/3000:  45%|████▍     | 1345/3000 [09:57&lt;14:46,  1.87it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1345/3000:  45%|████▍     | 1345/3000 [09:57&lt;14:46,  1.87it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.29e+6]Epoch 1346/3000:  45%|████▍     | 1345/3000 [09:57&lt;14:46,  1.87it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.29e+6]Epoch 1346/3000:  45%|████▍     | 1346/3000 [09:58&lt;14:04,  1.96it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.29e+6]Epoch 1346/3000:  45%|████▍     | 1346/3000 [09:58&lt;14:04,  1.96it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6] Epoch 1347/3000:  45%|████▍     | 1346/3000 [09:58&lt;14:04,  1.96it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1347/3000:  45%|████▍     | 1347/3000 [09:58&lt;12:53,  2.14it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.29e+6]Epoch 1347/3000:  45%|████▍     | 1347/3000 [09:58&lt;12:53,  2.14it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1348/3000:  45%|████▍     | 1347/3000 [09:58&lt;12:53,  2.14it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1348/3000:  45%|████▍     | 1348/3000 [09:59&lt;12:59,  2.12it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1348/3000:  45%|████▍     | 1348/3000 [09:59&lt;12:59,  2.12it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.28e+6]Epoch 1349/3000:  45%|████▍     | 1348/3000 [09:59&lt;12:59,  2.12it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.28e+6]Epoch 1349/3000:  45%|████▍     | 1349/3000 [09:59&lt;12:44,  2.16it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.28e+6]Epoch 1349/3000:  45%|████▍     | 1349/3000 [09:59&lt;12:44,  2.16it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.28e+6] Epoch 1350/3000:  45%|████▍     | 1349/3000 [09:59&lt;12:44,  2.16it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.28e+6]Epoch 1350/3000:  45%|████▌     | 1350/3000 [10:00&lt;13:27,  2.04it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.28e+6]Epoch 1350/3000:  45%|████▌     | 1350/3000 [10:00&lt;13:27,  2.04it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.28e+6]Epoch 1351/3000:  45%|████▌     | 1350/3000 [10:00&lt;13:27,  2.04it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.28e+6]Epoch 1351/3000:  45%|████▌     | 1351/3000 [10:00&lt;12:52,  2.13it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.28e+6]Epoch 1351/3000:  45%|████▌     | 1351/3000 [10:00&lt;12:52,  2.13it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1352/3000:  45%|████▌     | 1351/3000 [10:00&lt;12:52,  2.13it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1352/3000:  45%|████▌     | 1352/3000 [10:01&lt;13:55,  1.97it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1352/3000:  45%|████▌     | 1352/3000 [10:01&lt;13:55,  1.97it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.28e+6]Epoch 1353/3000:  45%|████▌     | 1352/3000 [10:01&lt;13:55,  1.97it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.28e+6]Epoch 1353/3000:  45%|████▌     | 1353/3000 [10:01&lt;14:54,  1.84it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.28e+6]Epoch 1353/3000:  45%|████▌     | 1353/3000 [10:01&lt;14:54,  1.84it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.28e+6]Epoch 1354/3000:  45%|████▌     | 1353/3000 [10:01&lt;14:54,  1.84it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.28e+6]Epoch 1354/3000:  45%|████▌     | 1354/3000 [10:02&lt;14:05,  1.95it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.28e+6]Epoch 1354/3000:  45%|████▌     | 1354/3000 [10:02&lt;14:05,  1.95it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.28e+6]Epoch 1355/3000:  45%|████▌     | 1354/3000 [10:02&lt;14:05,  1.95it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.28e+6]Epoch 1355/3000:  45%|████▌     | 1355/3000 [10:02&lt;11:04,  2.47it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.28e+6]Epoch 1355/3000:  45%|████▌     | 1355/3000 [10:02&lt;11:04,  2.47it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1356/3000:  45%|████▌     | 1355/3000 [10:02&lt;11:04,  2.47it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1356/3000:  45%|████▌     | 1356/3000 [10:02&lt;11:45,  2.33it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1356/3000:  45%|████▌     | 1356/3000 [10:02&lt;11:45,  2.33it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1357/3000:  45%|████▌     | 1356/3000 [10:02&lt;11:45,  2.33it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1357/3000:  45%|████▌     | 1357/3000 [10:03&lt;10:18,  2.66it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1357/3000:  45%|████▌     | 1357/3000 [10:03&lt;10:18,  2.66it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.28e+6]Epoch 1358/3000:  45%|████▌     | 1357/3000 [10:03&lt;10:18,  2.66it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.28e+6]Epoch 1358/3000:  45%|████▌     | 1358/3000 [10:03&lt;11:26,  2.39it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.28e+6]Epoch 1358/3000:  45%|████▌     | 1358/3000 [10:03&lt;11:26,  2.39it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.28e+6]Epoch 1359/3000:  45%|████▌     | 1358/3000 [10:03&lt;11:26,  2.39it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.28e+6]Epoch 1359/3000:  45%|████▌     | 1359/3000 [10:04&lt;13:17,  2.06it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.28e+6]Epoch 1359/3000:  45%|████▌     | 1359/3000 [10:04&lt;13:17,  2.06it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.28e+6]Epoch 1360/3000:  45%|████▌     | 1359/3000 [10:04&lt;13:17,  2.06it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.28e+6]Epoch 1360/3000:  45%|████▌     | 1360/3000 [10:04&lt;12:07,  2.25it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.28e+6]Epoch 1360/3000:  45%|████▌     | 1360/3000 [10:04&lt;12:07,  2.25it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.28e+6]Epoch 1361/3000:  45%|████▌     | 1360/3000 [10:04&lt;12:07,  2.25it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.28e+6]Epoch 1361/3000:  45%|████▌     | 1361/3000 [10:05&lt;11:24,  2.39it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.28e+6]Epoch 1361/3000:  45%|████▌     | 1361/3000 [10:05&lt;11:24,  2.39it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.28e+6]Epoch 1362/3000:  45%|████▌     | 1361/3000 [10:05&lt;11:24,  2.39it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.28e+6]Epoch 1362/3000:  45%|████▌     | 1362/3000 [10:05&lt;13:17,  2.05it/s, v_num=1, train_loss_step=1.34e+6, train_loss_epoch=1.28e+6]Epoch 1362/3000:  45%|████▌     | 1362/3000 [10:05&lt;13:17,  2.05it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.28e+6]Epoch 1363/3000:  45%|████▌     | 1362/3000 [10:05&lt;13:17,  2.05it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.28e+6]Epoch 1363/3000:  45%|████▌     | 1363/3000 [10:06&lt;14:19,  1.90it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.28e+6]Epoch 1363/3000:  45%|████▌     | 1363/3000 [10:06&lt;14:19,  1.90it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1364/3000:  45%|████▌     | 1363/3000 [10:06&lt;14:19,  1.90it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1364/3000:  45%|████▌     | 1364/3000 [10:06&lt;12:45,  2.14it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1364/3000:  45%|████▌     | 1364/3000 [10:06&lt;12:45,  2.14it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1365/3000:  45%|████▌     | 1364/3000 [10:06&lt;12:45,  2.14it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1365/3000:  46%|████▌     | 1365/3000 [10:07&lt;12:24,  2.20it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.28e+6]Epoch 1365/3000:  46%|████▌     | 1365/3000 [10:07&lt;12:24,  2.20it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.28e+6]Epoch 1366/3000:  46%|████▌     | 1365/3000 [10:07&lt;12:24,  2.20it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.28e+6]Epoch 1366/3000:  46%|████▌     | 1366/3000 [10:07&lt;13:20,  2.04it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.28e+6]Epoch 1366/3000:  46%|████▌     | 1366/3000 [10:07&lt;13:20,  2.04it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.28e+6]Epoch 1367/3000:  46%|████▌     | 1366/3000 [10:07&lt;13:20,  2.04it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.28e+6]Epoch 1367/3000:  46%|████▌     | 1367/3000 [10:08&lt;13:32,  2.01it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.28e+6]Epoch 1367/3000:  46%|████▌     | 1367/3000 [10:08&lt;13:32,  2.01it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.28e+6]Epoch 1368/3000:  46%|████▌     | 1367/3000 [10:08&lt;13:32,  2.01it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.28e+6]Epoch 1368/3000:  46%|████▌     | 1368/3000 [10:08&lt;14:13,  1.91it/s, v_num=1, train_loss_step=1.32e+6, train_loss_epoch=1.28e+6]Epoch 1368/3000:  46%|████▌     | 1368/3000 [10:08&lt;14:13,  1.91it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.28e+6]Epoch 1369/3000:  46%|████▌     | 1368/3000 [10:08&lt;14:13,  1.91it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.28e+6]Epoch 1369/3000:  46%|████▌     | 1369/3000 [10:09&lt;13:51,  1.96it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.28e+6]Epoch 1369/3000:  46%|████▌     | 1369/3000 [10:09&lt;13:51,  1.96it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.28e+6]Epoch 1370/3000:  46%|████▌     | 1369/3000 [10:09&lt;13:51,  1.96it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.28e+6]Epoch 1370/3000:  46%|████▌     | 1370/3000 [10:09&lt;13:07,  2.07it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.28e+6]Epoch 1370/3000:  46%|████▌     | 1370/3000 [10:09&lt;13:07,  2.07it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.28e+6]Epoch 1371/3000:  46%|████▌     | 1370/3000 [10:09&lt;13:07,  2.07it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.28e+6]Epoch 1371/3000:  46%|████▌     | 1371/3000 [10:10&lt;12:58,  2.09it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.28e+6]Epoch 1371/3000:  46%|████▌     | 1371/3000 [10:10&lt;12:58,  2.09it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.27e+6]Epoch 1372/3000:  46%|████▌     | 1371/3000 [10:10&lt;12:58,  2.09it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.27e+6]Epoch 1372/3000:  46%|████▌     | 1372/3000 [10:10&lt;13:29,  2.01it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.27e+6]Epoch 1372/3000:  46%|████▌     | 1372/3000 [10:10&lt;13:29,  2.01it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1373/3000:  46%|████▌     | 1372/3000 [10:10&lt;13:29,  2.01it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1373/3000:  46%|████▌     | 1373/3000 [10:10&lt;11:49,  2.29it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1373/3000:  46%|████▌     | 1373/3000 [10:10&lt;11:49,  2.29it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1374/3000:  46%|████▌     | 1373/3000 [10:11&lt;11:49,  2.29it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1374/3000:  46%|████▌     | 1374/3000 [10:11&lt;11:15,  2.41it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1374/3000:  46%|████▌     | 1374/3000 [10:11&lt;11:15,  2.41it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1375/3000:  46%|████▌     | 1374/3000 [10:11&lt;11:15,  2.41it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1375/3000:  46%|████▌     | 1375/3000 [10:11&lt;11:23,  2.38it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1375/3000:  46%|████▌     | 1375/3000 [10:11&lt;11:23,  2.38it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.27e+6]Epoch 1376/3000:  46%|████▌     | 1375/3000 [10:11&lt;11:23,  2.38it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.27e+6]Epoch 1376/3000:  46%|████▌     | 1376/3000 [10:12&lt;13:01,  2.08it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.27e+6]Epoch 1376/3000:  46%|████▌     | 1376/3000 [10:12&lt;13:01,  2.08it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.27e+6]Epoch 1377/3000:  46%|████▌     | 1376/3000 [10:12&lt;13:01,  2.08it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.27e+6]Epoch 1377/3000:  46%|████▌     | 1377/3000 [10:13&lt;15:42,  1.72it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.27e+6]Epoch 1377/3000:  46%|████▌     | 1377/3000 [10:13&lt;15:42,  1.72it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.27e+6]Epoch 1378/3000:  46%|████▌     | 1377/3000 [10:13&lt;15:42,  1.72it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.27e+6]Epoch 1378/3000:  46%|████▌     | 1378/3000 [10:13&lt;15:09,  1.78it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.27e+6]Epoch 1378/3000:  46%|████▌     | 1378/3000 [10:13&lt;15:09,  1.78it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1379/3000:  46%|████▌     | 1378/3000 [10:13&lt;15:09,  1.78it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1379/3000:  46%|████▌     | 1379/3000 [10:14&lt;13:46,  1.96it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1379/3000:  46%|████▌     | 1379/3000 [10:14&lt;13:46,  1.96it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1380/3000:  46%|████▌     | 1379/3000 [10:14&lt;13:46,  1.96it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1380/3000:  46%|████▌     | 1380/3000 [10:14&lt;11:31,  2.34it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1380/3000:  46%|████▌     | 1380/3000 [10:14&lt;11:31,  2.34it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.27e+6]Epoch 1381/3000:  46%|████▌     | 1380/3000 [10:14&lt;11:31,  2.34it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.27e+6]Epoch 1381/3000:  46%|████▌     | 1381/3000 [10:14&lt;09:15,  2.91it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.27e+6]Epoch 1381/3000:  46%|████▌     | 1381/3000 [10:14&lt;09:15,  2.91it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.27e+6]Epoch 1382/3000:  46%|████▌     | 1381/3000 [10:14&lt;09:15,  2.91it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.27e+6]Epoch 1382/3000:  46%|████▌     | 1382/3000 [10:14&lt;09:27,  2.85it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.27e+6]Epoch 1382/3000:  46%|████▌     | 1382/3000 [10:14&lt;09:27,  2.85it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1383/3000:  46%|████▌     | 1382/3000 [10:14&lt;09:27,  2.85it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1383/3000:  46%|████▌     | 1383/3000 [10:15&lt;10:10,  2.65it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.27e+6]Epoch 1383/3000:  46%|████▌     | 1383/3000 [10:15&lt;10:10,  2.65it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.27e+6]Epoch 1384/3000:  46%|████▌     | 1383/3000 [10:15&lt;10:10,  2.65it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.27e+6]Epoch 1384/3000:  46%|████▌     | 1384/3000 [10:15&lt;11:42,  2.30it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.27e+6]Epoch 1384/3000:  46%|████▌     | 1384/3000 [10:15&lt;11:42,  2.30it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.27e+6]Epoch 1385/3000:  46%|████▌     | 1384/3000 [10:15&lt;11:42,  2.30it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.27e+6]Epoch 1385/3000:  46%|████▌     | 1385/3000 [10:16&lt;11:54,  2.26it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.27e+6]Epoch 1385/3000:  46%|████▌     | 1385/3000 [10:16&lt;11:54,  2.26it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.27e+6]Epoch 1386/3000:  46%|████▌     | 1385/3000 [10:16&lt;11:54,  2.26it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.27e+6]Epoch 1386/3000:  46%|████▌     | 1386/3000 [10:16&lt;13:08,  2.05it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.27e+6]Epoch 1386/3000:  46%|████▌     | 1386/3000 [10:16&lt;13:08,  2.05it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.27e+6]Epoch 1387/3000:  46%|████▌     | 1386/3000 [10:16&lt;13:08,  2.05it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.27e+6]Epoch 1387/3000:  46%|████▌     | 1387/3000 [10:17&lt;12:09,  2.21it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.27e+6]Epoch 1387/3000:  46%|████▌     | 1387/3000 [10:17&lt;12:09,  2.21it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.27e+6]Epoch 1388/3000:  46%|████▌     | 1387/3000 [10:17&lt;12:09,  2.21it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.27e+6]Epoch 1388/3000:  46%|████▋     | 1388/3000 [10:17&lt;09:34,  2.81it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.27e+6]Epoch 1388/3000:  46%|████▋     | 1388/3000 [10:17&lt;09:34,  2.81it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.27e+6]Epoch 1389/3000:  46%|████▋     | 1388/3000 [10:17&lt;09:34,  2.81it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.27e+6]Epoch 1389/3000:  46%|████▋     | 1389/3000 [10:17&lt;09:42,  2.77it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.27e+6]Epoch 1389/3000:  46%|████▋     | 1389/3000 [10:17&lt;09:42,  2.77it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.27e+6]Epoch 1390/3000:  46%|████▋     | 1389/3000 [10:17&lt;09:42,  2.77it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.27e+6]Epoch 1390/3000:  46%|████▋     | 1390/3000 [10:18&lt;11:33,  2.32it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.27e+6]Epoch 1390/3000:  46%|████▋     | 1390/3000 [10:18&lt;11:33,  2.32it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.27e+6]Epoch 1391/3000:  46%|████▋     | 1390/3000 [10:18&lt;11:33,  2.32it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.27e+6]Epoch 1391/3000:  46%|████▋     | 1391/3000 [10:18&lt;12:15,  2.19it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.27e+6]Epoch 1391/3000:  46%|████▋     | 1391/3000 [10:18&lt;12:15,  2.19it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.27e+6] Epoch 1392/3000:  46%|████▋     | 1391/3000 [10:18&lt;12:15,  2.19it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.27e+6]Epoch 1392/3000:  46%|████▋     | 1392/3000 [10:19&lt;12:58,  2.07it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.27e+6]Epoch 1392/3000:  46%|████▋     | 1392/3000 [10:19&lt;12:58,  2.07it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.27e+6]Epoch 1393/3000:  46%|████▋     | 1392/3000 [10:19&lt;12:58,  2.07it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.27e+6]Epoch 1393/3000:  46%|████▋     | 1393/3000 [10:19&lt;11:59,  2.23it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.27e+6]Epoch 1393/3000:  46%|████▋     | 1393/3000 [10:19&lt;11:59,  2.23it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.27e+6]Epoch 1394/3000:  46%|████▋     | 1393/3000 [10:19&lt;11:59,  2.23it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.27e+6]Epoch 1394/3000:  46%|████▋     | 1394/3000 [10:20&lt;11:02,  2.42it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.27e+6]Epoch 1394/3000:  46%|████▋     | 1394/3000 [10:20&lt;11:02,  2.42it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.27e+6]Epoch 1395/3000:  46%|████▋     | 1394/3000 [10:20&lt;11:02,  2.42it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.27e+6]Epoch 1395/3000:  46%|████▋     | 1395/3000 [10:20&lt;12:43,  2.10it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.27e+6]Epoch 1395/3000:  46%|████▋     | 1395/3000 [10:20&lt;12:43,  2.10it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.27e+6]Epoch 1396/3000:  46%|████▋     | 1395/3000 [10:20&lt;12:43,  2.10it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.27e+6]Epoch 1396/3000:  47%|████▋     | 1396/3000 [10:21&lt;13:33,  1.97it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.27e+6]Epoch 1396/3000:  47%|████▋     | 1396/3000 [10:21&lt;13:33,  1.97it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.26e+6]Epoch 1397/3000:  47%|████▋     | 1396/3000 [10:21&lt;13:33,  1.97it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.26e+6]Epoch 1397/3000:  47%|████▋     | 1397/3000 [10:21&lt;13:59,  1.91it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.26e+6]Epoch 1397/3000:  47%|████▋     | 1397/3000 [10:21&lt;13:59,  1.91it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1398/3000:  47%|████▋     | 1397/3000 [10:21&lt;13:59,  1.91it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1398/3000:  47%|████▋     | 1398/3000 [10:22&lt;14:24,  1.85it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1398/3000:  47%|████▋     | 1398/3000 [10:22&lt;14:24,  1.85it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.26e+6]Epoch 1399/3000:  47%|████▋     | 1398/3000 [10:22&lt;14:24,  1.85it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.26e+6]Epoch 1399/3000:  47%|████▋     | 1399/3000 [10:23&lt;14:24,  1.85it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.26e+6]Epoch 1399/3000:  47%|████▋     | 1399/3000 [10:23&lt;14:24,  1.85it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1400/3000:  47%|████▋     | 1399/3000 [10:23&lt;14:24,  1.85it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1400/3000:  47%|████▋     | 1400/3000 [10:23&lt;13:22,  1.99it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1400/3000:  47%|████▋     | 1400/3000 [10:23&lt;13:22,  1.99it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1401/3000:  47%|████▋     | 1400/3000 [10:23&lt;13:22,  1.99it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1401/3000:  47%|████▋     | 1401/3000 [10:23&lt;13:33,  1.97it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1401/3000:  47%|████▋     | 1401/3000 [10:23&lt;13:33,  1.97it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1402/3000:  47%|████▋     | 1401/3000 [10:24&lt;13:33,  1.97it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1402/3000:  47%|████▋     | 1402/3000 [10:24&lt;13:06,  2.03it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1402/3000:  47%|████▋     | 1402/3000 [10:24&lt;13:06,  2.03it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1403/3000:  47%|████▋     | 1402/3000 [10:24&lt;13:06,  2.03it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1403/3000:  47%|████▋     | 1403/3000 [10:24&lt;13:06,  2.03it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1403/3000:  47%|████▋     | 1403/3000 [10:24&lt;13:06,  2.03it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1404/3000:  47%|████▋     | 1403/3000 [10:24&lt;13:06,  2.03it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1404/3000:  47%|████▋     | 1404/3000 [10:25&lt;13:34,  1.96it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1404/3000:  47%|████▋     | 1404/3000 [10:25&lt;13:34,  1.96it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.26e+6]Epoch 1405/3000:  47%|████▋     | 1404/3000 [10:25&lt;13:34,  1.96it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.26e+6]Epoch 1405/3000:  47%|████▋     | 1405/3000 [10:25&lt;12:59,  2.05it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.26e+6]Epoch 1405/3000:  47%|████▋     | 1405/3000 [10:25&lt;12:59,  2.05it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.26e+6]Epoch 1406/3000:  47%|████▋     | 1405/3000 [10:25&lt;12:59,  2.05it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.26e+6]Epoch 1406/3000:  47%|████▋     | 1406/3000 [10:26&lt;12:30,  2.12it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.26e+6]Epoch 1406/3000:  47%|████▋     | 1406/3000 [10:26&lt;12:30,  2.12it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1407/3000:  47%|████▋     | 1406/3000 [10:26&lt;12:30,  2.12it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1407/3000:  47%|████▋     | 1407/3000 [10:26&lt;12:01,  2.21it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1407/3000:  47%|████▋     | 1407/3000 [10:26&lt;12:01,  2.21it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1408/3000:  47%|████▋     | 1407/3000 [10:26&lt;12:01,  2.21it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1408/3000:  47%|████▋     | 1408/3000 [10:27&lt;13:23,  1.98it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1408/3000:  47%|████▋     | 1408/3000 [10:27&lt;13:23,  1.98it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.26e+6]Epoch 1409/3000:  47%|████▋     | 1408/3000 [10:27&lt;13:23,  1.98it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.26e+6]Epoch 1409/3000:  47%|████▋     | 1409/3000 [10:27&lt;13:00,  2.04it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.26e+6]Epoch 1409/3000:  47%|████▋     | 1409/3000 [10:27&lt;13:00,  2.04it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1410/3000:  47%|████▋     | 1409/3000 [10:27&lt;13:00,  2.04it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1410/3000:  47%|████▋     | 1410/3000 [10:28&lt;13:03,  2.03it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1410/3000:  47%|████▋     | 1410/3000 [10:28&lt;13:03,  2.03it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1411/3000:  47%|████▋     | 1410/3000 [10:28&lt;13:03,  2.03it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1411/3000:  47%|████▋     | 1411/3000 [10:28&lt;12:38,  2.10it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1411/3000:  47%|████▋     | 1411/3000 [10:28&lt;12:38,  2.10it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1412/3000:  47%|████▋     | 1411/3000 [10:28&lt;12:38,  2.10it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1412/3000:  47%|████▋     | 1412/3000 [10:29&lt;13:34,  1.95it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1412/3000:  47%|████▋     | 1412/3000 [10:29&lt;13:34,  1.95it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.26e+6]Epoch 1413/3000:  47%|████▋     | 1412/3000 [10:29&lt;13:34,  1.95it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.26e+6]Epoch 1413/3000:  47%|████▋     | 1413/3000 [10:29&lt;14:21,  1.84it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.26e+6]Epoch 1413/3000:  47%|████▋     | 1413/3000 [10:30&lt;14:21,  1.84it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1414/3000:  47%|████▋     | 1413/3000 [10:30&lt;14:21,  1.84it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1414/3000:  47%|████▋     | 1414/3000 [10:30&lt;14:34,  1.81it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.26e+6]Epoch 1414/3000:  47%|████▋     | 1414/3000 [10:30&lt;14:34,  1.81it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.26e+6]Epoch 1415/3000:  47%|████▋     | 1414/3000 [10:30&lt;14:34,  1.81it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.26e+6]Epoch 1415/3000:  47%|████▋     | 1415/3000 [10:30&lt;13:31,  1.95it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.26e+6]Epoch 1415/3000:  47%|████▋     | 1415/3000 [10:30&lt;13:31,  1.95it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1416/3000:  47%|████▋     | 1415/3000 [10:30&lt;13:31,  1.95it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1416/3000:  47%|████▋     | 1416/3000 [10:31&lt;12:16,  2.15it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1416/3000:  47%|████▋     | 1416/3000 [10:31&lt;12:16,  2.15it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.26e+6] Epoch 1417/3000:  47%|████▋     | 1416/3000 [10:31&lt;12:16,  2.15it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.26e+6]Epoch 1417/3000:  47%|████▋     | 1417/3000 [10:31&lt;11:09,  2.36it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.26e+6]Epoch 1417/3000:  47%|████▋     | 1417/3000 [10:31&lt;11:09,  2.36it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.26e+6]Epoch 1418/3000:  47%|████▋     | 1417/3000 [10:31&lt;11:09,  2.36it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.26e+6]Epoch 1418/3000:  47%|████▋     | 1418/3000 [10:32&lt;11:43,  2.25it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.26e+6]Epoch 1418/3000:  47%|████▋     | 1418/3000 [10:32&lt;11:43,  2.25it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.26e+6]Epoch 1419/3000:  47%|████▋     | 1418/3000 [10:32&lt;11:43,  2.25it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.26e+6]Epoch 1419/3000:  47%|████▋     | 1419/3000 [10:32&lt;11:27,  2.30it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.26e+6]Epoch 1419/3000:  47%|████▋     | 1419/3000 [10:32&lt;11:27,  2.30it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.26e+6] Epoch 1420/3000:  47%|████▋     | 1419/3000 [10:32&lt;11:27,  2.30it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.26e+6]Epoch 1420/3000:  47%|████▋     | 1420/3000 [10:33&lt;13:06,  2.01it/s, v_num=1, train_loss_step=1.3e+6, train_loss_epoch=1.26e+6]Epoch 1420/3000:  47%|████▋     | 1420/3000 [10:33&lt;13:06,  2.01it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1421/3000:  47%|████▋     | 1420/3000 [10:33&lt;13:06,  2.01it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1421/3000:  47%|████▋     | 1421/3000 [10:33&lt;13:42,  1.92it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1421/3000:  47%|████▋     | 1421/3000 [10:33&lt;13:42,  1.92it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1422/3000:  47%|████▋     | 1421/3000 [10:33&lt;13:42,  1.92it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1422/3000:  47%|████▋     | 1422/3000 [10:34&lt;12:29,  2.11it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.26e+6]Epoch 1422/3000:  47%|████▋     | 1422/3000 [10:34&lt;12:29,  2.11it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1423/3000:  47%|████▋     | 1422/3000 [10:34&lt;12:29,  2.11it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1423/3000:  47%|████▋     | 1423/3000 [10:34&lt;13:15,  1.98it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1423/3000:  47%|████▋     | 1423/3000 [10:34&lt;13:15,  1.98it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1424/3000:  47%|████▋     | 1423/3000 [10:34&lt;13:15,  1.98it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1424/3000:  47%|████▋     | 1424/3000 [10:35&lt;12:54,  2.03it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1424/3000:  47%|████▋     | 1424/3000 [10:35&lt;12:54,  2.03it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.25e+6]Epoch 1425/3000:  47%|████▋     | 1424/3000 [10:35&lt;12:54,  2.03it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.25e+6]Epoch 1425/3000:  48%|████▊     | 1425/3000 [10:35&lt;13:20,  1.97it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.25e+6]Epoch 1425/3000:  48%|████▊     | 1425/3000 [10:35&lt;13:20,  1.97it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.25e+6]Epoch 1426/3000:  48%|████▊     | 1425/3000 [10:35&lt;13:20,  1.97it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.25e+6]Epoch 1426/3000:  48%|████▊     | 1426/3000 [10:36&lt;13:00,  2.02it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.25e+6]Epoch 1426/3000:  48%|████▊     | 1426/3000 [10:36&lt;13:00,  2.02it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.25e+6]Epoch 1427/3000:  48%|████▊     | 1426/3000 [10:36&lt;13:00,  2.02it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.25e+6]Epoch 1427/3000:  48%|████▊     | 1427/3000 [10:36&lt;11:59,  2.19it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.25e+6]Epoch 1427/3000:  48%|████▊     | 1427/3000 [10:36&lt;11:59,  2.19it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1428/3000:  48%|████▊     | 1427/3000 [10:36&lt;11:59,  2.19it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1428/3000:  48%|████▊     | 1428/3000 [10:36&lt;11:30,  2.28it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1428/3000:  48%|████▊     | 1428/3000 [10:36&lt;11:30,  2.28it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1429/3000:  48%|████▊     | 1428/3000 [10:36&lt;11:30,  2.28it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1429/3000:  48%|████▊     | 1429/3000 [10:37&lt;12:56,  2.02it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1429/3000:  48%|████▊     | 1429/3000 [10:37&lt;12:56,  2.02it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1430/3000:  48%|████▊     | 1429/3000 [10:37&lt;12:56,  2.02it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1430/3000:  48%|████▊     | 1430/3000 [10:38&lt;12:22,  2.12it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1430/3000:  48%|████▊     | 1430/3000 [10:38&lt;12:22,  2.12it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1431/3000:  48%|████▊     | 1430/3000 [10:38&lt;12:22,  2.12it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1431/3000:  48%|████▊     | 1431/3000 [10:38&lt;12:42,  2.06it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1431/3000:  48%|████▊     | 1431/3000 [10:38&lt;12:42,  2.06it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.25e+6]Epoch 1432/3000:  48%|████▊     | 1431/3000 [10:38&lt;12:42,  2.06it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.25e+6]Epoch 1432/3000:  48%|████▊     | 1432/3000 [10:38&lt;12:26,  2.10it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.25e+6]Epoch 1432/3000:  48%|████▊     | 1432/3000 [10:38&lt;12:26,  2.10it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1433/3000:  48%|████▊     | 1432/3000 [10:38&lt;12:26,  2.10it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1433/3000:  48%|████▊     | 1433/3000 [10:39&lt;11:56,  2.19it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1433/3000:  48%|████▊     | 1433/3000 [10:39&lt;11:56,  2.19it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.25e+6]Epoch 1434/3000:  48%|████▊     | 1433/3000 [10:39&lt;11:56,  2.19it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.25e+6]Epoch 1434/3000:  48%|████▊     | 1434/3000 [10:39&lt;11:21,  2.30it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.25e+6]Epoch 1434/3000:  48%|████▊     | 1434/3000 [10:39&lt;11:21,  2.30it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.25e+6] Epoch 1435/3000:  48%|████▊     | 1434/3000 [10:39&lt;11:21,  2.30it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.25e+6]Epoch 1435/3000:  48%|████▊     | 1435/3000 [10:40&lt;11:07,  2.34it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.25e+6]Epoch 1435/3000:  48%|████▊     | 1435/3000 [10:40&lt;11:07,  2.34it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.25e+6]Epoch 1436/3000:  48%|████▊     | 1435/3000 [10:40&lt;11:07,  2.34it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.25e+6]Epoch 1436/3000:  48%|████▊     | 1436/3000 [10:40&lt;11:22,  2.29it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.25e+6]Epoch 1436/3000:  48%|████▊     | 1436/3000 [10:40&lt;11:22,  2.29it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.25e+6]Epoch 1437/3000:  48%|████▊     | 1436/3000 [10:40&lt;11:22,  2.29it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.25e+6]Epoch 1437/3000:  48%|████▊     | 1437/3000 [10:41&lt;11:15,  2.31it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.25e+6]Epoch 1437/3000:  48%|████▊     | 1437/3000 [10:41&lt;11:15,  2.31it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1438/3000:  48%|████▊     | 1437/3000 [10:41&lt;11:15,  2.31it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1438/3000:  48%|████▊     | 1438/3000 [10:41&lt;11:57,  2.18it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1438/3000:  48%|████▊     | 1438/3000 [10:41&lt;11:57,  2.18it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.25e+6]Epoch 1439/3000:  48%|████▊     | 1438/3000 [10:41&lt;11:57,  2.18it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.25e+6]Epoch 1439/3000:  48%|████▊     | 1439/3000 [10:42&lt;11:43,  2.22it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.25e+6]Epoch 1439/3000:  48%|████▊     | 1439/3000 [10:42&lt;11:43,  2.22it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1440/3000:  48%|████▊     | 1439/3000 [10:42&lt;11:43,  2.22it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1440/3000:  48%|████▊     | 1440/3000 [10:42&lt;10:30,  2.47it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1440/3000:  48%|████▊     | 1440/3000 [10:42&lt;10:30,  2.47it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.25e+6]Epoch 1441/3000:  48%|████▊     | 1440/3000 [10:42&lt;10:30,  2.47it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.25e+6]Epoch 1441/3000:  48%|████▊     | 1441/3000 [10:42&lt;11:00,  2.36it/s, v_num=1, train_loss_step=1.29e+6, train_loss_epoch=1.25e+6]Epoch 1441/3000:  48%|████▊     | 1441/3000 [10:42&lt;11:00,  2.36it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.25e+6]Epoch 1442/3000:  48%|████▊     | 1441/3000 [10:42&lt;11:00,  2.36it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.25e+6]Epoch 1442/3000:  48%|████▊     | 1442/3000 [10:43&lt;12:29,  2.08it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.25e+6]Epoch 1442/3000:  48%|████▊     | 1442/3000 [10:43&lt;12:29,  2.08it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1443/3000:  48%|████▊     | 1442/3000 [10:43&lt;12:29,  2.08it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1443/3000:  48%|████▊     | 1443/3000 [10:43&lt;12:46,  2.03it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1443/3000:  48%|████▊     | 1443/3000 [10:43&lt;12:46,  2.03it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.25e+6]Epoch 1444/3000:  48%|████▊     | 1443/3000 [10:43&lt;12:46,  2.03it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.25e+6]Epoch 1444/3000:  48%|████▊     | 1444/3000 [10:44&lt;12:48,  2.03it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.25e+6]Epoch 1444/3000:  48%|████▊     | 1444/3000 [10:44&lt;12:48,  2.03it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.25e+6]Epoch 1445/3000:  48%|████▊     | 1444/3000 [10:44&lt;12:48,  2.03it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.25e+6]Epoch 1445/3000:  48%|████▊     | 1445/3000 [10:45&lt;13:59,  1.85it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.25e+6]Epoch 1445/3000:  48%|████▊     | 1445/3000 [10:45&lt;13:59,  1.85it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1446/3000:  48%|████▊     | 1445/3000 [10:45&lt;13:59,  1.85it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1446/3000:  48%|████▊     | 1446/3000 [10:45&lt;13:41,  1.89it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1446/3000:  48%|████▊     | 1446/3000 [10:45&lt;13:41,  1.89it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.25e+6]Epoch 1447/3000:  48%|████▊     | 1446/3000 [10:45&lt;13:41,  1.89it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.25e+6]Epoch 1447/3000:  48%|████▊     | 1447/3000 [10:46&lt;12:59,  1.99it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.25e+6]Epoch 1447/3000:  48%|████▊     | 1447/3000 [10:46&lt;12:59,  1.99it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1448/3000:  48%|████▊     | 1447/3000 [10:46&lt;12:59,  1.99it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1448/3000:  48%|████▊     | 1448/3000 [10:46&lt;11:45,  2.20it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.25e+6]Epoch 1448/3000:  48%|████▊     | 1448/3000 [10:46&lt;11:45,  2.20it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.25e+6]Epoch 1449/3000:  48%|████▊     | 1448/3000 [10:46&lt;11:45,  2.20it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.25e+6]Epoch 1449/3000:  48%|████▊     | 1449/3000 [10:46&lt;13:05,  1.98it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.25e+6]Epoch 1449/3000:  48%|████▊     | 1449/3000 [10:46&lt;13:05,  1.98it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1450/3000:  48%|████▊     | 1449/3000 [10:47&lt;13:05,  1.98it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1450/3000:  48%|████▊     | 1450/3000 [10:47&lt;12:11,  2.12it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.25e+6]Epoch 1450/3000:  48%|████▊     | 1450/3000 [10:47&lt;12:11,  2.12it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.24e+6]Epoch 1451/3000:  48%|████▊     | 1450/3000 [10:47&lt;12:11,  2.12it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.24e+6]Epoch 1451/3000:  48%|████▊     | 1451/3000 [10:47&lt;11:32,  2.24it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.24e+6]Epoch 1451/3000:  48%|████▊     | 1451/3000 [10:47&lt;11:32,  2.24it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1452/3000:  48%|████▊     | 1451/3000 [10:47&lt;11:32,  2.24it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1452/3000:  48%|████▊     | 1452/3000 [10:48&lt;10:31,  2.45it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1452/3000:  48%|████▊     | 1452/3000 [10:48&lt;10:31,  2.45it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1453/3000:  48%|████▊     | 1452/3000 [10:48&lt;10:31,  2.45it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1453/3000:  48%|████▊     | 1453/3000 [10:48&lt;11:24,  2.26it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1453/3000:  48%|████▊     | 1453/3000 [10:48&lt;11:24,  2.26it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1454/3000:  48%|████▊     | 1453/3000 [10:48&lt;11:24,  2.26it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1454/3000:  48%|████▊     | 1454/3000 [10:48&lt;10:56,  2.36it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1454/3000:  48%|████▊     | 1454/3000 [10:48&lt;10:56,  2.36it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1455/3000:  48%|████▊     | 1454/3000 [10:49&lt;10:56,  2.36it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1455/3000:  48%|████▊     | 1455/3000 [10:49&lt;09:39,  2.67it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1455/3000:  48%|████▊     | 1455/3000 [10:49&lt;09:39,  2.67it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1456/3000:  48%|████▊     | 1455/3000 [10:49&lt;09:39,  2.67it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1456/3000:  49%|████▊     | 1456/3000 [10:49&lt;10:10,  2.53it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1456/3000:  49%|████▊     | 1456/3000 [10:49&lt;10:10,  2.53it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.24e+6]Epoch 1457/3000:  49%|████▊     | 1456/3000 [10:49&lt;10:10,  2.53it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.24e+6]Epoch 1457/3000:  49%|████▊     | 1457/3000 [10:49&lt;09:29,  2.71it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.24e+6]Epoch 1457/3000:  49%|████▊     | 1457/3000 [10:50&lt;09:29,  2.71it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.24e+6]Epoch 1458/3000:  49%|████▊     | 1457/3000 [10:50&lt;09:29,  2.71it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.24e+6]Epoch 1458/3000:  49%|████▊     | 1458/3000 [10:50&lt;10:22,  2.48it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.24e+6]Epoch 1458/3000:  49%|████▊     | 1458/3000 [10:50&lt;10:22,  2.48it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1459/3000:  49%|████▊     | 1458/3000 [10:50&lt;10:22,  2.48it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1459/3000:  49%|████▊     | 1459/3000 [10:51&lt;11:15,  2.28it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1459/3000:  49%|████▊     | 1459/3000 [10:51&lt;11:15,  2.28it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1460/3000:  49%|████▊     | 1459/3000 [10:51&lt;11:15,  2.28it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1460/3000:  49%|████▊     | 1460/3000 [10:51&lt;12:02,  2.13it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1460/3000:  49%|████▊     | 1460/3000 [10:51&lt;12:02,  2.13it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.24e+6]Epoch 1461/3000:  49%|████▊     | 1460/3000 [10:51&lt;12:02,  2.13it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.24e+6]Epoch 1461/3000:  49%|████▊     | 1461/3000 [10:51&lt;11:26,  2.24it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.24e+6]Epoch 1461/3000:  49%|████▊     | 1461/3000 [10:51&lt;11:26,  2.24it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1462/3000:  49%|████▊     | 1461/3000 [10:51&lt;11:26,  2.24it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1462/3000:  49%|████▊     | 1462/3000 [10:52&lt;11:04,  2.31it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1462/3000:  49%|████▊     | 1462/3000 [10:52&lt;11:04,  2.31it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.24e+6]Epoch 1463/3000:  49%|████▊     | 1462/3000 [10:52&lt;11:04,  2.31it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.24e+6]Epoch 1463/3000:  49%|████▉     | 1463/3000 [10:52&lt;11:53,  2.15it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.24e+6]Epoch 1463/3000:  49%|████▉     | 1463/3000 [10:52&lt;11:53,  2.15it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.24e+6]Epoch 1464/3000:  49%|████▉     | 1463/3000 [10:52&lt;11:53,  2.15it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.24e+6]Epoch 1464/3000:  49%|████▉     | 1464/3000 [10:53&lt;10:51,  2.36it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.24e+6]Epoch 1464/3000:  49%|████▉     | 1464/3000 [10:53&lt;10:51,  2.36it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.24e+6]Epoch 1465/3000:  49%|████▉     | 1464/3000 [10:53&lt;10:51,  2.36it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.24e+6]Epoch 1465/3000:  49%|████▉     | 1465/3000 [10:53&lt;10:28,  2.44it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.24e+6]Epoch 1465/3000:  49%|████▉     | 1465/3000 [10:53&lt;10:28,  2.44it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.24e+6]Epoch 1466/3000:  49%|████▉     | 1465/3000 [10:53&lt;10:28,  2.44it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.24e+6]Epoch 1466/3000:  49%|████▉     | 1466/3000 [10:54&lt;12:12,  2.09it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.24e+6]Epoch 1466/3000:  49%|████▉     | 1466/3000 [10:54&lt;12:12,  2.09it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1467/3000:  49%|████▉     | 1466/3000 [10:54&lt;12:12,  2.09it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1467/3000:  49%|████▉     | 1467/3000 [10:54&lt;12:27,  2.05it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1467/3000:  49%|████▉     | 1467/3000 [10:54&lt;12:27,  2.05it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.24e+6]Epoch 1468/3000:  49%|████▉     | 1467/3000 [10:54&lt;12:27,  2.05it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.24e+6]Epoch 1468/3000:  49%|████▉     | 1468/3000 [10:55&lt;13:42,  1.86it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.24e+6]Epoch 1468/3000:  49%|████▉     | 1468/3000 [10:55&lt;13:42,  1.86it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.24e+6]Epoch 1469/3000:  49%|████▉     | 1468/3000 [10:55&lt;13:42,  1.86it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.24e+6]Epoch 1469/3000:  49%|████▉     | 1469/3000 [10:55&lt;13:56,  1.83it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.24e+6]Epoch 1469/3000:  49%|████▉     | 1469/3000 [10:55&lt;13:56,  1.83it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.24e+6]Epoch 1470/3000:  49%|████▉     | 1469/3000 [10:55&lt;13:56,  1.83it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.24e+6]Epoch 1470/3000:  49%|████▉     | 1470/3000 [10:56&lt;13:25,  1.90it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.24e+6]Epoch 1470/3000:  49%|████▉     | 1470/3000 [10:56&lt;13:25,  1.90it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1471/3000:  49%|████▉     | 1470/3000 [10:56&lt;13:25,  1.90it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1471/3000:  49%|████▉     | 1471/3000 [10:56&lt;13:00,  1.96it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1471/3000:  49%|████▉     | 1471/3000 [10:56&lt;13:00,  1.96it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1472/3000:  49%|████▉     | 1471/3000 [10:56&lt;13:00,  1.96it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1472/3000:  49%|████▉     | 1472/3000 [10:57&lt;12:35,  2.02it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1472/3000:  49%|████▉     | 1472/3000 [10:57&lt;12:35,  2.02it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1473/3000:  49%|████▉     | 1472/3000 [10:57&lt;12:35,  2.02it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1473/3000:  49%|████▉     | 1473/3000 [10:57&lt;12:16,  2.07it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1473/3000:  49%|████▉     | 1473/3000 [10:57&lt;12:16,  2.07it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.24e+6]Epoch 1474/3000:  49%|████▉     | 1473/3000 [10:57&lt;12:16,  2.07it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.24e+6]Epoch 1474/3000:  49%|████▉     | 1474/3000 [10:58&lt;13:06,  1.94it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.24e+6]Epoch 1474/3000:  49%|████▉     | 1474/3000 [10:58&lt;13:06,  1.94it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1475/3000:  49%|████▉     | 1474/3000 [10:58&lt;13:06,  1.94it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1475/3000:  49%|████▉     | 1475/3000 [10:58&lt;12:30,  2.03it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.24e+6]Epoch 1475/3000:  49%|████▉     | 1475/3000 [10:58&lt;12:30,  2.03it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.24e+6]Epoch 1476/3000:  49%|████▉     | 1475/3000 [10:58&lt;12:30,  2.03it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.24e+6]Epoch 1476/3000:  49%|████▉     | 1476/3000 [10:59&lt;13:05,  1.94it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.24e+6]Epoch 1476/3000:  49%|████▉     | 1476/3000 [10:59&lt;13:05,  1.94it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.24e+6]Epoch 1477/3000:  49%|████▉     | 1476/3000 [10:59&lt;13:05,  1.94it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.24e+6]Epoch 1477/3000:  49%|████▉     | 1477/3000 [10:59&lt;12:18,  2.06it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.24e+6]Epoch 1477/3000:  49%|████▉     | 1477/3000 [10:59&lt;12:18,  2.06it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.24e+6]Epoch 1478/3000:  49%|████▉     | 1477/3000 [10:59&lt;12:18,  2.06it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.24e+6]Epoch 1478/3000:  49%|████▉     | 1478/3000 [11:00&lt;11:08,  2.28it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.24e+6]Epoch 1478/3000:  49%|████▉     | 1478/3000 [11:00&lt;11:08,  2.28it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.24e+6]Epoch 1479/3000:  49%|████▉     | 1478/3000 [11:00&lt;11:08,  2.28it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.24e+6]Epoch 1479/3000:  49%|████▉     | 1479/3000 [11:00&lt;11:35,  2.19it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.24e+6]Epoch 1479/3000:  49%|████▉     | 1479/3000 [11:00&lt;11:35,  2.19it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1480/3000:  49%|████▉     | 1479/3000 [11:00&lt;11:35,  2.19it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1480/3000:  49%|████▉     | 1480/3000 [11:01&lt;11:25,  2.22it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1480/3000:  49%|████▉     | 1480/3000 [11:01&lt;11:25,  2.22it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1481/3000:  49%|████▉     | 1480/3000 [11:01&lt;11:25,  2.22it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1481/3000:  49%|████▉     | 1481/3000 [11:01&lt;11:56,  2.12it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1481/3000:  49%|████▉     | 1481/3000 [11:01&lt;11:56,  2.12it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1482/3000:  49%|████▉     | 1481/3000 [11:01&lt;11:56,  2.12it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1482/3000:  49%|████▉     | 1482/3000 [11:02&lt;11:25,  2.21it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1482/3000:  49%|████▉     | 1482/3000 [11:02&lt;11:25,  2.21it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1483/3000:  49%|████▉     | 1482/3000 [11:02&lt;11:25,  2.21it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1483/3000:  49%|████▉     | 1483/3000 [11:02&lt;11:28,  2.20it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1483/3000:  49%|████▉     | 1483/3000 [11:02&lt;11:28,  2.20it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.23e+6]Epoch 1484/3000:  49%|████▉     | 1483/3000 [11:02&lt;11:28,  2.20it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.23e+6]Epoch 1484/3000:  49%|████▉     | 1484/3000 [11:02&lt;10:30,  2.40it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.23e+6]Epoch 1484/3000:  49%|████▉     | 1484/3000 [11:02&lt;10:30,  2.40it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.23e+6]Epoch 1485/3000:  49%|████▉     | 1484/3000 [11:02&lt;10:30,  2.40it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.23e+6]Epoch 1485/3000:  50%|████▉     | 1485/3000 [11:03&lt;10:48,  2.34it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.23e+6]Epoch 1485/3000:  50%|████▉     | 1485/3000 [11:03&lt;10:48,  2.34it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1486/3000:  50%|████▉     | 1485/3000 [11:03&lt;10:48,  2.34it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1486/3000:  50%|████▉     | 1486/3000 [11:03&lt;11:23,  2.21it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1486/3000:  50%|████▉     | 1486/3000 [11:03&lt;11:23,  2.21it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1487/3000:  50%|████▉     | 1486/3000 [11:03&lt;11:23,  2.21it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1487/3000:  50%|████▉     | 1487/3000 [11:04&lt;11:16,  2.24it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1487/3000:  50%|████▉     | 1487/3000 [11:04&lt;11:16,  2.24it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1488/3000:  50%|████▉     | 1487/3000 [11:04&lt;11:16,  2.24it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1488/3000:  50%|████▉     | 1488/3000 [11:04&lt;12:06,  2.08it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1488/3000:  50%|████▉     | 1488/3000 [11:04&lt;12:06,  2.08it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1489/3000:  50%|████▉     | 1488/3000 [11:04&lt;12:06,  2.08it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1489/3000:  50%|████▉     | 1489/3000 [11:05&lt;11:19,  2.23it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1489/3000:  50%|████▉     | 1489/3000 [11:05&lt;11:19,  2.23it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1490/3000:  50%|████▉     | 1489/3000 [11:05&lt;11:19,  2.23it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1490/3000:  50%|████▉     | 1490/3000 [11:05&lt;11:54,  2.11it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1490/3000:  50%|████▉     | 1490/3000 [11:05&lt;11:54,  2.11it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.23e+6]Epoch 1491/3000:  50%|████▉     | 1490/3000 [11:05&lt;11:54,  2.11it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.23e+6]Epoch 1491/3000:  50%|████▉     | 1491/3000 [11:06&lt;11:43,  2.14it/s, v_num=1, train_loss_step=1.27e+6, train_loss_epoch=1.23e+6]Epoch 1491/3000:  50%|████▉     | 1491/3000 [11:06&lt;11:43,  2.14it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.23e+6]Epoch 1492/3000:  50%|████▉     | 1491/3000 [11:06&lt;11:43,  2.14it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.23e+6]Epoch 1492/3000:  50%|████▉     | 1492/3000 [11:06&lt;11:00,  2.28it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.23e+6]Epoch 1492/3000:  50%|████▉     | 1492/3000 [11:06&lt;11:00,  2.28it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1493/3000:  50%|████▉     | 1492/3000 [11:06&lt;11:00,  2.28it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1493/3000:  50%|████▉     | 1493/3000 [11:06&lt;10:37,  2.36it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1493/3000:  50%|████▉     | 1493/3000 [11:06&lt;10:37,  2.36it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.23e+6] Epoch 1494/3000:  50%|████▉     | 1493/3000 [11:06&lt;10:37,  2.36it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.23e+6]Epoch 1494/3000:  50%|████▉     | 1494/3000 [11:07&lt;11:58,  2.09it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.23e+6]Epoch 1494/3000:  50%|████▉     | 1494/3000 [11:07&lt;11:58,  2.09it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.23e+6]Epoch 1495/3000:  50%|████▉     | 1494/3000 [11:07&lt;11:58,  2.09it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.23e+6]Epoch 1495/3000:  50%|████▉     | 1495/3000 [11:08&lt;13:06,  1.91it/s, v_num=1, train_loss_step=1.26e+6, train_loss_epoch=1.23e+6]Epoch 1495/3000:  50%|████▉     | 1495/3000 [11:08&lt;13:06,  1.91it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.23e+6]Epoch 1496/3000:  50%|████▉     | 1495/3000 [11:08&lt;13:06,  1.91it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.23e+6]Epoch 1496/3000:  50%|████▉     | 1496/3000 [11:08&lt;13:30,  1.86it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.23e+6]Epoch 1496/3000:  50%|████▉     | 1496/3000 [11:08&lt;13:30,  1.86it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.23e+6]Epoch 1497/3000:  50%|████▉     | 1496/3000 [11:08&lt;13:30,  1.86it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.23e+6]Epoch 1497/3000:  50%|████▉     | 1497/3000 [11:09&lt;14:00,  1.79it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.23e+6]Epoch 1497/3000:  50%|████▉     | 1497/3000 [11:09&lt;14:00,  1.79it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.23e+6]Epoch 1498/3000:  50%|████▉     | 1497/3000 [11:09&lt;14:00,  1.79it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.23e+6]Epoch 1498/3000:  50%|████▉     | 1498/3000 [11:09&lt;10:58,  2.28it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.23e+6]Epoch 1498/3000:  50%|████▉     | 1498/3000 [11:09&lt;10:58,  2.28it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.23e+6]Epoch 1499/3000:  50%|████▉     | 1498/3000 [11:09&lt;10:58,  2.28it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.23e+6]Epoch 1499/3000:  50%|████▉     | 1499/3000 [11:09&lt;11:01,  2.27it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.23e+6]Epoch 1499/3000:  50%|████▉     | 1499/3000 [11:09&lt;11:01,  2.27it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.23e+6]Epoch 1500/3000:  50%|████▉     | 1499/3000 [11:09&lt;11:01,  2.27it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.23e+6]Epoch 1500/3000:  50%|█████     | 1500/3000 [11:10&lt;11:33,  2.16it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.23e+6]Epoch 1500/3000:  50%|█████     | 1500/3000 [11:10&lt;11:33,  2.16it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1501/3000:  50%|█████     | 1500/3000 [11:10&lt;11:33,  2.16it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1501/3000:  50%|█████     | 1501/3000 [11:10&lt;11:50,  2.11it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1501/3000:  50%|█████     | 1501/3000 [11:10&lt;11:50,  2.11it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.23e+6]Epoch 1502/3000:  50%|█████     | 1501/3000 [11:10&lt;11:50,  2.11it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.23e+6]Epoch 1502/3000:  50%|█████     | 1502/3000 [11:11&lt;11:15,  2.22it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.23e+6]Epoch 1502/3000:  50%|█████     | 1502/3000 [11:11&lt;11:15,  2.22it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.23e+6]Epoch 1503/3000:  50%|█████     | 1502/3000 [11:11&lt;11:15,  2.22it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.23e+6]Epoch 1503/3000:  50%|█████     | 1503/3000 [11:11&lt;11:19,  2.20it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.23e+6]Epoch 1503/3000:  50%|█████     | 1503/3000 [11:11&lt;11:19,  2.20it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1504/3000:  50%|█████     | 1503/3000 [11:11&lt;11:19,  2.20it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1504/3000:  50%|█████     | 1504/3000 [11:12&lt;12:10,  2.05it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.23e+6]Epoch 1504/3000:  50%|█████     | 1504/3000 [11:12&lt;12:10,  2.05it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.23e+6]Epoch 1505/3000:  50%|█████     | 1504/3000 [11:12&lt;12:10,  2.05it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.23e+6]Epoch 1505/3000:  50%|█████     | 1505/3000 [11:12&lt;12:43,  1.96it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.23e+6]Epoch 1505/3000:  50%|█████     | 1505/3000 [11:12&lt;12:43,  1.96it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1506/3000:  50%|█████     | 1505/3000 [11:12&lt;12:43,  1.96it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1506/3000:  50%|█████     | 1506/3000 [11:13&lt;11:04,  2.25it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1506/3000:  50%|█████     | 1506/3000 [11:13&lt;11:04,  2.25it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.23e+6]Epoch 1507/3000:  50%|█████     | 1506/3000 [11:13&lt;11:04,  2.25it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.23e+6]Epoch 1507/3000:  50%|█████     | 1507/3000 [11:13&lt;10:06,  2.46it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.23e+6]Epoch 1507/3000:  50%|█████     | 1507/3000 [11:13&lt;10:06,  2.46it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1508/3000:  50%|█████     | 1507/3000 [11:13&lt;10:06,  2.46it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1508/3000:  50%|█████     | 1508/3000 [11:13&lt;09:43,  2.56it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.23e+6]Epoch 1508/3000:  50%|█████     | 1508/3000 [11:13&lt;09:43,  2.56it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.23e+6]Epoch 1509/3000:  50%|█████     | 1508/3000 [11:13&lt;09:43,  2.56it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.23e+6]Epoch 1509/3000:  50%|█████     | 1509/3000 [11:14&lt;10:22,  2.40it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.23e+6]Epoch 1509/3000:  50%|█████     | 1509/3000 [11:14&lt;10:22,  2.40it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1510/3000:  50%|█████     | 1509/3000 [11:14&lt;10:22,  2.40it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1510/3000:  50%|█████     | 1510/3000 [11:14&lt;08:28,  2.93it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1510/3000:  50%|█████     | 1510/3000 [11:14&lt;08:28,  2.93it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.22e+6]Epoch 1511/3000:  50%|█████     | 1510/3000 [11:14&lt;08:28,  2.93it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.22e+6]Epoch 1511/3000:  50%|█████     | 1511/3000 [11:14&lt;06:47,  3.66it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.22e+6]Epoch 1511/3000:  50%|█████     | 1511/3000 [11:14&lt;06:47,  3.66it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.22e+6]Epoch 1512/3000:  50%|█████     | 1511/3000 [11:14&lt;06:47,  3.66it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.22e+6]Epoch 1512/3000:  50%|█████     | 1512/3000 [11:14&lt;05:57,  4.16it/s, v_num=1, train_loss_step=1.25e+6, train_loss_epoch=1.22e+6]Epoch 1512/3000:  50%|█████     | 1512/3000 [11:14&lt;05:57,  4.16it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.22e+6] Epoch 1513/3000:  50%|█████     | 1512/3000 [11:14&lt;05:57,  4.16it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.22e+6]Epoch 1513/3000:  50%|█████     | 1513/3000 [11:14&lt;05:57,  4.16it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.22e+6]Epoch 1514/3000:  50%|█████     | 1513/3000 [11:14&lt;05:57,  4.16it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.22e+6]Epoch 1514/3000:  50%|█████     | 1514/3000 [11:14&lt;04:11,  5.91it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.22e+6]Epoch 1514/3000:  50%|█████     | 1514/3000 [11:14&lt;04:11,  5.91it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.22e+6]Epoch 1515/3000:  50%|█████     | 1514/3000 [11:14&lt;04:11,  5.91it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.22e+6]Epoch 1515/3000:  50%|█████     | 1515/3000 [11:15&lt;04:11,  5.91it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.22e+6]Epoch 1516/3000:  50%|█████     | 1515/3000 [11:15&lt;04:11,  5.91it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.22e+6]Epoch 1516/3000:  51%|█████     | 1516/3000 [11:15&lt;03:20,  7.40it/s, v_num=1, train_loss_step=1.28e+6, train_loss_epoch=1.22e+6]Epoch 1516/3000:  51%|█████     | 1516/3000 [11:15&lt;03:20,  7.40it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.22e+6]Epoch 1517/3000:  51%|█████     | 1516/3000 [11:15&lt;03:20,  7.40it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.22e+6]Epoch 1517/3000:  51%|█████     | 1517/3000 [11:15&lt;04:35,  5.37it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.22e+6]Epoch 1517/3000:  51%|█████     | 1517/3000 [11:15&lt;04:35,  5.37it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.22e+6]Epoch 1518/3000:  51%|█████     | 1517/3000 [11:15&lt;04:35,  5.37it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.22e+6]Epoch 1518/3000:  51%|█████     | 1518/3000 [11:16&lt;06:39,  3.71it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.22e+6]Epoch 1518/3000:  51%|█████     | 1518/3000 [11:16&lt;06:39,  3.71it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1519/3000:  51%|█████     | 1518/3000 [11:16&lt;06:39,  3.71it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1519/3000:  51%|█████     | 1519/3000 [11:16&lt;06:22,  3.88it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1519/3000:  51%|█████     | 1519/3000 [11:16&lt;06:22,  3.88it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.22e+6]Epoch 1520/3000:  51%|█████     | 1519/3000 [11:16&lt;06:22,  3.88it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.22e+6]Epoch 1520/3000:  51%|█████     | 1520/3000 [11:16&lt;08:13,  3.00it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.22e+6]Epoch 1520/3000:  51%|█████     | 1520/3000 [11:16&lt;08:13,  3.00it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1521/3000:  51%|█████     | 1520/3000 [11:16&lt;08:13,  3.00it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1521/3000:  51%|█████     | 1521/3000 [11:17&lt;09:02,  2.73it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1521/3000:  51%|█████     | 1521/3000 [11:17&lt;09:02,  2.73it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1522/3000:  51%|█████     | 1521/3000 [11:17&lt;09:02,  2.73it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1522/3000:  51%|█████     | 1522/3000 [11:17&lt;09:43,  2.53it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1522/3000:  51%|█████     | 1522/3000 [11:17&lt;09:43,  2.53it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.22e+6]Epoch 1523/3000:  51%|█████     | 1522/3000 [11:17&lt;09:43,  2.53it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.22e+6]Epoch 1523/3000:  51%|█████     | 1523/3000 [11:18&lt;10:43,  2.30it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.22e+6]Epoch 1523/3000:  51%|█████     | 1523/3000 [11:18&lt;10:43,  2.30it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.22e+6]Epoch 1524/3000:  51%|█████     | 1523/3000 [11:18&lt;10:43,  2.30it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.22e+6]Epoch 1524/3000:  51%|█████     | 1524/3000 [11:18&lt;10:43,  2.29it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.22e+6]Epoch 1524/3000:  51%|█████     | 1524/3000 [11:18&lt;10:43,  2.29it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.22e+6]Epoch 1525/3000:  51%|█████     | 1524/3000 [11:18&lt;10:43,  2.29it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.22e+6]Epoch 1525/3000:  51%|█████     | 1525/3000 [11:19&lt;11:12,  2.19it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.22e+6]Epoch 1525/3000:  51%|█████     | 1525/3000 [11:19&lt;11:12,  2.19it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.22e+6]Epoch 1526/3000:  51%|█████     | 1525/3000 [11:19&lt;11:12,  2.19it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.22e+6]Epoch 1526/3000:  51%|█████     | 1526/3000 [11:19&lt;11:32,  2.13it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.22e+6]Epoch 1526/3000:  51%|█████     | 1526/3000 [11:19&lt;11:32,  2.13it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.22e+6]Epoch 1527/3000:  51%|█████     | 1526/3000 [11:19&lt;11:32,  2.13it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.22e+6]Epoch 1527/3000:  51%|█████     | 1527/3000 [11:20&lt;11:54,  2.06it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.22e+6]Epoch 1527/3000:  51%|█████     | 1527/3000 [11:20&lt;11:54,  2.06it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1528/3000:  51%|█████     | 1527/3000 [11:20&lt;11:54,  2.06it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1528/3000:  51%|█████     | 1528/3000 [11:20&lt;11:50,  2.07it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1528/3000:  51%|█████     | 1528/3000 [11:20&lt;11:50,  2.07it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.22e+6]Epoch 1529/3000:  51%|█████     | 1528/3000 [11:20&lt;11:50,  2.07it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.22e+6]Epoch 1529/3000:  51%|█████     | 1529/3000 [11:21&lt;11:28,  2.14it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.22e+6]Epoch 1529/3000:  51%|█████     | 1529/3000 [11:21&lt;11:28,  2.14it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.22e+6] Epoch 1530/3000:  51%|█████     | 1529/3000 [11:21&lt;11:28,  2.14it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.22e+6]Epoch 1530/3000:  51%|█████     | 1530/3000 [11:21&lt;12:23,  1.98it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.22e+6]Epoch 1530/3000:  51%|█████     | 1530/3000 [11:21&lt;12:23,  1.98it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1531/3000:  51%|█████     | 1530/3000 [11:21&lt;12:23,  1.98it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1531/3000:  51%|█████     | 1531/3000 [11:22&lt;11:39,  2.10it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1531/3000:  51%|█████     | 1531/3000 [11:22&lt;11:39,  2.10it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1532/3000:  51%|█████     | 1531/3000 [11:22&lt;11:39,  2.10it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1532/3000:  51%|█████     | 1532/3000 [11:22&lt;11:54,  2.05it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1532/3000:  51%|█████     | 1532/3000 [11:22&lt;11:54,  2.05it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1533/3000:  51%|█████     | 1532/3000 [11:22&lt;11:54,  2.05it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1533/3000:  51%|█████     | 1533/3000 [11:23&lt;12:47,  1.91it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1533/3000:  51%|█████     | 1533/3000 [11:23&lt;12:47,  1.91it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.22e+6]Epoch 1534/3000:  51%|█████     | 1533/3000 [11:23&lt;12:47,  1.91it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.22e+6]Epoch 1534/3000:  51%|█████     | 1534/3000 [11:23&lt;11:58,  2.04it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.22e+6]Epoch 1534/3000:  51%|█████     | 1534/3000 [11:23&lt;11:58,  2.04it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.22e+6] Epoch 1535/3000:  51%|█████     | 1534/3000 [11:23&lt;11:58,  2.04it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.22e+6]Epoch 1535/3000:  51%|█████     | 1535/3000 [11:24&lt;11:29,  2.12it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.22e+6]Epoch 1535/3000:  51%|█████     | 1535/3000 [11:24&lt;11:29,  2.12it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.22e+6]Epoch 1536/3000:  51%|█████     | 1535/3000 [11:24&lt;11:29,  2.12it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.22e+6]Epoch 1536/3000:  51%|█████     | 1536/3000 [11:24&lt;10:58,  2.22it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.22e+6]Epoch 1536/3000:  51%|█████     | 1536/3000 [11:24&lt;10:58,  2.22it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.22e+6]Epoch 1537/3000:  51%|█████     | 1536/3000 [11:24&lt;10:58,  2.22it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.22e+6]Epoch 1537/3000:  51%|█████     | 1537/3000 [11:24&lt;11:09,  2.19it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.22e+6]Epoch 1537/3000:  51%|█████     | 1537/3000 [11:24&lt;11:09,  2.19it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1538/3000:  51%|█████     | 1537/3000 [11:24&lt;11:09,  2.19it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1538/3000:  51%|█████▏    | 1538/3000 [11:25&lt;11:50,  2.06it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.22e+6]Epoch 1538/3000:  51%|█████▏    | 1538/3000 [11:25&lt;11:50,  2.06it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.22e+6] Epoch 1539/3000:  51%|█████▏    | 1538/3000 [11:25&lt;11:50,  2.06it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.22e+6]Epoch 1539/3000:  51%|█████▏    | 1539/3000 [11:25&lt;11:13,  2.17it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.22e+6]Epoch 1539/3000:  51%|█████▏    | 1539/3000 [11:25&lt;11:13,  2.17it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.22e+6]Epoch 1540/3000:  51%|█████▏    | 1539/3000 [11:25&lt;11:13,  2.17it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.22e+6]Epoch 1540/3000:  51%|█████▏    | 1540/3000 [11:26&lt;12:23,  1.96it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.22e+6]Epoch 1540/3000:  51%|█████▏    | 1540/3000 [11:26&lt;12:23,  1.96it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1541/3000:  51%|█████▏    | 1540/3000 [11:26&lt;12:23,  1.96it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1541/3000:  51%|█████▏    | 1541/3000 [11:27&lt;13:01,  1.87it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1541/3000:  51%|█████▏    | 1541/3000 [11:27&lt;13:01,  1.87it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1542/3000:  51%|█████▏    | 1541/3000 [11:27&lt;13:01,  1.87it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1542/3000:  51%|█████▏    | 1542/3000 [11:27&lt;12:23,  1.96it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1542/3000:  51%|█████▏    | 1542/3000 [11:27&lt;12:23,  1.96it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1543/3000:  51%|█████▏    | 1542/3000 [11:27&lt;12:23,  1.96it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1543/3000:  51%|█████▏    | 1543/3000 [11:28&lt;12:37,  1.92it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1543/3000:  51%|█████▏    | 1543/3000 [11:28&lt;12:37,  1.92it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1544/3000:  51%|█████▏    | 1543/3000 [11:28&lt;12:37,  1.92it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1544/3000:  51%|█████▏    | 1544/3000 [11:28&lt;14:14,  1.70it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1544/3000:  51%|█████▏    | 1544/3000 [11:28&lt;14:14,  1.70it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.21e+6]Epoch 1545/3000:  51%|█████▏    | 1544/3000 [11:28&lt;14:14,  1.70it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.21e+6]Epoch 1545/3000:  52%|█████▏    | 1545/3000 [11:29&lt;12:37,  1.92it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.21e+6]Epoch 1545/3000:  52%|█████▏    | 1545/3000 [11:29&lt;12:37,  1.92it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1546/3000:  52%|█████▏    | 1545/3000 [11:29&lt;12:37,  1.92it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1546/3000:  52%|█████▏    | 1546/3000 [11:29&lt;11:49,  2.05it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1546/3000:  52%|█████▏    | 1546/3000 [11:29&lt;11:49,  2.05it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1547/3000:  52%|█████▏    | 1546/3000 [11:29&lt;11:49,  2.05it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1547/3000:  52%|█████▏    | 1547/3000 [11:29&lt;10:03,  2.41it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1547/3000:  52%|█████▏    | 1547/3000 [11:29&lt;10:03,  2.41it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1548/3000:  52%|█████▏    | 1547/3000 [11:29&lt;10:03,  2.41it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1548/3000:  52%|█████▏    | 1548/3000 [11:30&lt;10:10,  2.38it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1548/3000:  52%|█████▏    | 1548/3000 [11:30&lt;10:10,  2.38it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1549/3000:  52%|█████▏    | 1548/3000 [11:30&lt;10:10,  2.38it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1549/3000:  52%|█████▏    | 1549/3000 [11:30&lt;09:26,  2.56it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1549/3000:  52%|█████▏    | 1549/3000 [11:30&lt;09:26,  2.56it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.21e+6]Epoch 1550/3000:  52%|█████▏    | 1549/3000 [11:30&lt;09:26,  2.56it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.21e+6]Epoch 1550/3000:  52%|█████▏    | 1550/3000 [11:31&lt;10:09,  2.38it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.21e+6]Epoch 1550/3000:  52%|█████▏    | 1550/3000 [11:31&lt;10:09,  2.38it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.21e+6] Epoch 1551/3000:  52%|█████▏    | 1550/3000 [11:31&lt;10:09,  2.38it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.21e+6]Epoch 1551/3000:  52%|█████▏    | 1551/3000 [11:31&lt;09:19,  2.59it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.21e+6]Epoch 1551/3000:  52%|█████▏    | 1551/3000 [11:31&lt;09:19,  2.59it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.21e+6]Epoch 1552/3000:  52%|█████▏    | 1551/3000 [11:31&lt;09:19,  2.59it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.21e+6]Epoch 1552/3000:  52%|█████▏    | 1552/3000 [11:31&lt;09:55,  2.43it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.21e+6]Epoch 1552/3000:  52%|█████▏    | 1552/3000 [11:31&lt;09:55,  2.43it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.21e+6]Epoch 1553/3000:  52%|█████▏    | 1552/3000 [11:31&lt;09:55,  2.43it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.21e+6]Epoch 1553/3000:  52%|█████▏    | 1553/3000 [11:32&lt;09:56,  2.43it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.21e+6]Epoch 1553/3000:  52%|█████▏    | 1553/3000 [11:32&lt;09:56,  2.43it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.21e+6]Epoch 1554/3000:  52%|█████▏    | 1553/3000 [11:32&lt;09:56,  2.43it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.21e+6]Epoch 1554/3000:  52%|█████▏    | 1554/3000 [11:32&lt;09:18,  2.59it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.21e+6]Epoch 1554/3000:  52%|█████▏    | 1554/3000 [11:32&lt;09:18,  2.59it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.21e+6]Epoch 1555/3000:  52%|█████▏    | 1554/3000 [11:32&lt;09:18,  2.59it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.21e+6]Epoch 1555/3000:  52%|█████▏    | 1555/3000 [11:33&lt;11:01,  2.19it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.21e+6]Epoch 1555/3000:  52%|█████▏    | 1555/3000 [11:33&lt;11:01,  2.19it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.21e+6] Epoch 1556/3000:  52%|█████▏    | 1555/3000 [11:33&lt;11:01,  2.19it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.21e+6]Epoch 1556/3000:  52%|█████▏    | 1556/3000 [11:33&lt;12:56,  1.86it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.21e+6]Epoch 1556/3000:  52%|█████▏    | 1556/3000 [11:33&lt;12:56,  1.86it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1557/3000:  52%|█████▏    | 1556/3000 [11:34&lt;12:56,  1.86it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1557/3000:  52%|█████▏    | 1557/3000 [11:34&lt;11:44,  2.05it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1557/3000:  52%|█████▏    | 1557/3000 [11:34&lt;11:44,  2.05it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1558/3000:  52%|█████▏    | 1557/3000 [11:34&lt;11:44,  2.05it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1558/3000:  52%|█████▏    | 1558/3000 [11:34&lt;11:59,  2.00it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1558/3000:  52%|█████▏    | 1558/3000 [11:34&lt;11:59,  2.00it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1559/3000:  52%|█████▏    | 1558/3000 [11:34&lt;11:59,  2.00it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1559/3000:  52%|█████▏    | 1559/3000 [11:35&lt;09:56,  2.41it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1559/3000:  52%|█████▏    | 1559/3000 [11:35&lt;09:56,  2.41it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1560/3000:  52%|█████▏    | 1559/3000 [11:35&lt;09:56,  2.41it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1560/3000:  52%|█████▏    | 1560/3000 [11:35&lt;10:48,  2.22it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1560/3000:  52%|█████▏    | 1560/3000 [11:35&lt;10:48,  2.22it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1561/3000:  52%|█████▏    | 1560/3000 [11:35&lt;10:48,  2.22it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1561/3000:  52%|█████▏    | 1561/3000 [11:36&lt;11:27,  2.09it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1561/3000:  52%|█████▏    | 1561/3000 [11:36&lt;11:27,  2.09it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.21e+6] Epoch 1562/3000:  52%|█████▏    | 1561/3000 [11:36&lt;11:27,  2.09it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.21e+6]Epoch 1562/3000:  52%|█████▏    | 1562/3000 [11:36&lt;10:18,  2.32it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.21e+6]Epoch 1562/3000:  52%|█████▏    | 1562/3000 [11:36&lt;10:18,  2.32it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1563/3000:  52%|█████▏    | 1562/3000 [11:36&lt;10:18,  2.32it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1563/3000:  52%|█████▏    | 1563/3000 [11:36&lt;09:46,  2.45it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1563/3000:  52%|█████▏    | 1563/3000 [11:36&lt;09:46,  2.45it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1564/3000:  52%|█████▏    | 1563/3000 [11:36&lt;09:46,  2.45it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1564/3000:  52%|█████▏    | 1564/3000 [11:37&lt;09:15,  2.58it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1564/3000:  52%|█████▏    | 1564/3000 [11:37&lt;09:15,  2.58it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.21e+6]Epoch 1565/3000:  52%|█████▏    | 1564/3000 [11:37&lt;09:15,  2.58it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.21e+6]Epoch 1565/3000:  52%|█████▏    | 1565/3000 [11:37&lt;10:12,  2.34it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.21e+6]Epoch 1565/3000:  52%|█████▏    | 1565/3000 [11:37&lt;10:12,  2.34it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1566/3000:  52%|█████▏    | 1565/3000 [11:37&lt;10:12,  2.34it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1566/3000:  52%|█████▏    | 1566/3000 [11:38&lt;10:14,  2.33it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.21e+6]Epoch 1566/3000:  52%|█████▏    | 1566/3000 [11:38&lt;10:14,  2.33it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1567/3000:  52%|█████▏    | 1566/3000 [11:38&lt;10:14,  2.33it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1567/3000:  52%|█████▏    | 1567/3000 [11:38&lt;10:59,  2.17it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.21e+6]Epoch 1567/3000:  52%|█████▏    | 1567/3000 [11:38&lt;10:59,  2.17it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.21e+6]Epoch 1568/3000:  52%|█████▏    | 1567/3000 [11:38&lt;10:59,  2.17it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.21e+6]Epoch 1568/3000:  52%|█████▏    | 1568/3000 [11:39&lt;10:55,  2.19it/s, v_num=1, train_loss_step=1.24e+6, train_loss_epoch=1.21e+6]Epoch 1568/3000:  52%|█████▏    | 1568/3000 [11:39&lt;10:55,  2.19it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.21e+6] Epoch 1569/3000:  52%|█████▏    | 1568/3000 [11:39&lt;10:55,  2.19it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.21e+6]Epoch 1569/3000:  52%|█████▏    | 1569/3000 [11:39&lt;12:12,  1.95it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.21e+6]Epoch 1569/3000:  52%|█████▏    | 1569/3000 [11:39&lt;12:12,  1.95it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.21e+6]Epoch 1570/3000:  52%|█████▏    | 1569/3000 [11:39&lt;12:12,  1.95it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.21e+6]Epoch 1570/3000:  52%|█████▏    | 1570/3000 [11:40&lt;10:20,  2.30it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.21e+6]Epoch 1570/3000:  52%|█████▏    | 1570/3000 [11:40&lt;10:20,  2.30it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.21e+6]Epoch 1571/3000:  52%|█████▏    | 1570/3000 [11:40&lt;10:20,  2.30it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.21e+6]Epoch 1571/3000:  52%|█████▏    | 1571/3000 [11:40&lt;10:51,  2.19it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.21e+6]Epoch 1571/3000:  52%|█████▏    | 1571/3000 [11:40&lt;10:51,  2.19it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.21e+6]Epoch 1572/3000:  52%|█████▏    | 1571/3000 [11:40&lt;10:51,  2.19it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.21e+6]Epoch 1572/3000:  52%|█████▏    | 1572/3000 [11:40&lt;10:52,  2.19it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.21e+6]Epoch 1572/3000:  52%|█████▏    | 1572/3000 [11:40&lt;10:52,  2.19it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.21e+6] Epoch 1573/3000:  52%|█████▏    | 1572/3000 [11:41&lt;10:52,  2.19it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.21e+6]Epoch 1573/3000:  52%|█████▏    | 1573/3000 [11:41&lt;10:24,  2.28it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.21e+6]Epoch 1573/3000:  52%|█████▏    | 1573/3000 [11:41&lt;10:24,  2.28it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1574/3000:  52%|█████▏    | 1573/3000 [11:41&lt;10:24,  2.28it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1574/3000:  52%|█████▏    | 1574/3000 [11:41&lt;10:06,  2.35it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1574/3000:  52%|█████▏    | 1574/3000 [11:41&lt;10:06,  2.35it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.2e+6]Epoch 1575/3000:  52%|█████▏    | 1574/3000 [11:41&lt;10:06,  2.35it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.2e+6]Epoch 1575/3000:  52%|█████▎    | 1575/3000 [11:42&lt;11:41,  2.03it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.2e+6]Epoch 1575/3000:  52%|█████▎    | 1575/3000 [11:42&lt;11:41,  2.03it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6] Epoch 1576/3000:  52%|█████▎    | 1575/3000 [11:42&lt;11:41,  2.03it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1576/3000:  53%|█████▎    | 1576/3000 [11:42&lt;11:38,  2.04it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1576/3000:  53%|█████▎    | 1576/3000 [11:42&lt;11:38,  2.04it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1577/3000:  53%|█████▎    | 1576/3000 [11:42&lt;11:38,  2.04it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1577/3000:  53%|█████▎    | 1577/3000 [11:43&lt;10:53,  2.18it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1577/3000:  53%|█████▎    | 1577/3000 [11:43&lt;10:53,  2.18it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.2e+6]Epoch 1578/3000:  53%|█████▎    | 1577/3000 [11:43&lt;10:53,  2.18it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.2e+6]Epoch 1578/3000:  53%|█████▎    | 1578/3000 [11:43&lt;11:23,  2.08it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.2e+6]Epoch 1578/3000:  53%|█████▎    | 1578/3000 [11:43&lt;11:23,  2.08it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6] Epoch 1579/3000:  53%|█████▎    | 1578/3000 [11:43&lt;11:23,  2.08it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1579/3000:  53%|█████▎    | 1579/3000 [11:44&lt;11:45,  2.01it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1579/3000:  53%|█████▎    | 1579/3000 [11:44&lt;11:45,  2.01it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1580/3000:  53%|█████▎    | 1579/3000 [11:44&lt;11:45,  2.01it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1580/3000:  53%|█████▎    | 1580/3000 [11:45&lt;12:55,  1.83it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1580/3000:  53%|█████▎    | 1580/3000 [11:45&lt;12:55,  1.83it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.2e+6]Epoch 1581/3000:  53%|█████▎    | 1580/3000 [11:45&lt;12:55,  1.83it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.2e+6]Epoch 1581/3000:  53%|█████▎    | 1581/3000 [11:45&lt;10:43,  2.20it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.2e+6]Epoch 1581/3000:  53%|█████▎    | 1581/3000 [11:45&lt;10:43,  2.20it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.2e+6]Epoch 1582/3000:  53%|█████▎    | 1581/3000 [11:45&lt;10:43,  2.20it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.2e+6]Epoch 1582/3000:  53%|█████▎    | 1582/3000 [11:45&lt;09:10,  2.57it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.2e+6]Epoch 1582/3000:  53%|█████▎    | 1582/3000 [11:45&lt;09:10,  2.57it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1583/3000:  53%|█████▎    | 1582/3000 [11:45&lt;09:10,  2.57it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1583/3000:  53%|█████▎    | 1583/3000 [11:45&lt;09:07,  2.59it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1583/3000:  53%|█████▎    | 1583/3000 [11:45&lt;09:07,  2.59it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6] Epoch 1584/3000:  53%|█████▎    | 1583/3000 [11:45&lt;09:07,  2.59it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1584/3000:  53%|█████▎    | 1584/3000 [11:46&lt;09:34,  2.47it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1584/3000:  53%|█████▎    | 1584/3000 [11:46&lt;09:34,  2.47it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1585/3000:  53%|█████▎    | 1584/3000 [11:46&lt;09:34,  2.47it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1585/3000:  53%|█████▎    | 1585/3000 [11:46&lt;10:05,  2.34it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1585/3000:  53%|█████▎    | 1585/3000 [11:46&lt;10:05,  2.34it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.2e+6]Epoch 1586/3000:  53%|█████▎    | 1585/3000 [11:46&lt;10:05,  2.34it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.2e+6]Epoch 1586/3000:  53%|█████▎    | 1586/3000 [11:47&lt;11:22,  2.07it/s, v_num=1, train_loss_step=1.23e+6, train_loss_epoch=1.2e+6]Epoch 1586/3000:  53%|█████▎    | 1586/3000 [11:47&lt;11:22,  2.07it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.2e+6]Epoch 1587/3000:  53%|█████▎    | 1586/3000 [11:47&lt;11:22,  2.07it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.2e+6]Epoch 1587/3000:  53%|█████▎    | 1587/3000 [11:48&lt;12:32,  1.88it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.2e+6]Epoch 1587/3000:  53%|█████▎    | 1587/3000 [11:48&lt;12:32,  1.88it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1588/3000:  53%|█████▎    | 1587/3000 [11:48&lt;12:32,  1.88it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1588/3000:  53%|█████▎    | 1588/3000 [11:48&lt;13:04,  1.80it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1588/3000:  53%|█████▎    | 1588/3000 [11:48&lt;13:04,  1.80it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1589/3000:  53%|█████▎    | 1588/3000 [11:48&lt;13:04,  1.80it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1589/3000:  53%|█████▎    | 1589/3000 [11:49&lt;11:23,  2.06it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1589/3000:  53%|█████▎    | 1589/3000 [11:49&lt;11:23,  2.06it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6] Epoch 1590/3000:  53%|█████▎    | 1589/3000 [11:49&lt;11:23,  2.06it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1590/3000:  53%|█████▎    | 1590/3000 [11:49&lt;10:46,  2.18it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1590/3000:  53%|█████▎    | 1590/3000 [11:49&lt;10:46,  2.18it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.2e+6]Epoch 1591/3000:  53%|█████▎    | 1590/3000 [11:49&lt;10:46,  2.18it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.2e+6]Epoch 1591/3000:  53%|█████▎    | 1591/3000 [11:49&lt;10:45,  2.18it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.2e+6]Epoch 1591/3000:  53%|█████▎    | 1591/3000 [11:49&lt;10:45,  2.18it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1592/3000:  53%|█████▎    | 1591/3000 [11:49&lt;10:45,  2.18it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1592/3000:  53%|█████▎    | 1592/3000 [11:50&lt;10:33,  2.22it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1592/3000:  53%|█████▎    | 1592/3000 [11:50&lt;10:33,  2.22it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.2e+6]Epoch 1593/3000:  53%|█████▎    | 1592/3000 [11:50&lt;10:33,  2.22it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.2e+6]Epoch 1593/3000:  53%|█████▎    | 1593/3000 [11:50&lt;10:05,  2.32it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.2e+6]Epoch 1593/3000:  53%|█████▎    | 1593/3000 [11:50&lt;10:05,  2.32it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1594/3000:  53%|█████▎    | 1593/3000 [11:50&lt;10:05,  2.32it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1594/3000:  53%|█████▎    | 1594/3000 [11:50&lt;09:02,  2.59it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1594/3000:  53%|█████▎    | 1594/3000 [11:50&lt;09:02,  2.59it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6] Epoch 1595/3000:  53%|█████▎    | 1594/3000 [11:50&lt;09:02,  2.59it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1595/3000:  53%|█████▎    | 1595/3000 [11:51&lt;10:20,  2.26it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1595/3000:  53%|█████▎    | 1595/3000 [11:51&lt;10:20,  2.26it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1596/3000:  53%|█████▎    | 1595/3000 [11:51&lt;10:20,  2.26it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1596/3000:  53%|█████▎    | 1596/3000 [11:52&lt;11:52,  1.97it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1596/3000:  53%|█████▎    | 1596/3000 [11:52&lt;11:52,  1.97it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.2e+6]Epoch 1597/3000:  53%|█████▎    | 1596/3000 [11:52&lt;11:52,  1.97it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.2e+6]Epoch 1597/3000:  53%|█████▎    | 1597/3000 [11:52&lt;11:28,  2.04it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.2e+6]Epoch 1597/3000:  53%|█████▎    | 1597/3000 [11:52&lt;11:28,  2.04it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1598/3000:  53%|█████▎    | 1597/3000 [11:52&lt;11:28,  2.04it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1598/3000:  53%|█████▎    | 1598/3000 [11:53&lt;11:11,  2.09it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1598/3000:  53%|█████▎    | 1598/3000 [11:53&lt;11:11,  2.09it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6] Epoch 1599/3000:  53%|█████▎    | 1598/3000 [11:53&lt;11:11,  2.09it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1599/3000:  53%|█████▎    | 1599/3000 [11:53&lt;11:39,  2.00it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1599/3000:  53%|█████▎    | 1599/3000 [11:53&lt;11:39,  2.00it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1600/3000:  53%|█████▎    | 1599/3000 [11:53&lt;11:39,  2.00it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1600/3000:  53%|█████▎    | 1600/3000 [11:54&lt;12:50,  1.82it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1600/3000:  53%|█████▎    | 1600/3000 [11:54&lt;12:50,  1.82it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1601/3000:  53%|█████▎    | 1600/3000 [11:54&lt;12:50,  1.82it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1601/3000:  53%|█████▎    | 1601/3000 [11:54&lt;12:21,  1.89it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1601/3000:  53%|█████▎    | 1601/3000 [11:54&lt;12:21,  1.89it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1602/3000:  53%|█████▎    | 1601/3000 [11:54&lt;12:21,  1.89it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1602/3000:  53%|█████▎    | 1602/3000 [11:55&lt;12:06,  1.92it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1602/3000:  53%|█████▎    | 1602/3000 [11:55&lt;12:06,  1.92it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6] Epoch 1603/3000:  53%|█████▎    | 1602/3000 [11:55&lt;12:06,  1.92it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1603/3000:  53%|█████▎    | 1603/3000 [11:55&lt;12:02,  1.93it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1603/3000:  53%|█████▎    | 1603/3000 [11:55&lt;12:02,  1.93it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1604/3000:  53%|█████▎    | 1603/3000 [11:55&lt;12:02,  1.93it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1604/3000:  53%|█████▎    | 1604/3000 [11:56&lt;10:53,  2.14it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.2e+6]Epoch 1604/3000:  53%|█████▎    | 1604/3000 [11:56&lt;10:53,  2.14it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6] Epoch 1605/3000:  53%|█████▎    | 1604/3000 [11:56&lt;10:53,  2.14it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1605/3000:  54%|█████▎    | 1605/3000 [11:56&lt;11:08,  2.09it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.2e+6]Epoch 1605/3000:  54%|█████▎    | 1605/3000 [11:56&lt;11:08,  2.09it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1606/3000:  54%|█████▎    | 1605/3000 [11:56&lt;11:08,  2.09it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1606/3000:  54%|█████▎    | 1606/3000 [11:57&lt;11:07,  2.09it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.2e+6]Epoch 1606/3000:  54%|█████▎    | 1606/3000 [11:57&lt;11:07,  2.09it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.2e+6]Epoch 1607/3000:  54%|█████▎    | 1606/3000 [11:57&lt;11:07,  2.09it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.2e+6]Epoch 1607/3000:  54%|█████▎    | 1607/3000 [11:57&lt;11:46,  1.97it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.2e+6]Epoch 1607/3000:  54%|█████▎    | 1607/3000 [11:57&lt;11:46,  1.97it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.2e+6]Epoch 1608/3000:  54%|█████▎    | 1607/3000 [11:57&lt;11:46,  1.97it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.2e+6]Epoch 1608/3000:  54%|█████▎    | 1608/3000 [11:58&lt;11:59,  1.94it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.2e+6]Epoch 1608/3000:  54%|█████▎    | 1608/3000 [11:58&lt;11:59,  1.94it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1609/3000:  54%|█████▎    | 1608/3000 [11:58&lt;11:59,  1.94it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1609/3000:  54%|█████▎    | 1609/3000 [11:58&lt;10:52,  2.13it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1609/3000:  54%|█████▎    | 1609/3000 [11:58&lt;10:52,  2.13it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.19e+6]Epoch 1610/3000:  54%|█████▎    | 1609/3000 [11:58&lt;10:52,  2.13it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.19e+6]Epoch 1610/3000:  54%|█████▎    | 1610/3000 [11:59&lt;10:30,  2.21it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.19e+6]Epoch 1610/3000:  54%|█████▎    | 1610/3000 [11:59&lt;10:30,  2.21it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6] Epoch 1611/3000:  54%|█████▎    | 1610/3000 [11:59&lt;10:30,  2.21it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1611/3000:  54%|█████▎    | 1611/3000 [11:59&lt;10:13,  2.26it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1611/3000:  54%|█████▎    | 1611/3000 [11:59&lt;10:13,  2.26it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1612/3000:  54%|█████▎    | 1611/3000 [11:59&lt;10:13,  2.26it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1612/3000:  54%|█████▎    | 1612/3000 [11:59&lt;10:05,  2.29it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1612/3000:  54%|█████▎    | 1612/3000 [11:59&lt;10:05,  2.29it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1613/3000:  54%|█████▎    | 1612/3000 [11:59&lt;10:05,  2.29it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1613/3000:  54%|█████▍    | 1613/3000 [12:00&lt;10:10,  2.27it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1613/3000:  54%|█████▍    | 1613/3000 [12:00&lt;10:10,  2.27it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1614/3000:  54%|█████▍    | 1613/3000 [12:00&lt;10:10,  2.27it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1614/3000:  54%|█████▍    | 1614/3000 [12:00&lt;10:40,  2.16it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1614/3000:  54%|█████▍    | 1614/3000 [12:00&lt;10:40,  2.16it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6] Epoch 1615/3000:  54%|█████▍    | 1614/3000 [12:00&lt;10:40,  2.16it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1615/3000:  54%|█████▍    | 1615/3000 [12:01&lt;11:07,  2.08it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1615/3000:  54%|█████▍    | 1615/3000 [12:01&lt;11:07,  2.08it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.19e+6]Epoch 1616/3000:  54%|█████▍    | 1615/3000 [12:01&lt;11:07,  2.08it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.19e+6]Epoch 1616/3000:  54%|█████▍    | 1616/3000 [12:01&lt;09:16,  2.49it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.19e+6]Epoch 1616/3000:  54%|█████▍    | 1616/3000 [12:01&lt;09:16,  2.49it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1617/3000:  54%|█████▍    | 1616/3000 [12:01&lt;09:16,  2.49it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1617/3000:  54%|█████▍    | 1617/3000 [12:02&lt;09:58,  2.31it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1617/3000:  54%|█████▍    | 1617/3000 [12:02&lt;09:58,  2.31it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1618/3000:  54%|█████▍    | 1617/3000 [12:02&lt;09:58,  2.31it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1618/3000:  54%|█████▍    | 1618/3000 [12:02&lt;11:24,  2.02it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1618/3000:  54%|█████▍    | 1618/3000 [12:02&lt;11:24,  2.02it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6] Epoch 1619/3000:  54%|█████▍    | 1618/3000 [12:02&lt;11:24,  2.02it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1619/3000:  54%|█████▍    | 1619/3000 [12:03&lt;11:04,  2.08it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1619/3000:  54%|█████▍    | 1619/3000 [12:03&lt;11:04,  2.08it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.19e+6]Epoch 1620/3000:  54%|█████▍    | 1619/3000 [12:03&lt;11:04,  2.08it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.19e+6]Epoch 1620/3000:  54%|█████▍    | 1620/3000 [12:03&lt;11:18,  2.03it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.19e+6]Epoch 1620/3000:  54%|█████▍    | 1620/3000 [12:03&lt;11:18,  2.03it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1621/3000:  54%|█████▍    | 1620/3000 [12:03&lt;11:18,  2.03it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1621/3000:  54%|█████▍    | 1621/3000 [12:04&lt;10:51,  2.12it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1621/3000:  54%|█████▍    | 1621/3000 [12:04&lt;10:51,  2.12it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1622/3000:  54%|█████▍    | 1621/3000 [12:04&lt;10:51,  2.12it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1622/3000:  54%|█████▍    | 1622/3000 [12:04&lt;08:24,  2.73it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1622/3000:  54%|█████▍    | 1622/3000 [12:04&lt;08:24,  2.73it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6] Epoch 1623/3000:  54%|█████▍    | 1622/3000 [12:04&lt;08:24,  2.73it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1623/3000:  54%|█████▍    | 1623/3000 [12:04&lt;08:59,  2.55it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1623/3000:  54%|█████▍    | 1623/3000 [12:04&lt;08:59,  2.55it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1624/3000:  54%|█████▍    | 1623/3000 [12:04&lt;08:59,  2.55it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1624/3000:  54%|█████▍    | 1624/3000 [12:05&lt;09:41,  2.37it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1624/3000:  54%|█████▍    | 1624/3000 [12:05&lt;09:41,  2.37it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.19e+6]Epoch 1625/3000:  54%|█████▍    | 1624/3000 [12:05&lt;09:41,  2.37it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.19e+6]Epoch 1625/3000:  54%|█████▍    | 1625/3000 [12:05&lt;08:45,  2.62it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.19e+6]Epoch 1625/3000:  54%|█████▍    | 1625/3000 [12:05&lt;08:45,  2.62it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.19e+6]Epoch 1626/3000:  54%|█████▍    | 1625/3000 [12:05&lt;08:45,  2.62it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.19e+6]Epoch 1626/3000:  54%|█████▍    | 1626/3000 [12:05&lt;08:52,  2.58it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.19e+6]Epoch 1626/3000:  54%|█████▍    | 1626/3000 [12:05&lt;08:52,  2.58it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6] Epoch 1627/3000:  54%|█████▍    | 1626/3000 [12:05&lt;08:52,  2.58it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1627/3000:  54%|█████▍    | 1627/3000 [12:06&lt;09:36,  2.38it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1627/3000:  54%|█████▍    | 1627/3000 [12:06&lt;09:36,  2.38it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1628/3000:  54%|█████▍    | 1627/3000 [12:06&lt;09:36,  2.38it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1628/3000:  54%|█████▍    | 1628/3000 [12:06&lt;10:35,  2.16it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1628/3000:  54%|█████▍    | 1628/3000 [12:06&lt;10:35,  2.16it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1629/3000:  54%|█████▍    | 1628/3000 [12:06&lt;10:35,  2.16it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1629/3000:  54%|█████▍    | 1629/3000 [12:07&lt;10:23,  2.20it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1629/3000:  54%|█████▍    | 1629/3000 [12:07&lt;10:23,  2.20it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1630/3000:  54%|█████▍    | 1629/3000 [12:07&lt;10:23,  2.20it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1630/3000:  54%|█████▍    | 1630/3000 [12:07&lt;09:26,  2.42it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1630/3000:  54%|█████▍    | 1630/3000 [12:07&lt;09:26,  2.42it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1631/3000:  54%|█████▍    | 1630/3000 [12:07&lt;09:26,  2.42it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1631/3000:  54%|█████▍    | 1631/3000 [12:08&lt;11:03,  2.06it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1631/3000:  54%|█████▍    | 1631/3000 [12:08&lt;11:03,  2.06it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1632/3000:  54%|█████▍    | 1631/3000 [12:08&lt;11:03,  2.06it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1632/3000:  54%|█████▍    | 1632/3000 [12:08&lt;11:04,  2.06it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1632/3000:  54%|█████▍    | 1632/3000 [12:08&lt;11:04,  2.06it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.19e+6]Epoch 1633/3000:  54%|█████▍    | 1632/3000 [12:08&lt;11:04,  2.06it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.19e+6]Epoch 1633/3000:  54%|█████▍    | 1633/3000 [12:09&lt;11:29,  1.98it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.19e+6]Epoch 1633/3000:  54%|█████▍    | 1633/3000 [12:09&lt;11:29,  1.98it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6] Epoch 1634/3000:  54%|█████▍    | 1633/3000 [12:09&lt;11:29,  1.98it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1634/3000:  54%|█████▍    | 1634/3000 [12:09&lt;10:33,  2.16it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1634/3000:  54%|█████▍    | 1634/3000 [12:09&lt;10:33,  2.16it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.19e+6]Epoch 1635/3000:  54%|█████▍    | 1634/3000 [12:09&lt;10:33,  2.16it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.19e+6]Epoch 1635/3000:  55%|█████▍    | 1635/3000 [12:10&lt;09:15,  2.46it/s, v_num=1, train_loss_step=1.22e+6, train_loss_epoch=1.19e+6]Epoch 1635/3000:  55%|█████▍    | 1635/3000 [12:10&lt;09:15,  2.46it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1636/3000:  55%|█████▍    | 1635/3000 [12:10&lt;09:15,  2.46it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1636/3000:  55%|█████▍    | 1636/3000 [12:10&lt;08:46,  2.59it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1636/3000:  55%|█████▍    | 1636/3000 [12:10&lt;08:46,  2.59it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1637/3000:  55%|█████▍    | 1636/3000 [12:10&lt;08:46,  2.59it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1637/3000:  55%|█████▍    | 1637/3000 [12:10&lt;08:17,  2.74it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.19e+6]Epoch 1637/3000:  55%|█████▍    | 1637/3000 [12:10&lt;08:17,  2.74it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.19e+6]Epoch 1638/3000:  55%|█████▍    | 1637/3000 [12:10&lt;08:17,  2.74it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.19e+6]Epoch 1638/3000:  55%|█████▍    | 1638/3000 [12:10&lt;07:44,  2.94it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.19e+6]Epoch 1638/3000:  55%|█████▍    | 1638/3000 [12:10&lt;07:44,  2.94it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1639/3000:  55%|█████▍    | 1638/3000 [12:10&lt;07:44,  2.94it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1639/3000:  55%|█████▍    | 1639/3000 [12:11&lt;08:38,  2.63it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1639/3000:  55%|█████▍    | 1639/3000 [12:11&lt;08:38,  2.63it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.19e+6]Epoch 1640/3000:  55%|█████▍    | 1639/3000 [12:11&lt;08:38,  2.63it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.19e+6]Epoch 1640/3000:  55%|█████▍    | 1640/3000 [12:12&lt;11:00,  2.06it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.19e+6]Epoch 1640/3000:  55%|█████▍    | 1640/3000 [12:12&lt;11:00,  2.06it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1641/3000:  55%|█████▍    | 1640/3000 [12:12&lt;11:00,  2.06it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1641/3000:  55%|█████▍    | 1641/3000 [12:12&lt;10:19,  2.19it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1641/3000:  55%|█████▍    | 1641/3000 [12:12&lt;10:19,  2.19it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6] Epoch 1642/3000:  55%|█████▍    | 1641/3000 [12:12&lt;10:19,  2.19it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1642/3000:  55%|█████▍    | 1642/3000 [12:13&lt;11:11,  2.02it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.19e+6]Epoch 1642/3000:  55%|█████▍    | 1642/3000 [12:13&lt;11:11,  2.02it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1643/3000:  55%|█████▍    | 1642/3000 [12:13&lt;11:11,  2.02it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1643/3000:  55%|█████▍    | 1643/3000 [12:13&lt;11:01,  2.05it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1643/3000:  55%|█████▍    | 1643/3000 [12:13&lt;11:01,  2.05it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1644/3000:  55%|█████▍    | 1643/3000 [12:13&lt;11:01,  2.05it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1644/3000:  55%|█████▍    | 1644/3000 [12:14&lt;10:57,  2.06it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1644/3000:  55%|█████▍    | 1644/3000 [12:14&lt;10:57,  2.06it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1645/3000:  55%|█████▍    | 1644/3000 [12:14&lt;10:57,  2.06it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1645/3000:  55%|█████▍    | 1645/3000 [12:14&lt;11:02,  2.04it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.19e+6]Epoch 1645/3000:  55%|█████▍    | 1645/3000 [12:14&lt;11:02,  2.04it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1646/3000:  55%|█████▍    | 1645/3000 [12:14&lt;11:02,  2.04it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1646/3000:  55%|█████▍    | 1646/3000 [12:15&lt;10:50,  2.08it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1646/3000:  55%|█████▍    | 1646/3000 [12:15&lt;10:50,  2.08it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6] Epoch 1647/3000:  55%|█████▍    | 1646/3000 [12:15&lt;10:50,  2.08it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6]Epoch 1647/3000:  55%|█████▍    | 1647/3000 [12:15&lt;11:10,  2.02it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6]Epoch 1647/3000:  55%|█████▍    | 1647/3000 [12:15&lt;11:10,  2.02it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1648/3000:  55%|█████▍    | 1647/3000 [12:15&lt;11:10,  2.02it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1648/3000:  55%|█████▍    | 1648/3000 [12:15&lt;10:38,  2.12it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1648/3000:  55%|█████▍    | 1648/3000 [12:15&lt;10:38,  2.12it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6] Epoch 1649/3000:  55%|█████▍    | 1648/3000 [12:15&lt;10:38,  2.12it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6]Epoch 1649/3000:  55%|█████▍    | 1649/3000 [12:16&lt;11:17,  2.00it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6]Epoch 1649/3000:  55%|█████▍    | 1649/3000 [12:16&lt;11:17,  2.00it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1650/3000:  55%|█████▍    | 1649/3000 [12:16&lt;11:17,  2.00it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1650/3000:  55%|█████▌    | 1650/3000 [12:17&lt;12:24,  1.81it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1650/3000:  55%|█████▌    | 1650/3000 [12:17&lt;12:24,  1.81it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1651/3000:  55%|█████▌    | 1650/3000 [12:17&lt;12:24,  1.81it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1651/3000:  55%|█████▌    | 1651/3000 [12:17&lt;12:29,  1.80it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1651/3000:  55%|█████▌    | 1651/3000 [12:17&lt;12:29,  1.80it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6] Epoch 1652/3000:  55%|█████▌    | 1651/3000 [12:17&lt;12:29,  1.80it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6]Epoch 1652/3000:  55%|█████▌    | 1652/3000 [12:18&lt;11:46,  1.91it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6]Epoch 1652/3000:  55%|█████▌    | 1652/3000 [12:18&lt;11:46,  1.91it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1653/3000:  55%|█████▌    | 1652/3000 [12:18&lt;11:46,  1.91it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1653/3000:  55%|█████▌    | 1653/3000 [12:18&lt;11:08,  2.02it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1653/3000:  55%|█████▌    | 1653/3000 [12:18&lt;11:08,  2.02it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.18e+6]Epoch 1654/3000:  55%|█████▌    | 1653/3000 [12:18&lt;11:08,  2.02it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.18e+6]Epoch 1654/3000:  55%|█████▌    | 1654/3000 [12:19&lt;10:50,  2.07it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.18e+6]Epoch 1654/3000:  55%|█████▌    | 1654/3000 [12:19&lt;10:50,  2.07it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1655/3000:  55%|█████▌    | 1654/3000 [12:19&lt;10:50,  2.07it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1655/3000:  55%|█████▌    | 1655/3000 [12:19&lt;10:55,  2.05it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1655/3000:  55%|█████▌    | 1655/3000 [12:19&lt;10:55,  2.05it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1656/3000:  55%|█████▌    | 1655/3000 [12:19&lt;10:55,  2.05it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1656/3000:  55%|█████▌    | 1656/3000 [12:20&lt;11:46,  1.90it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1656/3000:  55%|█████▌    | 1656/3000 [12:20&lt;11:46,  1.90it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1657/3000:  55%|█████▌    | 1656/3000 [12:20&lt;11:46,  1.90it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1657/3000:  55%|█████▌    | 1657/3000 [12:20&lt;11:24,  1.96it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1657/3000:  55%|█████▌    | 1657/3000 [12:20&lt;11:24,  1.96it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.18e+6]Epoch 1658/3000:  55%|█████▌    | 1657/3000 [12:20&lt;11:24,  1.96it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.18e+6]Epoch 1658/3000:  55%|█████▌    | 1658/3000 [12:21&lt;10:57,  2.04it/s, v_num=1, train_loss_step=1.21e+6, train_loss_epoch=1.18e+6]Epoch 1658/3000:  55%|█████▌    | 1658/3000 [12:21&lt;10:57,  2.04it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1659/3000:  55%|█████▌    | 1658/3000 [12:21&lt;10:57,  2.04it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1659/3000:  55%|█████▌    | 1659/3000 [12:21&lt;10:03,  2.22it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1659/3000:  55%|█████▌    | 1659/3000 [12:21&lt;10:03,  2.22it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1660/3000:  55%|█████▌    | 1659/3000 [12:21&lt;10:03,  2.22it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1660/3000:  55%|█████▌    | 1660/3000 [12:21&lt;08:47,  2.54it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1660/3000:  55%|█████▌    | 1660/3000 [12:21&lt;08:47,  2.54it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1661/3000:  55%|█████▌    | 1660/3000 [12:21&lt;08:47,  2.54it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1661/3000:  55%|█████▌    | 1661/3000 [12:22&lt;09:06,  2.45it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1661/3000:  55%|█████▌    | 1661/3000 [12:22&lt;09:06,  2.45it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1662/3000:  55%|█████▌    | 1661/3000 [12:22&lt;09:06,  2.45it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1662/3000:  55%|█████▌    | 1662/3000 [12:22&lt;08:22,  2.67it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1662/3000:  55%|█████▌    | 1662/3000 [12:22&lt;08:22,  2.67it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.18e+6]Epoch 1663/3000:  55%|█████▌    | 1662/3000 [12:22&lt;08:22,  2.67it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.18e+6]Epoch 1663/3000:  55%|█████▌    | 1663/3000 [12:23&lt;09:27,  2.36it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.18e+6]Epoch 1663/3000:  55%|█████▌    | 1663/3000 [12:23&lt;09:27,  2.36it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1664/3000:  55%|█████▌    | 1663/3000 [12:23&lt;09:27,  2.36it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1664/3000:  55%|█████▌    | 1664/3000 [12:23&lt;09:53,  2.25it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1664/3000:  55%|█████▌    | 1664/3000 [12:23&lt;09:53,  2.25it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6] Epoch 1665/3000:  55%|█████▌    | 1664/3000 [12:23&lt;09:53,  2.25it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6]Epoch 1665/3000:  56%|█████▌    | 1665/3000 [12:24&lt;10:22,  2.15it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6]Epoch 1665/3000:  56%|█████▌    | 1665/3000 [12:24&lt;10:22,  2.15it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1666/3000:  56%|█████▌    | 1665/3000 [12:24&lt;10:22,  2.15it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1666/3000:  56%|█████▌    | 1666/3000 [12:24&lt;10:03,  2.21it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1666/3000:  56%|█████▌    | 1666/3000 [12:24&lt;10:03,  2.21it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1667/3000:  56%|█████▌    | 1666/3000 [12:24&lt;10:03,  2.21it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1667/3000:  56%|█████▌    | 1667/3000 [12:24&lt;10:21,  2.15it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1667/3000:  56%|█████▌    | 1667/3000 [12:24&lt;10:21,  2.15it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1668/3000:  56%|█████▌    | 1667/3000 [12:24&lt;10:21,  2.15it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1668/3000:  56%|█████▌    | 1668/3000 [12:25&lt;10:23,  2.14it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1668/3000:  56%|█████▌    | 1668/3000 [12:25&lt;10:23,  2.14it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1669/3000:  56%|█████▌    | 1668/3000 [12:25&lt;10:23,  2.14it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1669/3000:  56%|█████▌    | 1669/3000 [12:26&lt;11:24,  1.94it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1669/3000:  56%|█████▌    | 1669/3000 [12:26&lt;11:24,  1.94it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6] Epoch 1670/3000:  56%|█████▌    | 1669/3000 [12:26&lt;11:24,  1.94it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6]Epoch 1670/3000:  56%|█████▌    | 1670/3000 [12:26&lt;12:12,  1.81it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6]Epoch 1670/3000:  56%|█████▌    | 1670/3000 [12:26&lt;12:12,  1.81it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1671/3000:  56%|█████▌    | 1670/3000 [12:26&lt;12:12,  1.81it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1671/3000:  56%|█████▌    | 1671/3000 [12:27&lt;11:48,  1.88it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1671/3000:  56%|█████▌    | 1671/3000 [12:27&lt;11:48,  1.88it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1672/3000:  56%|█████▌    | 1671/3000 [12:27&lt;11:48,  1.88it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1672/3000:  56%|█████▌    | 1672/3000 [12:27&lt;09:38,  2.30it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1672/3000:  56%|█████▌    | 1672/3000 [12:27&lt;09:38,  2.30it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6] Epoch 1673/3000:  56%|█████▌    | 1672/3000 [12:27&lt;09:38,  2.30it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6]Epoch 1673/3000:  56%|█████▌    | 1673/3000 [12:27&lt;09:38,  2.30it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6]Epoch 1674/3000:  56%|█████▌    | 1673/3000 [12:27&lt;09:38,  2.30it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6]Epoch 1674/3000:  56%|█████▌    | 1674/3000 [12:27&lt;06:11,  3.57it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.18e+6]Epoch 1674/3000:  56%|█████▌    | 1674/3000 [12:27&lt;06:11,  3.57it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1675/3000:  56%|█████▌    | 1674/3000 [12:27&lt;06:11,  3.57it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1675/3000:  56%|█████▌    | 1675/3000 [12:27&lt;05:13,  4.23it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1675/3000:  56%|█████▌    | 1675/3000 [12:27&lt;05:13,  4.23it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1676/3000:  56%|█████▌    | 1675/3000 [12:27&lt;05:13,  4.23it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1676/3000:  56%|█████▌    | 1676/3000 [12:27&lt;05:12,  4.23it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.18e+6]Epoch 1677/3000:  56%|█████▌    | 1676/3000 [12:27&lt;05:12,  4.23it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.18e+6]Epoch 1677/3000:  56%|█████▌    | 1677/3000 [12:27&lt;04:00,  5.51it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.18e+6]Epoch 1677/3000:  56%|█████▌    | 1677/3000 [12:27&lt;04:00,  5.51it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1678/3000:  56%|█████▌    | 1677/3000 [12:27&lt;04:00,  5.51it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1678/3000:  56%|█████▌    | 1678/3000 [12:28&lt;05:07,  4.30it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1678/3000:  56%|█████▌    | 1678/3000 [12:28&lt;05:07,  4.30it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1679/3000:  56%|█████▌    | 1678/3000 [12:28&lt;05:07,  4.30it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1679/3000:  56%|█████▌    | 1679/3000 [12:28&lt;05:14,  4.20it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.18e+6]Epoch 1679/3000:  56%|█████▌    | 1679/3000 [12:28&lt;05:14,  4.20it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1680/3000:  56%|█████▌    | 1679/3000 [12:28&lt;05:14,  4.20it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1680/3000:  56%|█████▌    | 1680/3000 [12:29&lt;06:41,  3.29it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1680/3000:  56%|█████▌    | 1680/3000 [12:29&lt;06:41,  3.29it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1681/3000:  56%|█████▌    | 1680/3000 [12:29&lt;06:41,  3.29it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1681/3000:  56%|█████▌    | 1681/3000 [12:29&lt;07:53,  2.78it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1681/3000:  56%|█████▌    | 1681/3000 [12:29&lt;07:53,  2.78it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1682/3000:  56%|█████▌    | 1681/3000 [12:29&lt;07:53,  2.78it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1682/3000:  56%|█████▌    | 1682/3000 [12:30&lt;08:31,  2.58it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.18e+6]Epoch 1682/3000:  56%|█████▌    | 1682/3000 [12:30&lt;08:31,  2.58it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1683/3000:  56%|█████▌    | 1682/3000 [12:30&lt;08:31,  2.58it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1683/3000:  56%|█████▌    | 1683/3000 [12:30&lt;08:38,  2.54it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1683/3000:  56%|█████▌    | 1683/3000 [12:30&lt;08:38,  2.54it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1684/3000:  56%|█████▌    | 1683/3000 [12:30&lt;08:38,  2.54it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1684/3000:  56%|█████▌    | 1684/3000 [12:30&lt;08:46,  2.50it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.18e+6]Epoch 1684/3000:  56%|█████▌    | 1684/3000 [12:30&lt;08:46,  2.50it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.17e+6]Epoch 1685/3000:  56%|█████▌    | 1684/3000 [12:30&lt;08:46,  2.50it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.17e+6]Epoch 1685/3000:  56%|█████▌    | 1685/3000 [12:31&lt;09:23,  2.33it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.17e+6]Epoch 1685/3000:  56%|█████▌    | 1685/3000 [12:31&lt;09:23,  2.33it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1686/3000:  56%|█████▌    | 1685/3000 [12:31&lt;09:23,  2.33it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1686/3000:  56%|█████▌    | 1686/3000 [12:31&lt;08:38,  2.53it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1686/3000:  56%|█████▌    | 1686/3000 [12:31&lt;08:38,  2.53it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1687/3000:  56%|█████▌    | 1686/3000 [12:31&lt;08:38,  2.53it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1687/3000:  56%|█████▌    | 1687/3000 [12:32&lt;08:49,  2.48it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1687/3000:  56%|█████▌    | 1687/3000 [12:32&lt;08:49,  2.48it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.17e+6] Epoch 1688/3000:  56%|█████▌    | 1687/3000 [12:32&lt;08:49,  2.48it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.17e+6]Epoch 1688/3000:  56%|█████▋    | 1688/3000 [12:32&lt;09:20,  2.34it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.17e+6]Epoch 1688/3000:  56%|█████▋    | 1688/3000 [12:32&lt;09:20,  2.34it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.17e+6]Epoch 1689/3000:  56%|█████▋    | 1688/3000 [12:32&lt;09:20,  2.34it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.17e+6]Epoch 1689/3000:  56%|█████▋    | 1689/3000 [12:33&lt;09:50,  2.22it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.17e+6]Epoch 1689/3000:  56%|█████▋    | 1689/3000 [12:33&lt;09:50,  2.22it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1690/3000:  56%|█████▋    | 1689/3000 [12:33&lt;09:50,  2.22it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1690/3000:  56%|█████▋    | 1690/3000 [12:33&lt;10:24,  2.10it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1690/3000:  56%|█████▋    | 1690/3000 [12:33&lt;10:24,  2.10it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.17e+6]Epoch 1691/3000:  56%|█████▋    | 1690/3000 [12:33&lt;10:24,  2.10it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.17e+6]Epoch 1691/3000:  56%|█████▋    | 1691/3000 [12:34&lt;10:26,  2.09it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.17e+6]Epoch 1691/3000:  56%|█████▋    | 1691/3000 [12:34&lt;10:26,  2.09it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.17e+6]Epoch 1692/3000:  56%|█████▋    | 1691/3000 [12:34&lt;10:26,  2.09it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.17e+6]Epoch 1692/3000:  56%|█████▋    | 1692/3000 [12:34&lt;10:40,  2.04it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.17e+6]Epoch 1692/3000:  56%|█████▋    | 1692/3000 [12:34&lt;10:40,  2.04it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1693/3000:  56%|█████▋    | 1692/3000 [12:34&lt;10:40,  2.04it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1693/3000:  56%|█████▋    | 1693/3000 [12:35&lt;10:42,  2.04it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1693/3000:  56%|█████▋    | 1693/3000 [12:35&lt;10:42,  2.04it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1694/3000:  56%|█████▋    | 1693/3000 [12:35&lt;10:42,  2.04it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1694/3000:  56%|█████▋    | 1694/3000 [12:35&lt;10:33,  2.06it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1694/3000:  56%|█████▋    | 1694/3000 [12:35&lt;10:33,  2.06it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.17e+6] Epoch 1695/3000:  56%|█████▋    | 1694/3000 [12:35&lt;10:33,  2.06it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.17e+6]Epoch 1695/3000:  56%|█████▋    | 1695/3000 [12:36&lt;10:17,  2.11it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.17e+6]Epoch 1695/3000:  56%|█████▋    | 1695/3000 [12:36&lt;10:17,  2.11it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.17e+6]Epoch 1696/3000:  56%|█████▋    | 1695/3000 [12:36&lt;10:17,  2.11it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.17e+6]Epoch 1696/3000:  57%|█████▋    | 1696/3000 [12:36&lt;10:53,  2.00it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.17e+6]Epoch 1696/3000:  57%|█████▋    | 1696/3000 [12:36&lt;10:53,  2.00it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1697/3000:  57%|█████▋    | 1696/3000 [12:36&lt;10:53,  2.00it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1697/3000:  57%|█████▋    | 1697/3000 [12:37&lt;11:25,  1.90it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1697/3000:  57%|█████▋    | 1697/3000 [12:37&lt;11:25,  1.90it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.17e+6]Epoch 1698/3000:  57%|█████▋    | 1697/3000 [12:37&lt;11:25,  1.90it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.17e+6]Epoch 1698/3000:  57%|█████▋    | 1698/3000 [12:37&lt;11:36,  1.87it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.17e+6]Epoch 1698/3000:  57%|█████▋    | 1698/3000 [12:37&lt;11:36,  1.87it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1699/3000:  57%|█████▋    | 1698/3000 [12:37&lt;11:36,  1.87it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1699/3000:  57%|█████▋    | 1699/3000 [12:38&lt;10:49,  2.00it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1699/3000:  57%|█████▋    | 1699/3000 [12:38&lt;10:49,  2.00it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1700/3000:  57%|█████▋    | 1699/3000 [12:38&lt;10:49,  2.00it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1700/3000:  57%|█████▋    | 1700/3000 [12:38&lt;10:10,  2.13it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1700/3000:  57%|█████▋    | 1700/3000 [12:38&lt;10:10,  2.13it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1701/3000:  57%|█████▋    | 1700/3000 [12:38&lt;10:10,  2.13it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1701/3000:  57%|█████▋    | 1701/3000 [12:38&lt;09:53,  2.19it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1701/3000:  57%|█████▋    | 1701/3000 [12:38&lt;09:53,  2.19it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1702/3000:  57%|█████▋    | 1701/3000 [12:38&lt;09:53,  2.19it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1702/3000:  57%|█████▋    | 1702/3000 [12:39&lt;09:46,  2.21it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1702/3000:  57%|█████▋    | 1702/3000 [12:39&lt;09:46,  2.21it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1703/3000:  57%|█████▋    | 1702/3000 [12:39&lt;09:46,  2.21it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1703/3000:  57%|█████▋    | 1703/3000 [12:39&lt;09:30,  2.27it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1703/3000:  57%|█████▋    | 1703/3000 [12:39&lt;09:30,  2.27it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.17e+6] Epoch 1704/3000:  57%|█████▋    | 1703/3000 [12:39&lt;09:30,  2.27it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.17e+6]Epoch 1704/3000:  57%|█████▋    | 1704/3000 [12:40&lt;10:48,  2.00it/s, v_num=1, train_loss_step=1.2e+6, train_loss_epoch=1.17e+6]Epoch 1704/3000:  57%|█████▋    | 1704/3000 [12:40&lt;10:48,  2.00it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1705/3000:  57%|█████▋    | 1704/3000 [12:40&lt;10:48,  2.00it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1705/3000:  57%|█████▋    | 1705/3000 [12:40&lt;08:56,  2.41it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1705/3000:  57%|█████▋    | 1705/3000 [12:40&lt;08:56,  2.41it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1706/3000:  57%|█████▋    | 1705/3000 [12:40&lt;08:56,  2.41it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1706/3000:  57%|█████▋    | 1706/3000 [12:41&lt;10:08,  2.13it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1706/3000:  57%|█████▋    | 1706/3000 [12:41&lt;10:08,  2.13it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.17e+6]Epoch 1707/3000:  57%|█████▋    | 1706/3000 [12:41&lt;10:08,  2.13it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.17e+6]Epoch 1707/3000:  57%|█████▋    | 1707/3000 [12:41&lt;11:27,  1.88it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.17e+6]Epoch 1707/3000:  57%|█████▋    | 1707/3000 [12:41&lt;11:27,  1.88it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.17e+6]Epoch 1708/3000:  57%|█████▋    | 1707/3000 [12:41&lt;11:27,  1.88it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.17e+6]Epoch 1708/3000:  57%|█████▋    | 1708/3000 [12:42&lt;10:21,  2.08it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.17e+6]Epoch 1708/3000:  57%|█████▋    | 1708/3000 [12:42&lt;10:21,  2.08it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1709/3000:  57%|█████▋    | 1708/3000 [12:42&lt;10:21,  2.08it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1709/3000:  57%|█████▋    | 1709/3000 [12:42&lt;09:55,  2.17it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1709/3000:  57%|█████▋    | 1709/3000 [12:42&lt;09:55,  2.17it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1710/3000:  57%|█████▋    | 1709/3000 [12:42&lt;09:55,  2.17it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1710/3000:  57%|█████▋    | 1710/3000 [12:43&lt;10:26,  2.06it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1710/3000:  57%|█████▋    | 1710/3000 [12:43&lt;10:26,  2.06it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1711/3000:  57%|█████▋    | 1710/3000 [12:43&lt;10:26,  2.06it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1711/3000:  57%|█████▋    | 1711/3000 [12:43&lt;10:20,  2.08it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1711/3000:  57%|█████▋    | 1711/3000 [12:43&lt;10:20,  2.08it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.17e+6]Epoch 1712/3000:  57%|█████▋    | 1711/3000 [12:43&lt;10:20,  2.08it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.17e+6]Epoch 1712/3000:  57%|█████▋    | 1712/3000 [12:44&lt;10:35,  2.03it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.17e+6]Epoch 1712/3000:  57%|█████▋    | 1712/3000 [12:44&lt;10:35,  2.03it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1713/3000:  57%|█████▋    | 1712/3000 [12:44&lt;10:35,  2.03it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1713/3000:  57%|█████▋    | 1713/3000 [12:44&lt;11:09,  1.92it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1713/3000:  57%|█████▋    | 1713/3000 [12:44&lt;11:09,  1.92it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1714/3000:  57%|█████▋    | 1713/3000 [12:44&lt;11:09,  1.92it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1714/3000:  57%|█████▋    | 1714/3000 [12:45&lt;11:14,  1.91it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1714/3000:  57%|█████▋    | 1714/3000 [12:45&lt;11:14,  1.91it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1715/3000:  57%|█████▋    | 1714/3000 [12:45&lt;11:14,  1.91it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1715/3000:  57%|█████▋    | 1715/3000 [12:45&lt;11:09,  1.92it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1715/3000:  57%|█████▋    | 1715/3000 [12:45&lt;11:09,  1.92it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1716/3000:  57%|█████▋    | 1715/3000 [12:45&lt;11:09,  1.92it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1716/3000:  57%|█████▋    | 1716/3000 [12:46&lt;09:54,  2.16it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1716/3000:  57%|█████▋    | 1716/3000 [12:46&lt;09:54,  2.16it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1717/3000:  57%|█████▋    | 1716/3000 [12:46&lt;09:54,  2.16it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1717/3000:  57%|█████▋    | 1717/3000 [12:46&lt;08:38,  2.48it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1717/3000:  57%|█████▋    | 1717/3000 [12:46&lt;08:38,  2.48it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.17e+6]Epoch 1718/3000:  57%|█████▋    | 1717/3000 [12:46&lt;08:38,  2.48it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.17e+6]Epoch 1718/3000:  57%|█████▋    | 1718/3000 [12:46&lt;08:28,  2.52it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.17e+6]Epoch 1718/3000:  57%|█████▋    | 1718/3000 [12:46&lt;08:28,  2.52it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1719/3000:  57%|█████▋    | 1718/3000 [12:46&lt;08:28,  2.52it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1719/3000:  57%|█████▋    | 1719/3000 [12:47&lt;09:15,  2.31it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1719/3000:  57%|█████▋    | 1719/3000 [12:47&lt;09:15,  2.31it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1720/3000:  57%|█████▋    | 1719/3000 [12:47&lt;09:15,  2.31it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1720/3000:  57%|█████▋    | 1720/3000 [12:47&lt;09:50,  2.17it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1720/3000:  57%|█████▋    | 1720/3000 [12:47&lt;09:50,  2.17it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1721/3000:  57%|█████▋    | 1720/3000 [12:47&lt;09:50,  2.17it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1721/3000:  57%|█████▋    | 1721/3000 [12:48&lt;09:44,  2.19it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1721/3000:  57%|█████▋    | 1721/3000 [12:48&lt;09:44,  2.19it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.17e+6]Epoch 1722/3000:  57%|█████▋    | 1721/3000 [12:48&lt;09:44,  2.19it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.17e+6]Epoch 1722/3000:  57%|█████▋    | 1722/3000 [12:48&lt;09:47,  2.18it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.17e+6]Epoch 1722/3000:  57%|█████▋    | 1722/3000 [12:48&lt;09:47,  2.18it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1723/3000:  57%|█████▋    | 1722/3000 [12:48&lt;09:47,  2.18it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1723/3000:  57%|█████▋    | 1723/3000 [12:49&lt;09:45,  2.18it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.17e+6]Epoch 1723/3000:  57%|█████▋    | 1723/3000 [12:49&lt;09:45,  2.18it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1724/3000:  57%|█████▋    | 1723/3000 [12:49&lt;09:45,  2.18it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1724/3000:  57%|█████▋    | 1724/3000 [12:49&lt;10:26,  2.04it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.17e+6]Epoch 1724/3000:  57%|█████▋    | 1724/3000 [12:49&lt;10:26,  2.04it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.17e+6]Epoch 1725/3000:  57%|█████▋    | 1724/3000 [12:49&lt;10:26,  2.04it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.17e+6]Epoch 1725/3000:  57%|█████▊    | 1725/3000 [12:50&lt;08:51,  2.40it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.17e+6]Epoch 1725/3000:  57%|█████▊    | 1725/3000 [12:50&lt;08:51,  2.40it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1726/3000:  57%|█████▊    | 1725/3000 [12:50&lt;08:51,  2.40it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1726/3000:  58%|█████▊    | 1726/3000 [12:50&lt;08:33,  2.48it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1726/3000:  58%|█████▊    | 1726/3000 [12:50&lt;08:33,  2.48it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1727/3000:  58%|█████▊    | 1726/3000 [12:50&lt;08:33,  2.48it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1727/3000:  58%|█████▊    | 1727/3000 [12:50&lt;09:11,  2.31it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1727/3000:  58%|█████▊    | 1727/3000 [12:50&lt;09:11,  2.31it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.16e+6]Epoch 1728/3000:  58%|█████▊    | 1727/3000 [12:50&lt;09:11,  2.31it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.16e+6]Epoch 1728/3000:  58%|█████▊    | 1728/3000 [12:51&lt;09:20,  2.27it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.16e+6]Epoch 1728/3000:  58%|█████▊    | 1728/3000 [12:51&lt;09:20,  2.27it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1729/3000:  58%|█████▊    | 1728/3000 [12:51&lt;09:20,  2.27it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1729/3000:  58%|█████▊    | 1729/3000 [12:51&lt;08:45,  2.42it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1729/3000:  58%|█████▊    | 1729/3000 [12:51&lt;08:45,  2.42it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1730/3000:  58%|█████▊    | 1729/3000 [12:51&lt;08:45,  2.42it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1730/3000:  58%|█████▊    | 1730/3000 [12:52&lt;09:04,  2.33it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1730/3000:  58%|█████▊    | 1730/3000 [12:52&lt;09:04,  2.33it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1731/3000:  58%|█████▊    | 1730/3000 [12:52&lt;09:04,  2.33it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1731/3000:  58%|█████▊    | 1731/3000 [12:52&lt;09:40,  2.18it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1731/3000:  58%|█████▊    | 1731/3000 [12:52&lt;09:40,  2.18it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.16e+6]Epoch 1732/3000:  58%|█████▊    | 1731/3000 [12:52&lt;09:40,  2.18it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.16e+6]Epoch 1732/3000:  58%|█████▊    | 1732/3000 [12:53&lt;10:04,  2.10it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.16e+6]Epoch 1732/3000:  58%|█████▊    | 1732/3000 [12:53&lt;10:04,  2.10it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1733/3000:  58%|█████▊    | 1732/3000 [12:53&lt;10:04,  2.10it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1733/3000:  58%|█████▊    | 1733/3000 [12:53&lt;10:20,  2.04it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1733/3000:  58%|█████▊    | 1733/3000 [12:53&lt;10:20,  2.04it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1734/3000:  58%|█████▊    | 1733/3000 [12:53&lt;10:20,  2.04it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1734/3000:  58%|█████▊    | 1734/3000 [12:54&lt;09:52,  2.14it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1734/3000:  58%|█████▊    | 1734/3000 [12:54&lt;09:52,  2.14it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.16e+6]Epoch 1735/3000:  58%|█████▊    | 1734/3000 [12:54&lt;09:52,  2.14it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.16e+6]Epoch 1735/3000:  58%|█████▊    | 1735/3000 [12:54&lt;09:40,  2.18it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.16e+6]Epoch 1735/3000:  58%|█████▊    | 1735/3000 [12:54&lt;09:40,  2.18it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.16e+6]Epoch 1736/3000:  58%|█████▊    | 1735/3000 [12:54&lt;09:40,  2.18it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.16e+6]Epoch 1736/3000:  58%|█████▊    | 1736/3000 [12:55&lt;10:19,  2.04it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.16e+6]Epoch 1736/3000:  58%|█████▊    | 1736/3000 [12:55&lt;10:19,  2.04it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.16e+6]Epoch 1737/3000:  58%|█████▊    | 1736/3000 [12:55&lt;10:19,  2.04it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.16e+6]Epoch 1737/3000:  58%|█████▊    | 1737/3000 [12:55&lt;10:41,  1.97it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.16e+6]Epoch 1737/3000:  58%|█████▊    | 1737/3000 [12:55&lt;10:41,  1.97it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1738/3000:  58%|█████▊    | 1737/3000 [12:55&lt;10:41,  1.97it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1738/3000:  58%|█████▊    | 1738/3000 [12:56&lt;10:48,  1.95it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1738/3000:  58%|█████▊    | 1738/3000 [12:56&lt;10:48,  1.95it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1739/3000:  58%|█████▊    | 1738/3000 [12:56&lt;10:48,  1.95it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1739/3000:  58%|█████▊    | 1739/3000 [12:56&lt;10:43,  1.96it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1739/3000:  58%|█████▊    | 1739/3000 [12:56&lt;10:43,  1.96it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1740/3000:  58%|█████▊    | 1739/3000 [12:56&lt;10:43,  1.96it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1740/3000:  58%|█████▊    | 1740/3000 [12:57&lt;10:17,  2.04it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1740/3000:  58%|█████▊    | 1740/3000 [12:57&lt;10:17,  2.04it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.16e+6]Epoch 1741/3000:  58%|█████▊    | 1740/3000 [12:57&lt;10:17,  2.04it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.16e+6]Epoch 1741/3000:  58%|█████▊    | 1741/3000 [12:57&lt;11:06,  1.89it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.16e+6]Epoch 1741/3000:  58%|█████▊    | 1741/3000 [12:57&lt;11:06,  1.89it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1742/3000:  58%|█████▊    | 1741/3000 [12:57&lt;11:06,  1.89it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1742/3000:  58%|█████▊    | 1742/3000 [12:58&lt;09:24,  2.23it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1742/3000:  58%|█████▊    | 1742/3000 [12:58&lt;09:24,  2.23it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.16e+6]Epoch 1743/3000:  58%|█████▊    | 1742/3000 [12:58&lt;09:24,  2.23it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.16e+6]Epoch 1743/3000:  58%|█████▊    | 1743/3000 [12:58&lt;08:58,  2.34it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.16e+6]Epoch 1743/3000:  58%|█████▊    | 1743/3000 [12:58&lt;08:58,  2.34it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1744/3000:  58%|█████▊    | 1743/3000 [12:58&lt;08:58,  2.34it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1744/3000:  58%|█████▊    | 1744/3000 [12:58&lt;08:27,  2.47it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1744/3000:  58%|█████▊    | 1744/3000 [12:58&lt;08:27,  2.47it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1745/3000:  58%|█████▊    | 1744/3000 [12:58&lt;08:27,  2.47it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1745/3000:  58%|█████▊    | 1745/3000 [12:59&lt;08:57,  2.34it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1745/3000:  58%|█████▊    | 1745/3000 [12:59&lt;08:57,  2.34it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.16e+6]Epoch 1746/3000:  58%|█████▊    | 1745/3000 [12:59&lt;08:57,  2.34it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.16e+6]Epoch 1746/3000:  58%|█████▊    | 1746/3000 [13:00&lt;10:29,  1.99it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.16e+6]Epoch 1746/3000:  58%|█████▊    | 1746/3000 [13:00&lt;10:29,  1.99it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1747/3000:  58%|█████▊    | 1746/3000 [13:00&lt;10:29,  1.99it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1747/3000:  58%|█████▊    | 1747/3000 [13:00&lt;11:36,  1.80it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1747/3000:  58%|█████▊    | 1747/3000 [13:00&lt;11:36,  1.80it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.16e+6]Epoch 1748/3000:  58%|█████▊    | 1747/3000 [13:00&lt;11:36,  1.80it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.16e+6]Epoch 1748/3000:  58%|█████▊    | 1748/3000 [13:01&lt;11:00,  1.90it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.16e+6]Epoch 1748/3000:  58%|█████▊    | 1748/3000 [13:01&lt;11:00,  1.90it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1749/3000:  58%|█████▊    | 1748/3000 [13:01&lt;11:00,  1.90it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1749/3000:  58%|█████▊    | 1749/3000 [13:01&lt;10:45,  1.94it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1749/3000:  58%|█████▊    | 1749/3000 [13:01&lt;10:45,  1.94it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1750/3000:  58%|█████▊    | 1749/3000 [13:01&lt;10:45,  1.94it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1750/3000:  58%|█████▊    | 1750/3000 [13:02&lt;10:26,  2.00it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1750/3000:  58%|█████▊    | 1750/3000 [13:02&lt;10:26,  2.00it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1751/3000:  58%|█████▊    | 1750/3000 [13:02&lt;10:26,  2.00it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1751/3000:  58%|█████▊    | 1751/3000 [13:02&lt;09:48,  2.12it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1751/3000:  58%|█████▊    | 1751/3000 [13:02&lt;09:48,  2.12it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1752/3000:  58%|█████▊    | 1751/3000 [13:02&lt;09:48,  2.12it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1752/3000:  58%|█████▊    | 1752/3000 [13:02&lt;09:44,  2.14it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1752/3000:  58%|█████▊    | 1752/3000 [13:02&lt;09:44,  2.14it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1753/3000:  58%|█████▊    | 1752/3000 [13:02&lt;09:44,  2.14it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1753/3000:  58%|█████▊    | 1753/3000 [13:03&lt;10:15,  2.02it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1753/3000:  58%|█████▊    | 1753/3000 [13:03&lt;10:15,  2.02it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1754/3000:  58%|█████▊    | 1753/3000 [13:03&lt;10:15,  2.02it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1754/3000:  58%|█████▊    | 1754/3000 [13:04&lt;10:30,  1.98it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1754/3000:  58%|█████▊    | 1754/3000 [13:04&lt;10:30,  1.98it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1755/3000:  58%|█████▊    | 1754/3000 [13:04&lt;10:30,  1.98it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1755/3000:  58%|█████▊    | 1755/3000 [13:04&lt;10:20,  2.01it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1755/3000:  58%|█████▊    | 1755/3000 [13:04&lt;10:20,  2.01it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1756/3000:  58%|█████▊    | 1755/3000 [13:04&lt;10:20,  2.01it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1756/3000:  59%|█████▊    | 1756/3000 [13:05&lt;10:42,  1.94it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1756/3000:  59%|█████▊    | 1756/3000 [13:05&lt;10:42,  1.94it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1757/3000:  59%|█████▊    | 1756/3000 [13:05&lt;10:42,  1.94it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1757/3000:  59%|█████▊    | 1757/3000 [13:05&lt;09:39,  2.15it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1757/3000:  59%|█████▊    | 1757/3000 [13:05&lt;09:39,  2.15it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1758/3000:  59%|█████▊    | 1757/3000 [13:05&lt;09:39,  2.15it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1758/3000:  59%|█████▊    | 1758/3000 [13:05&lt;08:10,  2.53it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1758/3000:  59%|█████▊    | 1758/3000 [13:05&lt;08:10,  2.53it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1759/3000:  59%|█████▊    | 1758/3000 [13:05&lt;08:10,  2.53it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1759/3000:  59%|█████▊    | 1759/3000 [13:06&lt;09:25,  2.20it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1759/3000:  59%|█████▊    | 1759/3000 [13:06&lt;09:25,  2.20it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.16e+6]Epoch 1760/3000:  59%|█████▊    | 1759/3000 [13:06&lt;09:25,  2.20it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.16e+6]Epoch 1760/3000:  59%|█████▊    | 1760/3000 [13:06&lt;10:06,  2.04it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.16e+6]Epoch 1760/3000:  59%|█████▊    | 1760/3000 [13:06&lt;10:06,  2.04it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1761/3000:  59%|█████▊    | 1760/3000 [13:06&lt;10:06,  2.04it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1761/3000:  59%|█████▊    | 1761/3000 [13:07&lt;10:35,  1.95it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1761/3000:  59%|█████▊    | 1761/3000 [13:07&lt;10:35,  1.95it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1762/3000:  59%|█████▊    | 1761/3000 [13:07&lt;10:35,  1.95it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1762/3000:  59%|█████▊    | 1762/3000 [13:07&lt;09:49,  2.10it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1762/3000:  59%|█████▊    | 1762/3000 [13:07&lt;09:49,  2.10it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1763/3000:  59%|█████▊    | 1762/3000 [13:07&lt;09:49,  2.10it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1763/3000:  59%|█████▉    | 1763/3000 [13:08&lt;09:00,  2.29it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1763/3000:  59%|█████▉    | 1763/3000 [13:08&lt;09:00,  2.29it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1764/3000:  59%|█████▉    | 1763/3000 [13:08&lt;09:00,  2.29it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1764/3000:  59%|█████▉    | 1764/3000 [13:08&lt;09:34,  2.15it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1764/3000:  59%|█████▉    | 1764/3000 [13:08&lt;09:34,  2.15it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1765/3000:  59%|█████▉    | 1764/3000 [13:08&lt;09:34,  2.15it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1765/3000:  59%|█████▉    | 1765/3000 [13:09&lt;08:55,  2.31it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1765/3000:  59%|█████▉    | 1765/3000 [13:09&lt;08:55,  2.31it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1766/3000:  59%|█████▉    | 1765/3000 [13:09&lt;08:55,  2.31it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1766/3000:  59%|█████▉    | 1766/3000 [13:09&lt;09:22,  2.19it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.16e+6]Epoch 1766/3000:  59%|█████▉    | 1766/3000 [13:09&lt;09:22,  2.19it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1767/3000:  59%|█████▉    | 1766/3000 [13:09&lt;09:22,  2.19it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1767/3000:  59%|█████▉    | 1767/3000 [13:10&lt;10:02,  2.04it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.16e+6]Epoch 1767/3000:  59%|█████▉    | 1767/3000 [13:10&lt;10:02,  2.04it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1768/3000:  59%|█████▉    | 1767/3000 [13:10&lt;10:02,  2.04it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1768/3000:  59%|█████▉    | 1768/3000 [13:10&lt;09:49,  2.09it/s, v_num=1, train_loss_step=1.19e+6, train_loss_epoch=1.16e+6]Epoch 1768/3000:  59%|█████▉    | 1768/3000 [13:10&lt;09:49,  2.09it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1769/3000:  59%|█████▉    | 1768/3000 [13:10&lt;09:49,  2.09it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1769/3000:  59%|█████▉    | 1769/3000 [13:11&lt;09:59,  2.05it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.16e+6]Epoch 1769/3000:  59%|█████▉    | 1769/3000 [13:11&lt;09:59,  2.05it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.15e+6]Epoch 1770/3000:  59%|█████▉    | 1769/3000 [13:11&lt;09:59,  2.05it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.15e+6]Epoch 1770/3000:  59%|█████▉    | 1770/3000 [13:11&lt;10:13,  2.01it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.15e+6]Epoch 1770/3000:  59%|█████▉    | 1770/3000 [13:11&lt;10:13,  2.01it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1771/3000:  59%|█████▉    | 1770/3000 [13:11&lt;10:13,  2.01it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1771/3000:  59%|█████▉    | 1771/3000 [13:12&lt;10:04,  2.03it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1771/3000:  59%|█████▉    | 1771/3000 [13:12&lt;10:04,  2.03it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1772/3000:  59%|█████▉    | 1771/3000 [13:12&lt;10:04,  2.03it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1772/3000:  59%|█████▉    | 1772/3000 [13:12&lt;10:10,  2.01it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1772/3000:  59%|█████▉    | 1772/3000 [13:12&lt;10:10,  2.01it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1773/3000:  59%|█████▉    | 1772/3000 [13:12&lt;10:10,  2.01it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1773/3000:  59%|█████▉    | 1773/3000 [13:13&lt;10:14,  2.00it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1773/3000:  59%|█████▉    | 1773/3000 [13:13&lt;10:14,  2.00it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1774/3000:  59%|█████▉    | 1773/3000 [13:13&lt;10:14,  2.00it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1774/3000:  59%|█████▉    | 1774/3000 [13:13&lt;08:39,  2.36it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1774/3000:  59%|█████▉    | 1774/3000 [13:13&lt;08:39,  2.36it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.15e+6]Epoch 1775/3000:  59%|█████▉    | 1774/3000 [13:13&lt;08:39,  2.36it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.15e+6]Epoch 1775/3000:  59%|█████▉    | 1775/3000 [13:13&lt;08:49,  2.31it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.15e+6]Epoch 1775/3000:  59%|█████▉    | 1775/3000 [13:13&lt;08:49,  2.31it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1776/3000:  59%|█████▉    | 1775/3000 [13:13&lt;08:49,  2.31it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1776/3000:  59%|█████▉    | 1776/3000 [13:14&lt;08:12,  2.48it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1776/3000:  59%|█████▉    | 1776/3000 [13:14&lt;08:12,  2.48it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.15e+6]Epoch 1777/3000:  59%|█████▉    | 1776/3000 [13:14&lt;08:12,  2.48it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.15e+6]Epoch 1777/3000:  59%|█████▉    | 1777/3000 [13:14&lt;08:16,  2.46it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.15e+6]Epoch 1777/3000:  59%|█████▉    | 1777/3000 [13:14&lt;08:16,  2.46it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1778/3000:  59%|█████▉    | 1777/3000 [13:14&lt;08:16,  2.46it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1778/3000:  59%|█████▉    | 1778/3000 [13:14&lt;08:19,  2.45it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1778/3000:  59%|█████▉    | 1778/3000 [13:14&lt;08:19,  2.45it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1779/3000:  59%|█████▉    | 1778/3000 [13:14&lt;08:19,  2.45it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1779/3000:  59%|█████▉    | 1779/3000 [13:15&lt;08:02,  2.53it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1779/3000:  59%|█████▉    | 1779/3000 [13:15&lt;08:02,  2.53it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1780/3000:  59%|█████▉    | 1779/3000 [13:15&lt;08:02,  2.53it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1780/3000:  59%|█████▉    | 1780/3000 [13:15&lt;08:13,  2.47it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1780/3000:  59%|█████▉    | 1780/3000 [13:15&lt;08:13,  2.47it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.15e+6]Epoch 1781/3000:  59%|█████▉    | 1780/3000 [13:15&lt;08:13,  2.47it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.15e+6]Epoch 1781/3000:  59%|█████▉    | 1781/3000 [13:16&lt;09:04,  2.24it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.15e+6]Epoch 1781/3000:  59%|█████▉    | 1781/3000 [13:16&lt;09:04,  2.24it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1782/3000:  59%|█████▉    | 1781/3000 [13:16&lt;09:04,  2.24it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1782/3000:  59%|█████▉    | 1782/3000 [13:16&lt;09:31,  2.13it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1782/3000:  59%|█████▉    | 1782/3000 [13:16&lt;09:31,  2.13it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1783/3000:  59%|█████▉    | 1782/3000 [13:16&lt;09:31,  2.13it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1783/3000:  59%|█████▉    | 1783/3000 [13:17&lt;09:03,  2.24it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1783/3000:  59%|█████▉    | 1783/3000 [13:17&lt;09:03,  2.24it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1784/3000:  59%|█████▉    | 1783/3000 [13:17&lt;09:03,  2.24it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1784/3000:  59%|█████▉    | 1784/3000 [13:17&lt;09:36,  2.11it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1784/3000:  59%|█████▉    | 1784/3000 [13:17&lt;09:36,  2.11it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1785/3000:  59%|█████▉    | 1784/3000 [13:17&lt;09:36,  2.11it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1785/3000:  60%|█████▉    | 1785/3000 [13:18&lt;10:05,  2.01it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1785/3000:  60%|█████▉    | 1785/3000 [13:18&lt;10:05,  2.01it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1786/3000:  60%|█████▉    | 1785/3000 [13:18&lt;10:05,  2.01it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1786/3000:  60%|█████▉    | 1786/3000 [13:18&lt;09:34,  2.11it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1786/3000:  60%|█████▉    | 1786/3000 [13:18&lt;09:34,  2.11it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1787/3000:  60%|█████▉    | 1786/3000 [13:18&lt;09:34,  2.11it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1787/3000:  60%|█████▉    | 1787/3000 [13:19&lt;09:07,  2.22it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1787/3000:  60%|█████▉    | 1787/3000 [13:19&lt;09:07,  2.22it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.15e+6]Epoch 1788/3000:  60%|█████▉    | 1787/3000 [13:19&lt;09:07,  2.22it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.15e+6]Epoch 1788/3000:  60%|█████▉    | 1788/3000 [13:19&lt;08:09,  2.48it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.15e+6]Epoch 1788/3000:  60%|█████▉    | 1788/3000 [13:19&lt;08:09,  2.48it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1789/3000:  60%|█████▉    | 1788/3000 [13:19&lt;08:09,  2.48it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1789/3000:  60%|█████▉    | 1789/3000 [13:19&lt;08:09,  2.47it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1789/3000:  60%|█████▉    | 1789/3000 [13:19&lt;08:09,  2.47it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1790/3000:  60%|█████▉    | 1789/3000 [13:19&lt;08:09,  2.47it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1790/3000:  60%|█████▉    | 1790/3000 [13:20&lt;09:42,  2.08it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1790/3000:  60%|█████▉    | 1790/3000 [13:20&lt;09:42,  2.08it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1791/3000:  60%|█████▉    | 1790/3000 [13:20&lt;09:42,  2.08it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1791/3000:  60%|█████▉    | 1791/3000 [13:20&lt;09:14,  2.18it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1791/3000:  60%|█████▉    | 1791/3000 [13:20&lt;09:14,  2.18it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1792/3000:  60%|█████▉    | 1791/3000 [13:20&lt;09:14,  2.18it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1792/3000:  60%|█████▉    | 1792/3000 [13:21&lt;09:06,  2.21it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1792/3000:  60%|█████▉    | 1792/3000 [13:21&lt;09:06,  2.21it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1793/3000:  60%|█████▉    | 1792/3000 [13:21&lt;09:06,  2.21it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1793/3000:  60%|█████▉    | 1793/3000 [13:21&lt;07:55,  2.54it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1793/3000:  60%|█████▉    | 1793/3000 [13:21&lt;07:55,  2.54it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1794/3000:  60%|█████▉    | 1793/3000 [13:21&lt;07:55,  2.54it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1794/3000:  60%|█████▉    | 1794/3000 [13:21&lt;07:19,  2.74it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1794/3000:  60%|█████▉    | 1794/3000 [13:21&lt;07:19,  2.74it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1795/3000:  60%|█████▉    | 1794/3000 [13:21&lt;07:19,  2.74it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1795/3000:  60%|█████▉    | 1795/3000 [13:22&lt;07:14,  2.77it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1795/3000:  60%|█████▉    | 1795/3000 [13:22&lt;07:14,  2.77it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1796/3000:  60%|█████▉    | 1795/3000 [13:22&lt;07:14,  2.77it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1796/3000:  60%|█████▉    | 1796/3000 [13:22&lt;09:04,  2.21it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1796/3000:  60%|█████▉    | 1796/3000 [13:22&lt;09:04,  2.21it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1797/3000:  60%|█████▉    | 1796/3000 [13:22&lt;09:04,  2.21it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1797/3000:  60%|█████▉    | 1797/3000 [13:23&lt;09:23,  2.13it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1797/3000:  60%|█████▉    | 1797/3000 [13:23&lt;09:23,  2.13it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.15e+6]Epoch 1798/3000:  60%|█████▉    | 1797/3000 [13:23&lt;09:23,  2.13it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.15e+6]Epoch 1798/3000:  60%|█████▉    | 1798/3000 [13:23&lt;09:34,  2.09it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.15e+6]Epoch 1798/3000:  60%|█████▉    | 1798/3000 [13:23&lt;09:34,  2.09it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1799/3000:  60%|█████▉    | 1798/3000 [13:23&lt;09:34,  2.09it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1799/3000:  60%|█████▉    | 1799/3000 [13:24&lt;09:43,  2.06it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1799/3000:  60%|█████▉    | 1799/3000 [13:24&lt;09:43,  2.06it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1800/3000:  60%|█████▉    | 1799/3000 [13:24&lt;09:43,  2.06it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1800/3000:  60%|██████    | 1800/3000 [13:24&lt;09:49,  2.04it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1800/3000:  60%|██████    | 1800/3000 [13:24&lt;09:49,  2.04it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1801/3000:  60%|██████    | 1800/3000 [13:24&lt;09:49,  2.04it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1801/3000:  60%|██████    | 1801/3000 [13:25&lt;09:16,  2.16it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1801/3000:  60%|██████    | 1801/3000 [13:25&lt;09:16,  2.16it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.15e+6]Epoch 1802/3000:  60%|██████    | 1801/3000 [13:25&lt;09:16,  2.16it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.15e+6]Epoch 1802/3000:  60%|██████    | 1802/3000 [13:25&lt;09:49,  2.03it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.15e+6]Epoch 1802/3000:  60%|██████    | 1802/3000 [13:25&lt;09:49,  2.03it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.15e+6]Epoch 1803/3000:  60%|██████    | 1802/3000 [13:25&lt;09:49,  2.03it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.15e+6]Epoch 1803/3000:  60%|██████    | 1803/3000 [13:26&lt;10:58,  1.82it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.15e+6]Epoch 1803/3000:  60%|██████    | 1803/3000 [13:26&lt;10:58,  1.82it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.15e+6]Epoch 1804/3000:  60%|██████    | 1803/3000 [13:26&lt;10:58,  1.82it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.15e+6]Epoch 1804/3000:  60%|██████    | 1804/3000 [13:27&lt;10:32,  1.89it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.15e+6]Epoch 1804/3000:  60%|██████    | 1804/3000 [13:27&lt;10:32,  1.89it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1805/3000:  60%|██████    | 1804/3000 [13:27&lt;10:32,  1.89it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1805/3000:  60%|██████    | 1805/3000 [13:27&lt;09:49,  2.03it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1805/3000:  60%|██████    | 1805/3000 [13:27&lt;09:49,  2.03it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.15e+6]Epoch 1806/3000:  60%|██████    | 1805/3000 [13:27&lt;09:49,  2.03it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.15e+6]Epoch 1806/3000:  60%|██████    | 1806/3000 [13:27&lt;09:20,  2.13it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.15e+6]Epoch 1806/3000:  60%|██████    | 1806/3000 [13:27&lt;09:20,  2.13it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.15e+6]Epoch 1807/3000:  60%|██████    | 1806/3000 [13:27&lt;09:20,  2.13it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.15e+6]Epoch 1807/3000:  60%|██████    | 1807/3000 [13:28&lt;10:23,  1.91it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.15e+6]Epoch 1807/3000:  60%|██████    | 1807/3000 [13:28&lt;10:23,  1.91it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.15e+6]Epoch 1808/3000:  60%|██████    | 1807/3000 [13:28&lt;10:23,  1.91it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.15e+6]Epoch 1808/3000:  60%|██████    | 1808/3000 [13:28&lt;09:59,  1.99it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.15e+6]Epoch 1808/3000:  60%|██████    | 1808/3000 [13:28&lt;09:59,  1.99it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1809/3000:  60%|██████    | 1808/3000 [13:28&lt;09:59,  1.99it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1809/3000:  60%|██████    | 1809/3000 [13:29&lt;09:48,  2.02it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1809/3000:  60%|██████    | 1809/3000 [13:29&lt;09:48,  2.02it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1810/3000:  60%|██████    | 1809/3000 [13:29&lt;09:48,  2.02it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1810/3000:  60%|██████    | 1810/3000 [13:29&lt;08:32,  2.32it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.15e+6]Epoch 1810/3000:  60%|██████    | 1810/3000 [13:29&lt;08:32,  2.32it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.15e+6]Epoch 1811/3000:  60%|██████    | 1810/3000 [13:29&lt;08:32,  2.32it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.15e+6]Epoch 1811/3000:  60%|██████    | 1811/3000 [13:30&lt;08:38,  2.29it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.15e+6]Epoch 1811/3000:  60%|██████    | 1811/3000 [13:30&lt;08:38,  2.29it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1812/3000:  60%|██████    | 1811/3000 [13:30&lt;08:38,  2.29it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1812/3000:  60%|██████    | 1812/3000 [13:30&lt;08:25,  2.35it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1812/3000:  60%|██████    | 1812/3000 [13:30&lt;08:25,  2.35it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1813/3000:  60%|██████    | 1812/3000 [13:30&lt;08:25,  2.35it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1813/3000:  60%|██████    | 1813/3000 [13:31&lt;08:55,  2.22it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.15e+6]Epoch 1813/3000:  60%|██████    | 1813/3000 [13:31&lt;08:55,  2.22it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1814/3000:  60%|██████    | 1813/3000 [13:31&lt;08:55,  2.22it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1814/3000:  60%|██████    | 1814/3000 [13:31&lt;09:49,  2.01it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.15e+6]Epoch 1814/3000:  60%|██████    | 1814/3000 [13:31&lt;09:49,  2.01it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.15e+6]Epoch 1815/3000:  60%|██████    | 1814/3000 [13:31&lt;09:49,  2.01it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.15e+6]Epoch 1815/3000:  60%|██████    | 1815/3000 [13:32&lt;10:09,  1.94it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.15e+6]Epoch 1815/3000:  60%|██████    | 1815/3000 [13:32&lt;10:09,  1.94it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1816/3000:  60%|██████    | 1815/3000 [13:32&lt;10:09,  1.94it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1816/3000:  61%|██████    | 1816/3000 [13:32&lt;09:15,  2.13it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1816/3000:  61%|██████    | 1816/3000 [13:32&lt;09:15,  2.13it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1817/3000:  61%|██████    | 1816/3000 [13:32&lt;09:15,  2.13it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1817/3000:  61%|██████    | 1817/3000 [13:32&lt;07:40,  2.57it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1817/3000:  61%|██████    | 1817/3000 [13:32&lt;07:40,  2.57it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1818/3000:  61%|██████    | 1817/3000 [13:32&lt;07:40,  2.57it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1818/3000:  61%|██████    | 1818/3000 [13:33&lt;07:51,  2.51it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1818/3000:  61%|██████    | 1818/3000 [13:33&lt;07:51,  2.51it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1819/3000:  61%|██████    | 1818/3000 [13:33&lt;07:51,  2.51it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1819/3000:  61%|██████    | 1819/3000 [13:33&lt;08:46,  2.24it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1819/3000:  61%|██████    | 1819/3000 [13:33&lt;08:46,  2.24it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.14e+6]Epoch 1820/3000:  61%|██████    | 1819/3000 [13:33&lt;08:46,  2.24it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.14e+6]Epoch 1820/3000:  61%|██████    | 1820/3000 [13:34&lt;08:17,  2.37it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.14e+6]Epoch 1820/3000:  61%|██████    | 1820/3000 [13:34&lt;08:17,  2.37it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1821/3000:  61%|██████    | 1820/3000 [13:34&lt;08:17,  2.37it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1821/3000:  61%|██████    | 1821/3000 [13:34&lt;07:45,  2.53it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1821/3000:  61%|██████    | 1821/3000 [13:34&lt;07:45,  2.53it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.14e+6]Epoch 1822/3000:  61%|██████    | 1821/3000 [13:34&lt;07:45,  2.53it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.14e+6]Epoch 1822/3000:  61%|██████    | 1822/3000 [13:34&lt;07:20,  2.67it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.14e+6]Epoch 1822/3000:  61%|██████    | 1822/3000 [13:34&lt;07:20,  2.67it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1823/3000:  61%|██████    | 1822/3000 [13:34&lt;07:20,  2.67it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1823/3000:  61%|██████    | 1823/3000 [13:35&lt;07:14,  2.71it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1823/3000:  61%|██████    | 1823/3000 [13:35&lt;07:14,  2.71it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1824/3000:  61%|██████    | 1823/3000 [13:35&lt;07:14,  2.71it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1824/3000:  61%|██████    | 1824/3000 [13:35&lt;07:40,  2.55it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1824/3000:  61%|██████    | 1824/3000 [13:35&lt;07:40,  2.55it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.14e+6]Epoch 1825/3000:  61%|██████    | 1824/3000 [13:35&lt;07:40,  2.55it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.14e+6]Epoch 1825/3000:  61%|██████    | 1825/3000 [13:36&lt;08:23,  2.33it/s, v_num=1, train_loss_step=1.18e+6, train_loss_epoch=1.14e+6]Epoch 1825/3000:  61%|██████    | 1825/3000 [13:36&lt;08:23,  2.33it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1826/3000:  61%|██████    | 1825/3000 [13:36&lt;08:23,  2.33it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1826/3000:  61%|██████    | 1826/3000 [13:36&lt;09:21,  2.09it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1826/3000:  61%|██████    | 1826/3000 [13:36&lt;09:21,  2.09it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1827/3000:  61%|██████    | 1826/3000 [13:36&lt;09:21,  2.09it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1827/3000:  61%|██████    | 1827/3000 [13:37&lt;09:21,  2.09it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1827/3000:  61%|██████    | 1827/3000 [13:37&lt;09:21,  2.09it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1828/3000:  61%|██████    | 1827/3000 [13:37&lt;09:21,  2.09it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1828/3000:  61%|██████    | 1828/3000 [13:37&lt;09:32,  2.05it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1828/3000:  61%|██████    | 1828/3000 [13:37&lt;09:32,  2.05it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.14e+6]Epoch 1829/3000:  61%|██████    | 1828/3000 [13:37&lt;09:32,  2.05it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.14e+6]Epoch 1829/3000:  61%|██████    | 1829/3000 [13:38&lt;09:32,  2.05it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.14e+6]Epoch 1829/3000:  61%|██████    | 1829/3000 [13:38&lt;09:32,  2.05it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1830/3000:  61%|██████    | 1829/3000 [13:38&lt;09:32,  2.05it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1830/3000:  61%|██████    | 1830/3000 [13:38&lt;09:55,  1.97it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1830/3000:  61%|██████    | 1830/3000 [13:38&lt;09:55,  1.97it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1831/3000:  61%|██████    | 1830/3000 [13:38&lt;09:55,  1.97it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1831/3000:  61%|██████    | 1831/3000 [13:39&lt;10:25,  1.87it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1831/3000:  61%|██████    | 1831/3000 [13:39&lt;10:25,  1.87it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1832/3000:  61%|██████    | 1831/3000 [13:39&lt;10:25,  1.87it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1832/3000:  61%|██████    | 1832/3000 [13:39&lt;09:10,  2.12it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1832/3000:  61%|██████    | 1832/3000 [13:39&lt;09:10,  2.12it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1833/3000:  61%|██████    | 1832/3000 [13:39&lt;09:10,  2.12it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1833/3000:  61%|██████    | 1833/3000 [13:40&lt;08:47,  2.21it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1833/3000:  61%|██████    | 1833/3000 [13:40&lt;08:47,  2.21it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1834/3000:  61%|██████    | 1833/3000 [13:40&lt;08:47,  2.21it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1834/3000:  61%|██████    | 1834/3000 [13:40&lt;09:02,  2.15it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1834/3000:  61%|██████    | 1834/3000 [13:40&lt;09:02,  2.15it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1835/3000:  61%|██████    | 1834/3000 [13:40&lt;09:02,  2.15it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1835/3000:  61%|██████    | 1835/3000 [13:40&lt;08:40,  2.24it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1835/3000:  61%|██████    | 1835/3000 [13:40&lt;08:40,  2.24it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.14e+6]Epoch 1836/3000:  61%|██████    | 1835/3000 [13:40&lt;08:40,  2.24it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.14e+6]Epoch 1836/3000:  61%|██████    | 1836/3000 [13:41&lt;08:41,  2.23it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.14e+6]Epoch 1836/3000:  61%|██████    | 1836/3000 [13:41&lt;08:41,  2.23it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1837/3000:  61%|██████    | 1836/3000 [13:41&lt;08:41,  2.23it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1837/3000:  61%|██████    | 1837/3000 [13:41&lt;08:20,  2.32it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1837/3000:  61%|██████    | 1837/3000 [13:41&lt;08:20,  2.32it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1838/3000:  61%|██████    | 1837/3000 [13:41&lt;08:20,  2.32it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1838/3000:  61%|██████▏   | 1838/3000 [13:42&lt;08:25,  2.30it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1838/3000:  61%|██████▏   | 1838/3000 [13:42&lt;08:25,  2.30it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1839/3000:  61%|██████▏   | 1838/3000 [13:42&lt;08:25,  2.30it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1839/3000:  61%|██████▏   | 1839/3000 [13:42&lt;08:59,  2.15it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1839/3000:  61%|██████▏   | 1839/3000 [13:42&lt;08:59,  2.15it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.14e+6]Epoch 1840/3000:  61%|██████▏   | 1839/3000 [13:42&lt;08:59,  2.15it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.14e+6]Epoch 1840/3000:  61%|██████▏   | 1840/3000 [13:43&lt;09:20,  2.07it/s, v_num=1, train_loss_step=1.17e+6, train_loss_epoch=1.14e+6]Epoch 1840/3000:  61%|██████▏   | 1840/3000 [13:43&lt;09:20,  2.07it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1841/3000:  61%|██████▏   | 1840/3000 [13:43&lt;09:20,  2.07it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1841/3000:  61%|██████▏   | 1841/3000 [13:43&lt;10:06,  1.91it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1841/3000:  61%|██████▏   | 1841/3000 [13:43&lt;10:06,  1.91it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1842/3000:  61%|██████▏   | 1841/3000 [13:43&lt;10:06,  1.91it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1842/3000:  61%|██████▏   | 1842/3000 [13:44&lt;10:21,  1.86it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1842/3000:  61%|██████▏   | 1842/3000 [13:44&lt;10:21,  1.86it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1843/3000:  61%|██████▏   | 1842/3000 [13:44&lt;10:21,  1.86it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1843/3000:  61%|██████▏   | 1843/3000 [13:44&lt;09:38,  2.00it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1843/3000:  61%|██████▏   | 1843/3000 [13:44&lt;09:38,  2.00it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1844/3000:  61%|██████▏   | 1843/3000 [13:44&lt;09:38,  2.00it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1844/3000:  61%|██████▏   | 1844/3000 [13:45&lt;08:53,  2.17it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1844/3000:  61%|██████▏   | 1844/3000 [13:45&lt;08:53,  2.17it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.14e+6]Epoch 1845/3000:  61%|██████▏   | 1844/3000 [13:45&lt;08:53,  2.17it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.14e+6]Epoch 1845/3000:  62%|██████▏   | 1845/3000 [13:45&lt;08:32,  2.26it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.14e+6]Epoch 1845/3000:  62%|██████▏   | 1845/3000 [13:45&lt;08:32,  2.26it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1846/3000:  62%|██████▏   | 1845/3000 [13:45&lt;08:32,  2.26it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1846/3000:  62%|██████▏   | 1846/3000 [13:46&lt;07:51,  2.45it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1846/3000:  62%|██████▏   | 1846/3000 [13:46&lt;07:51,  2.45it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1847/3000:  62%|██████▏   | 1846/3000 [13:46&lt;07:51,  2.45it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1847/3000:  62%|██████▏   | 1847/3000 [13:46&lt;07:43,  2.49it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1847/3000:  62%|██████▏   | 1847/3000 [13:46&lt;07:43,  2.49it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1848/3000:  62%|██████▏   | 1847/3000 [13:46&lt;07:43,  2.49it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1848/3000:  62%|██████▏   | 1848/3000 [13:46&lt;08:02,  2.39it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1848/3000:  62%|██████▏   | 1848/3000 [13:46&lt;08:02,  2.39it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1849/3000:  62%|██████▏   | 1848/3000 [13:46&lt;08:02,  2.39it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1849/3000:  62%|██████▏   | 1849/3000 [13:47&lt;07:55,  2.42it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1849/3000:  62%|██████▏   | 1849/3000 [13:47&lt;07:55,  2.42it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.14e+6]Epoch 1850/3000:  62%|██████▏   | 1849/3000 [13:47&lt;07:55,  2.42it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.14e+6]Epoch 1850/3000:  62%|██████▏   | 1850/3000 [13:47&lt;08:28,  2.26it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.14e+6]Epoch 1850/3000:  62%|██████▏   | 1850/3000 [13:47&lt;08:28,  2.26it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1851/3000:  62%|██████▏   | 1850/3000 [13:47&lt;08:28,  2.26it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1851/3000:  62%|██████▏   | 1851/3000 [13:48&lt;08:06,  2.36it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1851/3000:  62%|██████▏   | 1851/3000 [13:48&lt;08:06,  2.36it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1852/3000:  62%|██████▏   | 1851/3000 [13:48&lt;08:06,  2.36it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1852/3000:  62%|██████▏   | 1852/3000 [13:48&lt;08:15,  2.32it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.14e+6]Epoch 1852/3000:  62%|██████▏   | 1852/3000 [13:48&lt;08:15,  2.32it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1853/3000:  62%|██████▏   | 1852/3000 [13:48&lt;08:15,  2.32it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1853/3000:  62%|██████▏   | 1853/3000 [13:49&lt;09:13,  2.07it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1853/3000:  62%|██████▏   | 1853/3000 [13:49&lt;09:13,  2.07it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1854/3000:  62%|██████▏   | 1853/3000 [13:49&lt;09:13,  2.07it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1854/3000:  62%|██████▏   | 1854/3000 [13:49&lt;08:21,  2.29it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1854/3000:  62%|██████▏   | 1854/3000 [13:49&lt;08:21,  2.29it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1855/3000:  62%|██████▏   | 1854/3000 [13:49&lt;08:21,  2.29it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1855/3000:  62%|██████▏   | 1855/3000 [13:50&lt;08:57,  2.13it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1855/3000:  62%|██████▏   | 1855/3000 [13:50&lt;08:57,  2.13it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1856/3000:  62%|██████▏   | 1855/3000 [13:50&lt;08:57,  2.13it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1856/3000:  62%|██████▏   | 1856/3000 [13:50&lt;09:33,  1.99it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1856/3000:  62%|██████▏   | 1856/3000 [13:50&lt;09:33,  1.99it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1857/3000:  62%|██████▏   | 1856/3000 [13:50&lt;09:33,  1.99it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1857/3000:  62%|██████▏   | 1857/3000 [13:51&lt;10:28,  1.82it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1857/3000:  62%|██████▏   | 1857/3000 [13:51&lt;10:28,  1.82it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1858/3000:  62%|██████▏   | 1857/3000 [13:51&lt;10:28,  1.82it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1858/3000:  62%|██████▏   | 1858/3000 [13:51&lt;10:31,  1.81it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1858/3000:  62%|██████▏   | 1858/3000 [13:51&lt;10:31,  1.81it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1859/3000:  62%|██████▏   | 1858/3000 [13:51&lt;10:31,  1.81it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1859/3000:  62%|██████▏   | 1859/3000 [13:52&lt;10:00,  1.90it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1859/3000:  62%|██████▏   | 1859/3000 [13:52&lt;10:00,  1.90it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1860/3000:  62%|██████▏   | 1859/3000 [13:52&lt;10:00,  1.90it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1860/3000:  62%|██████▏   | 1860/3000 [13:52&lt;09:38,  1.97it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.14e+6]Epoch 1860/3000:  62%|██████▏   | 1860/3000 [13:52&lt;09:38,  1.97it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1861/3000:  62%|██████▏   | 1860/3000 [13:52&lt;09:38,  1.97it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1861/3000:  62%|██████▏   | 1861/3000 [13:53&lt;08:20,  2.28it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.14e+6]Epoch 1861/3000:  62%|██████▏   | 1861/3000 [13:53&lt;08:20,  2.28it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1862/3000:  62%|██████▏   | 1861/3000 [13:53&lt;08:20,  2.28it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1862/3000:  62%|██████▏   | 1862/3000 [13:53&lt;08:06,  2.34it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1862/3000:  62%|██████▏   | 1862/3000 [13:53&lt;08:06,  2.34it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1863/3000:  62%|██████▏   | 1862/3000 [13:53&lt;08:06,  2.34it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1863/3000:  62%|██████▏   | 1863/3000 [13:53&lt;07:38,  2.48it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.14e+6]Epoch 1863/3000:  62%|██████▏   | 1863/3000 [13:53&lt;07:38,  2.48it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1864/3000:  62%|██████▏   | 1863/3000 [13:53&lt;07:38,  2.48it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1864/3000:  62%|██████▏   | 1864/3000 [13:54&lt;07:37,  2.48it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.14e+6]Epoch 1864/3000:  62%|██████▏   | 1864/3000 [13:54&lt;07:37,  2.48it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1865/3000:  62%|██████▏   | 1864/3000 [13:54&lt;07:37,  2.48it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1865/3000:  62%|██████▏   | 1865/3000 [13:54&lt;07:50,  2.41it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1865/3000:  62%|██████▏   | 1865/3000 [13:54&lt;07:50,  2.41it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1866/3000:  62%|██████▏   | 1865/3000 [13:54&lt;07:50,  2.41it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1866/3000:  62%|██████▏   | 1866/3000 [13:55&lt;08:49,  2.14it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1866/3000:  62%|██████▏   | 1866/3000 [13:55&lt;08:49,  2.14it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1867/3000:  62%|██████▏   | 1866/3000 [13:55&lt;08:49,  2.14it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1867/3000:  62%|██████▏   | 1867/3000 [13:55&lt;09:10,  2.06it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1867/3000:  62%|██████▏   | 1867/3000 [13:55&lt;09:10,  2.06it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1868/3000:  62%|██████▏   | 1867/3000 [13:55&lt;09:10,  2.06it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1868/3000:  62%|██████▏   | 1868/3000 [13:56&lt;09:24,  2.01it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1868/3000:  62%|██████▏   | 1868/3000 [13:56&lt;09:24,  2.01it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1869/3000:  62%|██████▏   | 1868/3000 [13:56&lt;09:24,  2.01it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1869/3000:  62%|██████▏   | 1869/3000 [13:56&lt;08:56,  2.11it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1869/3000:  62%|██████▏   | 1869/3000 [13:56&lt;08:56,  2.11it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1870/3000:  62%|██████▏   | 1869/3000 [13:56&lt;08:56,  2.11it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1870/3000:  62%|██████▏   | 1870/3000 [13:57&lt;07:57,  2.37it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1870/3000:  62%|██████▏   | 1870/3000 [13:57&lt;07:57,  2.37it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1871/3000:  62%|██████▏   | 1870/3000 [13:57&lt;07:57,  2.37it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1871/3000:  62%|██████▏   | 1871/3000 [13:57&lt;06:31,  2.88it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1871/3000:  62%|██████▏   | 1871/3000 [13:57&lt;06:31,  2.88it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1872/3000:  62%|██████▏   | 1871/3000 [13:57&lt;06:31,  2.88it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1872/3000:  62%|██████▏   | 1872/3000 [13:57&lt;05:55,  3.17it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1872/3000:  62%|██████▏   | 1872/3000 [13:57&lt;05:55,  3.17it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1873/3000:  62%|██████▏   | 1872/3000 [13:57&lt;05:55,  3.17it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1873/3000:  62%|██████▏   | 1873/3000 [13:57&lt;04:51,  3.86it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1873/3000:  62%|██████▏   | 1873/3000 [13:57&lt;04:51,  3.86it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1874/3000:  62%|██████▏   | 1873/3000 [13:57&lt;04:51,  3.86it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1874/3000:  62%|██████▏   | 1874/3000 [13:57&lt;03:59,  4.70it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1874/3000:  62%|██████▏   | 1874/3000 [13:57&lt;03:59,  4.70it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1875/3000:  62%|██████▏   | 1874/3000 [13:57&lt;03:59,  4.70it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1875/3000:  62%|██████▎   | 1875/3000 [13:58&lt;04:52,  3.84it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1875/3000:  62%|██████▎   | 1875/3000 [13:58&lt;04:52,  3.84it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1876/3000:  62%|██████▎   | 1875/3000 [13:58&lt;04:52,  3.84it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1876/3000:  63%|██████▎   | 1876/3000 [13:58&lt;04:53,  3.83it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1876/3000:  63%|██████▎   | 1876/3000 [13:58&lt;04:53,  3.83it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1877/3000:  63%|██████▎   | 1876/3000 [13:58&lt;04:53,  3.83it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1877/3000:  63%|██████▎   | 1877/3000 [13:58&lt;05:45,  3.25it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1877/3000:  63%|██████▎   | 1877/3000 [13:58&lt;05:45,  3.25it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1878/3000:  63%|██████▎   | 1877/3000 [13:58&lt;05:45,  3.25it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1878/3000:  63%|██████▎   | 1878/3000 [13:59&lt;06:51,  2.73it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1878/3000:  63%|██████▎   | 1878/3000 [13:59&lt;06:51,  2.73it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1879/3000:  63%|██████▎   | 1878/3000 [13:59&lt;06:51,  2.73it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1879/3000:  63%|██████▎   | 1879/3000 [13:59&lt;07:53,  2.37it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1879/3000:  63%|██████▎   | 1879/3000 [13:59&lt;07:53,  2.37it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1880/3000:  63%|██████▎   | 1879/3000 [13:59&lt;07:53,  2.37it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1880/3000:  63%|██████▎   | 1880/3000 [14:00&lt;07:14,  2.58it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1880/3000:  63%|██████▎   | 1880/3000 [14:00&lt;07:14,  2.58it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1881/3000:  63%|██████▎   | 1880/3000 [14:00&lt;07:14,  2.58it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1881/3000:  63%|██████▎   | 1881/3000 [14:00&lt;06:32,  2.85it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1881/3000:  63%|██████▎   | 1881/3000 [14:00&lt;06:32,  2.85it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.13e+6] Epoch 1882/3000:  63%|██████▎   | 1881/3000 [14:00&lt;06:32,  2.85it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.13e+6]Epoch 1882/3000:  63%|██████▎   | 1882/3000 [14:00&lt;07:29,  2.49it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.13e+6]Epoch 1882/3000:  63%|██████▎   | 1882/3000 [14:00&lt;07:29,  2.49it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1883/3000:  63%|██████▎   | 1882/3000 [14:00&lt;07:29,  2.49it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1883/3000:  63%|██████▎   | 1883/3000 [14:01&lt;07:57,  2.34it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1883/3000:  63%|██████▎   | 1883/3000 [14:01&lt;07:57,  2.34it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1884/3000:  63%|██████▎   | 1883/3000 [14:01&lt;07:57,  2.34it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1884/3000:  63%|██████▎   | 1884/3000 [14:01&lt;07:48,  2.38it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1884/3000:  63%|██████▎   | 1884/3000 [14:01&lt;07:48,  2.38it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1885/3000:  63%|██████▎   | 1884/3000 [14:01&lt;07:48,  2.38it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1885/3000:  63%|██████▎   | 1885/3000 [14:02&lt;08:51,  2.10it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1885/3000:  63%|██████▎   | 1885/3000 [14:02&lt;08:51,  2.10it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1886/3000:  63%|██████▎   | 1885/3000 [14:02&lt;08:51,  2.10it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1886/3000:  63%|██████▎   | 1886/3000 [14:02&lt;08:18,  2.23it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1886/3000:  63%|██████▎   | 1886/3000 [14:02&lt;08:18,  2.23it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.13e+6] Epoch 1887/3000:  63%|██████▎   | 1886/3000 [14:02&lt;08:18,  2.23it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.13e+6]Epoch 1887/3000:  63%|██████▎   | 1887/3000 [14:03&lt;08:19,  2.23it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.13e+6]Epoch 1887/3000:  63%|██████▎   | 1887/3000 [14:03&lt;08:19,  2.23it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1888/3000:  63%|██████▎   | 1887/3000 [14:03&lt;08:19,  2.23it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1888/3000:  63%|██████▎   | 1888/3000 [14:03&lt;09:00,  2.06it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1888/3000:  63%|██████▎   | 1888/3000 [14:03&lt;09:00,  2.06it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1889/3000:  63%|██████▎   | 1888/3000 [14:03&lt;09:00,  2.06it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1889/3000:  63%|██████▎   | 1889/3000 [14:04&lt;08:48,  2.10it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1889/3000:  63%|██████▎   | 1889/3000 [14:04&lt;08:48,  2.10it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1890/3000:  63%|██████▎   | 1889/3000 [14:04&lt;08:48,  2.10it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1890/3000:  63%|██████▎   | 1890/3000 [14:04&lt;09:08,  2.02it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1890/3000:  63%|██████▎   | 1890/3000 [14:04&lt;09:08,  2.02it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.13e+6]Epoch 1891/3000:  63%|██████▎   | 1890/3000 [14:04&lt;09:08,  2.02it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.13e+6]Epoch 1891/3000:  63%|██████▎   | 1891/3000 [14:05&lt;08:56,  2.07it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.13e+6]Epoch 1891/3000:  63%|██████▎   | 1891/3000 [14:05&lt;08:56,  2.07it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1892/3000:  63%|██████▎   | 1891/3000 [14:05&lt;08:56,  2.07it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1892/3000:  63%|██████▎   | 1892/3000 [14:05&lt;07:52,  2.35it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1892/3000:  63%|██████▎   | 1892/3000 [14:05&lt;07:52,  2.35it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1893/3000:  63%|██████▎   | 1892/3000 [14:05&lt;07:52,  2.35it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1893/3000:  63%|██████▎   | 1893/3000 [14:06&lt;08:38,  2.13it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1893/3000:  63%|██████▎   | 1893/3000 [14:06&lt;08:38,  2.13it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.13e+6]Epoch 1894/3000:  63%|██████▎   | 1893/3000 [14:06&lt;08:38,  2.13it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.13e+6]Epoch 1894/3000:  63%|██████▎   | 1894/3000 [14:06&lt;08:55,  2.06it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.13e+6]Epoch 1894/3000:  63%|██████▎   | 1894/3000 [14:06&lt;08:55,  2.06it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1895/3000:  63%|██████▎   | 1894/3000 [14:06&lt;08:55,  2.06it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1895/3000:  63%|██████▎   | 1895/3000 [14:07&lt;09:13,  2.00it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1895/3000:  63%|██████▎   | 1895/3000 [14:07&lt;09:13,  2.00it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1896/3000:  63%|██████▎   | 1895/3000 [14:07&lt;09:13,  2.00it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1896/3000:  63%|██████▎   | 1896/3000 [14:07&lt;09:23,  1.96it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1896/3000:  63%|██████▎   | 1896/3000 [14:07&lt;09:23,  1.96it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1897/3000:  63%|██████▎   | 1896/3000 [14:07&lt;09:23,  1.96it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1897/3000:  63%|██████▎   | 1897/3000 [14:08&lt;09:31,  1.93it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1897/3000:  63%|██████▎   | 1897/3000 [14:08&lt;09:31,  1.93it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1898/3000:  63%|██████▎   | 1897/3000 [14:08&lt;09:31,  1.93it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1898/3000:  63%|██████▎   | 1898/3000 [14:08&lt;09:00,  2.04it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1898/3000:  63%|██████▎   | 1898/3000 [14:08&lt;09:00,  2.04it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1899/3000:  63%|██████▎   | 1898/3000 [14:08&lt;09:00,  2.04it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1899/3000:  63%|██████▎   | 1899/3000 [14:09&lt;09:38,  1.90it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1899/3000:  63%|██████▎   | 1899/3000 [14:09&lt;09:38,  1.90it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1900/3000:  63%|██████▎   | 1899/3000 [14:09&lt;09:38,  1.90it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1900/3000:  63%|██████▎   | 1900/3000 [14:09&lt;09:16,  1.98it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1900/3000:  63%|██████▎   | 1900/3000 [14:09&lt;09:16,  1.98it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1901/3000:  63%|██████▎   | 1900/3000 [14:09&lt;09:16,  1.98it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1901/3000:  63%|██████▎   | 1901/3000 [14:09&lt;08:00,  2.29it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1901/3000:  63%|██████▎   | 1901/3000 [14:09&lt;08:00,  2.29it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.13e+6]Epoch 1902/3000:  63%|██████▎   | 1901/3000 [14:10&lt;08:00,  2.29it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.13e+6]Epoch 1902/3000:  63%|██████▎   | 1902/3000 [14:10&lt;08:12,  2.23it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.13e+6]Epoch 1902/3000:  63%|██████▎   | 1902/3000 [14:10&lt;08:12,  2.23it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1903/3000:  63%|██████▎   | 1902/3000 [14:10&lt;08:12,  2.23it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1903/3000:  63%|██████▎   | 1903/3000 [14:11&lt;09:00,  2.03it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1903/3000:  63%|██████▎   | 1903/3000 [14:11&lt;09:00,  2.03it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1904/3000:  63%|██████▎   | 1903/3000 [14:11&lt;09:00,  2.03it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1904/3000:  63%|██████▎   | 1904/3000 [14:11&lt;07:37,  2.39it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1904/3000:  63%|██████▎   | 1904/3000 [14:11&lt;07:37,  2.39it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1905/3000:  63%|██████▎   | 1904/3000 [14:11&lt;07:37,  2.39it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1905/3000:  64%|██████▎   | 1905/3000 [14:11&lt;08:18,  2.20it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1905/3000:  64%|██████▎   | 1905/3000 [14:11&lt;08:18,  2.20it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1906/3000:  64%|██████▎   | 1905/3000 [14:11&lt;08:18,  2.20it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1906/3000:  64%|██████▎   | 1906/3000 [14:12&lt;08:06,  2.25it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.13e+6]Epoch 1906/3000:  64%|██████▎   | 1906/3000 [14:12&lt;08:06,  2.25it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1907/3000:  64%|██████▎   | 1906/3000 [14:12&lt;08:06,  2.25it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1907/3000:  64%|██████▎   | 1907/3000 [14:12&lt;07:25,  2.46it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1907/3000:  64%|██████▎   | 1907/3000 [14:12&lt;07:25,  2.46it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.13e+6]Epoch 1908/3000:  64%|██████▎   | 1907/3000 [14:12&lt;07:25,  2.46it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.13e+6]Epoch 1908/3000:  64%|██████▎   | 1908/3000 [14:12&lt;07:03,  2.58it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.13e+6]Epoch 1908/3000:  64%|██████▎   | 1908/3000 [14:12&lt;07:03,  2.58it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1909/3000:  64%|██████▎   | 1908/3000 [14:12&lt;07:03,  2.58it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1909/3000:  64%|██████▎   | 1909/3000 [14:13&lt;06:56,  2.62it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.13e+6]Epoch 1909/3000:  64%|██████▎   | 1909/3000 [14:13&lt;06:56,  2.62it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1910/3000:  64%|██████▎   | 1909/3000 [14:13&lt;06:56,  2.62it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1910/3000:  64%|██████▎   | 1910/3000 [14:13&lt;07:22,  2.46it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1910/3000:  64%|██████▎   | 1910/3000 [14:13&lt;07:22,  2.46it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1911/3000:  64%|██████▎   | 1910/3000 [14:13&lt;07:22,  2.46it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1911/3000:  64%|██████▎   | 1911/3000 [14:14&lt;08:04,  2.25it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1911/3000:  64%|██████▎   | 1911/3000 [14:14&lt;08:04,  2.25it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.13e+6] Epoch 1912/3000:  64%|██████▎   | 1911/3000 [14:14&lt;08:04,  2.25it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.13e+6]Epoch 1912/3000:  64%|██████▎   | 1912/3000 [14:14&lt;07:52,  2.30it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.13e+6]Epoch 1912/3000:  64%|██████▎   | 1912/3000 [14:14&lt;07:52,  2.30it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1913/3000:  64%|██████▎   | 1912/3000 [14:14&lt;07:52,  2.30it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1913/3000:  64%|██████▍   | 1913/3000 [14:15&lt;08:36,  2.10it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1913/3000:  64%|██████▍   | 1913/3000 [14:15&lt;08:36,  2.10it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1914/3000:  64%|██████▍   | 1913/3000 [14:15&lt;08:36,  2.10it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1914/3000:  64%|██████▍   | 1914/3000 [14:15&lt;08:40,  2.08it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1914/3000:  64%|██████▍   | 1914/3000 [14:15&lt;08:40,  2.08it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1915/3000:  64%|██████▍   | 1914/3000 [14:15&lt;08:40,  2.08it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1915/3000:  64%|██████▍   | 1915/3000 [14:16&lt;09:39,  1.87it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1915/3000:  64%|██████▍   | 1915/3000 [14:16&lt;09:39,  1.87it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1916/3000:  64%|██████▍   | 1915/3000 [14:16&lt;09:39,  1.87it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1916/3000:  64%|██████▍   | 1916/3000 [14:16&lt;09:17,  1.94it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.13e+6]Epoch 1916/3000:  64%|██████▍   | 1916/3000 [14:16&lt;09:17,  1.94it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1917/3000:  64%|██████▍   | 1916/3000 [14:16&lt;09:17,  1.94it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1917/3000:  64%|██████▍   | 1917/3000 [14:17&lt;09:19,  1.93it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.13e+6]Epoch 1917/3000:  64%|██████▍   | 1917/3000 [14:17&lt;09:19,  1.93it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1918/3000:  64%|██████▍   | 1917/3000 [14:17&lt;09:19,  1.93it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1918/3000:  64%|██████▍   | 1918/3000 [14:17&lt;09:08,  1.97it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1918/3000:  64%|██████▍   | 1918/3000 [14:17&lt;09:08,  1.97it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1919/3000:  64%|██████▍   | 1918/3000 [14:17&lt;09:08,  1.97it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1919/3000:  64%|██████▍   | 1919/3000 [14:18&lt;09:05,  1.98it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1919/3000:  64%|██████▍   | 1919/3000 [14:18&lt;09:05,  1.98it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1920/3000:  64%|██████▍   | 1919/3000 [14:18&lt;09:05,  1.98it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1920/3000:  64%|██████▍   | 1920/3000 [14:18&lt;08:37,  2.09it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1920/3000:  64%|██████▍   | 1920/3000 [14:18&lt;08:37,  2.09it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1921/3000:  64%|██████▍   | 1920/3000 [14:18&lt;08:37,  2.09it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1921/3000:  64%|██████▍   | 1921/3000 [14:19&lt;08:14,  2.18it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1921/3000:  64%|██████▍   | 1921/3000 [14:19&lt;08:14,  2.18it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1922/3000:  64%|██████▍   | 1921/3000 [14:19&lt;08:14,  2.18it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1922/3000:  64%|██████▍   | 1922/3000 [14:19&lt;08:14,  2.18it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1922/3000:  64%|██████▍   | 1922/3000 [14:19&lt;08:14,  2.18it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1923/3000:  64%|██████▍   | 1922/3000 [14:19&lt;08:14,  2.18it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1923/3000:  64%|██████▍   | 1923/3000 [14:20&lt;07:50,  2.29it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1923/3000:  64%|██████▍   | 1923/3000 [14:20&lt;07:50,  2.29it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.12e+6] Epoch 1924/3000:  64%|██████▍   | 1923/3000 [14:20&lt;07:50,  2.29it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.12e+6]Epoch 1924/3000:  64%|██████▍   | 1924/3000 [14:20&lt;08:51,  2.03it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.12e+6]Epoch 1924/3000:  64%|██████▍   | 1924/3000 [14:20&lt;08:51,  2.03it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1925/3000:  64%|██████▍   | 1924/3000 [14:20&lt;08:51,  2.03it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1925/3000:  64%|██████▍   | 1925/3000 [14:21&lt;08:55,  2.01it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1925/3000:  64%|██████▍   | 1925/3000 [14:21&lt;08:55,  2.01it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1926/3000:  64%|██████▍   | 1925/3000 [14:21&lt;08:55,  2.01it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1926/3000:  64%|██████▍   | 1926/3000 [14:21&lt;09:01,  1.98it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1926/3000:  64%|██████▍   | 1926/3000 [14:21&lt;09:01,  1.98it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1927/3000:  64%|██████▍   | 1926/3000 [14:21&lt;09:01,  1.98it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1927/3000:  64%|██████▍   | 1927/3000 [14:22&lt;09:18,  1.92it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1927/3000:  64%|██████▍   | 1927/3000 [14:22&lt;09:18,  1.92it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1928/3000:  64%|██████▍   | 1927/3000 [14:22&lt;09:18,  1.92it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1928/3000:  64%|██████▍   | 1928/3000 [14:22&lt;09:42,  1.84it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1928/3000:  64%|██████▍   | 1928/3000 [14:22&lt;09:42,  1.84it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.12e+6]Epoch 1929/3000:  64%|██████▍   | 1928/3000 [14:22&lt;09:42,  1.84it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.12e+6]Epoch 1929/3000:  64%|██████▍   | 1929/3000 [14:23&lt;09:01,  1.98it/s, v_num=1, train_loss_step=1.16e+6, train_loss_epoch=1.12e+6]Epoch 1929/3000:  64%|██████▍   | 1929/3000 [14:23&lt;09:01,  1.98it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1930/3000:  64%|██████▍   | 1929/3000 [14:23&lt;09:01,  1.98it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1930/3000:  64%|██████▍   | 1930/3000 [14:23&lt;09:20,  1.91it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1930/3000:  64%|██████▍   | 1930/3000 [14:23&lt;09:20,  1.91it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1931/3000:  64%|██████▍   | 1930/3000 [14:23&lt;09:20,  1.91it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1931/3000:  64%|██████▍   | 1931/3000 [14:24&lt;07:42,  2.31it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1931/3000:  64%|██████▍   | 1931/3000 [14:24&lt;07:42,  2.31it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1932/3000:  64%|██████▍   | 1931/3000 [14:24&lt;07:42,  2.31it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1932/3000:  64%|██████▍   | 1932/3000 [14:24&lt;08:10,  2.18it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1932/3000:  64%|██████▍   | 1932/3000 [14:24&lt;08:10,  2.18it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1933/3000:  64%|██████▍   | 1932/3000 [14:24&lt;08:10,  2.18it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1933/3000:  64%|██████▍   | 1933/3000 [14:24&lt;07:43,  2.30it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1933/3000:  64%|██████▍   | 1933/3000 [14:24&lt;07:43,  2.30it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1934/3000:  64%|██████▍   | 1933/3000 [14:25&lt;07:43,  2.30it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1934/3000:  64%|██████▍   | 1934/3000 [14:25&lt;08:08,  2.18it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1934/3000:  64%|██████▍   | 1934/3000 [14:25&lt;08:08,  2.18it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1935/3000:  64%|██████▍   | 1934/3000 [14:25&lt;08:08,  2.18it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1935/3000:  64%|██████▍   | 1935/3000 [14:26&lt;09:03,  1.96it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1935/3000:  64%|██████▍   | 1935/3000 [14:26&lt;09:03,  1.96it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1936/3000:  64%|██████▍   | 1935/3000 [14:26&lt;09:03,  1.96it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1936/3000:  65%|██████▍   | 1936/3000 [14:26&lt;09:07,  1.94it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1936/3000:  65%|██████▍   | 1936/3000 [14:26&lt;09:07,  1.94it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1937/3000:  65%|██████▍   | 1936/3000 [14:26&lt;09:07,  1.94it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1937/3000:  65%|██████▍   | 1937/3000 [14:27&lt;09:32,  1.86it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1937/3000:  65%|██████▍   | 1937/3000 [14:27&lt;09:32,  1.86it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1938/3000:  65%|██████▍   | 1937/3000 [14:27&lt;09:32,  1.86it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1938/3000:  65%|██████▍   | 1938/3000 [14:27&lt;09:52,  1.79it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1938/3000:  65%|██████▍   | 1938/3000 [14:27&lt;09:52,  1.79it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1939/3000:  65%|██████▍   | 1938/3000 [14:27&lt;09:52,  1.79it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1939/3000:  65%|██████▍   | 1939/3000 [14:28&lt;09:05,  1.94it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1939/3000:  65%|██████▍   | 1939/3000 [14:28&lt;09:05,  1.94it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.12e+6]Epoch 1940/3000:  65%|██████▍   | 1939/3000 [14:28&lt;09:05,  1.94it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.12e+6]Epoch 1940/3000:  65%|██████▍   | 1940/3000 [14:28&lt;08:56,  1.98it/s, v_num=1, train_loss_step=1.15e+6, train_loss_epoch=1.12e+6]Epoch 1940/3000:  65%|██████▍   | 1940/3000 [14:28&lt;08:56,  1.98it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1941/3000:  65%|██████▍   | 1940/3000 [14:28&lt;08:56,  1.98it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1941/3000:  65%|██████▍   | 1941/3000 [14:29&lt;08:19,  2.12it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1941/3000:  65%|██████▍   | 1941/3000 [14:29&lt;08:19,  2.12it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1942/3000:  65%|██████▍   | 1941/3000 [14:29&lt;08:19,  2.12it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1942/3000:  65%|██████▍   | 1942/3000 [14:29&lt;08:22,  2.10it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1942/3000:  65%|██████▍   | 1942/3000 [14:29&lt;08:22,  2.10it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1943/3000:  65%|██████▍   | 1942/3000 [14:29&lt;08:22,  2.10it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1943/3000:  65%|██████▍   | 1943/3000 [14:30&lt;08:41,  2.03it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1943/3000:  65%|██████▍   | 1943/3000 [14:30&lt;08:41,  2.03it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1944/3000:  65%|██████▍   | 1943/3000 [14:30&lt;08:41,  2.03it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1944/3000:  65%|██████▍   | 1944/3000 [14:30&lt;09:21,  1.88it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1944/3000:  65%|██████▍   | 1944/3000 [14:30&lt;09:21,  1.88it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1945/3000:  65%|██████▍   | 1944/3000 [14:30&lt;09:21,  1.88it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1945/3000:  65%|██████▍   | 1945/3000 [14:31&lt;08:58,  1.96it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1945/3000:  65%|██████▍   | 1945/3000 [14:31&lt;08:58,  1.96it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1946/3000:  65%|██████▍   | 1945/3000 [14:31&lt;08:58,  1.96it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1946/3000:  65%|██████▍   | 1946/3000 [14:31&lt;08:43,  2.02it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1946/3000:  65%|██████▍   | 1946/3000 [14:31&lt;08:43,  2.02it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1947/3000:  65%|██████▍   | 1946/3000 [14:31&lt;08:43,  2.02it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1947/3000:  65%|██████▍   | 1947/3000 [14:32&lt;09:23,  1.87it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1947/3000:  65%|██████▍   | 1947/3000 [14:32&lt;09:23,  1.87it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1948/3000:  65%|██████▍   | 1947/3000 [14:32&lt;09:23,  1.87it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1948/3000:  65%|██████▍   | 1948/3000 [14:32&lt;08:47,  1.99it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1948/3000:  65%|██████▍   | 1948/3000 [14:32&lt;08:47,  1.99it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1949/3000:  65%|██████▍   | 1948/3000 [14:32&lt;08:47,  1.99it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1949/3000:  65%|██████▍   | 1949/3000 [14:33&lt;08:33,  2.05it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1949/3000:  65%|██████▍   | 1949/3000 [14:33&lt;08:33,  2.05it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1950/3000:  65%|██████▍   | 1949/3000 [14:33&lt;08:33,  2.05it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1950/3000:  65%|██████▌   | 1950/3000 [14:33&lt;08:26,  2.07it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1950/3000:  65%|██████▌   | 1950/3000 [14:33&lt;08:26,  2.07it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1951/3000:  65%|██████▌   | 1950/3000 [14:33&lt;08:26,  2.07it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1951/3000:  65%|██████▌   | 1951/3000 [14:34&lt;08:22,  2.09it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1951/3000:  65%|██████▌   | 1951/3000 [14:34&lt;08:22,  2.09it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1952/3000:  65%|██████▌   | 1951/3000 [14:34&lt;08:22,  2.09it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1952/3000:  65%|██████▌   | 1952/3000 [14:34&lt;07:58,  2.19it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1952/3000:  65%|██████▌   | 1952/3000 [14:34&lt;07:58,  2.19it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1953/3000:  65%|██████▌   | 1952/3000 [14:34&lt;07:58,  2.19it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1953/3000:  65%|██████▌   | 1953/3000 [14:35&lt;08:10,  2.14it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1953/3000:  65%|██████▌   | 1953/3000 [14:35&lt;08:10,  2.14it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1954/3000:  65%|██████▌   | 1953/3000 [14:35&lt;08:10,  2.14it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1954/3000:  65%|██████▌   | 1954/3000 [14:35&lt;08:31,  2.04it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1954/3000:  65%|██████▌   | 1954/3000 [14:35&lt;08:31,  2.04it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1955/3000:  65%|██████▌   | 1954/3000 [14:35&lt;08:31,  2.04it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1955/3000:  65%|██████▌   | 1955/3000 [14:36&lt;09:00,  1.93it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1955/3000:  65%|██████▌   | 1955/3000 [14:36&lt;09:00,  1.93it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.12e+6] Epoch 1956/3000:  65%|██████▌   | 1955/3000 [14:36&lt;09:00,  1.93it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.12e+6]Epoch 1956/3000:  65%|██████▌   | 1956/3000 [14:36&lt;09:15,  1.88it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.12e+6]Epoch 1956/3000:  65%|██████▌   | 1956/3000 [14:36&lt;09:15,  1.88it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.12e+6]Epoch 1957/3000:  65%|██████▌   | 1956/3000 [14:36&lt;09:15,  1.88it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.12e+6]Epoch 1957/3000:  65%|██████▌   | 1957/3000 [14:37&lt;09:01,  1.93it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.12e+6]Epoch 1957/3000:  65%|██████▌   | 1957/3000 [14:37&lt;09:01,  1.93it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1958/3000:  65%|██████▌   | 1957/3000 [14:37&lt;09:01,  1.93it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1958/3000:  65%|██████▌   | 1958/3000 [14:37&lt;07:57,  2.18it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1958/3000:  65%|██████▌   | 1958/3000 [14:37&lt;07:57,  2.18it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1959/3000:  65%|██████▌   | 1958/3000 [14:37&lt;07:57,  2.18it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1959/3000:  65%|██████▌   | 1959/3000 [14:38&lt;08:24,  2.06it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1959/3000:  65%|██████▌   | 1959/3000 [14:38&lt;08:24,  2.06it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1960/3000:  65%|██████▌   | 1959/3000 [14:38&lt;08:24,  2.06it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1960/3000:  65%|██████▌   | 1960/3000 [14:38&lt;07:28,  2.32it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1960/3000:  65%|██████▌   | 1960/3000 [14:38&lt;07:28,  2.32it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1961/3000:  65%|██████▌   | 1960/3000 [14:38&lt;07:28,  2.32it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1961/3000:  65%|██████▌   | 1961/3000 [14:38&lt;07:54,  2.19it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.12e+6]Epoch 1961/3000:  65%|██████▌   | 1961/3000 [14:38&lt;07:54,  2.19it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.12e+6] Epoch 1962/3000:  65%|██████▌   | 1961/3000 [14:38&lt;07:54,  2.19it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.12e+6]Epoch 1962/3000:  65%|██████▌   | 1962/3000 [14:39&lt;08:13,  2.10it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.12e+6]Epoch 1962/3000:  65%|██████▌   | 1962/3000 [14:39&lt;08:13,  2.10it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1963/3000:  65%|██████▌   | 1962/3000 [14:39&lt;08:13,  2.10it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1963/3000:  65%|██████▌   | 1963/3000 [14:39&lt;08:37,  2.00it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1963/3000:  65%|██████▌   | 1963/3000 [14:39&lt;08:37,  2.00it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1964/3000:  65%|██████▌   | 1963/3000 [14:40&lt;08:37,  2.00it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1964/3000:  65%|██████▌   | 1964/3000 [14:40&lt;08:44,  1.97it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1964/3000:  65%|██████▌   | 1964/3000 [14:40&lt;08:44,  1.97it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.12e+6]Epoch 1965/3000:  65%|██████▌   | 1964/3000 [14:40&lt;08:44,  1.97it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.12e+6]Epoch 1965/3000:  66%|██████▌   | 1965/3000 [14:40&lt;08:08,  2.12it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.12e+6]Epoch 1965/3000:  66%|██████▌   | 1965/3000 [14:40&lt;08:08,  2.12it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1966/3000:  66%|██████▌   | 1965/3000 [14:40&lt;08:08,  2.12it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1966/3000:  66%|██████▌   | 1966/3000 [14:41&lt;08:19,  2.07it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1966/3000:  66%|██████▌   | 1966/3000 [14:41&lt;08:19,  2.07it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1967/3000:  66%|██████▌   | 1966/3000 [14:41&lt;08:19,  2.07it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1967/3000:  66%|██████▌   | 1967/3000 [14:41&lt;08:45,  1.97it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1967/3000:  66%|██████▌   | 1967/3000 [14:41&lt;08:45,  1.97it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1968/3000:  66%|██████▌   | 1967/3000 [14:41&lt;08:45,  1.97it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1968/3000:  66%|██████▌   | 1968/3000 [14:42&lt;09:12,  1.87it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1968/3000:  66%|██████▌   | 1968/3000 [14:42&lt;09:12,  1.87it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1969/3000:  66%|██████▌   | 1968/3000 [14:42&lt;09:12,  1.87it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1969/3000:  66%|██████▌   | 1969/3000 [14:43&lt;09:10,  1.87it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1969/3000:  66%|██████▌   | 1969/3000 [14:43&lt;09:10,  1.87it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1970/3000:  66%|██████▌   | 1969/3000 [14:43&lt;09:10,  1.87it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1970/3000:  66%|██████▌   | 1970/3000 [14:43&lt;08:35,  2.00it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.12e+6]Epoch 1970/3000:  66%|██████▌   | 1970/3000 [14:43&lt;08:35,  2.00it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.12e+6]Epoch 1971/3000:  66%|██████▌   | 1970/3000 [14:43&lt;08:35,  2.00it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.12e+6]Epoch 1971/3000:  66%|██████▌   | 1971/3000 [14:44&lt;08:24,  2.04it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.12e+6]Epoch 1971/3000:  66%|██████▌   | 1971/3000 [14:44&lt;08:24,  2.04it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1972/3000:  66%|██████▌   | 1971/3000 [14:44&lt;08:24,  2.04it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1972/3000:  66%|██████▌   | 1972/3000 [14:44&lt;08:39,  1.98it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.12e+6]Epoch 1972/3000:  66%|██████▌   | 1972/3000 [14:44&lt;08:39,  1.98it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1973/3000:  66%|██████▌   | 1972/3000 [14:44&lt;08:39,  1.98it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1973/3000:  66%|██████▌   | 1973/3000 [14:44&lt;07:52,  2.17it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1973/3000:  66%|██████▌   | 1973/3000 [14:44&lt;07:52,  2.17it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1974/3000:  66%|██████▌   | 1973/3000 [14:44&lt;07:52,  2.17it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1974/3000:  66%|██████▌   | 1974/3000 [14:45&lt;08:01,  2.13it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.12e+6]Epoch 1974/3000:  66%|██████▌   | 1974/3000 [14:45&lt;08:01,  2.13it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 1975/3000:  66%|██████▌   | 1974/3000 [14:45&lt;08:01,  2.13it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 1975/3000:  66%|██████▌   | 1975/3000 [14:45&lt;08:15,  2.07it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 1975/3000:  66%|██████▌   | 1975/3000 [14:45&lt;08:15,  2.07it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 1976/3000:  66%|██████▌   | 1975/3000 [14:45&lt;08:15,  2.07it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 1976/3000:  66%|██████▌   | 1976/3000 [14:46&lt;07:00,  2.44it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 1976/3000:  66%|██████▌   | 1976/3000 [14:46&lt;07:00,  2.44it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.11e+6]Epoch 1977/3000:  66%|██████▌   | 1976/3000 [14:46&lt;07:00,  2.44it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.11e+6]Epoch 1977/3000:  66%|██████▌   | 1977/3000 [14:46&lt;06:33,  2.60it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.11e+6]Epoch 1977/3000:  66%|██████▌   | 1977/3000 [14:46&lt;06:33,  2.60it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1978/3000:  66%|██████▌   | 1977/3000 [14:46&lt;06:33,  2.60it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1978/3000:  66%|██████▌   | 1978/3000 [14:46&lt;06:57,  2.45it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1978/3000:  66%|██████▌   | 1978/3000 [14:46&lt;06:57,  2.45it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 1979/3000:  66%|██████▌   | 1978/3000 [14:46&lt;06:57,  2.45it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 1979/3000:  66%|██████▌   | 1979/3000 [14:47&lt;07:41,  2.21it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 1979/3000:  66%|██████▌   | 1979/3000 [14:47&lt;07:41,  2.21it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1980/3000:  66%|██████▌   | 1979/3000 [14:47&lt;07:41,  2.21it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1980/3000:  66%|██████▌   | 1980/3000 [14:48&lt;08:14,  2.06it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1980/3000:  66%|██████▌   | 1980/3000 [14:48&lt;08:14,  2.06it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1981/3000:  66%|██████▌   | 1980/3000 [14:48&lt;08:14,  2.06it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1981/3000:  66%|██████▌   | 1981/3000 [14:48&lt;08:29,  2.00it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1981/3000:  66%|██████▌   | 1981/3000 [14:48&lt;08:29,  2.00it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1982/3000:  66%|██████▌   | 1981/3000 [14:48&lt;08:29,  2.00it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1982/3000:  66%|██████▌   | 1982/3000 [14:49&lt;09:19,  1.82it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1982/3000:  66%|██████▌   | 1982/3000 [14:49&lt;09:19,  1.82it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 1983/3000:  66%|██████▌   | 1982/3000 [14:49&lt;09:19,  1.82it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 1983/3000:  66%|██████▌   | 1983/3000 [14:49&lt;09:26,  1.79it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 1983/3000:  66%|██████▌   | 1983/3000 [14:49&lt;09:26,  1.79it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1984/3000:  66%|██████▌   | 1983/3000 [14:49&lt;09:26,  1.79it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1984/3000:  66%|██████▌   | 1984/3000 [14:50&lt;08:47,  1.93it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1984/3000:  66%|██████▌   | 1984/3000 [14:50&lt;08:47,  1.93it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6] Epoch 1985/3000:  66%|██████▌   | 1984/3000 [14:50&lt;08:47,  1.93it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 1985/3000:  66%|██████▌   | 1985/3000 [14:50&lt;08:08,  2.08it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 1985/3000:  66%|██████▌   | 1985/3000 [14:50&lt;08:08,  2.08it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1986/3000:  66%|██████▌   | 1985/3000 [14:50&lt;08:08,  2.08it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1986/3000:  66%|██████▌   | 1986/3000 [14:51&lt;07:29,  2.25it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1986/3000:  66%|██████▌   | 1986/3000 [14:51&lt;07:29,  2.25it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.11e+6]Epoch 1987/3000:  66%|██████▌   | 1986/3000 [14:51&lt;07:29,  2.25it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.11e+6]Epoch 1987/3000:  66%|██████▌   | 1987/3000 [14:51&lt;07:29,  2.26it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.11e+6]Epoch 1987/3000:  66%|██████▌   | 1987/3000 [14:51&lt;07:29,  2.26it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6] Epoch 1988/3000:  66%|██████▌   | 1987/3000 [14:51&lt;07:29,  2.26it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 1988/3000:  66%|██████▋   | 1988/3000 [14:51&lt;07:32,  2.24it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 1988/3000:  66%|██████▋   | 1988/3000 [14:51&lt;07:32,  2.24it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1989/3000:  66%|██████▋   | 1988/3000 [14:51&lt;07:32,  2.24it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1989/3000:  66%|██████▋   | 1989/3000 [14:52&lt;06:51,  2.46it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1989/3000:  66%|██████▋   | 1989/3000 [14:52&lt;06:51,  2.46it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 1990/3000:  66%|██████▋   | 1989/3000 [14:52&lt;06:51,  2.46it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 1990/3000:  66%|██████▋   | 1990/3000 [14:52&lt;07:14,  2.32it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 1990/3000:  66%|██████▋   | 1990/3000 [14:52&lt;07:14,  2.32it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.11e+6]Epoch 1991/3000:  66%|██████▋   | 1990/3000 [14:52&lt;07:14,  2.32it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.11e+6]Epoch 1991/3000:  66%|██████▋   | 1991/3000 [14:53&lt;07:35,  2.21it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.11e+6]Epoch 1991/3000:  66%|██████▋   | 1991/3000 [14:53&lt;07:35,  2.21it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1992/3000:  66%|██████▋   | 1991/3000 [14:53&lt;07:35,  2.21it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1992/3000:  66%|██████▋   | 1992/3000 [14:53&lt;07:30,  2.24it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1992/3000:  66%|██████▋   | 1992/3000 [14:53&lt;07:30,  2.24it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1993/3000:  66%|██████▋   | 1992/3000 [14:53&lt;07:30,  2.24it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1993/3000:  66%|██████▋   | 1993/3000 [14:54&lt;07:28,  2.24it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1993/3000:  66%|██████▋   | 1993/3000 [14:54&lt;07:28,  2.24it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1994/3000:  66%|██████▋   | 1993/3000 [14:54&lt;07:28,  2.24it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1994/3000:  66%|██████▋   | 1994/3000 [14:54&lt;07:42,  2.18it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1994/3000:  66%|██████▋   | 1994/3000 [14:54&lt;07:42,  2.18it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1995/3000:  66%|██████▋   | 1994/3000 [14:54&lt;07:42,  2.18it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1995/3000:  66%|██████▋   | 1995/3000 [14:55&lt;08:29,  1.97it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1995/3000:  66%|██████▋   | 1995/3000 [14:55&lt;08:29,  1.97it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 1996/3000:  66%|██████▋   | 1995/3000 [14:55&lt;08:29,  1.97it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 1996/3000:  67%|██████▋   | 1996/3000 [14:55&lt;08:01,  2.09it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 1996/3000:  67%|██████▋   | 1996/3000 [14:55&lt;08:01,  2.09it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6] Epoch 1997/3000:  67%|██████▋   | 1996/3000 [14:55&lt;08:01,  2.09it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 1997/3000:  67%|██████▋   | 1997/3000 [14:56&lt;07:42,  2.17it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 1997/3000:  67%|██████▋   | 1997/3000 [14:56&lt;07:42,  2.17it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1998/3000:  67%|██████▋   | 1997/3000 [14:56&lt;07:42,  2.17it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1998/3000:  67%|██████▋   | 1998/3000 [14:56&lt;07:31,  2.22it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1998/3000:  67%|██████▋   | 1998/3000 [14:56&lt;07:31,  2.22it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1999/3000:  67%|██████▋   | 1998/3000 [14:56&lt;07:31,  2.22it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1999/3000:  67%|██████▋   | 1999/3000 [14:56&lt;07:45,  2.15it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 1999/3000:  67%|██████▋   | 1999/3000 [14:56&lt;07:45,  2.15it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6] Epoch 2000/3000:  67%|██████▋   | 1999/3000 [14:56&lt;07:45,  2.15it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2000/3000:  67%|██████▋   | 2000/3000 [14:57&lt;08:05,  2.06it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2000/3000:  67%|██████▋   | 2000/3000 [14:57&lt;08:05,  2.06it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.11e+6]Epoch 2001/3000:  67%|██████▋   | 2000/3000 [14:57&lt;08:05,  2.06it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.11e+6]Epoch 2001/3000:  67%|██████▋   | 2001/3000 [14:57&lt;07:29,  2.22it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.11e+6]Epoch 2001/3000:  67%|██████▋   | 2001/3000 [14:57&lt;07:29,  2.22it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2002/3000:  67%|██████▋   | 2001/3000 [14:57&lt;07:29,  2.22it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2002/3000:  67%|██████▋   | 2002/3000 [14:58&lt;07:21,  2.26it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2002/3000:  67%|██████▋   | 2002/3000 [14:58&lt;07:21,  2.26it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6] Epoch 2003/3000:  67%|██████▋   | 2002/3000 [14:58&lt;07:21,  2.26it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2003/3000:  67%|██████▋   | 2003/3000 [14:58&lt;07:46,  2.14it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2003/3000:  67%|██████▋   | 2003/3000 [14:58&lt;07:46,  2.14it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2004/3000:  67%|██████▋   | 2003/3000 [14:58&lt;07:46,  2.14it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2004/3000:  67%|██████▋   | 2004/3000 [14:59&lt;08:14,  2.01it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2004/3000:  67%|██████▋   | 2004/3000 [14:59&lt;08:14,  2.01it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6] Epoch 2005/3000:  67%|██████▋   | 2004/3000 [14:59&lt;08:14,  2.01it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2005/3000:  67%|██████▋   | 2005/3000 [14:59&lt;08:06,  2.05it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2005/3000:  67%|██████▋   | 2005/3000 [14:59&lt;08:06,  2.05it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 2006/3000:  67%|██████▋   | 2005/3000 [14:59&lt;08:06,  2.05it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 2006/3000:  67%|██████▋   | 2006/3000 [15:00&lt;08:32,  1.94it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 2006/3000:  67%|██████▋   | 2006/3000 [15:00&lt;08:32,  1.94it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6] Epoch 2007/3000:  67%|██████▋   | 2006/3000 [15:00&lt;08:32,  1.94it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2007/3000:  67%|██████▋   | 2007/3000 [15:00&lt;07:43,  2.14it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2007/3000:  67%|██████▋   | 2007/3000 [15:00&lt;07:43,  2.14it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2008/3000:  67%|██████▋   | 2007/3000 [15:00&lt;07:43,  2.14it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2008/3000:  67%|██████▋   | 2008/3000 [15:01&lt;08:19,  1.99it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2008/3000:  67%|██████▋   | 2008/3000 [15:01&lt;08:19,  1.99it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 2009/3000:  67%|██████▋   | 2008/3000 [15:01&lt;08:19,  1.99it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 2009/3000:  67%|██████▋   | 2009/3000 [15:01&lt;08:37,  1.92it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.11e+6]Epoch 2009/3000:  67%|██████▋   | 2009/3000 [15:01&lt;08:37,  1.92it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.11e+6]Epoch 2010/3000:  67%|██████▋   | 2009/3000 [15:01&lt;08:37,  1.92it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.11e+6]Epoch 2010/3000:  67%|██████▋   | 2010/3000 [15:02&lt;08:31,  1.93it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.11e+6]Epoch 2010/3000:  67%|██████▋   | 2010/3000 [15:02&lt;08:31,  1.93it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2011/3000:  67%|██████▋   | 2010/3000 [15:02&lt;08:31,  1.93it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2011/3000:  67%|██████▋   | 2011/3000 [15:02&lt;08:04,  2.04it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2011/3000:  67%|██████▋   | 2011/3000 [15:02&lt;08:04,  2.04it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2012/3000:  67%|██████▋   | 2011/3000 [15:02&lt;08:04,  2.04it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2012/3000:  67%|██████▋   | 2012/3000 [15:03&lt;07:27,  2.21it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2012/3000:  67%|██████▋   | 2012/3000 [15:03&lt;07:27,  2.21it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2013/3000:  67%|██████▋   | 2012/3000 [15:03&lt;07:27,  2.21it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2013/3000:  67%|██████▋   | 2013/3000 [15:03&lt;07:51,  2.09it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2013/3000:  67%|██████▋   | 2013/3000 [15:03&lt;07:51,  2.09it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6] Epoch 2014/3000:  67%|██████▋   | 2013/3000 [15:03&lt;07:51,  2.09it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2014/3000:  67%|██████▋   | 2014/3000 [15:04&lt;07:31,  2.18it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2014/3000:  67%|██████▋   | 2014/3000 [15:04&lt;07:31,  2.18it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.11e+6]Epoch 2015/3000:  67%|██████▋   | 2014/3000 [15:04&lt;07:31,  2.18it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.11e+6]Epoch 2015/3000:  67%|██████▋   | 2015/3000 [15:04&lt;07:29,  2.19it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.11e+6]Epoch 2015/3000:  67%|██████▋   | 2015/3000 [15:04&lt;07:29,  2.19it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2016/3000:  67%|██████▋   | 2015/3000 [15:04&lt;07:29,  2.19it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2016/3000:  67%|██████▋   | 2016/3000 [15:05&lt;07:54,  2.07it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2016/3000:  67%|██████▋   | 2016/3000 [15:05&lt;07:54,  2.07it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2017/3000:  67%|██████▋   | 2016/3000 [15:05&lt;07:54,  2.07it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2017/3000:  67%|██████▋   | 2017/3000 [15:05&lt;08:47,  1.86it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2017/3000:  67%|██████▋   | 2017/3000 [15:05&lt;08:47,  1.86it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6] Epoch 2018/3000:  67%|██████▋   | 2017/3000 [15:05&lt;08:47,  1.86it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2018/3000:  67%|██████▋   | 2018/3000 [15:06&lt;08:35,  1.90it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2018/3000:  67%|██████▋   | 2018/3000 [15:06&lt;08:35,  1.90it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2019/3000:  67%|██████▋   | 2018/3000 [15:06&lt;08:35,  1.90it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2019/3000:  67%|██████▋   | 2019/3000 [15:06&lt;07:22,  2.22it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2019/3000:  67%|██████▋   | 2019/3000 [15:06&lt;07:22,  2.22it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2020/3000:  67%|██████▋   | 2019/3000 [15:06&lt;07:22,  2.22it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2020/3000:  67%|██████▋   | 2020/3000 [15:06&lt;07:06,  2.30it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2020/3000:  67%|██████▋   | 2020/3000 [15:07&lt;07:06,  2.30it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.11e+6]Epoch 2021/3000:  67%|██████▋   | 2020/3000 [15:07&lt;07:06,  2.30it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.11e+6]Epoch 2021/3000:  67%|██████▋   | 2021/3000 [15:07&lt;06:40,  2.44it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.11e+6]Epoch 2021/3000:  67%|██████▋   | 2021/3000 [15:07&lt;06:40,  2.44it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2022/3000:  67%|██████▋   | 2021/3000 [15:07&lt;06:40,  2.44it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2022/3000:  67%|██████▋   | 2022/3000 [15:07&lt;05:50,  2.79it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2022/3000:  67%|██████▋   | 2022/3000 [15:07&lt;05:50,  2.79it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2023/3000:  67%|██████▋   | 2022/3000 [15:07&lt;05:50,  2.79it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2023/3000:  67%|██████▋   | 2023/3000 [15:07&lt;05:14,  3.11it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2023/3000:  67%|██████▋   | 2023/3000 [15:07&lt;05:14,  3.11it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2024/3000:  67%|██████▋   | 2023/3000 [15:07&lt;05:14,  3.11it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2024/3000:  67%|██████▋   | 2024/3000 [15:08&lt;06:15,  2.60it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2024/3000:  67%|██████▋   | 2024/3000 [15:08&lt;06:15,  2.60it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2025/3000:  67%|██████▋   | 2024/3000 [15:08&lt;06:15,  2.60it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2025/3000:  68%|██████▊   | 2025/3000 [15:08&lt;06:32,  2.48it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2025/3000:  68%|██████▊   | 2025/3000 [15:08&lt;06:32,  2.48it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2026/3000:  68%|██████▊   | 2025/3000 [15:08&lt;06:32,  2.48it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2026/3000:  68%|██████▊   | 2026/3000 [15:09&lt;07:21,  2.21it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2026/3000:  68%|██████▊   | 2026/3000 [15:09&lt;07:21,  2.21it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6] Epoch 2027/3000:  68%|██████▊   | 2026/3000 [15:09&lt;07:21,  2.21it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2027/3000:  68%|██████▊   | 2027/3000 [15:09&lt;07:10,  2.26it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2027/3000:  68%|██████▊   | 2027/3000 [15:09&lt;07:10,  2.26it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2028/3000:  68%|██████▊   | 2027/3000 [15:09&lt;07:10,  2.26it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2028/3000:  68%|██████▊   | 2028/3000 [15:10&lt;07:34,  2.14it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2028/3000:  68%|██████▊   | 2028/3000 [15:10&lt;07:34,  2.14it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2029/3000:  68%|██████▊   | 2028/3000 [15:10&lt;07:34,  2.14it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2029/3000:  68%|██████▊   | 2029/3000 [15:10&lt;07:25,  2.18it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2029/3000:  68%|██████▊   | 2029/3000 [15:10&lt;07:25,  2.18it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2030/3000:  68%|██████▊   | 2029/3000 [15:10&lt;07:25,  2.18it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2030/3000:  68%|██████▊   | 2030/3000 [15:11&lt;08:00,  2.02it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2030/3000:  68%|██████▊   | 2030/3000 [15:11&lt;08:00,  2.02it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6] Epoch 2031/3000:  68%|██████▊   | 2030/3000 [15:11&lt;08:00,  2.02it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2031/3000:  68%|██████▊   | 2031/3000 [15:11&lt;08:01,  2.01it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.11e+6]Epoch 2031/3000:  68%|██████▊   | 2031/3000 [15:11&lt;08:01,  2.01it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2032/3000:  68%|██████▊   | 2031/3000 [15:11&lt;08:01,  2.01it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2032/3000:  68%|██████▊   | 2032/3000 [15:12&lt;07:58,  2.02it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2032/3000:  68%|██████▊   | 2032/3000 [15:12&lt;07:58,  2.02it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.11e+6]Epoch 2033/3000:  68%|██████▊   | 2032/3000 [15:12&lt;07:58,  2.02it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.11e+6]Epoch 2033/3000:  68%|██████▊   | 2033/3000 [15:12&lt;07:12,  2.23it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.11e+6]Epoch 2033/3000:  68%|██████▊   | 2033/3000 [15:12&lt;07:12,  2.23it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2034/3000:  68%|██████▊   | 2033/3000 [15:12&lt;07:12,  2.23it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2034/3000:  68%|██████▊   | 2034/3000 [15:13&lt;07:19,  2.20it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.11e+6]Epoch 2034/3000:  68%|██████▊   | 2034/3000 [15:13&lt;07:19,  2.20it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2035/3000:  68%|██████▊   | 2034/3000 [15:13&lt;07:19,  2.20it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2035/3000:  68%|██████▊   | 2035/3000 [15:13&lt;07:22,  2.18it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2035/3000:  68%|██████▊   | 2035/3000 [15:13&lt;07:22,  2.18it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2036/3000:  68%|██████▊   | 2035/3000 [15:13&lt;07:22,  2.18it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2036/3000:  68%|██████▊   | 2036/3000 [15:14&lt;07:53,  2.03it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.11e+6]Epoch 2036/3000:  68%|██████▊   | 2036/3000 [15:14&lt;07:53,  2.03it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.1e+6] Epoch 2037/3000:  68%|██████▊   | 2036/3000 [15:14&lt;07:53,  2.03it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.1e+6]Epoch 2037/3000:  68%|██████▊   | 2037/3000 [15:14&lt;07:39,  2.10it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.1e+6]Epoch 2037/3000:  68%|██████▊   | 2037/3000 [15:14&lt;07:39,  2.10it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6] Epoch 2038/3000:  68%|██████▊   | 2037/3000 [15:14&lt;07:39,  2.10it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2038/3000:  68%|██████▊   | 2038/3000 [15:14&lt;07:08,  2.24it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2038/3000:  68%|██████▊   | 2038/3000 [15:14&lt;07:08,  2.24it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2039/3000:  68%|██████▊   | 2038/3000 [15:14&lt;07:08,  2.24it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2039/3000:  68%|██████▊   | 2039/3000 [15:15&lt;07:08,  2.25it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2039/3000:  68%|██████▊   | 2039/3000 [15:15&lt;07:08,  2.25it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2040/3000:  68%|██████▊   | 2039/3000 [15:15&lt;07:08,  2.25it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2040/3000:  68%|██████▊   | 2040/3000 [15:16&lt;07:57,  2.01it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2040/3000:  68%|██████▊   | 2040/3000 [15:16&lt;07:57,  2.01it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2041/3000:  68%|██████▊   | 2040/3000 [15:16&lt;07:57,  2.01it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2041/3000:  68%|██████▊   | 2041/3000 [15:16&lt;07:52,  2.03it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2041/3000:  68%|██████▊   | 2041/3000 [15:16&lt;07:52,  2.03it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.1e+6]Epoch 2042/3000:  68%|██████▊   | 2041/3000 [15:16&lt;07:52,  2.03it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.1e+6]Epoch 2042/3000:  68%|██████▊   | 2042/3000 [15:17&lt;07:52,  2.03it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.1e+6]Epoch 2042/3000:  68%|██████▊   | 2042/3000 [15:17&lt;07:52,  2.03it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2043/3000:  68%|██████▊   | 2042/3000 [15:17&lt;07:52,  2.03it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2043/3000:  68%|██████▊   | 2043/3000 [15:17&lt;07:09,  2.23it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2043/3000:  68%|██████▊   | 2043/3000 [15:17&lt;07:09,  2.23it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6] Epoch 2044/3000:  68%|██████▊   | 2043/3000 [15:17&lt;07:09,  2.23it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2044/3000:  68%|██████▊   | 2044/3000 [15:17&lt;07:08,  2.23it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2044/3000:  68%|██████▊   | 2044/3000 [15:17&lt;07:08,  2.23it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.1e+6]Epoch 2045/3000:  68%|██████▊   | 2044/3000 [15:17&lt;07:08,  2.23it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.1e+6]Epoch 2045/3000:  68%|██████▊   | 2045/3000 [15:18&lt;07:11,  2.21it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.1e+6]Epoch 2045/3000:  68%|██████▊   | 2045/3000 [15:18&lt;07:11,  2.21it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2046/3000:  68%|██████▊   | 2045/3000 [15:18&lt;07:11,  2.21it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2046/3000:  68%|██████▊   | 2046/3000 [15:18&lt;07:14,  2.19it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2046/3000:  68%|██████▊   | 2046/3000 [15:18&lt;07:14,  2.19it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2047/3000:  68%|██████▊   | 2046/3000 [15:18&lt;07:14,  2.19it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2047/3000:  68%|██████▊   | 2047/3000 [15:19&lt;07:44,  2.05it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2047/3000:  68%|██████▊   | 2047/3000 [15:19&lt;07:44,  2.05it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6] Epoch 2048/3000:  68%|██████▊   | 2047/3000 [15:19&lt;07:44,  2.05it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2048/3000:  68%|██████▊   | 2048/3000 [15:19&lt;07:24,  2.14it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2048/3000:  68%|██████▊   | 2048/3000 [15:19&lt;07:24,  2.14it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2049/3000:  68%|██████▊   | 2048/3000 [15:19&lt;07:24,  2.14it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2049/3000:  68%|██████▊   | 2049/3000 [15:20&lt;07:09,  2.22it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2049/3000:  68%|██████▊   | 2049/3000 [15:20&lt;07:09,  2.22it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2050/3000:  68%|██████▊   | 2049/3000 [15:20&lt;07:09,  2.22it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2050/3000:  68%|██████▊   | 2050/3000 [15:20&lt;06:49,  2.32it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2050/3000:  68%|██████▊   | 2050/3000 [15:20&lt;06:49,  2.32it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6] Epoch 2051/3000:  68%|██████▊   | 2050/3000 [15:20&lt;06:49,  2.32it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2051/3000:  68%|██████▊   | 2051/3000 [15:20&lt;06:44,  2.35it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2051/3000:  68%|██████▊   | 2051/3000 [15:20&lt;06:44,  2.35it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.1e+6]Epoch 2052/3000:  68%|██████▊   | 2051/3000 [15:20&lt;06:44,  2.35it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.1e+6]Epoch 2052/3000:  68%|██████▊   | 2052/3000 [15:21&lt;06:40,  2.37it/s, v_num=1, train_loss_step=1.14e+6, train_loss_epoch=1.1e+6]Epoch 2052/3000:  68%|██████▊   | 2052/3000 [15:21&lt;06:40,  2.37it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6] Epoch 2053/3000:  68%|██████▊   | 2052/3000 [15:21&lt;06:40,  2.37it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2053/3000:  68%|██████▊   | 2053/3000 [15:22&lt;07:50,  2.01it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2053/3000:  68%|██████▊   | 2053/3000 [15:22&lt;07:50,  2.01it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2054/3000:  68%|██████▊   | 2053/3000 [15:22&lt;07:50,  2.01it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2054/3000:  68%|██████▊   | 2054/3000 [15:22&lt;07:49,  2.01it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2054/3000:  68%|██████▊   | 2054/3000 [15:22&lt;07:49,  2.01it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2055/3000:  68%|██████▊   | 2054/3000 [15:22&lt;07:49,  2.01it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2055/3000:  68%|██████▊   | 2055/3000 [15:22&lt;07:41,  2.05it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2055/3000:  68%|██████▊   | 2055/3000 [15:22&lt;07:41,  2.05it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2056/3000:  68%|██████▊   | 2055/3000 [15:22&lt;07:41,  2.05it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2056/3000:  69%|██████▊   | 2056/3000 [15:23&lt;07:47,  2.02it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2056/3000:  69%|██████▊   | 2056/3000 [15:23&lt;07:47,  2.02it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6] Epoch 2057/3000:  69%|██████▊   | 2056/3000 [15:23&lt;07:47,  2.02it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2057/3000:  69%|██████▊   | 2057/3000 [15:24&lt;07:49,  2.01it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2057/3000:  69%|██████▊   | 2057/3000 [15:24&lt;07:49,  2.01it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2058/3000:  69%|██████▊   | 2057/3000 [15:24&lt;07:49,  2.01it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2058/3000:  69%|██████▊   | 2058/3000 [15:24&lt;07:42,  2.04it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2058/3000:  69%|██████▊   | 2058/3000 [15:24&lt;07:42,  2.04it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2059/3000:  69%|██████▊   | 2058/3000 [15:24&lt;07:42,  2.04it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2059/3000:  69%|██████▊   | 2059/3000 [15:24&lt;07:36,  2.06it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2059/3000:  69%|██████▊   | 2059/3000 [15:24&lt;07:36,  2.06it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2060/3000:  69%|██████▊   | 2059/3000 [15:24&lt;07:36,  2.06it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2060/3000:  69%|██████▊   | 2060/3000 [15:25&lt;06:41,  2.34it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2060/3000:  69%|██████▊   | 2060/3000 [15:25&lt;06:41,  2.34it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2061/3000:  69%|██████▊   | 2060/3000 [15:25&lt;06:41,  2.34it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2061/3000:  69%|██████▊   | 2061/3000 [15:25&lt;06:27,  2.42it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2061/3000:  69%|██████▊   | 2061/3000 [15:25&lt;06:27,  2.42it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6] Epoch 2062/3000:  69%|██████▊   | 2061/3000 [15:25&lt;06:27,  2.42it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2062/3000:  69%|██████▊   | 2062/3000 [15:26&lt;07:03,  2.21it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2062/3000:  69%|██████▊   | 2062/3000 [15:26&lt;07:03,  2.21it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2063/3000:  69%|██████▊   | 2062/3000 [15:26&lt;07:03,  2.21it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2063/3000:  69%|██████▉   | 2063/3000 [15:26&lt;07:22,  2.12it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2063/3000:  69%|██████▉   | 2063/3000 [15:26&lt;07:22,  2.12it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2064/3000:  69%|██████▉   | 2063/3000 [15:26&lt;07:22,  2.12it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2064/3000:  69%|██████▉   | 2064/3000 [15:27&lt;07:28,  2.09it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2064/3000:  69%|██████▉   | 2064/3000 [15:27&lt;07:28,  2.09it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2065/3000:  69%|██████▉   | 2064/3000 [15:27&lt;07:28,  2.09it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2065/3000:  69%|██████▉   | 2065/3000 [15:27&lt;07:23,  2.11it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2065/3000:  69%|██████▉   | 2065/3000 [15:27&lt;07:23,  2.11it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2066/3000:  69%|██████▉   | 2065/3000 [15:27&lt;07:23,  2.11it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2066/3000:  69%|██████▉   | 2066/3000 [15:28&lt;07:07,  2.18it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2066/3000:  69%|██████▉   | 2066/3000 [15:28&lt;07:07,  2.18it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2067/3000:  69%|██████▉   | 2066/3000 [15:28&lt;07:07,  2.18it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2067/3000:  69%|██████▉   | 2067/3000 [15:28&lt;07:50,  1.98it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2067/3000:  69%|██████▉   | 2067/3000 [15:28&lt;07:50,  1.98it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2068/3000:  69%|██████▉   | 2067/3000 [15:28&lt;07:50,  1.98it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2068/3000:  69%|██████▉   | 2068/3000 [15:29&lt;08:19,  1.87it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2068/3000:  69%|██████▉   | 2068/3000 [15:29&lt;08:19,  1.87it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2069/3000:  69%|██████▉   | 2068/3000 [15:29&lt;08:19,  1.87it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2069/3000:  69%|██████▉   | 2069/3000 [15:29&lt;08:14,  1.88it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2069/3000:  69%|██████▉   | 2069/3000 [15:29&lt;08:14,  1.88it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6] Epoch 2070/3000:  69%|██████▉   | 2069/3000 [15:29&lt;08:14,  1.88it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2070/3000:  69%|██████▉   | 2070/3000 [15:30&lt;07:55,  1.96it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2070/3000:  69%|██████▉   | 2070/3000 [15:30&lt;07:55,  1.96it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.1e+6]Epoch 2071/3000:  69%|██████▉   | 2070/3000 [15:30&lt;07:55,  1.96it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.1e+6]Epoch 2071/3000:  69%|██████▉   | 2071/3000 [15:30&lt;08:15,  1.87it/s, v_num=1, train_loss_step=1.13e+6, train_loss_epoch=1.1e+6]Epoch 2071/3000:  69%|██████▉   | 2071/3000 [15:30&lt;08:15,  1.87it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2072/3000:  69%|██████▉   | 2071/3000 [15:30&lt;08:15,  1.87it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2072/3000:  69%|██████▉   | 2072/3000 [15:31&lt;07:55,  1.95it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2072/3000:  69%|██████▉   | 2072/3000 [15:31&lt;07:55,  1.95it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6] Epoch 2073/3000:  69%|██████▉   | 2072/3000 [15:31&lt;07:55,  1.95it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2073/3000:  69%|██████▉   | 2073/3000 [15:31&lt;07:21,  2.10it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2073/3000:  69%|██████▉   | 2073/3000 [15:31&lt;07:21,  2.10it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2074/3000:  69%|██████▉   | 2073/3000 [15:31&lt;07:21,  2.10it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2074/3000:  69%|██████▉   | 2074/3000 [15:32&lt;07:28,  2.06it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2074/3000:  69%|██████▉   | 2074/3000 [15:32&lt;07:28,  2.06it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2075/3000:  69%|██████▉   | 2074/3000 [15:32&lt;07:28,  2.06it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2075/3000:  69%|██████▉   | 2075/3000 [15:32&lt;08:32,  1.81it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2075/3000:  69%|██████▉   | 2075/3000 [15:32&lt;08:32,  1.81it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2076/3000:  69%|██████▉   | 2075/3000 [15:32&lt;08:32,  1.81it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2076/3000:  69%|██████▉   | 2076/3000 [15:33&lt;08:15,  1.86it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2076/3000:  69%|██████▉   | 2076/3000 [15:33&lt;08:15,  1.86it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6] Epoch 2077/3000:  69%|██████▉   | 2076/3000 [15:33&lt;08:15,  1.86it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2077/3000:  69%|██████▉   | 2077/3000 [15:33&lt;07:25,  2.07it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2077/3000:  69%|██████▉   | 2077/3000 [15:33&lt;07:25,  2.07it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2078/3000:  69%|██████▉   | 2077/3000 [15:33&lt;07:25,  2.07it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2078/3000:  69%|██████▉   | 2078/3000 [15:34&lt;07:11,  2.14it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2078/3000:  69%|██████▉   | 2078/3000 [15:34&lt;07:11,  2.14it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6] Epoch 2079/3000:  69%|██████▉   | 2078/3000 [15:34&lt;07:11,  2.14it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2079/3000:  69%|██████▉   | 2079/3000 [15:34&lt;06:23,  2.40it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2079/3000:  69%|██████▉   | 2079/3000 [15:34&lt;06:23,  2.40it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2080/3000:  69%|██████▉   | 2079/3000 [15:34&lt;06:23,  2.40it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2080/3000:  69%|██████▉   | 2080/3000 [15:34&lt;06:05,  2.52it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2080/3000:  69%|██████▉   | 2080/3000 [15:34&lt;06:05,  2.52it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2081/3000:  69%|██████▉   | 2080/3000 [15:34&lt;06:05,  2.52it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2081/3000:  69%|██████▉   | 2081/3000 [15:35&lt;05:19,  2.88it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2081/3000:  69%|██████▉   | 2081/3000 [15:35&lt;05:19,  2.88it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6] Epoch 2082/3000:  69%|██████▉   | 2081/3000 [15:35&lt;05:19,  2.88it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2082/3000:  69%|██████▉   | 2082/3000 [15:35&lt;05:09,  2.96it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2082/3000:  69%|██████▉   | 2082/3000 [15:35&lt;05:09,  2.96it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2083/3000:  69%|██████▉   | 2082/3000 [15:35&lt;05:09,  2.96it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2083/3000:  69%|██████▉   | 2083/3000 [15:35&lt;05:35,  2.73it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2083/3000:  69%|██████▉   | 2083/3000 [15:35&lt;05:35,  2.73it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2084/3000:  69%|██████▉   | 2083/3000 [15:35&lt;05:35,  2.73it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2084/3000:  69%|██████▉   | 2084/3000 [15:36&lt;06:44,  2.27it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2084/3000:  69%|██████▉   | 2084/3000 [15:36&lt;06:44,  2.27it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2085/3000:  69%|██████▉   | 2084/3000 [15:36&lt;06:44,  2.27it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2085/3000:  70%|██████▉   | 2085/3000 [15:36&lt;06:54,  2.21it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2085/3000:  70%|██████▉   | 2085/3000 [15:36&lt;06:54,  2.21it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6] Epoch 2086/3000:  70%|██████▉   | 2085/3000 [15:36&lt;06:54,  2.21it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2086/3000:  70%|██████▉   | 2086/3000 [15:37&lt;06:58,  2.18it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2086/3000:  70%|██████▉   | 2086/3000 [15:37&lt;06:58,  2.18it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2087/3000:  70%|██████▉   | 2086/3000 [15:37&lt;06:58,  2.18it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2087/3000:  70%|██████▉   | 2087/3000 [15:37&lt;07:09,  2.12it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2087/3000:  70%|██████▉   | 2087/3000 [15:37&lt;07:09,  2.12it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2088/3000:  70%|██████▉   | 2087/3000 [15:37&lt;07:09,  2.12it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2088/3000:  70%|██████▉   | 2088/3000 [15:38&lt;08:07,  1.87it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2088/3000:  70%|██████▉   | 2088/3000 [15:38&lt;08:07,  1.87it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2089/3000:  70%|██████▉   | 2088/3000 [15:38&lt;08:07,  1.87it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2089/3000:  70%|██████▉   | 2089/3000 [15:38&lt;07:31,  2.02it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2089/3000:  70%|██████▉   | 2089/3000 [15:38&lt;07:31,  2.02it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2090/3000:  70%|██████▉   | 2089/3000 [15:39&lt;07:31,  2.02it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2090/3000:  70%|██████▉   | 2090/3000 [15:39&lt;07:41,  1.97it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2090/3000:  70%|██████▉   | 2090/3000 [15:39&lt;07:41,  1.97it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2091/3000:  70%|██████▉   | 2090/3000 [15:39&lt;07:41,  1.97it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2091/3000:  70%|██████▉   | 2091/3000 [15:39&lt;07:19,  2.07it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2091/3000:  70%|██████▉   | 2091/3000 [15:39&lt;07:19,  2.07it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2092/3000:  70%|██████▉   | 2091/3000 [15:39&lt;07:19,  2.07it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2092/3000:  70%|██████▉   | 2092/3000 [15:40&lt;07:16,  2.08it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2092/3000:  70%|██████▉   | 2092/3000 [15:40&lt;07:16,  2.08it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6] Epoch 2093/3000:  70%|██████▉   | 2092/3000 [15:40&lt;07:16,  2.08it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2093/3000:  70%|██████▉   | 2093/3000 [15:41&lt;07:50,  1.93it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2093/3000:  70%|██████▉   | 2093/3000 [15:41&lt;07:50,  1.93it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2094/3000:  70%|██████▉   | 2093/3000 [15:41&lt;07:50,  1.93it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2094/3000:  70%|██████▉   | 2094/3000 [15:41&lt;08:16,  1.82it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2094/3000:  70%|██████▉   | 2094/3000 [15:41&lt;08:16,  1.82it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2095/3000:  70%|██████▉   | 2094/3000 [15:41&lt;08:16,  1.82it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2095/3000:  70%|██████▉   | 2095/3000 [15:42&lt;08:25,  1.79it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2095/3000:  70%|██████▉   | 2095/3000 [15:42&lt;08:25,  1.79it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2096/3000:  70%|██████▉   | 2095/3000 [15:42&lt;08:25,  1.79it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2096/3000:  70%|██████▉   | 2096/3000 [15:42&lt;07:53,  1.91it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.1e+6]Epoch 2096/3000:  70%|██████▉   | 2096/3000 [15:42&lt;07:53,  1.91it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6] Epoch 2097/3000:  70%|██████▉   | 2096/3000 [15:42&lt;07:53,  1.91it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2097/3000:  70%|██████▉   | 2097/3000 [15:43&lt;08:01,  1.88it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2097/3000:  70%|██████▉   | 2097/3000 [15:43&lt;08:01,  1.88it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2098/3000:  70%|██████▉   | 2097/3000 [15:43&lt;08:01,  1.88it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2098/3000:  70%|██████▉   | 2098/3000 [15:43&lt;08:20,  1.80it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2098/3000:  70%|██████▉   | 2098/3000 [15:43&lt;08:20,  1.80it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2099/3000:  70%|██████▉   | 2098/3000 [15:43&lt;08:20,  1.80it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2099/3000:  70%|██████▉   | 2099/3000 [15:44&lt;07:52,  1.91it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2099/3000:  70%|██████▉   | 2099/3000 [15:44&lt;07:52,  1.91it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6] Epoch 2100/3000:  70%|██████▉   | 2099/3000 [15:44&lt;07:52,  1.91it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2100/3000:  70%|███████   | 2100/3000 [15:44&lt;06:53,  2.18it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.1e+6]Epoch 2100/3000:  70%|███████   | 2100/3000 [15:44&lt;06:53,  2.18it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2101/3000:  70%|███████   | 2100/3000 [15:44&lt;06:53,  2.18it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2101/3000:  70%|███████   | 2101/3000 [15:45&lt;06:58,  2.15it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2101/3000:  70%|███████   | 2101/3000 [15:45&lt;06:58,  2.15it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2102/3000:  70%|███████   | 2101/3000 [15:45&lt;06:58,  2.15it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2102/3000:  70%|███████   | 2102/3000 [15:45&lt;06:56,  2.16it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.1e+6]Epoch 2102/3000:  70%|███████   | 2102/3000 [15:45&lt;06:56,  2.16it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2103/3000:  70%|███████   | 2102/3000 [15:45&lt;06:56,  2.16it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2103/3000:  70%|███████   | 2103/3000 [15:46&lt;07:26,  2.01it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.1e+6]Epoch 2103/3000:  70%|███████   | 2103/3000 [15:46&lt;07:26,  2.01it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2104/3000:  70%|███████   | 2103/3000 [15:46&lt;07:26,  2.01it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2104/3000:  70%|███████   | 2104/3000 [15:46&lt;07:08,  2.09it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2104/3000:  70%|███████   | 2104/3000 [15:46&lt;07:08,  2.09it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2105/3000:  70%|███████   | 2104/3000 [15:46&lt;07:08,  2.09it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2105/3000:  70%|███████   | 2105/3000 [15:47&lt;07:20,  2.03it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2105/3000:  70%|███████   | 2105/3000 [15:47&lt;07:20,  2.03it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2106/3000:  70%|███████   | 2105/3000 [15:47&lt;07:20,  2.03it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2106/3000:  70%|███████   | 2106/3000 [15:47&lt;07:21,  2.02it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2106/3000:  70%|███████   | 2106/3000 [15:47&lt;07:21,  2.02it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2107/3000:  70%|███████   | 2106/3000 [15:47&lt;07:21,  2.02it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2107/3000:  70%|███████   | 2107/3000 [15:47&lt;06:52,  2.17it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2107/3000:  70%|███████   | 2107/3000 [15:47&lt;06:52,  2.17it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2108/3000:  70%|███████   | 2107/3000 [15:47&lt;06:52,  2.17it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2108/3000:  70%|███████   | 2108/3000 [15:48&lt;07:02,  2.11it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2108/3000:  70%|███████   | 2108/3000 [15:48&lt;07:02,  2.11it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2109/3000:  70%|███████   | 2108/3000 [15:48&lt;07:02,  2.11it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2109/3000:  70%|███████   | 2109/3000 [15:49&lt;07:33,  1.96it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2109/3000:  70%|███████   | 2109/3000 [15:49&lt;07:33,  1.96it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2110/3000:  70%|███████   | 2109/3000 [15:49&lt;07:33,  1.96it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2110/3000:  70%|███████   | 2110/3000 [15:49&lt;07:14,  2.05it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2110/3000:  70%|███████   | 2110/3000 [15:49&lt;07:14,  2.05it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2111/3000:  70%|███████   | 2110/3000 [15:49&lt;07:14,  2.05it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2111/3000:  70%|███████   | 2111/3000 [15:49&lt;06:32,  2.27it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2111/3000:  70%|███████   | 2111/3000 [15:49&lt;06:32,  2.27it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2112/3000:  70%|███████   | 2111/3000 [15:49&lt;06:32,  2.27it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2112/3000:  70%|███████   | 2112/3000 [15:50&lt;06:47,  2.18it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2112/3000:  70%|███████   | 2112/3000 [15:50&lt;06:47,  2.18it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6] Epoch 2113/3000:  70%|███████   | 2112/3000 [15:50&lt;06:47,  2.18it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2113/3000:  70%|███████   | 2113/3000 [15:50&lt;06:09,  2.40it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2113/3000:  70%|███████   | 2113/3000 [15:50&lt;06:09,  2.40it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2114/3000:  70%|███████   | 2113/3000 [15:50&lt;06:09,  2.40it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2114/3000:  70%|███████   | 2114/3000 [15:50&lt;05:25,  2.72it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2114/3000:  70%|███████   | 2114/3000 [15:50&lt;05:25,  2.72it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6] Epoch 2115/3000:  70%|███████   | 2114/3000 [15:50&lt;05:25,  2.72it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2115/3000:  70%|███████   | 2115/3000 [15:51&lt;05:46,  2.56it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2115/3000:  70%|███████   | 2115/3000 [15:51&lt;05:46,  2.56it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2116/3000:  70%|███████   | 2115/3000 [15:51&lt;05:46,  2.56it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2116/3000:  71%|███████   | 2116/3000 [15:51&lt;05:45,  2.56it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2116/3000:  71%|███████   | 2116/3000 [15:51&lt;05:45,  2.56it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2117/3000:  71%|███████   | 2116/3000 [15:51&lt;05:45,  2.56it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2117/3000:  71%|███████   | 2117/3000 [15:52&lt;05:46,  2.55it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2117/3000:  71%|███████   | 2117/3000 [15:52&lt;05:46,  2.55it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6] Epoch 2118/3000:  71%|███████   | 2117/3000 [15:52&lt;05:46,  2.55it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2118/3000:  71%|███████   | 2118/3000 [15:52&lt;06:06,  2.41it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2118/3000:  71%|███████   | 2118/3000 [15:52&lt;06:06,  2.41it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2119/3000:  71%|███████   | 2118/3000 [15:52&lt;06:06,  2.41it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2119/3000:  71%|███████   | 2119/3000 [15:53&lt;06:31,  2.25it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2119/3000:  71%|███████   | 2119/3000 [15:53&lt;06:31,  2.25it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6] Epoch 2120/3000:  71%|███████   | 2119/3000 [15:53&lt;06:31,  2.25it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2120/3000:  71%|███████   | 2120/3000 [15:53&lt;05:35,  2.62it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2120/3000:  71%|███████   | 2120/3000 [15:53&lt;05:35,  2.62it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.09e+6]Epoch 2121/3000:  71%|███████   | 2120/3000 [15:53&lt;05:35,  2.62it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.09e+6]Epoch 2121/3000:  71%|███████   | 2121/3000 [15:53&lt;05:56,  2.46it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.09e+6]Epoch 2121/3000:  71%|███████   | 2121/3000 [15:53&lt;05:56,  2.46it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2122/3000:  71%|███████   | 2121/3000 [15:53&lt;05:56,  2.46it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2122/3000:  71%|███████   | 2122/3000 [15:54&lt;07:18,  2.00it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2122/3000:  71%|███████   | 2122/3000 [15:54&lt;07:18,  2.00it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2123/3000:  71%|███████   | 2122/3000 [15:54&lt;07:18,  2.00it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2123/3000:  71%|███████   | 2123/3000 [15:54&lt;06:26,  2.27it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2123/3000:  71%|███████   | 2123/3000 [15:54&lt;06:26,  2.27it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2124/3000:  71%|███████   | 2123/3000 [15:54&lt;06:26,  2.27it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2124/3000:  71%|███████   | 2124/3000 [15:55&lt;06:22,  2.29it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2124/3000:  71%|███████   | 2124/3000 [15:55&lt;06:22,  2.29it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2125/3000:  71%|███████   | 2124/3000 [15:55&lt;06:22,  2.29it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2125/3000:  71%|███████   | 2125/3000 [15:55&lt;06:18,  2.31it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2125/3000:  71%|███████   | 2125/3000 [15:55&lt;06:18,  2.31it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6] Epoch 2126/3000:  71%|███████   | 2125/3000 [15:55&lt;06:18,  2.31it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2126/3000:  71%|███████   | 2126/3000 [15:56&lt;06:50,  2.13it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2126/3000:  71%|███████   | 2126/3000 [15:56&lt;06:50,  2.13it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2127/3000:  71%|███████   | 2126/3000 [15:56&lt;06:50,  2.13it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2127/3000:  71%|███████   | 2127/3000 [15:56&lt;06:51,  2.12it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2127/3000:  71%|███████   | 2127/3000 [15:56&lt;06:51,  2.12it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6] Epoch 2128/3000:  71%|███████   | 2127/3000 [15:56&lt;06:51,  2.12it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2128/3000:  71%|███████   | 2128/3000 [15:57&lt;06:41,  2.17it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2128/3000:  71%|███████   | 2128/3000 [15:57&lt;06:41,  2.17it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2129/3000:  71%|███████   | 2128/3000 [15:57&lt;06:41,  2.17it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2129/3000:  71%|███████   | 2129/3000 [15:57&lt;06:56,  2.09it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2129/3000:  71%|███████   | 2129/3000 [15:57&lt;06:56,  2.09it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2130/3000:  71%|███████   | 2129/3000 [15:57&lt;06:56,  2.09it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2130/3000:  71%|███████   | 2130/3000 [15:58&lt;06:29,  2.24it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2130/3000:  71%|███████   | 2130/3000 [15:58&lt;06:29,  2.24it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2131/3000:  71%|███████   | 2130/3000 [15:58&lt;06:29,  2.24it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2131/3000:  71%|███████   | 2131/3000 [15:58&lt;06:40,  2.17it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2131/3000:  71%|███████   | 2131/3000 [15:58&lt;06:40,  2.17it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.09e+6]Epoch 2132/3000:  71%|███████   | 2131/3000 [15:58&lt;06:40,  2.17it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.09e+6]Epoch 2132/3000:  71%|███████   | 2132/3000 [15:59&lt;07:06,  2.04it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.09e+6]Epoch 2132/3000:  71%|███████   | 2132/3000 [15:59&lt;07:06,  2.04it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2133/3000:  71%|███████   | 2132/3000 [15:59&lt;07:06,  2.04it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2133/3000:  71%|███████   | 2133/3000 [15:59&lt;06:49,  2.12it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2133/3000:  71%|███████   | 2133/3000 [15:59&lt;06:49,  2.12it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2134/3000:  71%|███████   | 2133/3000 [15:59&lt;06:49,  2.12it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2134/3000:  71%|███████   | 2134/3000 [15:59&lt;06:41,  2.16it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2134/3000:  71%|███████   | 2134/3000 [15:59&lt;06:41,  2.16it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2135/3000:  71%|███████   | 2134/3000 [15:59&lt;06:41,  2.16it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2135/3000:  71%|███████   | 2135/3000 [16:00&lt;06:52,  2.10it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2135/3000:  71%|███████   | 2135/3000 [16:00&lt;06:52,  2.10it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2136/3000:  71%|███████   | 2135/3000 [16:00&lt;06:52,  2.10it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2136/3000:  71%|███████   | 2136/3000 [16:00&lt;06:42,  2.15it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2136/3000:  71%|███████   | 2136/3000 [16:00&lt;06:42,  2.15it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.09e+6]Epoch 2137/3000:  71%|███████   | 2136/3000 [16:00&lt;06:42,  2.15it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.09e+6]Epoch 2137/3000:  71%|███████   | 2137/3000 [16:01&lt;06:14,  2.31it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.09e+6]Epoch 2137/3000:  71%|███████   | 2137/3000 [16:01&lt;06:14,  2.31it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2138/3000:  71%|███████   | 2137/3000 [16:01&lt;06:14,  2.31it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2138/3000:  71%|███████▏  | 2138/3000 [16:01&lt;06:51,  2.09it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2138/3000:  71%|███████▏  | 2138/3000 [16:01&lt;06:51,  2.09it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6] Epoch 2139/3000:  71%|███████▏  | 2138/3000 [16:01&lt;06:51,  2.09it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2139/3000:  71%|███████▏  | 2139/3000 [16:02&lt;07:12,  1.99it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2139/3000:  71%|███████▏  | 2139/3000 [16:02&lt;07:12,  1.99it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2140/3000:  71%|███████▏  | 2139/3000 [16:02&lt;07:12,  1.99it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2140/3000:  71%|███████▏  | 2140/3000 [16:02&lt;06:49,  2.10it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2140/3000:  71%|███████▏  | 2140/3000 [16:02&lt;06:49,  2.10it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2141/3000:  71%|███████▏  | 2140/3000 [16:02&lt;06:49,  2.10it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2141/3000:  71%|███████▏  | 2141/3000 [16:03&lt;06:40,  2.14it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2141/3000:  71%|███████▏  | 2141/3000 [16:03&lt;06:40,  2.14it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.09e+6]Epoch 2142/3000:  71%|███████▏  | 2141/3000 [16:03&lt;06:40,  2.14it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.09e+6]Epoch 2142/3000:  71%|███████▏  | 2142/3000 [16:03&lt;07:15,  1.97it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.09e+6]Epoch 2142/3000:  71%|███████▏  | 2142/3000 [16:03&lt;07:15,  1.97it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2143/3000:  71%|███████▏  | 2142/3000 [16:03&lt;07:15,  1.97it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2143/3000:  71%|███████▏  | 2143/3000 [16:04&lt;06:25,  2.23it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2143/3000:  71%|███████▏  | 2143/3000 [16:04&lt;06:25,  2.23it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2144/3000:  71%|███████▏  | 2143/3000 [16:04&lt;06:25,  2.23it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2144/3000:  71%|███████▏  | 2144/3000 [16:04&lt;06:36,  2.16it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2144/3000:  71%|███████▏  | 2144/3000 [16:04&lt;06:36,  2.16it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2145/3000:  71%|███████▏  | 2144/3000 [16:04&lt;06:36,  2.16it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2145/3000:  72%|███████▏  | 2145/3000 [16:05&lt;07:20,  1.94it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2145/3000:  72%|███████▏  | 2145/3000 [16:05&lt;07:20,  1.94it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6] Epoch 2146/3000:  72%|███████▏  | 2145/3000 [16:05&lt;07:20,  1.94it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2146/3000:  72%|███████▏  | 2146/3000 [16:05&lt;07:27,  1.91it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2146/3000:  72%|███████▏  | 2146/3000 [16:05&lt;07:27,  1.91it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2147/3000:  72%|███████▏  | 2146/3000 [16:05&lt;07:27,  1.91it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2147/3000:  72%|███████▏  | 2147/3000 [16:06&lt;07:39,  1.85it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2147/3000:  72%|███████▏  | 2147/3000 [16:06&lt;07:39,  1.85it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2148/3000:  72%|███████▏  | 2147/3000 [16:06&lt;07:39,  1.85it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2148/3000:  72%|███████▏  | 2148/3000 [16:06&lt;07:18,  1.95it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2148/3000:  72%|███████▏  | 2148/3000 [16:06&lt;07:18,  1.95it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6] Epoch 2149/3000:  72%|███████▏  | 2148/3000 [16:06&lt;07:18,  1.95it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2149/3000:  72%|███████▏  | 2149/3000 [16:07&lt;06:08,  2.31it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.09e+6]Epoch 2149/3000:  72%|███████▏  | 2149/3000 [16:07&lt;06:08,  2.31it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2150/3000:  72%|███████▏  | 2149/3000 [16:07&lt;06:08,  2.31it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2150/3000:  72%|███████▏  | 2150/3000 [16:07&lt;06:04,  2.33it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2150/3000:  72%|███████▏  | 2150/3000 [16:07&lt;06:04,  2.33it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2151/3000:  72%|███████▏  | 2150/3000 [16:07&lt;06:04,  2.33it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2151/3000:  72%|███████▏  | 2151/3000 [16:08&lt;06:33,  2.16it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2151/3000:  72%|███████▏  | 2151/3000 [16:08&lt;06:33,  2.16it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2152/3000:  72%|███████▏  | 2151/3000 [16:08&lt;06:33,  2.16it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2152/3000:  72%|███████▏  | 2152/3000 [16:08&lt;06:31,  2.17it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2152/3000:  72%|███████▏  | 2152/3000 [16:08&lt;06:31,  2.17it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2153/3000:  72%|███████▏  | 2152/3000 [16:08&lt;06:31,  2.17it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2153/3000:  72%|███████▏  | 2153/3000 [16:08&lt;05:43,  2.47it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2153/3000:  72%|███████▏  | 2153/3000 [16:08&lt;05:43,  2.47it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2154/3000:  72%|███████▏  | 2153/3000 [16:08&lt;05:43,  2.47it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2154/3000:  72%|███████▏  | 2154/3000 [16:08&lt;04:31,  3.11it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2154/3000:  72%|███████▏  | 2154/3000 [16:08&lt;04:31,  3.11it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2155/3000:  72%|███████▏  | 2154/3000 [16:08&lt;04:31,  3.11it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2155/3000:  72%|███████▏  | 2155/3000 [16:09&lt;04:31,  3.11it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2156/3000:  72%|███████▏  | 2155/3000 [16:09&lt;04:31,  3.11it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2156/3000:  72%|███████▏  | 2156/3000 [16:09&lt;04:17,  3.28it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2156/3000:  72%|███████▏  | 2156/3000 [16:09&lt;04:17,  3.28it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2157/3000:  72%|███████▏  | 2156/3000 [16:09&lt;04:17,  3.28it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2157/3000:  72%|███████▏  | 2157/3000 [16:09&lt;04:27,  3.15it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2157/3000:  72%|███████▏  | 2157/3000 [16:09&lt;04:27,  3.15it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2158/3000:  72%|███████▏  | 2157/3000 [16:09&lt;04:27,  3.15it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2158/3000:  72%|███████▏  | 2158/3000 [16:10&lt;04:44,  2.96it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2158/3000:  72%|███████▏  | 2158/3000 [16:10&lt;04:44,  2.96it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.09e+6]Epoch 2159/3000:  72%|███████▏  | 2158/3000 [16:10&lt;04:44,  2.96it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.09e+6]Epoch 2159/3000:  72%|███████▏  | 2159/3000 [16:10&lt;05:43,  2.45it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.09e+6]Epoch 2159/3000:  72%|███████▏  | 2159/3000 [16:10&lt;05:43,  2.45it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.09e+6]Epoch 2160/3000:  72%|███████▏  | 2159/3000 [16:10&lt;05:43,  2.45it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.09e+6]Epoch 2160/3000:  72%|███████▏  | 2160/3000 [16:11&lt;05:55,  2.36it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.09e+6]Epoch 2160/3000:  72%|███████▏  | 2160/3000 [16:11&lt;05:55,  2.36it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2161/3000:  72%|███████▏  | 2160/3000 [16:11&lt;05:55,  2.36it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2161/3000:  72%|███████▏  | 2161/3000 [16:11&lt;06:17,  2.22it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2161/3000:  72%|███████▏  | 2161/3000 [16:11&lt;06:17,  2.22it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2162/3000:  72%|███████▏  | 2161/3000 [16:11&lt;06:17,  2.22it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2162/3000:  72%|███████▏  | 2162/3000 [16:12&lt;06:04,  2.30it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2162/3000:  72%|███████▏  | 2162/3000 [16:12&lt;06:04,  2.30it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2163/3000:  72%|███████▏  | 2162/3000 [16:12&lt;06:04,  2.30it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2163/3000:  72%|███████▏  | 2163/3000 [16:12&lt;06:11,  2.25it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2163/3000:  72%|███████▏  | 2163/3000 [16:12&lt;06:11,  2.25it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2164/3000:  72%|███████▏  | 2163/3000 [16:12&lt;06:11,  2.25it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2164/3000:  72%|███████▏  | 2164/3000 [16:13&lt;06:13,  2.24it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2164/3000:  72%|███████▏  | 2164/3000 [16:13&lt;06:13,  2.24it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.09e+6]Epoch 2165/3000:  72%|███████▏  | 2164/3000 [16:13&lt;06:13,  2.24it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.09e+6]Epoch 2165/3000:  72%|███████▏  | 2165/3000 [16:13&lt;06:34,  2.12it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.09e+6]Epoch 2165/3000:  72%|███████▏  | 2165/3000 [16:13&lt;06:34,  2.12it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2166/3000:  72%|███████▏  | 2165/3000 [16:13&lt;06:34,  2.12it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2166/3000:  72%|███████▏  | 2166/3000 [16:13&lt;05:24,  2.57it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2166/3000:  72%|███████▏  | 2166/3000 [16:13&lt;05:24,  2.57it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2167/3000:  72%|███████▏  | 2166/3000 [16:13&lt;05:24,  2.57it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2167/3000:  72%|███████▏  | 2167/3000 [16:14&lt;04:37,  3.01it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2167/3000:  72%|███████▏  | 2167/3000 [16:14&lt;04:37,  3.01it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2168/3000:  72%|███████▏  | 2167/3000 [16:14&lt;04:37,  3.01it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2168/3000:  72%|███████▏  | 2168/3000 [16:14&lt;04:25,  3.14it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2168/3000:  72%|███████▏  | 2168/3000 [16:14&lt;04:25,  3.14it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2169/3000:  72%|███████▏  | 2168/3000 [16:14&lt;04:25,  3.14it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2169/3000:  72%|███████▏  | 2169/3000 [16:14&lt;05:18,  2.61it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2169/3000:  72%|███████▏  | 2169/3000 [16:14&lt;05:18,  2.61it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2170/3000:  72%|███████▏  | 2169/3000 [16:14&lt;05:18,  2.61it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2170/3000:  72%|███████▏  | 2170/3000 [16:15&lt;05:36,  2.46it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.09e+6]Epoch 2170/3000:  72%|███████▏  | 2170/3000 [16:15&lt;05:36,  2.46it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2171/3000:  72%|███████▏  | 2170/3000 [16:15&lt;05:36,  2.46it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2171/3000:  72%|███████▏  | 2171/3000 [16:15&lt;06:27,  2.14it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2171/3000:  72%|███████▏  | 2171/3000 [16:15&lt;06:27,  2.14it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2172/3000:  72%|███████▏  | 2171/3000 [16:15&lt;06:27,  2.14it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2172/3000:  72%|███████▏  | 2172/3000 [16:16&lt;05:50,  2.36it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2172/3000:  72%|███████▏  | 2172/3000 [16:16&lt;05:50,  2.36it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2173/3000:  72%|███████▏  | 2172/3000 [16:16&lt;05:50,  2.36it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2173/3000:  72%|███████▏  | 2173/3000 [16:16&lt;05:56,  2.32it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2173/3000:  72%|███████▏  | 2173/3000 [16:16&lt;05:56,  2.32it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2174/3000:  72%|███████▏  | 2173/3000 [16:16&lt;05:56,  2.32it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2174/3000:  72%|███████▏  | 2174/3000 [16:17&lt;05:51,  2.35it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2174/3000:  72%|███████▏  | 2174/3000 [16:17&lt;05:51,  2.35it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2175/3000:  72%|███████▏  | 2174/3000 [16:17&lt;05:51,  2.35it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2175/3000:  72%|███████▎  | 2175/3000 [16:17&lt;05:23,  2.55it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.09e+6]Epoch 2175/3000:  72%|███████▎  | 2175/3000 [16:17&lt;05:23,  2.55it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2176/3000:  72%|███████▎  | 2175/3000 [16:17&lt;05:23,  2.55it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2176/3000:  73%|███████▎  | 2176/3000 [16:17&lt;05:33,  2.47it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.09e+6]Epoch 2176/3000:  73%|███████▎  | 2176/3000 [16:17&lt;05:33,  2.47it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6] Epoch 2177/3000:  73%|███████▎  | 2176/3000 [16:17&lt;05:33,  2.47it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6]Epoch 2177/3000:  73%|███████▎  | 2177/3000 [16:18&lt;06:09,  2.23it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6]Epoch 2177/3000:  73%|███████▎  | 2177/3000 [16:18&lt;06:09,  2.23it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2178/3000:  73%|███████▎  | 2177/3000 [16:18&lt;06:09,  2.23it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2178/3000:  73%|███████▎  | 2178/3000 [16:18&lt;05:52,  2.33it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2178/3000:  73%|███████▎  | 2178/3000 [16:18&lt;05:52,  2.33it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2179/3000:  73%|███████▎  | 2178/3000 [16:18&lt;05:52,  2.33it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2179/3000:  73%|███████▎  | 2179/3000 [16:19&lt;05:49,  2.35it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2179/3000:  73%|███████▎  | 2179/3000 [16:19&lt;05:49,  2.35it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2180/3000:  73%|███████▎  | 2179/3000 [16:19&lt;05:49,  2.35it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2180/3000:  73%|███████▎  | 2180/3000 [16:19&lt;06:16,  2.18it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2180/3000:  73%|███████▎  | 2180/3000 [16:19&lt;06:16,  2.18it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6] Epoch 2181/3000:  73%|███████▎  | 2180/3000 [16:19&lt;06:16,  2.18it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6]Epoch 2181/3000:  73%|███████▎  | 2181/3000 [16:20&lt;05:30,  2.47it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6]Epoch 2181/3000:  73%|███████▎  | 2181/3000 [16:20&lt;05:30,  2.47it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6]Epoch 2182/3000:  73%|███████▎  | 2181/3000 [16:20&lt;05:30,  2.47it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6]Epoch 2182/3000:  73%|███████▎  | 2182/3000 [16:20&lt;05:35,  2.44it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6]Epoch 2182/3000:  73%|███████▎  | 2182/3000 [16:20&lt;05:35,  2.44it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2183/3000:  73%|███████▎  | 2182/3000 [16:20&lt;05:35,  2.44it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2183/3000:  73%|███████▎  | 2183/3000 [16:20&lt;05:53,  2.31it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2183/3000:  73%|███████▎  | 2183/3000 [16:21&lt;05:53,  2.31it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2184/3000:  73%|███████▎  | 2183/3000 [16:21&lt;05:53,  2.31it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2184/3000:  73%|███████▎  | 2184/3000 [16:21&lt;06:21,  2.14it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2184/3000:  73%|███████▎  | 2184/3000 [16:21&lt;06:21,  2.14it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2185/3000:  73%|███████▎  | 2184/3000 [16:21&lt;06:21,  2.14it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2185/3000:  73%|███████▎  | 2185/3000 [16:22&lt;06:49,  1.99it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2185/3000:  73%|███████▎  | 2185/3000 [16:22&lt;06:49,  1.99it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2186/3000:  73%|███████▎  | 2185/3000 [16:22&lt;06:49,  1.99it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2186/3000:  73%|███████▎  | 2186/3000 [16:22&lt;06:01,  2.25it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2186/3000:  73%|███████▎  | 2186/3000 [16:22&lt;06:01,  2.25it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2187/3000:  73%|███████▎  | 2186/3000 [16:22&lt;06:01,  2.25it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2187/3000:  73%|███████▎  | 2187/3000 [16:22&lt;06:09,  2.20it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2187/3000:  73%|███████▎  | 2187/3000 [16:22&lt;06:09,  2.20it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2188/3000:  73%|███████▎  | 2187/3000 [16:22&lt;06:09,  2.20it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2188/3000:  73%|███████▎  | 2188/3000 [16:23&lt;05:59,  2.26it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2188/3000:  73%|███████▎  | 2188/3000 [16:23&lt;05:59,  2.26it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2189/3000:  73%|███████▎  | 2188/3000 [16:23&lt;05:59,  2.26it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2189/3000:  73%|███████▎  | 2189/3000 [16:23&lt;06:02,  2.24it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2189/3000:  73%|███████▎  | 2189/3000 [16:23&lt;06:02,  2.24it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6] Epoch 2190/3000:  73%|███████▎  | 2189/3000 [16:23&lt;06:02,  2.24it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6]Epoch 2190/3000:  73%|███████▎  | 2190/3000 [16:24&lt;06:25,  2.10it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6]Epoch 2190/3000:  73%|███████▎  | 2190/3000 [16:24&lt;06:25,  2.10it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2191/3000:  73%|███████▎  | 2190/3000 [16:24&lt;06:25,  2.10it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2191/3000:  73%|███████▎  | 2191/3000 [16:24&lt;06:05,  2.21it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2191/3000:  73%|███████▎  | 2191/3000 [16:24&lt;06:05,  2.21it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2192/3000:  73%|███████▎  | 2191/3000 [16:24&lt;06:05,  2.21it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2192/3000:  73%|███████▎  | 2192/3000 [16:25&lt;06:00,  2.24it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2192/3000:  73%|███████▎  | 2192/3000 [16:25&lt;06:00,  2.24it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2193/3000:  73%|███████▎  | 2192/3000 [16:25&lt;06:00,  2.24it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2193/3000:  73%|███████▎  | 2193/3000 [16:25&lt;06:08,  2.19it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2193/3000:  73%|███████▎  | 2193/3000 [16:25&lt;06:08,  2.19it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2194/3000:  73%|███████▎  | 2193/3000 [16:25&lt;06:08,  2.19it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2194/3000:  73%|███████▎  | 2194/3000 [16:26&lt;07:09,  1.88it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2194/3000:  73%|███████▎  | 2194/3000 [16:26&lt;07:09,  1.88it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2195/3000:  73%|███████▎  | 2194/3000 [16:26&lt;07:09,  1.88it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2195/3000:  73%|███████▎  | 2195/3000 [16:26&lt;07:32,  1.78it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2195/3000:  73%|███████▎  | 2195/3000 [16:26&lt;07:32,  1.78it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2196/3000:  73%|███████▎  | 2195/3000 [16:26&lt;07:32,  1.78it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2196/3000:  73%|███████▎  | 2196/3000 [16:27&lt;06:41,  2.00it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2196/3000:  73%|███████▎  | 2196/3000 [16:27&lt;06:41,  2.00it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2197/3000:  73%|███████▎  | 2196/3000 [16:27&lt;06:41,  2.00it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2197/3000:  73%|███████▎  | 2197/3000 [16:27&lt;06:54,  1.94it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2197/3000:  73%|███████▎  | 2197/3000 [16:27&lt;06:54,  1.94it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.08e+6]Epoch 2198/3000:  73%|███████▎  | 2197/3000 [16:27&lt;06:54,  1.94it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.08e+6]Epoch 2198/3000:  73%|███████▎  | 2198/3000 [16:28&lt;06:49,  1.96it/s, v_num=1, train_loss_step=1.11e+6, train_loss_epoch=1.08e+6]Epoch 2198/3000:  73%|███████▎  | 2198/3000 [16:28&lt;06:49,  1.96it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6] Epoch 2199/3000:  73%|███████▎  | 2198/3000 [16:28&lt;06:49,  1.96it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6]Epoch 2199/3000:  73%|███████▎  | 2199/3000 [16:28&lt;05:36,  2.38it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6]Epoch 2199/3000:  73%|███████▎  | 2199/3000 [16:28&lt;05:36,  2.38it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2200/3000:  73%|███████▎  | 2199/3000 [16:28&lt;05:36,  2.38it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2200/3000:  73%|███████▎  | 2200/3000 [16:28&lt;05:18,  2.51it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2200/3000:  73%|███████▎  | 2200/3000 [16:28&lt;05:18,  2.51it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6] Epoch 2201/3000:  73%|███████▎  | 2200/3000 [16:28&lt;05:18,  2.51it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6]Epoch 2201/3000:  73%|███████▎  | 2201/3000 [16:29&lt;05:50,  2.28it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6]Epoch 2201/3000:  73%|███████▎  | 2201/3000 [16:29&lt;05:50,  2.28it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2202/3000:  73%|███████▎  | 2201/3000 [16:29&lt;05:50,  2.28it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2202/3000:  73%|███████▎  | 2202/3000 [16:29&lt;05:55,  2.24it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2202/3000:  73%|███████▎  | 2202/3000 [16:29&lt;05:55,  2.24it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2203/3000:  73%|███████▎  | 2202/3000 [16:29&lt;05:55,  2.24it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2203/3000:  73%|███████▎  | 2203/3000 [16:30&lt;06:04,  2.19it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2203/3000:  73%|███████▎  | 2203/3000 [16:30&lt;06:04,  2.19it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2204/3000:  73%|███████▎  | 2203/3000 [16:30&lt;06:04,  2.19it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2204/3000:  73%|███████▎  | 2204/3000 [16:30&lt;06:25,  2.07it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2204/3000:  73%|███████▎  | 2204/3000 [16:30&lt;06:25,  2.07it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2205/3000:  73%|███████▎  | 2204/3000 [16:30&lt;06:25,  2.07it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2205/3000:  74%|███████▎  | 2205/3000 [16:31&lt;05:55,  2.24it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2205/3000:  74%|███████▎  | 2205/3000 [16:31&lt;05:55,  2.24it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2206/3000:  74%|███████▎  | 2205/3000 [16:31&lt;05:55,  2.24it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2206/3000:  74%|███████▎  | 2206/3000 [16:31&lt;06:31,  2.03it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2206/3000:  74%|███████▎  | 2206/3000 [16:31&lt;06:31,  2.03it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.08e+6]Epoch 2207/3000:  74%|███████▎  | 2206/3000 [16:31&lt;06:31,  2.03it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.08e+6]Epoch 2207/3000:  74%|███████▎  | 2207/3000 [16:32&lt;06:52,  1.92it/s, v_num=1, train_loss_step=1.12e+6, train_loss_epoch=1.08e+6]Epoch 2207/3000:  74%|███████▎  | 2207/3000 [16:32&lt;06:52,  1.92it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2208/3000:  74%|███████▎  | 2207/3000 [16:32&lt;06:52,  1.92it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2208/3000:  74%|███████▎  | 2208/3000 [16:32&lt;06:12,  2.13it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2208/3000:  74%|███████▎  | 2208/3000 [16:32&lt;06:12,  2.13it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2209/3000:  74%|███████▎  | 2208/3000 [16:32&lt;06:12,  2.13it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2209/3000:  74%|███████▎  | 2209/3000 [16:33&lt;06:29,  2.03it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2209/3000:  74%|███████▎  | 2209/3000 [16:33&lt;06:29,  2.03it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2210/3000:  74%|███████▎  | 2209/3000 [16:33&lt;06:29,  2.03it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2210/3000:  74%|███████▎  | 2210/3000 [16:33&lt;06:30,  2.02it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2210/3000:  74%|███████▎  | 2210/3000 [16:33&lt;06:30,  2.02it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2211/3000:  74%|███████▎  | 2210/3000 [16:33&lt;06:30,  2.02it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2211/3000:  74%|███████▎  | 2211/3000 [16:34&lt;06:55,  1.90it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2211/3000:  74%|███████▎  | 2211/3000 [16:34&lt;06:55,  1.90it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2212/3000:  74%|███████▎  | 2211/3000 [16:34&lt;06:55,  1.90it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2212/3000:  74%|███████▎  | 2212/3000 [16:35&lt;06:57,  1.89it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2212/3000:  74%|███████▎  | 2212/3000 [16:35&lt;06:57,  1.89it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2213/3000:  74%|███████▎  | 2212/3000 [16:35&lt;06:57,  1.89it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2213/3000:  74%|███████▍  | 2213/3000 [16:35&lt;06:57,  1.88it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2213/3000:  74%|███████▍  | 2213/3000 [16:35&lt;06:57,  1.88it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2214/3000:  74%|███████▍  | 2213/3000 [16:35&lt;06:57,  1.88it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2214/3000:  74%|███████▍  | 2214/3000 [16:36&lt;06:34,  1.99it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2214/3000:  74%|███████▍  | 2214/3000 [16:36&lt;06:34,  1.99it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2215/3000:  74%|███████▍  | 2214/3000 [16:36&lt;06:34,  1.99it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2215/3000:  74%|███████▍  | 2215/3000 [16:36&lt;06:00,  2.18it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2215/3000:  74%|███████▍  | 2215/3000 [16:36&lt;06:00,  2.18it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2216/3000:  74%|███████▍  | 2215/3000 [16:36&lt;06:00,  2.18it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2216/3000:  74%|███████▍  | 2216/3000 [16:36&lt;06:31,  2.00it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2216/3000:  74%|███████▍  | 2216/3000 [16:36&lt;06:31,  2.00it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2217/3000:  74%|███████▍  | 2216/3000 [16:36&lt;06:31,  2.00it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2217/3000:  74%|███████▍  | 2217/3000 [16:37&lt;07:12,  1.81it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2217/3000:  74%|███████▍  | 2217/3000 [16:37&lt;07:12,  1.81it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2218/3000:  74%|███████▍  | 2217/3000 [16:37&lt;07:12,  1.81it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2218/3000:  74%|███████▍  | 2218/3000 [16:38&lt;07:16,  1.79it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2218/3000:  74%|███████▍  | 2218/3000 [16:38&lt;07:16,  1.79it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2219/3000:  74%|███████▍  | 2218/3000 [16:38&lt;07:16,  1.79it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2219/3000:  74%|███████▍  | 2219/3000 [16:38&lt;07:19,  1.78it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2219/3000:  74%|███████▍  | 2219/3000 [16:38&lt;07:19,  1.78it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2220/3000:  74%|███████▍  | 2219/3000 [16:38&lt;07:19,  1.78it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2220/3000:  74%|███████▍  | 2220/3000 [16:39&lt;07:33,  1.72it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2220/3000:  74%|███████▍  | 2220/3000 [16:39&lt;07:33,  1.72it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2221/3000:  74%|███████▍  | 2220/3000 [16:39&lt;07:33,  1.72it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2221/3000:  74%|███████▍  | 2221/3000 [16:39&lt;07:04,  1.84it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2221/3000:  74%|███████▍  | 2221/3000 [16:39&lt;07:04,  1.84it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2222/3000:  74%|███████▍  | 2221/3000 [16:39&lt;07:04,  1.84it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2222/3000:  74%|███████▍  | 2222/3000 [16:40&lt;07:43,  1.68it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2222/3000:  74%|███████▍  | 2222/3000 [16:40&lt;07:43,  1.68it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2223/3000:  74%|███████▍  | 2222/3000 [16:40&lt;07:43,  1.68it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2223/3000:  74%|███████▍  | 2223/3000 [16:41&lt;07:16,  1.78it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2223/3000:  74%|███████▍  | 2223/3000 [16:41&lt;07:16,  1.78it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2224/3000:  74%|███████▍  | 2223/3000 [16:41&lt;07:16,  1.78it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2224/3000:  74%|███████▍  | 2224/3000 [16:41&lt;07:23,  1.75it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2224/3000:  74%|███████▍  | 2224/3000 [16:41&lt;07:23,  1.75it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2225/3000:  74%|███████▍  | 2224/3000 [16:41&lt;07:23,  1.75it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2225/3000:  74%|███████▍  | 2225/3000 [16:42&lt;07:27,  1.73it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2225/3000:  74%|███████▍  | 2225/3000 [16:42&lt;07:27,  1.73it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2226/3000:  74%|███████▍  | 2225/3000 [16:42&lt;07:27,  1.73it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2226/3000:  74%|███████▍  | 2226/3000 [16:42&lt;07:07,  1.81it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2226/3000:  74%|███████▍  | 2226/3000 [16:42&lt;07:07,  1.81it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2227/3000:  74%|███████▍  | 2226/3000 [16:42&lt;07:07,  1.81it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2227/3000:  74%|███████▍  | 2227/3000 [16:43&lt;07:29,  1.72it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2227/3000:  74%|███████▍  | 2227/3000 [16:43&lt;07:29,  1.72it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6] Epoch 2228/3000:  74%|███████▍  | 2227/3000 [16:43&lt;07:29,  1.72it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6]Epoch 2228/3000:  74%|███████▍  | 2228/3000 [16:43&lt;07:27,  1.73it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6]Epoch 2228/3000:  74%|███████▍  | 2228/3000 [16:43&lt;07:27,  1.73it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2229/3000:  74%|███████▍  | 2228/3000 [16:43&lt;07:27,  1.73it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2229/3000:  74%|███████▍  | 2229/3000 [16:44&lt;07:35,  1.69it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2229/3000:  74%|███████▍  | 2229/3000 [16:44&lt;07:35,  1.69it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2230/3000:  74%|███████▍  | 2229/3000 [16:44&lt;07:35,  1.69it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2230/3000:  74%|███████▍  | 2230/3000 [16:45&lt;07:33,  1.70it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2230/3000:  74%|███████▍  | 2230/3000 [16:45&lt;07:33,  1.70it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2231/3000:  74%|███████▍  | 2230/3000 [16:45&lt;07:33,  1.70it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2231/3000:  74%|███████▍  | 2231/3000 [16:45&lt;06:42,  1.91it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2231/3000:  74%|███████▍  | 2231/3000 [16:45&lt;06:42,  1.91it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2232/3000:  74%|███████▍  | 2231/3000 [16:45&lt;06:42,  1.91it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2232/3000:  74%|███████▍  | 2232/3000 [16:45&lt;06:18,  2.03it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2232/3000:  74%|███████▍  | 2232/3000 [16:45&lt;06:18,  2.03it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2233/3000:  74%|███████▍  | 2232/3000 [16:45&lt;06:18,  2.03it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2233/3000:  74%|███████▍  | 2233/3000 [16:46&lt;06:06,  2.09it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2233/3000:  74%|███████▍  | 2233/3000 [16:46&lt;06:06,  2.09it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6] Epoch 2234/3000:  74%|███████▍  | 2233/3000 [16:46&lt;06:06,  2.09it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6]Epoch 2234/3000:  74%|███████▍  | 2234/3000 [16:46&lt;05:24,  2.36it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.08e+6]Epoch 2234/3000:  74%|███████▍  | 2234/3000 [16:46&lt;05:24,  2.36it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2235/3000:  74%|███████▍  | 2234/3000 [16:46&lt;05:24,  2.36it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2235/3000:  74%|███████▍  | 2235/3000 [16:47&lt;05:04,  2.51it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2235/3000:  74%|███████▍  | 2235/3000 [16:47&lt;05:04,  2.51it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2236/3000:  74%|███████▍  | 2235/3000 [16:47&lt;05:04,  2.51it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2236/3000:  75%|███████▍  | 2236/3000 [16:47&lt;04:56,  2.58it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2236/3000:  75%|███████▍  | 2236/3000 [16:47&lt;04:56,  2.58it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2237/3000:  75%|███████▍  | 2236/3000 [16:47&lt;04:56,  2.58it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2237/3000:  75%|███████▍  | 2237/3000 [16:47&lt;04:43,  2.69it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2237/3000:  75%|███████▍  | 2237/3000 [16:47&lt;04:43,  2.69it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2238/3000:  75%|███████▍  | 2237/3000 [16:47&lt;04:43,  2.69it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2238/3000:  75%|███████▍  | 2238/3000 [16:48&lt;05:20,  2.38it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2238/3000:  75%|███████▍  | 2238/3000 [16:48&lt;05:20,  2.38it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2239/3000:  75%|███████▍  | 2238/3000 [16:48&lt;05:20,  2.38it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2239/3000:  75%|███████▍  | 2239/3000 [16:48&lt;06:05,  2.08it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2239/3000:  75%|███████▍  | 2239/3000 [16:48&lt;06:05,  2.08it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2240/3000:  75%|███████▍  | 2239/3000 [16:48&lt;06:05,  2.08it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2240/3000:  75%|███████▍  | 2240/3000 [16:49&lt;06:47,  1.86it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2240/3000:  75%|███████▍  | 2240/3000 [16:49&lt;06:47,  1.86it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2241/3000:  75%|███████▍  | 2240/3000 [16:49&lt;06:47,  1.86it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2241/3000:  75%|███████▍  | 2241/3000 [16:50&lt;06:28,  1.95it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2241/3000:  75%|███████▍  | 2241/3000 [16:50&lt;06:28,  1.95it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2242/3000:  75%|███████▍  | 2241/3000 [16:50&lt;06:28,  1.95it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2242/3000:  75%|███████▍  | 2242/3000 [16:50&lt;06:36,  1.91it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2242/3000:  75%|███████▍  | 2242/3000 [16:50&lt;06:36,  1.91it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2243/3000:  75%|███████▍  | 2242/3000 [16:50&lt;06:36,  1.91it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2243/3000:  75%|███████▍  | 2243/3000 [16:51&lt;06:32,  1.93it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2243/3000:  75%|███████▍  | 2243/3000 [16:51&lt;06:32,  1.93it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2244/3000:  75%|███████▍  | 2243/3000 [16:51&lt;06:32,  1.93it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2244/3000:  75%|███████▍  | 2244/3000 [16:51&lt;05:55,  2.13it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2244/3000:  75%|███████▍  | 2244/3000 [16:51&lt;05:55,  2.13it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2245/3000:  75%|███████▍  | 2244/3000 [16:51&lt;05:55,  2.13it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2245/3000:  75%|███████▍  | 2245/3000 [16:51&lt;04:42,  2.67it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2245/3000:  75%|███████▍  | 2245/3000 [16:51&lt;04:42,  2.67it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2246/3000:  75%|███████▍  | 2245/3000 [16:51&lt;04:42,  2.67it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2246/3000:  75%|███████▍  | 2246/3000 [16:52&lt;05:04,  2.48it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2246/3000:  75%|███████▍  | 2246/3000 [16:52&lt;05:04,  2.48it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2247/3000:  75%|███████▍  | 2246/3000 [16:52&lt;05:04,  2.48it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2247/3000:  75%|███████▍  | 2247/3000 [16:52&lt;04:52,  2.58it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2247/3000:  75%|███████▍  | 2247/3000 [16:52&lt;04:52,  2.58it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2248/3000:  75%|███████▍  | 2247/3000 [16:52&lt;04:52,  2.58it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2248/3000:  75%|███████▍  | 2248/3000 [16:52&lt;05:15,  2.38it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2248/3000:  75%|███████▍  | 2248/3000 [16:52&lt;05:15,  2.38it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2249/3000:  75%|███████▍  | 2248/3000 [16:52&lt;05:15,  2.38it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2249/3000:  75%|███████▍  | 2249/3000 [16:53&lt;06:11,  2.02it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2249/3000:  75%|███████▍  | 2249/3000 [16:53&lt;06:11,  2.02it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2250/3000:  75%|███████▍  | 2249/3000 [16:53&lt;06:11,  2.02it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2250/3000:  75%|███████▌  | 2250/3000 [16:54&lt;06:35,  1.90it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2250/3000:  75%|███████▌  | 2250/3000 [16:54&lt;06:35,  1.90it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2251/3000:  75%|███████▌  | 2250/3000 [16:54&lt;06:35,  1.90it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2251/3000:  75%|███████▌  | 2251/3000 [16:54&lt;06:13,  2.01it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2251/3000:  75%|███████▌  | 2251/3000 [16:54&lt;06:13,  2.01it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2252/3000:  75%|███████▌  | 2251/3000 [16:54&lt;06:13,  2.01it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2252/3000:  75%|███████▌  | 2252/3000 [16:54&lt;05:03,  2.46it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2252/3000:  75%|███████▌  | 2252/3000 [16:54&lt;05:03,  2.46it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2253/3000:  75%|███████▌  | 2252/3000 [16:54&lt;05:03,  2.46it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2253/3000:  75%|███████▌  | 2253/3000 [16:55&lt;04:45,  2.61it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2253/3000:  75%|███████▌  | 2253/3000 [16:55&lt;04:45,  2.61it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2254/3000:  75%|███████▌  | 2253/3000 [16:55&lt;04:45,  2.61it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2254/3000:  75%|███████▌  | 2254/3000 [16:55&lt;05:19,  2.33it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2254/3000:  75%|███████▌  | 2254/3000 [16:55&lt;05:19,  2.33it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2255/3000:  75%|███████▌  | 2254/3000 [16:55&lt;05:19,  2.33it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2255/3000:  75%|███████▌  | 2255/3000 [16:55&lt;04:54,  2.53it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.08e+6]Epoch 2255/3000:  75%|███████▌  | 2255/3000 [16:55&lt;04:54,  2.53it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2256/3000:  75%|███████▌  | 2255/3000 [16:55&lt;04:54,  2.53it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2256/3000:  75%|███████▌  | 2256/3000 [16:56&lt;04:56,  2.51it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.08e+6]Epoch 2256/3000:  75%|███████▌  | 2256/3000 [16:56&lt;04:56,  2.51it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2257/3000:  75%|███████▌  | 2256/3000 [16:56&lt;04:56,  2.51it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2257/3000:  75%|███████▌  | 2257/3000 [16:56&lt;04:44,  2.61it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.08e+6]Epoch 2257/3000:  75%|███████▌  | 2257/3000 [16:56&lt;04:44,  2.61it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2258/3000:  75%|███████▌  | 2257/3000 [16:56&lt;04:44,  2.61it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2258/3000:  75%|███████▌  | 2258/3000 [16:57&lt;05:10,  2.39it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.08e+6]Epoch 2258/3000:  75%|███████▌  | 2258/3000 [16:57&lt;05:10,  2.39it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2259/3000:  75%|███████▌  | 2258/3000 [16:57&lt;05:10,  2.39it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2259/3000:  75%|███████▌  | 2259/3000 [16:57&lt;05:18,  2.32it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2259/3000:  75%|███████▌  | 2259/3000 [16:57&lt;05:18,  2.32it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2260/3000:  75%|███████▌  | 2259/3000 [16:57&lt;05:18,  2.32it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2260/3000:  75%|███████▌  | 2260/3000 [16:58&lt;05:11,  2.37it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2260/3000:  75%|███████▌  | 2260/3000 [16:58&lt;05:11,  2.37it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.07e+6]Epoch 2261/3000:  75%|███████▌  | 2260/3000 [16:58&lt;05:11,  2.37it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.07e+6]Epoch 2261/3000:  75%|███████▌  | 2261/3000 [16:58&lt;04:29,  2.74it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.07e+6]Epoch 2261/3000:  75%|███████▌  | 2261/3000 [16:58&lt;04:29,  2.74it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2262/3000:  75%|███████▌  | 2261/3000 [16:58&lt;04:29,  2.74it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2262/3000:  75%|███████▌  | 2262/3000 [16:58&lt;04:33,  2.70it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2262/3000:  75%|███████▌  | 2262/3000 [16:58&lt;04:33,  2.70it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2263/3000:  75%|███████▌  | 2262/3000 [16:58&lt;04:33,  2.70it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2263/3000:  75%|███████▌  | 2263/3000 [16:59&lt;04:58,  2.47it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2263/3000:  75%|███████▌  | 2263/3000 [16:59&lt;04:58,  2.47it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2264/3000:  75%|███████▌  | 2263/3000 [16:59&lt;04:58,  2.47it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2264/3000:  75%|███████▌  | 2264/3000 [16:59&lt;04:53,  2.51it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2264/3000:  75%|███████▌  | 2264/3000 [16:59&lt;04:53,  2.51it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2265/3000:  75%|███████▌  | 2264/3000 [16:59&lt;04:53,  2.51it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2265/3000:  76%|███████▌  | 2265/3000 [17:00&lt;05:16,  2.33it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2265/3000:  76%|███████▌  | 2265/3000 [17:00&lt;05:16,  2.33it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2266/3000:  76%|███████▌  | 2265/3000 [17:00&lt;05:16,  2.33it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2266/3000:  76%|███████▌  | 2266/3000 [17:00&lt;05:32,  2.21it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2266/3000:  76%|███████▌  | 2266/3000 [17:00&lt;05:32,  2.21it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2267/3000:  76%|███████▌  | 2266/3000 [17:00&lt;05:32,  2.21it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2267/3000:  76%|███████▌  | 2267/3000 [17:01&lt;05:56,  2.06it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2267/3000:  76%|███████▌  | 2267/3000 [17:01&lt;05:56,  2.06it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2268/3000:  76%|███████▌  | 2267/3000 [17:01&lt;05:56,  2.06it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2268/3000:  76%|███████▌  | 2268/3000 [17:01&lt;06:26,  1.89it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2268/3000:  76%|███████▌  | 2268/3000 [17:01&lt;06:26,  1.89it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2269/3000:  76%|███████▌  | 2268/3000 [17:01&lt;06:26,  1.89it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2269/3000:  76%|███████▌  | 2269/3000 [17:02&lt;06:33,  1.86it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2269/3000:  76%|███████▌  | 2269/3000 [17:02&lt;06:33,  1.86it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2270/3000:  76%|███████▌  | 2269/3000 [17:02&lt;06:33,  1.86it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2270/3000:  76%|███████▌  | 2270/3000 [17:02&lt;05:56,  2.05it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2270/3000:  76%|███████▌  | 2270/3000 [17:02&lt;05:56,  2.05it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2271/3000:  76%|███████▌  | 2270/3000 [17:02&lt;05:56,  2.05it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2271/3000:  76%|███████▌  | 2271/3000 [17:03&lt;05:51,  2.08it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2271/3000:  76%|███████▌  | 2271/3000 [17:03&lt;05:51,  2.08it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2272/3000:  76%|███████▌  | 2271/3000 [17:03&lt;05:51,  2.08it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2272/3000:  76%|███████▌  | 2272/3000 [17:03&lt;05:00,  2.42it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2272/3000:  76%|███████▌  | 2272/3000 [17:03&lt;05:00,  2.42it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2273/3000:  76%|███████▌  | 2272/3000 [17:03&lt;05:00,  2.42it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2273/3000:  76%|███████▌  | 2273/3000 [17:03&lt;04:36,  2.63it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2273/3000:  76%|███████▌  | 2273/3000 [17:03&lt;04:36,  2.63it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2274/3000:  76%|███████▌  | 2273/3000 [17:03&lt;04:36,  2.63it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2274/3000:  76%|███████▌  | 2274/3000 [17:04&lt;04:47,  2.53it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2274/3000:  76%|███████▌  | 2274/3000 [17:04&lt;04:47,  2.53it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2275/3000:  76%|███████▌  | 2274/3000 [17:04&lt;04:47,  2.53it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2275/3000:  76%|███████▌  | 2275/3000 [17:04&lt;05:41,  2.13it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2275/3000:  76%|███████▌  | 2275/3000 [17:04&lt;05:41,  2.13it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2276/3000:  76%|███████▌  | 2275/3000 [17:04&lt;05:41,  2.13it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2276/3000:  76%|███████▌  | 2276/3000 [17:05&lt;05:21,  2.25it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2276/3000:  76%|███████▌  | 2276/3000 [17:05&lt;05:21,  2.25it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2277/3000:  76%|███████▌  | 2276/3000 [17:05&lt;05:21,  2.25it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2277/3000:  76%|███████▌  | 2277/3000 [17:05&lt;04:48,  2.51it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2277/3000:  76%|███████▌  | 2277/3000 [17:05&lt;04:48,  2.51it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2278/3000:  76%|███████▌  | 2277/3000 [17:05&lt;04:48,  2.51it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2278/3000:  76%|███████▌  | 2278/3000 [17:05&lt;05:04,  2.37it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2278/3000:  76%|███████▌  | 2278/3000 [17:05&lt;05:04,  2.37it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2279/3000:  76%|███████▌  | 2278/3000 [17:05&lt;05:04,  2.37it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2279/3000:  76%|███████▌  | 2279/3000 [17:06&lt;05:21,  2.24it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2279/3000:  76%|███████▌  | 2279/3000 [17:06&lt;05:21,  2.24it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2280/3000:  76%|███████▌  | 2279/3000 [17:06&lt;05:21,  2.24it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2280/3000:  76%|███████▌  | 2280/3000 [17:07&lt;05:47,  2.07it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2280/3000:  76%|███████▌  | 2280/3000 [17:07&lt;05:47,  2.07it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2281/3000:  76%|███████▌  | 2280/3000 [17:07&lt;05:47,  2.07it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2281/3000:  76%|███████▌  | 2281/3000 [17:07&lt;05:57,  2.01it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2281/3000:  76%|███████▌  | 2281/3000 [17:07&lt;05:57,  2.01it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2282/3000:  76%|███████▌  | 2281/3000 [17:07&lt;05:57,  2.01it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2282/3000:  76%|███████▌  | 2282/3000 [17:07&lt;05:27,  2.19it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2282/3000:  76%|███████▌  | 2282/3000 [17:07&lt;05:27,  2.19it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2283/3000:  76%|███████▌  | 2282/3000 [17:07&lt;05:27,  2.19it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2283/3000:  76%|███████▌  | 2283/3000 [17:08&lt;05:29,  2.18it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2283/3000:  76%|███████▌  | 2283/3000 [17:08&lt;05:29,  2.18it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2284/3000:  76%|███████▌  | 2283/3000 [17:08&lt;05:29,  2.18it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2284/3000:  76%|███████▌  | 2284/3000 [17:08&lt;05:16,  2.26it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2284/3000:  76%|███████▌  | 2284/3000 [17:08&lt;05:16,  2.26it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2285/3000:  76%|███████▌  | 2284/3000 [17:08&lt;05:16,  2.26it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2285/3000:  76%|███████▌  | 2285/3000 [17:09&lt;05:21,  2.23it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2285/3000:  76%|███████▌  | 2285/3000 [17:09&lt;05:21,  2.23it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2286/3000:  76%|███████▌  | 2285/3000 [17:09&lt;05:21,  2.23it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2286/3000:  76%|███████▌  | 2286/3000 [17:09&lt;04:09,  2.87it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2286/3000:  76%|███████▌  | 2286/3000 [17:09&lt;04:09,  2.87it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2287/3000:  76%|███████▌  | 2286/3000 [17:09&lt;04:09,  2.87it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2287/3000:  76%|███████▌  | 2287/3000 [17:09&lt;04:26,  2.67it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2287/3000:  76%|███████▌  | 2287/3000 [17:09&lt;04:26,  2.67it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2288/3000:  76%|███████▌  | 2287/3000 [17:09&lt;04:26,  2.67it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2288/3000:  76%|███████▋  | 2288/3000 [17:10&lt;04:49,  2.46it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2288/3000:  76%|███████▋  | 2288/3000 [17:10&lt;04:49,  2.46it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2289/3000:  76%|███████▋  | 2288/3000 [17:10&lt;04:49,  2.46it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2289/3000:  76%|███████▋  | 2289/3000 [17:10&lt;05:17,  2.24it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2289/3000:  76%|███████▋  | 2289/3000 [17:10&lt;05:17,  2.24it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2290/3000:  76%|███████▋  | 2289/3000 [17:10&lt;05:17,  2.24it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2290/3000:  76%|███████▋  | 2290/3000 [17:11&lt;05:49,  2.03it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2290/3000:  76%|███████▋  | 2290/3000 [17:11&lt;05:49,  2.03it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2291/3000:  76%|███████▋  | 2290/3000 [17:11&lt;05:49,  2.03it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2291/3000:  76%|███████▋  | 2291/3000 [17:11&lt;05:26,  2.17it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2291/3000:  76%|███████▋  | 2291/3000 [17:11&lt;05:26,  2.17it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2292/3000:  76%|███████▋  | 2291/3000 [17:11&lt;05:26,  2.17it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2292/3000:  76%|███████▋  | 2292/3000 [17:12&lt;04:46,  2.48it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2292/3000:  76%|███████▋  | 2292/3000 [17:12&lt;04:46,  2.48it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2293/3000:  76%|███████▋  | 2292/3000 [17:12&lt;04:46,  2.48it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2293/3000:  76%|███████▋  | 2293/3000 [17:12&lt;05:12,  2.26it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2293/3000:  76%|███████▋  | 2293/3000 [17:12&lt;05:12,  2.26it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2294/3000:  76%|███████▋  | 2293/3000 [17:12&lt;05:12,  2.26it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2294/3000:  76%|███████▋  | 2294/3000 [17:12&lt;04:56,  2.38it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2294/3000:  76%|███████▋  | 2294/3000 [17:12&lt;04:56,  2.38it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2295/3000:  76%|███████▋  | 2294/3000 [17:12&lt;04:56,  2.38it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2295/3000:  76%|███████▋  | 2295/3000 [17:13&lt;04:21,  2.70it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2295/3000:  76%|███████▋  | 2295/3000 [17:13&lt;04:21,  2.70it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2296/3000:  76%|███████▋  | 2295/3000 [17:13&lt;04:21,  2.70it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2296/3000:  77%|███████▋  | 2296/3000 [17:13&lt;04:47,  2.44it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2296/3000:  77%|███████▋  | 2296/3000 [17:13&lt;04:47,  2.44it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2297/3000:  77%|███████▋  | 2296/3000 [17:13&lt;04:47,  2.44it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2297/3000:  77%|███████▋  | 2297/3000 [17:14&lt;05:16,  2.22it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2297/3000:  77%|███████▋  | 2297/3000 [17:14&lt;05:16,  2.22it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2298/3000:  77%|███████▋  | 2297/3000 [17:14&lt;05:16,  2.22it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2298/3000:  77%|███████▋  | 2298/3000 [17:14&lt;05:00,  2.33it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2298/3000:  77%|███████▋  | 2298/3000 [17:14&lt;05:00,  2.33it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2299/3000:  77%|███████▋  | 2298/3000 [17:14&lt;05:00,  2.33it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2299/3000:  77%|███████▋  | 2299/3000 [17:15&lt;04:42,  2.48it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2299/3000:  77%|███████▋  | 2299/3000 [17:15&lt;04:42,  2.48it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2300/3000:  77%|███████▋  | 2299/3000 [17:15&lt;04:42,  2.48it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2300/3000:  77%|███████▋  | 2300/3000 [17:15&lt;04:54,  2.38it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2300/3000:  77%|███████▋  | 2300/3000 [17:15&lt;04:54,  2.38it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2301/3000:  77%|███████▋  | 2300/3000 [17:15&lt;04:54,  2.38it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2301/3000:  77%|███████▋  | 2301/3000 [17:16&lt;05:48,  2.01it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2301/3000:  77%|███████▋  | 2301/3000 [17:16&lt;05:48,  2.01it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2302/3000:  77%|███████▋  | 2301/3000 [17:16&lt;05:48,  2.01it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2302/3000:  77%|███████▋  | 2302/3000 [17:16&lt;04:56,  2.36it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2302/3000:  77%|███████▋  | 2302/3000 [17:16&lt;04:56,  2.36it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2303/3000:  77%|███████▋  | 2302/3000 [17:16&lt;04:56,  2.36it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2303/3000:  77%|███████▋  | 2303/3000 [17:16&lt;04:55,  2.36it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2303/3000:  77%|███████▋  | 2303/3000 [17:16&lt;04:55,  2.36it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2304/3000:  77%|███████▋  | 2303/3000 [17:16&lt;04:55,  2.36it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2304/3000:  77%|███████▋  | 2304/3000 [17:17&lt;05:41,  2.04it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2304/3000:  77%|███████▋  | 2304/3000 [17:17&lt;05:41,  2.04it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2305/3000:  77%|███████▋  | 2304/3000 [17:17&lt;05:41,  2.04it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2305/3000:  77%|███████▋  | 2305/3000 [17:17&lt;05:38,  2.05it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2305/3000:  77%|███████▋  | 2305/3000 [17:17&lt;05:38,  2.05it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2306/3000:  77%|███████▋  | 2305/3000 [17:17&lt;05:38,  2.05it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2306/3000:  77%|███████▋  | 2306/3000 [17:18&lt;05:35,  2.07it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2306/3000:  77%|███████▋  | 2306/3000 [17:18&lt;05:35,  2.07it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2307/3000:  77%|███████▋  | 2306/3000 [17:18&lt;05:35,  2.07it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2307/3000:  77%|███████▋  | 2307/3000 [17:18&lt;04:33,  2.54it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2307/3000:  77%|███████▋  | 2307/3000 [17:18&lt;04:33,  2.54it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2308/3000:  77%|███████▋  | 2307/3000 [17:18&lt;04:33,  2.54it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2308/3000:  77%|███████▋  | 2308/3000 [17:18&lt;04:13,  2.73it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2308/3000:  77%|███████▋  | 2308/3000 [17:18&lt;04:13,  2.73it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2309/3000:  77%|███████▋  | 2308/3000 [17:18&lt;04:13,  2.73it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2309/3000:  77%|███████▋  | 2309/3000 [17:19&lt;03:48,  3.03it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2309/3000:  77%|███████▋  | 2309/3000 [17:19&lt;03:48,  3.03it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2310/3000:  77%|███████▋  | 2309/3000 [17:19&lt;03:48,  3.03it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2310/3000:  77%|███████▋  | 2310/3000 [17:19&lt;04:01,  2.85it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2310/3000:  77%|███████▋  | 2310/3000 [17:19&lt;04:01,  2.85it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2311/3000:  77%|███████▋  | 2310/3000 [17:19&lt;04:01,  2.85it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2311/3000:  77%|███████▋  | 2311/3000 [17:20&lt;05:22,  2.13it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2311/3000:  77%|███████▋  | 2311/3000 [17:20&lt;05:22,  2.13it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2312/3000:  77%|███████▋  | 2311/3000 [17:20&lt;05:22,  2.13it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2312/3000:  77%|███████▋  | 2312/3000 [17:20&lt;04:53,  2.35it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2312/3000:  77%|███████▋  | 2312/3000 [17:20&lt;04:53,  2.35it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2313/3000:  77%|███████▋  | 2312/3000 [17:20&lt;04:53,  2.35it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2313/3000:  77%|███████▋  | 2313/3000 [17:21&lt;05:40,  2.02it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2313/3000:  77%|███████▋  | 2313/3000 [17:21&lt;05:40,  2.02it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2314/3000:  77%|███████▋  | 2313/3000 [17:21&lt;05:40,  2.02it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2314/3000:  77%|███████▋  | 2314/3000 [17:21&lt;05:35,  2.05it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2314/3000:  77%|███████▋  | 2314/3000 [17:21&lt;05:35,  2.05it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2315/3000:  77%|███████▋  | 2314/3000 [17:21&lt;05:35,  2.05it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2315/3000:  77%|███████▋  | 2315/3000 [17:22&lt;05:08,  2.22it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2315/3000:  77%|███████▋  | 2315/3000 [17:22&lt;05:08,  2.22it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2316/3000:  77%|███████▋  | 2315/3000 [17:22&lt;05:08,  2.22it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2316/3000:  77%|███████▋  | 2316/3000 [17:22&lt;05:37,  2.03it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2316/3000:  77%|███████▋  | 2316/3000 [17:22&lt;05:37,  2.03it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2317/3000:  77%|███████▋  | 2316/3000 [17:22&lt;05:37,  2.03it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2317/3000:  77%|███████▋  | 2317/3000 [17:23&lt;05:39,  2.01it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2317/3000:  77%|███████▋  | 2317/3000 [17:23&lt;05:39,  2.01it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2318/3000:  77%|███████▋  | 2317/3000 [17:23&lt;05:39,  2.01it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2318/3000:  77%|███████▋  | 2318/3000 [17:23&lt;05:28,  2.08it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2318/3000:  77%|███████▋  | 2318/3000 [17:23&lt;05:28,  2.08it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2319/3000:  77%|███████▋  | 2318/3000 [17:23&lt;05:28,  2.08it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2319/3000:  77%|███████▋  | 2319/3000 [17:24&lt;05:33,  2.04it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.07e+6]Epoch 2319/3000:  77%|███████▋  | 2319/3000 [17:24&lt;05:33,  2.04it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2320/3000:  77%|███████▋  | 2319/3000 [17:24&lt;05:33,  2.04it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2320/3000:  77%|███████▋  | 2320/3000 [17:24&lt;05:24,  2.09it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2320/3000:  77%|███████▋  | 2320/3000 [17:24&lt;05:24,  2.09it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2321/3000:  77%|███████▋  | 2320/3000 [17:24&lt;05:24,  2.09it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2321/3000:  77%|███████▋  | 2321/3000 [17:25&lt;06:00,  1.88it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2321/3000:  77%|███████▋  | 2321/3000 [17:25&lt;06:00,  1.88it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2322/3000:  77%|███████▋  | 2321/3000 [17:25&lt;06:00,  1.88it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2322/3000:  77%|███████▋  | 2322/3000 [17:25&lt;05:56,  1.90it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2322/3000:  77%|███████▋  | 2322/3000 [17:25&lt;05:56,  1.90it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2323/3000:  77%|███████▋  | 2322/3000 [17:25&lt;05:56,  1.90it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2323/3000:  77%|███████▋  | 2323/3000 [17:26&lt;06:02,  1.87it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2323/3000:  77%|███████▋  | 2323/3000 [17:26&lt;06:02,  1.87it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2324/3000:  77%|███████▋  | 2323/3000 [17:26&lt;06:02,  1.87it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2324/3000:  77%|███████▋  | 2324/3000 [17:26&lt;05:40,  1.98it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2324/3000:  77%|███████▋  | 2324/3000 [17:26&lt;05:40,  1.98it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2325/3000:  77%|███████▋  | 2324/3000 [17:26&lt;05:40,  1.98it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2325/3000:  78%|███████▊  | 2325/3000 [17:27&lt;05:53,  1.91it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2325/3000:  78%|███████▊  | 2325/3000 [17:27&lt;05:53,  1.91it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2326/3000:  78%|███████▊  | 2325/3000 [17:27&lt;05:53,  1.91it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2326/3000:  78%|███████▊  | 2326/3000 [17:27&lt;05:51,  1.92it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2326/3000:  78%|███████▊  | 2326/3000 [17:27&lt;05:51,  1.92it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2327/3000:  78%|███████▊  | 2326/3000 [17:27&lt;05:51,  1.92it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2327/3000:  78%|███████▊  | 2327/3000 [17:28&lt;05:40,  1.98it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2327/3000:  78%|███████▊  | 2327/3000 [17:28&lt;05:40,  1.98it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2328/3000:  78%|███████▊  | 2327/3000 [17:28&lt;05:40,  1.98it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2328/3000:  78%|███████▊  | 2328/3000 [17:28&lt;05:17,  2.12it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2328/3000:  78%|███████▊  | 2328/3000 [17:28&lt;05:17,  2.12it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2329/3000:  78%|███████▊  | 2328/3000 [17:28&lt;05:17,  2.12it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2329/3000:  78%|███████▊  | 2329/3000 [17:29&lt;05:40,  1.97it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2329/3000:  78%|███████▊  | 2329/3000 [17:29&lt;05:40,  1.97it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2330/3000:  78%|███████▊  | 2329/3000 [17:29&lt;05:40,  1.97it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2330/3000:  78%|███████▊  | 2330/3000 [17:29&lt;06:12,  1.80it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2330/3000:  78%|███████▊  | 2330/3000 [17:29&lt;06:12,  1.80it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2331/3000:  78%|███████▊  | 2330/3000 [17:30&lt;06:12,  1.80it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2331/3000:  78%|███████▊  | 2331/3000 [17:30&lt;05:50,  1.91it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2331/3000:  78%|███████▊  | 2331/3000 [17:30&lt;05:50,  1.91it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2332/3000:  78%|███████▊  | 2331/3000 [17:30&lt;05:50,  1.91it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2332/3000:  78%|███████▊  | 2332/3000 [17:30&lt;04:55,  2.26it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2332/3000:  78%|███████▊  | 2332/3000 [17:30&lt;04:55,  2.26it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2333/3000:  78%|███████▊  | 2332/3000 [17:30&lt;04:55,  2.26it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2333/3000:  78%|███████▊  | 2333/3000 [17:31&lt;05:02,  2.21it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2333/3000:  78%|███████▊  | 2333/3000 [17:31&lt;05:02,  2.21it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2334/3000:  78%|███████▊  | 2333/3000 [17:31&lt;05:02,  2.21it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2334/3000:  78%|███████▊  | 2334/3000 [17:31&lt;05:24,  2.05it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2334/3000:  78%|███████▊  | 2334/3000 [17:31&lt;05:24,  2.05it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.07e+6] Epoch 2335/3000:  78%|███████▊  | 2334/3000 [17:31&lt;05:24,  2.05it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.07e+6]Epoch 2335/3000:  78%|███████▊  | 2335/3000 [17:32&lt;04:59,  2.22it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.07e+6]Epoch 2335/3000:  78%|███████▊  | 2335/3000 [17:32&lt;04:59,  2.22it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.07e+6]Epoch 2336/3000:  78%|███████▊  | 2335/3000 [17:32&lt;04:59,  2.22it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.07e+6]Epoch 2336/3000:  78%|███████▊  | 2336/3000 [17:32&lt;05:16,  2.10it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.07e+6]Epoch 2336/3000:  78%|███████▊  | 2336/3000 [17:32&lt;05:16,  2.10it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2337/3000:  78%|███████▊  | 2336/3000 [17:32&lt;05:16,  2.10it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2337/3000:  78%|███████▊  | 2337/3000 [17:33&lt;05:05,  2.17it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2337/3000:  78%|███████▊  | 2337/3000 [17:33&lt;05:05,  2.17it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2338/3000:  78%|███████▊  | 2337/3000 [17:33&lt;05:05,  2.17it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2338/3000:  78%|███████▊  | 2338/3000 [17:33&lt;04:35,  2.40it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2338/3000:  78%|███████▊  | 2338/3000 [17:33&lt;04:35,  2.40it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2339/3000:  78%|███████▊  | 2338/3000 [17:33&lt;04:35,  2.40it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2339/3000:  78%|███████▊  | 2339/3000 [17:33&lt;03:40,  3.00it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2339/3000:  78%|███████▊  | 2339/3000 [17:33&lt;03:40,  3.00it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2340/3000:  78%|███████▊  | 2339/3000 [17:33&lt;03:40,  3.00it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2340/3000:  78%|███████▊  | 2340/3000 [17:33&lt;03:09,  3.49it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2340/3000:  78%|███████▊  | 2340/3000 [17:33&lt;03:09,  3.49it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2341/3000:  78%|███████▊  | 2340/3000 [17:33&lt;03:09,  3.49it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2341/3000:  78%|███████▊  | 2341/3000 [17:34&lt;03:24,  3.22it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2341/3000:  78%|███████▊  | 2341/3000 [17:34&lt;03:24,  3.22it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2342/3000:  78%|███████▊  | 2341/3000 [17:34&lt;03:24,  3.22it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2342/3000:  78%|███████▊  | 2342/3000 [17:34&lt;03:28,  3.15it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.07e+6]Epoch 2342/3000:  78%|███████▊  | 2342/3000 [17:34&lt;03:28,  3.15it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.07e+6]Epoch 2343/3000:  78%|███████▊  | 2342/3000 [17:34&lt;03:28,  3.15it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.07e+6]Epoch 2343/3000:  78%|███████▊  | 2343/3000 [17:34&lt;04:17,  2.55it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.07e+6]Epoch 2343/3000:  78%|███████▊  | 2343/3000 [17:34&lt;04:17,  2.55it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2344/3000:  78%|███████▊  | 2343/3000 [17:34&lt;04:17,  2.55it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2344/3000:  78%|███████▊  | 2344/3000 [17:35&lt;04:35,  2.38it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2344/3000:  78%|███████▊  | 2344/3000 [17:35&lt;04:35,  2.38it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2345/3000:  78%|███████▊  | 2344/3000 [17:35&lt;04:35,  2.38it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2345/3000:  78%|███████▊  | 2345/3000 [17:36&lt;05:06,  2.14it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.07e+6]Epoch 2345/3000:  78%|███████▊  | 2345/3000 [17:36&lt;05:06,  2.14it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2346/3000:  78%|███████▊  | 2345/3000 [17:36&lt;05:06,  2.14it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2346/3000:  78%|███████▊  | 2346/3000 [17:36&lt;04:09,  2.62it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2346/3000:  78%|███████▊  | 2346/3000 [17:36&lt;04:09,  2.62it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2347/3000:  78%|███████▊  | 2346/3000 [17:36&lt;04:09,  2.62it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2347/3000:  78%|███████▊  | 2347/3000 [17:36&lt;03:44,  2.91it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2347/3000:  78%|███████▊  | 2347/3000 [17:36&lt;03:44,  2.91it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2348/3000:  78%|███████▊  | 2347/3000 [17:36&lt;03:44,  2.91it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2348/3000:  78%|███████▊  | 2348/3000 [17:36&lt;03:09,  3.44it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.07e+6]Epoch 2348/3000:  78%|███████▊  | 2348/3000 [17:36&lt;03:09,  3.44it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2349/3000:  78%|███████▊  | 2348/3000 [17:36&lt;03:09,  3.44it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2349/3000:  78%|███████▊  | 2349/3000 [17:37&lt;03:53,  2.79it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.07e+6]Epoch 2349/3000:  78%|███████▊  | 2349/3000 [17:37&lt;03:53,  2.79it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2350/3000:  78%|███████▊  | 2349/3000 [17:37&lt;03:53,  2.79it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2350/3000:  78%|███████▊  | 2350/3000 [17:37&lt;03:52,  2.79it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2350/3000:  78%|███████▊  | 2350/3000 [17:37&lt;03:52,  2.79it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2351/3000:  78%|███████▊  | 2350/3000 [17:37&lt;03:52,  2.79it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2351/3000:  78%|███████▊  | 2351/3000 [17:38&lt;04:33,  2.38it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2351/3000:  78%|███████▊  | 2351/3000 [17:38&lt;04:33,  2.38it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2352/3000:  78%|███████▊  | 2351/3000 [17:38&lt;04:33,  2.38it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2352/3000:  78%|███████▊  | 2352/3000 [17:38&lt;04:34,  2.36it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2352/3000:  78%|███████▊  | 2352/3000 [17:38&lt;04:34,  2.36it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2353/3000:  78%|███████▊  | 2352/3000 [17:38&lt;04:34,  2.36it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2353/3000:  78%|███████▊  | 2353/3000 [17:39&lt;04:51,  2.22it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2353/3000:  78%|███████▊  | 2353/3000 [17:39&lt;04:51,  2.22it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.06e+6] Epoch 2354/3000:  78%|███████▊  | 2353/3000 [17:39&lt;04:51,  2.22it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.06e+6]Epoch 2354/3000:  78%|███████▊  | 2354/3000 [17:39&lt;05:15,  2.05it/s, v_num=1, train_loss_step=1.1e+6, train_loss_epoch=1.06e+6]Epoch 2354/3000:  78%|███████▊  | 2354/3000 [17:39&lt;05:15,  2.05it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.06e+6]Epoch 2355/3000:  78%|███████▊  | 2354/3000 [17:39&lt;05:15,  2.05it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.06e+6]Epoch 2355/3000:  78%|███████▊  | 2355/3000 [17:40&lt;05:01,  2.14it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.06e+6]Epoch 2355/3000:  78%|███████▊  | 2355/3000 [17:40&lt;05:01,  2.14it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2356/3000:  78%|███████▊  | 2355/3000 [17:40&lt;05:01,  2.14it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2356/3000:  79%|███████▊  | 2356/3000 [17:40&lt;05:00,  2.14it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2356/3000:  79%|███████▊  | 2356/3000 [17:40&lt;05:00,  2.14it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2357/3000:  79%|███████▊  | 2356/3000 [17:40&lt;05:00,  2.14it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2357/3000:  79%|███████▊  | 2357/3000 [17:41&lt;05:22,  1.99it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2357/3000:  79%|███████▊  | 2357/3000 [17:41&lt;05:22,  1.99it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2358/3000:  79%|███████▊  | 2357/3000 [17:41&lt;05:22,  1.99it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2358/3000:  79%|███████▊  | 2358/3000 [17:41&lt;05:19,  2.01it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2358/3000:  79%|███████▊  | 2358/3000 [17:41&lt;05:19,  2.01it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2359/3000:  79%|███████▊  | 2358/3000 [17:41&lt;05:19,  2.01it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2359/3000:  79%|███████▊  | 2359/3000 [17:42&lt;05:20,  2.00it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2359/3000:  79%|███████▊  | 2359/3000 [17:42&lt;05:20,  2.00it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2360/3000:  79%|███████▊  | 2359/3000 [17:42&lt;05:20,  2.00it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2360/3000:  79%|███████▊  | 2360/3000 [17:42&lt;05:56,  1.80it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2360/3000:  79%|███████▊  | 2360/3000 [17:42&lt;05:56,  1.80it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2361/3000:  79%|███████▊  | 2360/3000 [17:42&lt;05:56,  1.80it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2361/3000:  79%|███████▊  | 2361/3000 [17:43&lt;05:24,  1.97it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2361/3000:  79%|███████▊  | 2361/3000 [17:43&lt;05:24,  1.97it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2362/3000:  79%|███████▊  | 2361/3000 [17:43&lt;05:24,  1.97it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2362/3000:  79%|███████▊  | 2362/3000 [17:43&lt;05:26,  1.96it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2362/3000:  79%|███████▊  | 2362/3000 [17:43&lt;05:26,  1.96it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2363/3000:  79%|███████▊  | 2362/3000 [17:43&lt;05:26,  1.96it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2363/3000:  79%|███████▉  | 2363/3000 [17:44&lt;05:52,  1.81it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2363/3000:  79%|███████▉  | 2363/3000 [17:44&lt;05:52,  1.81it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2364/3000:  79%|███████▉  | 2363/3000 [17:44&lt;05:52,  1.81it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2364/3000:  79%|███████▉  | 2364/3000 [17:44&lt;05:43,  1.85it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2364/3000:  79%|███████▉  | 2364/3000 [17:44&lt;05:43,  1.85it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2365/3000:  79%|███████▉  | 2364/3000 [17:44&lt;05:43,  1.85it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2365/3000:  79%|███████▉  | 2365/3000 [17:45&lt;05:39,  1.87it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2365/3000:  79%|███████▉  | 2365/3000 [17:45&lt;05:39,  1.87it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2366/3000:  79%|███████▉  | 2365/3000 [17:45&lt;05:39,  1.87it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2366/3000:  79%|███████▉  | 2366/3000 [17:45&lt;05:38,  1.87it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2366/3000:  79%|███████▉  | 2366/3000 [17:45&lt;05:38,  1.87it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2367/3000:  79%|███████▉  | 2366/3000 [17:45&lt;05:38,  1.87it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2367/3000:  79%|███████▉  | 2367/3000 [17:46&lt;05:24,  1.95it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2367/3000:  79%|███████▉  | 2367/3000 [17:46&lt;05:24,  1.95it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2368/3000:  79%|███████▉  | 2367/3000 [17:46&lt;05:24,  1.95it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2368/3000:  79%|███████▉  | 2368/3000 [17:46&lt;05:27,  1.93it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2368/3000:  79%|███████▉  | 2368/3000 [17:46&lt;05:27,  1.93it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2369/3000:  79%|███████▉  | 2368/3000 [17:46&lt;05:27,  1.93it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2369/3000:  79%|███████▉  | 2369/3000 [17:47&lt;05:30,  1.91it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2369/3000:  79%|███████▉  | 2369/3000 [17:47&lt;05:30,  1.91it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2370/3000:  79%|███████▉  | 2369/3000 [17:47&lt;05:30,  1.91it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2370/3000:  79%|███████▉  | 2370/3000 [17:48&lt;06:00,  1.75it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2370/3000:  79%|███████▉  | 2370/3000 [17:48&lt;06:00,  1.75it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2371/3000:  79%|███████▉  | 2370/3000 [17:48&lt;06:00,  1.75it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2371/3000:  79%|███████▉  | 2371/3000 [17:48&lt;06:03,  1.73it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2371/3000:  79%|███████▉  | 2371/3000 [17:48&lt;06:03,  1.73it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2372/3000:  79%|███████▉  | 2371/3000 [17:48&lt;06:03,  1.73it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2372/3000:  79%|███████▉  | 2372/3000 [17:49&lt;05:53,  1.78it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2372/3000:  79%|███████▉  | 2372/3000 [17:49&lt;05:53,  1.78it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2373/3000:  79%|███████▉  | 2372/3000 [17:49&lt;05:53,  1.78it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2373/3000:  79%|███████▉  | 2373/3000 [17:49&lt;06:01,  1.74it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2373/3000:  79%|███████▉  | 2373/3000 [17:49&lt;06:01,  1.74it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2374/3000:  79%|███████▉  | 2373/3000 [17:49&lt;06:01,  1.74it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2374/3000:  79%|███████▉  | 2374/3000 [17:50&lt;05:49,  1.79it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2374/3000:  79%|███████▉  | 2374/3000 [17:50&lt;05:49,  1.79it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2375/3000:  79%|███████▉  | 2374/3000 [17:50&lt;05:49,  1.79it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2375/3000:  79%|███████▉  | 2375/3000 [17:50&lt;04:58,  2.10it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2375/3000:  79%|███████▉  | 2375/3000 [17:50&lt;04:58,  2.10it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2376/3000:  79%|███████▉  | 2375/3000 [17:50&lt;04:58,  2.10it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2376/3000:  79%|███████▉  | 2376/3000 [17:51&lt;05:19,  1.95it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2376/3000:  79%|███████▉  | 2376/3000 [17:51&lt;05:19,  1.95it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2377/3000:  79%|███████▉  | 2376/3000 [17:51&lt;05:19,  1.95it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2377/3000:  79%|███████▉  | 2377/3000 [17:51&lt;05:03,  2.05it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2377/3000:  79%|███████▉  | 2377/3000 [17:51&lt;05:03,  2.05it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2378/3000:  79%|███████▉  | 2377/3000 [17:51&lt;05:03,  2.05it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2378/3000:  79%|███████▉  | 2378/3000 [17:52&lt;05:05,  2.04it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2378/3000:  79%|███████▉  | 2378/3000 [17:52&lt;05:05,  2.04it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2379/3000:  79%|███████▉  | 2378/3000 [17:52&lt;05:05,  2.04it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2379/3000:  79%|███████▉  | 2379/3000 [17:52&lt;05:27,  1.90it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2379/3000:  79%|███████▉  | 2379/3000 [17:52&lt;05:27,  1.90it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2380/3000:  79%|███████▉  | 2379/3000 [17:52&lt;05:27,  1.90it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2380/3000:  79%|███████▉  | 2380/3000 [17:53&lt;05:45,  1.79it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2380/3000:  79%|███████▉  | 2380/3000 [17:53&lt;05:45,  1.79it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2381/3000:  79%|███████▉  | 2380/3000 [17:53&lt;05:45,  1.79it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2381/3000:  79%|███████▉  | 2381/3000 [17:53&lt;05:30,  1.87it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2381/3000:  79%|███████▉  | 2381/3000 [17:53&lt;05:30,  1.87it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2382/3000:  79%|███████▉  | 2381/3000 [17:53&lt;05:30,  1.87it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2382/3000:  79%|███████▉  | 2382/3000 [17:54&lt;05:00,  2.06it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2382/3000:  79%|███████▉  | 2382/3000 [17:54&lt;05:00,  2.06it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2383/3000:  79%|███████▉  | 2382/3000 [17:54&lt;05:00,  2.06it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2383/3000:  79%|███████▉  | 2383/3000 [17:54&lt;05:14,  1.96it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2383/3000:  79%|███████▉  | 2383/3000 [17:54&lt;05:14,  1.96it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2384/3000:  79%|███████▉  | 2383/3000 [17:54&lt;05:14,  1.96it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2384/3000:  79%|███████▉  | 2384/3000 [17:55&lt;04:54,  2.09it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2384/3000:  79%|███████▉  | 2384/3000 [17:55&lt;04:54,  2.09it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2385/3000:  79%|███████▉  | 2384/3000 [17:55&lt;04:54,  2.09it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2385/3000:  80%|███████▉  | 2385/3000 [17:55&lt;05:16,  1.94it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2385/3000:  80%|███████▉  | 2385/3000 [17:55&lt;05:16,  1.94it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2386/3000:  80%|███████▉  | 2385/3000 [17:55&lt;05:16,  1.94it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2386/3000:  80%|███████▉  | 2386/3000 [17:56&lt;05:24,  1.89it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2386/3000:  80%|███████▉  | 2386/3000 [17:56&lt;05:24,  1.89it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2387/3000:  80%|███████▉  | 2386/3000 [17:56&lt;05:24,  1.89it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2387/3000:  80%|███████▉  | 2387/3000 [17:56&lt;05:37,  1.82it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2387/3000:  80%|███████▉  | 2387/3000 [17:56&lt;05:37,  1.82it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2388/3000:  80%|███████▉  | 2387/3000 [17:56&lt;05:37,  1.82it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2388/3000:  80%|███████▉  | 2388/3000 [17:57&lt;05:42,  1.79it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2388/3000:  80%|███████▉  | 2388/3000 [17:57&lt;05:42,  1.79it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2389/3000:  80%|███████▉  | 2388/3000 [17:57&lt;05:42,  1.79it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2389/3000:  80%|███████▉  | 2389/3000 [17:57&lt;05:23,  1.89it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2389/3000:  80%|███████▉  | 2389/3000 [17:58&lt;05:23,  1.89it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2390/3000:  80%|███████▉  | 2389/3000 [17:58&lt;05:23,  1.89it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2390/3000:  80%|███████▉  | 2390/3000 [17:58&lt;05:04,  2.00it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2390/3000:  80%|███████▉  | 2390/3000 [17:58&lt;05:04,  2.00it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2391/3000:  80%|███████▉  | 2390/3000 [17:58&lt;05:04,  2.00it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2391/3000:  80%|███████▉  | 2391/3000 [17:58&lt;05:15,  1.93it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2391/3000:  80%|███████▉  | 2391/3000 [17:58&lt;05:15,  1.93it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2392/3000:  80%|███████▉  | 2391/3000 [17:59&lt;05:15,  1.93it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2392/3000:  80%|███████▉  | 2392/3000 [17:59&lt;05:08,  1.97it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2392/3000:  80%|███████▉  | 2392/3000 [17:59&lt;05:08,  1.97it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2393/3000:  80%|███████▉  | 2392/3000 [17:59&lt;05:08,  1.97it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2393/3000:  80%|███████▉  | 2393/3000 [17:59&lt;04:57,  2.04it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2393/3000:  80%|███████▉  | 2393/3000 [17:59&lt;04:57,  2.04it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2394/3000:  80%|███████▉  | 2393/3000 [17:59&lt;04:57,  2.04it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2394/3000:  80%|███████▉  | 2394/3000 [18:00&lt;04:18,  2.34it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2394/3000:  80%|███████▉  | 2394/3000 [18:00&lt;04:18,  2.34it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2395/3000:  80%|███████▉  | 2394/3000 [18:00&lt;04:18,  2.34it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2395/3000:  80%|███████▉  | 2395/3000 [18:00&lt;04:16,  2.35it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2395/3000:  80%|███████▉  | 2395/3000 [18:00&lt;04:16,  2.35it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2396/3000:  80%|███████▉  | 2395/3000 [18:00&lt;04:16,  2.35it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2396/3000:  80%|███████▉  | 2396/3000 [18:01&lt;04:49,  2.09it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2396/3000:  80%|███████▉  | 2396/3000 [18:01&lt;04:49,  2.09it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2397/3000:  80%|███████▉  | 2396/3000 [18:01&lt;04:49,  2.09it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2397/3000:  80%|███████▉  | 2397/3000 [18:01&lt;05:18,  1.89it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2397/3000:  80%|███████▉  | 2397/3000 [18:01&lt;05:18,  1.89it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2398/3000:  80%|███████▉  | 2397/3000 [18:01&lt;05:18,  1.89it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2398/3000:  80%|███████▉  | 2398/3000 [18:02&lt;05:19,  1.88it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2398/3000:  80%|███████▉  | 2398/3000 [18:02&lt;05:19,  1.88it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2399/3000:  80%|███████▉  | 2398/3000 [18:02&lt;05:19,  1.88it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2399/3000:  80%|███████▉  | 2399/3000 [18:02&lt;04:46,  2.10it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2399/3000:  80%|███████▉  | 2399/3000 [18:02&lt;04:46,  2.10it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2400/3000:  80%|███████▉  | 2399/3000 [18:02&lt;04:46,  2.10it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2400/3000:  80%|████████  | 2400/3000 [18:03&lt;04:30,  2.21it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2400/3000:  80%|████████  | 2400/3000 [18:03&lt;04:30,  2.21it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2401/3000:  80%|████████  | 2400/3000 [18:03&lt;04:30,  2.21it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2401/3000:  80%|████████  | 2401/3000 [18:03&lt;04:42,  2.12it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2401/3000:  80%|████████  | 2401/3000 [18:03&lt;04:42,  2.12it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2402/3000:  80%|████████  | 2401/3000 [18:03&lt;04:42,  2.12it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2402/3000:  80%|████████  | 2402/3000 [18:04&lt;04:37,  2.15it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2402/3000:  80%|████████  | 2402/3000 [18:04&lt;04:37,  2.15it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2403/3000:  80%|████████  | 2402/3000 [18:04&lt;04:37,  2.15it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2403/3000:  80%|████████  | 2403/3000 [18:04&lt;04:35,  2.17it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2403/3000:  80%|████████  | 2403/3000 [18:04&lt;04:35,  2.17it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2404/3000:  80%|████████  | 2403/3000 [18:04&lt;04:35,  2.17it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2404/3000:  80%|████████  | 2404/3000 [18:05&lt;04:42,  2.11it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2404/3000:  80%|████████  | 2404/3000 [18:05&lt;04:42,  2.11it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2405/3000:  80%|████████  | 2404/3000 [18:05&lt;04:42,  2.11it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2405/3000:  80%|████████  | 2405/3000 [18:05&lt;04:47,  2.07it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2405/3000:  80%|████████  | 2405/3000 [18:05&lt;04:47,  2.07it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2406/3000:  80%|████████  | 2405/3000 [18:05&lt;04:47,  2.07it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2406/3000:  80%|████████  | 2406/3000 [18:06&lt;04:46,  2.07it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2406/3000:  80%|████████  | 2406/3000 [18:06&lt;04:46,  2.07it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2407/3000:  80%|████████  | 2406/3000 [18:06&lt;04:46,  2.07it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2407/3000:  80%|████████  | 2407/3000 [18:06&lt;03:59,  2.48it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2407/3000:  80%|████████  | 2407/3000 [18:06&lt;03:59,  2.48it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2408/3000:  80%|████████  | 2407/3000 [18:06&lt;03:59,  2.48it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2408/3000:  80%|████████  | 2408/3000 [18:06&lt;03:31,  2.81it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2408/3000:  80%|████████  | 2408/3000 [18:06&lt;03:31,  2.81it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2409/3000:  80%|████████  | 2408/3000 [18:06&lt;03:31,  2.81it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2409/3000:  80%|████████  | 2409/3000 [18:06&lt;02:47,  3.53it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2409/3000:  80%|████████  | 2409/3000 [18:06&lt;02:47,  3.53it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2410/3000:  80%|████████  | 2409/3000 [18:06&lt;02:47,  3.53it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2410/3000:  80%|████████  | 2410/3000 [18:06&lt;02:29,  3.94it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2410/3000:  80%|████████  | 2410/3000 [18:06&lt;02:29,  3.94it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2411/3000:  80%|████████  | 2410/3000 [18:06&lt;02:29,  3.94it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2411/3000:  80%|████████  | 2411/3000 [18:07&lt;03:07,  3.14it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2411/3000:  80%|████████  | 2411/3000 [18:07&lt;03:07,  3.14it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2412/3000:  80%|████████  | 2411/3000 [18:07&lt;03:07,  3.14it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2412/3000:  80%|████████  | 2412/3000 [18:07&lt;03:37,  2.71it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2412/3000:  80%|████████  | 2412/3000 [18:07&lt;03:37,  2.71it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2413/3000:  80%|████████  | 2412/3000 [18:07&lt;03:37,  2.71it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2413/3000:  80%|████████  | 2413/3000 [18:08&lt;03:46,  2.59it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2413/3000:  80%|████████  | 2413/3000 [18:08&lt;03:46,  2.59it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2414/3000:  80%|████████  | 2413/3000 [18:08&lt;03:46,  2.59it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2414/3000:  80%|████████  | 2414/3000 [18:08&lt;04:47,  2.04it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2414/3000:  80%|████████  | 2414/3000 [18:08&lt;04:47,  2.04it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2415/3000:  80%|████████  | 2414/3000 [18:08&lt;04:47,  2.04it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2415/3000:  80%|████████  | 2415/3000 [18:09&lt;04:30,  2.16it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2415/3000:  80%|████████  | 2415/3000 [18:09&lt;04:30,  2.16it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2416/3000:  80%|████████  | 2415/3000 [18:09&lt;04:30,  2.16it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2416/3000:  81%|████████  | 2416/3000 [18:09&lt;04:35,  2.12it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2416/3000:  81%|████████  | 2416/3000 [18:09&lt;04:35,  2.12it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2417/3000:  81%|████████  | 2416/3000 [18:09&lt;04:35,  2.12it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2417/3000:  81%|████████  | 2417/3000 [18:10&lt;04:46,  2.03it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2417/3000:  81%|████████  | 2417/3000 [18:10&lt;04:46,  2.03it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2418/3000:  81%|████████  | 2417/3000 [18:10&lt;04:46,  2.03it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2418/3000:  81%|████████  | 2418/3000 [18:10&lt;04:41,  2.07it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2418/3000:  81%|████████  | 2418/3000 [18:10&lt;04:41,  2.07it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2419/3000:  81%|████████  | 2418/3000 [18:10&lt;04:41,  2.07it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2419/3000:  81%|████████  | 2419/3000 [18:11&lt;05:00,  1.93it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2419/3000:  81%|████████  | 2419/3000 [18:11&lt;05:00,  1.93it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2420/3000:  81%|████████  | 2419/3000 [18:11&lt;05:00,  1.93it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2420/3000:  81%|████████  | 2420/3000 [18:11&lt;04:56,  1.96it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2420/3000:  81%|████████  | 2420/3000 [18:11&lt;04:56,  1.96it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2421/3000:  81%|████████  | 2420/3000 [18:11&lt;04:56,  1.96it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2421/3000:  81%|████████  | 2421/3000 [18:12&lt;04:30,  2.14it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2421/3000:  81%|████████  | 2421/3000 [18:12&lt;04:30,  2.14it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2422/3000:  81%|████████  | 2421/3000 [18:12&lt;04:30,  2.14it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2422/3000:  81%|████████  | 2422/3000 [18:12&lt;03:42,  2.59it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2422/3000:  81%|████████  | 2422/3000 [18:12&lt;03:42,  2.59it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2423/3000:  81%|████████  | 2422/3000 [18:12&lt;03:42,  2.59it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2423/3000:  81%|████████  | 2423/3000 [18:12&lt;03:31,  2.73it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2423/3000:  81%|████████  | 2423/3000 [18:12&lt;03:31,  2.73it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2424/3000:  81%|████████  | 2423/3000 [18:12&lt;03:31,  2.73it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2424/3000:  81%|████████  | 2424/3000 [18:13&lt;03:57,  2.43it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2424/3000:  81%|████████  | 2424/3000 [18:13&lt;03:57,  2.43it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2425/3000:  81%|████████  | 2424/3000 [18:13&lt;03:57,  2.43it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2425/3000:  81%|████████  | 2425/3000 [18:13&lt;04:21,  2.20it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2425/3000:  81%|████████  | 2425/3000 [18:13&lt;04:21,  2.20it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2426/3000:  81%|████████  | 2425/3000 [18:13&lt;04:21,  2.20it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2426/3000:  81%|████████  | 2426/3000 [18:14&lt;04:22,  2.19it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2426/3000:  81%|████████  | 2426/3000 [18:14&lt;04:22,  2.19it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2427/3000:  81%|████████  | 2426/3000 [18:14&lt;04:22,  2.19it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2427/3000:  81%|████████  | 2427/3000 [18:14&lt;04:09,  2.30it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2427/3000:  81%|████████  | 2427/3000 [18:14&lt;04:09,  2.30it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2428/3000:  81%|████████  | 2427/3000 [18:14&lt;04:09,  2.30it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2428/3000:  81%|████████  | 2428/3000 [18:15&lt;03:57,  2.41it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2428/3000:  81%|████████  | 2428/3000 [18:15&lt;03:57,  2.41it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2429/3000:  81%|████████  | 2428/3000 [18:15&lt;03:57,  2.41it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2429/3000:  81%|████████  | 2429/3000 [18:15&lt;04:05,  2.33it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2429/3000:  81%|████████  | 2429/3000 [18:15&lt;04:05,  2.33it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2430/3000:  81%|████████  | 2429/3000 [18:15&lt;04:05,  2.33it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2430/3000:  81%|████████  | 2430/3000 [18:15&lt;04:01,  2.36it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2430/3000:  81%|████████  | 2430/3000 [18:15&lt;04:01,  2.36it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2431/3000:  81%|████████  | 2430/3000 [18:15&lt;04:01,  2.36it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2431/3000:  81%|████████  | 2431/3000 [18:16&lt;03:57,  2.39it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2431/3000:  81%|████████  | 2431/3000 [18:16&lt;03:57,  2.39it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2432/3000:  81%|████████  | 2431/3000 [18:16&lt;03:57,  2.39it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2432/3000:  81%|████████  | 2432/3000 [18:16&lt;03:48,  2.49it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2432/3000:  81%|████████  | 2432/3000 [18:16&lt;03:48,  2.49it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2433/3000:  81%|████████  | 2432/3000 [18:16&lt;03:48,  2.49it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2433/3000:  81%|████████  | 2433/3000 [18:17&lt;03:53,  2.43it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2433/3000:  81%|████████  | 2433/3000 [18:17&lt;03:53,  2.43it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2434/3000:  81%|████████  | 2433/3000 [18:17&lt;03:53,  2.43it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2434/3000:  81%|████████  | 2434/3000 [18:17&lt;04:02,  2.34it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2434/3000:  81%|████████  | 2434/3000 [18:17&lt;04:02,  2.34it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2435/3000:  81%|████████  | 2434/3000 [18:17&lt;04:02,  2.34it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2435/3000:  81%|████████  | 2435/3000 [18:18&lt;04:17,  2.19it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2435/3000:  81%|████████  | 2435/3000 [18:18&lt;04:17,  2.19it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2436/3000:  81%|████████  | 2435/3000 [18:18&lt;04:17,  2.19it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2436/3000:  81%|████████  | 2436/3000 [18:18&lt;04:35,  2.05it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2436/3000:  81%|████████  | 2436/3000 [18:18&lt;04:35,  2.05it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2437/3000:  81%|████████  | 2436/3000 [18:18&lt;04:35,  2.05it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2437/3000:  81%|████████  | 2437/3000 [18:19&lt;04:13,  2.22it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2437/3000:  81%|████████  | 2437/3000 [18:19&lt;04:13,  2.22it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2438/3000:  81%|████████  | 2437/3000 [18:19&lt;04:13,  2.22it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2438/3000:  81%|████████▏ | 2438/3000 [18:19&lt;04:25,  2.12it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.06e+6]Epoch 2438/3000:  81%|████████▏ | 2438/3000 [18:19&lt;04:25,  2.12it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2439/3000:  81%|████████▏ | 2438/3000 [18:19&lt;04:25,  2.12it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2439/3000:  81%|████████▏ | 2439/3000 [18:19&lt;04:07,  2.26it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2439/3000:  81%|████████▏ | 2439/3000 [18:19&lt;04:07,  2.26it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2440/3000:  81%|████████▏ | 2439/3000 [18:19&lt;04:07,  2.26it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2440/3000:  81%|████████▏ | 2440/3000 [18:20&lt;04:09,  2.24it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2440/3000:  81%|████████▏ | 2440/3000 [18:20&lt;04:09,  2.24it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2441/3000:  81%|████████▏ | 2440/3000 [18:20&lt;04:09,  2.24it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2441/3000:  81%|████████▏ | 2441/3000 [18:20&lt;04:22,  2.13it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2441/3000:  81%|████████▏ | 2441/3000 [18:20&lt;04:22,  2.13it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2442/3000:  81%|████████▏ | 2441/3000 [18:20&lt;04:22,  2.13it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2442/3000:  81%|████████▏ | 2442/3000 [18:21&lt;04:08,  2.25it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2442/3000:  81%|████████▏ | 2442/3000 [18:21&lt;04:08,  2.25it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2443/3000:  81%|████████▏ | 2442/3000 [18:21&lt;04:08,  2.25it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2443/3000:  81%|████████▏ | 2443/3000 [18:21&lt;04:01,  2.31it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2443/3000:  81%|████████▏ | 2443/3000 [18:21&lt;04:01,  2.31it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2444/3000:  81%|████████▏ | 2443/3000 [18:21&lt;04:01,  2.31it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2444/3000:  81%|████████▏ | 2444/3000 [18:22&lt;04:14,  2.18it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2444/3000:  81%|████████▏ | 2444/3000 [18:22&lt;04:14,  2.18it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2445/3000:  81%|████████▏ | 2444/3000 [18:22&lt;04:14,  2.18it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2445/3000:  82%|████████▏ | 2445/3000 [18:22&lt;03:57,  2.34it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2445/3000:  82%|████████▏ | 2445/3000 [18:22&lt;03:57,  2.34it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2446/3000:  82%|████████▏ | 2445/3000 [18:22&lt;03:57,  2.34it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2446/3000:  82%|████████▏ | 2446/3000 [18:22&lt;03:38,  2.53it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2446/3000:  82%|████████▏ | 2446/3000 [18:22&lt;03:38,  2.53it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2447/3000:  82%|████████▏ | 2446/3000 [18:22&lt;03:38,  2.53it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2447/3000:  82%|████████▏ | 2447/3000 [18:23&lt;03:18,  2.79it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2447/3000:  82%|████████▏ | 2447/3000 [18:23&lt;03:18,  2.79it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2448/3000:  82%|████████▏ | 2447/3000 [18:23&lt;03:18,  2.79it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2448/3000:  82%|████████▏ | 2448/3000 [18:23&lt;03:52,  2.37it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2448/3000:  82%|████████▏ | 2448/3000 [18:23&lt;03:52,  2.37it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2449/3000:  82%|████████▏ | 2448/3000 [18:23&lt;03:52,  2.37it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2449/3000:  82%|████████▏ | 2449/3000 [18:24&lt;04:01,  2.29it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.06e+6]Epoch 2449/3000:  82%|████████▏ | 2449/3000 [18:24&lt;04:01,  2.29it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2450/3000:  82%|████████▏ | 2449/3000 [18:24&lt;04:01,  2.29it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2450/3000:  82%|████████▏ | 2450/3000 [18:24&lt;04:13,  2.17it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.06e+6]Epoch 2450/3000:  82%|████████▏ | 2450/3000 [18:24&lt;04:13,  2.17it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2451/3000:  82%|████████▏ | 2450/3000 [18:24&lt;04:13,  2.17it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2451/3000:  82%|████████▏ | 2451/3000 [18:25&lt;04:16,  2.14it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2451/3000:  82%|████████▏ | 2451/3000 [18:25&lt;04:16,  2.14it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2452/3000:  82%|████████▏ | 2451/3000 [18:25&lt;04:16,  2.14it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2452/3000:  82%|████████▏ | 2452/3000 [18:25&lt;04:35,  1.99it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.06e+6]Epoch 2452/3000:  82%|████████▏ | 2452/3000 [18:25&lt;04:35,  1.99it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.06e+6]Epoch 2453/3000:  82%|████████▏ | 2452/3000 [18:25&lt;04:35,  1.99it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.06e+6]Epoch 2453/3000:  82%|████████▏ | 2453/3000 [18:26&lt;04:42,  1.94it/s, v_num=1, train_loss_step=1.09e+6, train_loss_epoch=1.06e+6]Epoch 2453/3000:  82%|████████▏ | 2453/3000 [18:26&lt;04:42,  1.94it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2454/3000:  82%|████████▏ | 2453/3000 [18:26&lt;04:42,  1.94it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2454/3000:  82%|████████▏ | 2454/3000 [18:26&lt;04:15,  2.13it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.06e+6]Epoch 2454/3000:  82%|████████▏ | 2454/3000 [18:26&lt;04:15,  2.13it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2455/3000:  82%|████████▏ | 2454/3000 [18:26&lt;04:15,  2.13it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2455/3000:  82%|████████▏ | 2455/3000 [18:27&lt;03:43,  2.44it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2455/3000:  82%|████████▏ | 2455/3000 [18:27&lt;03:43,  2.44it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2456/3000:  82%|████████▏ | 2455/3000 [18:27&lt;03:43,  2.44it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2456/3000:  82%|████████▏ | 2456/3000 [18:27&lt;03:41,  2.46it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2456/3000:  82%|████████▏ | 2456/3000 [18:27&lt;03:41,  2.46it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2457/3000:  82%|████████▏ | 2456/3000 [18:27&lt;03:41,  2.46it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2457/3000:  82%|████████▏ | 2457/3000 [18:27&lt;03:58,  2.28it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2457/3000:  82%|████████▏ | 2457/3000 [18:27&lt;03:58,  2.28it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2458/3000:  82%|████████▏ | 2457/3000 [18:27&lt;03:58,  2.28it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2458/3000:  82%|████████▏ | 2458/3000 [18:28&lt;03:45,  2.40it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2458/3000:  82%|████████▏ | 2458/3000 [18:28&lt;03:45,  2.40it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2459/3000:  82%|████████▏ | 2458/3000 [18:28&lt;03:45,  2.40it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2459/3000:  82%|████████▏ | 2459/3000 [18:28&lt;03:52,  2.33it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2459/3000:  82%|████████▏ | 2459/3000 [18:28&lt;03:52,  2.33it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2460/3000:  82%|████████▏ | 2459/3000 [18:28&lt;03:52,  2.33it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2460/3000:  82%|████████▏ | 2460/3000 [18:29&lt;03:49,  2.36it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2460/3000:  82%|████████▏ | 2460/3000 [18:29&lt;03:49,  2.36it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2461/3000:  82%|████████▏ | 2460/3000 [18:29&lt;03:49,  2.36it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2461/3000:  82%|████████▏ | 2461/3000 [18:29&lt;03:41,  2.44it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2461/3000:  82%|████████▏ | 2461/3000 [18:29&lt;03:41,  2.44it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2462/3000:  82%|████████▏ | 2461/3000 [18:29&lt;03:41,  2.44it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2462/3000:  82%|████████▏ | 2462/3000 [18:29&lt;03:39,  2.45it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2462/3000:  82%|████████▏ | 2462/3000 [18:29&lt;03:39,  2.45it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2463/3000:  82%|████████▏ | 2462/3000 [18:29&lt;03:39,  2.45it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2463/3000:  82%|████████▏ | 2463/3000 [18:30&lt;03:49,  2.34it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2463/3000:  82%|████████▏ | 2463/3000 [18:30&lt;03:49,  2.34it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2464/3000:  82%|████████▏ | 2463/3000 [18:30&lt;03:49,  2.34it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2464/3000:  82%|████████▏ | 2464/3000 [18:30&lt;03:56,  2.27it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2464/3000:  82%|████████▏ | 2464/3000 [18:30&lt;03:56,  2.27it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2465/3000:  82%|████████▏ | 2464/3000 [18:30&lt;03:56,  2.27it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2465/3000:  82%|████████▏ | 2465/3000 [18:31&lt;04:11,  2.12it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2465/3000:  82%|████████▏ | 2465/3000 [18:31&lt;04:11,  2.12it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2466/3000:  82%|████████▏ | 2465/3000 [18:31&lt;04:11,  2.12it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2466/3000:  82%|████████▏ | 2466/3000 [18:32&lt;04:37,  1.93it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2466/3000:  82%|████████▏ | 2466/3000 [18:32&lt;04:37,  1.93it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2467/3000:  82%|████████▏ | 2466/3000 [18:32&lt;04:37,  1.93it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2467/3000:  82%|████████▏ | 2467/3000 [18:32&lt;04:02,  2.20it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2467/3000:  82%|████████▏ | 2467/3000 [18:32&lt;04:02,  2.20it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2468/3000:  82%|████████▏ | 2467/3000 [18:32&lt;04:02,  2.20it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2468/3000:  82%|████████▏ | 2468/3000 [18:32&lt;03:26,  2.57it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2468/3000:  82%|████████▏ | 2468/3000 [18:32&lt;03:26,  2.57it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2469/3000:  82%|████████▏ | 2468/3000 [18:32&lt;03:26,  2.57it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2469/3000:  82%|████████▏ | 2469/3000 [18:32&lt;03:06,  2.85it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2469/3000:  82%|████████▏ | 2469/3000 [18:32&lt;03:06,  2.85it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2470/3000:  82%|████████▏ | 2469/3000 [18:32&lt;03:06,  2.85it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2470/3000:  82%|████████▏ | 2470/3000 [18:33&lt;02:58,  2.97it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2470/3000:  82%|████████▏ | 2470/3000 [18:33&lt;02:58,  2.97it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2471/3000:  82%|████████▏ | 2470/3000 [18:33&lt;02:58,  2.97it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2471/3000:  82%|████████▏ | 2471/3000 [18:33&lt;02:38,  3.33it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2471/3000:  82%|████████▏ | 2471/3000 [18:33&lt;02:38,  3.33it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2472/3000:  82%|████████▏ | 2471/3000 [18:33&lt;02:38,  3.33it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2472/3000:  82%|████████▏ | 2472/3000 [18:33&lt;02:27,  3.57it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2472/3000:  82%|████████▏ | 2472/3000 [18:33&lt;02:27,  3.57it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2473/3000:  82%|████████▏ | 2472/3000 [18:33&lt;02:27,  3.57it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2473/3000:  82%|████████▏ | 2473/3000 [18:33&lt;02:21,  3.71it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2473/3000:  82%|████████▏ | 2473/3000 [18:33&lt;02:21,  3.71it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2474/3000:  82%|████████▏ | 2473/3000 [18:33&lt;02:21,  3.71it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2474/3000:  82%|████████▏ | 2474/3000 [18:34&lt;02:14,  3.92it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2474/3000:  82%|████████▏ | 2474/3000 [18:34&lt;02:14,  3.92it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2475/3000:  82%|████████▏ | 2474/3000 [18:34&lt;02:14,  3.92it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2475/3000:  82%|████████▎ | 2475/3000 [18:34&lt;02:15,  3.88it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2475/3000:  82%|████████▎ | 2475/3000 [18:34&lt;02:15,  3.88it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2476/3000:  82%|████████▎ | 2475/3000 [18:34&lt;02:15,  3.88it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2476/3000:  83%|████████▎ | 2476/3000 [18:34&lt;02:12,  3.95it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2476/3000:  83%|████████▎ | 2476/3000 [18:34&lt;02:12,  3.95it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2477/3000:  83%|████████▎ | 2476/3000 [18:34&lt;02:12,  3.95it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2477/3000:  83%|████████▎ | 2477/3000 [18:34&lt;02:15,  3.86it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2477/3000:  83%|████████▎ | 2477/3000 [18:34&lt;02:15,  3.86it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2478/3000:  83%|████████▎ | 2477/3000 [18:34&lt;02:15,  3.86it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2478/3000:  83%|████████▎ | 2478/3000 [18:35&lt;02:15,  3.86it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2478/3000:  83%|████████▎ | 2478/3000 [18:35&lt;02:15,  3.86it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2479/3000:  83%|████████▎ | 2478/3000 [18:35&lt;02:15,  3.86it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2479/3000:  83%|████████▎ | 2479/3000 [18:35&lt;02:50,  3.06it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2479/3000:  83%|████████▎ | 2479/3000 [18:35&lt;02:50,  3.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2480/3000:  83%|████████▎ | 2479/3000 [18:35&lt;02:50,  3.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2480/3000:  83%|████████▎ | 2480/3000 [18:36&lt;03:08,  2.75it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2480/3000:  83%|████████▎ | 2480/3000 [18:36&lt;03:08,  2.75it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2481/3000:  83%|████████▎ | 2480/3000 [18:36&lt;03:08,  2.75it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2481/3000:  83%|████████▎ | 2481/3000 [18:36&lt;03:16,  2.65it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2481/3000:  83%|████████▎ | 2481/3000 [18:36&lt;03:16,  2.65it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2482/3000:  83%|████████▎ | 2481/3000 [18:36&lt;03:16,  2.65it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2482/3000:  83%|████████▎ | 2482/3000 [18:37&lt;03:46,  2.28it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2482/3000:  83%|████████▎ | 2482/3000 [18:37&lt;03:46,  2.28it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2483/3000:  83%|████████▎ | 2482/3000 [18:37&lt;03:46,  2.28it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2483/3000:  83%|████████▎ | 2483/3000 [18:37&lt;03:53,  2.21it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2483/3000:  83%|████████▎ | 2483/3000 [18:37&lt;03:53,  2.21it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2484/3000:  83%|████████▎ | 2483/3000 [18:37&lt;03:53,  2.21it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2484/3000:  83%|████████▎ | 2484/3000 [18:37&lt;03:46,  2.28it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2484/3000:  83%|████████▎ | 2484/3000 [18:37&lt;03:46,  2.28it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2485/3000:  83%|████████▎ | 2484/3000 [18:37&lt;03:46,  2.28it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2485/3000:  83%|████████▎ | 2485/3000 [18:38&lt;03:54,  2.19it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2485/3000:  83%|████████▎ | 2485/3000 [18:38&lt;03:54,  2.19it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2486/3000:  83%|████████▎ | 2485/3000 [18:38&lt;03:54,  2.19it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2486/3000:  83%|████████▎ | 2486/3000 [18:38&lt;04:09,  2.06it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2486/3000:  83%|████████▎ | 2486/3000 [18:38&lt;04:09,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2487/3000:  83%|████████▎ | 2486/3000 [18:38&lt;04:09,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2487/3000:  83%|████████▎ | 2487/3000 [18:39&lt;03:50,  2.23it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2487/3000:  83%|████████▎ | 2487/3000 [18:39&lt;03:50,  2.23it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2488/3000:  83%|████████▎ | 2487/3000 [18:39&lt;03:50,  2.23it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2488/3000:  83%|████████▎ | 2488/3000 [18:39&lt;03:36,  2.36it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2488/3000:  83%|████████▎ | 2488/3000 [18:39&lt;03:36,  2.36it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2489/3000:  83%|████████▎ | 2488/3000 [18:39&lt;03:36,  2.36it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2489/3000:  83%|████████▎ | 2489/3000 [18:40&lt;03:35,  2.37it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2489/3000:  83%|████████▎ | 2489/3000 [18:40&lt;03:35,  2.37it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2490/3000:  83%|████████▎ | 2489/3000 [18:40&lt;03:35,  2.37it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2490/3000:  83%|████████▎ | 2490/3000 [18:40&lt;03:38,  2.33it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2490/3000:  83%|████████▎ | 2490/3000 [18:40&lt;03:38,  2.33it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2491/3000:  83%|████████▎ | 2490/3000 [18:40&lt;03:38,  2.33it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2491/3000:  83%|████████▎ | 2491/3000 [18:41&lt;03:56,  2.15it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2491/3000:  83%|████████▎ | 2491/3000 [18:41&lt;03:56,  2.15it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2492/3000:  83%|████████▎ | 2491/3000 [18:41&lt;03:56,  2.15it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2492/3000:  83%|████████▎ | 2492/3000 [18:41&lt;04:09,  2.03it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2492/3000:  83%|████████▎ | 2492/3000 [18:41&lt;04:09,  2.03it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2493/3000:  83%|████████▎ | 2492/3000 [18:41&lt;04:09,  2.03it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2493/3000:  83%|████████▎ | 2493/3000 [18:42&lt;04:11,  2.01it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2493/3000:  83%|████████▎ | 2493/3000 [18:42&lt;04:11,  2.01it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2494/3000:  83%|████████▎ | 2493/3000 [18:42&lt;04:11,  2.01it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2494/3000:  83%|████████▎ | 2494/3000 [18:42&lt;04:15,  1.98it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2494/3000:  83%|████████▎ | 2494/3000 [18:42&lt;04:15,  1.98it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2495/3000:  83%|████████▎ | 2494/3000 [18:42&lt;04:15,  1.98it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2495/3000:  83%|████████▎ | 2495/3000 [18:42&lt;03:38,  2.32it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2495/3000:  83%|████████▎ | 2495/3000 [18:42&lt;03:38,  2.32it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2496/3000:  83%|████████▎ | 2495/3000 [18:42&lt;03:38,  2.32it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2496/3000:  83%|████████▎ | 2496/3000 [18:43&lt;03:38,  2.30it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2496/3000:  83%|████████▎ | 2496/3000 [18:43&lt;03:38,  2.30it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2497/3000:  83%|████████▎ | 2496/3000 [18:43&lt;03:38,  2.30it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2497/3000:  83%|████████▎ | 2497/3000 [18:43&lt;03:49,  2.19it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2497/3000:  83%|████████▎ | 2497/3000 [18:43&lt;03:49,  2.19it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2498/3000:  83%|████████▎ | 2497/3000 [18:43&lt;03:49,  2.19it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2498/3000:  83%|████████▎ | 2498/3000 [18:44&lt;03:45,  2.22it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2498/3000:  83%|████████▎ | 2498/3000 [18:44&lt;03:45,  2.22it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2499/3000:  83%|████████▎ | 2498/3000 [18:44&lt;03:45,  2.22it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2499/3000:  83%|████████▎ | 2499/3000 [18:44&lt;03:40,  2.27it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2499/3000:  83%|████████▎ | 2499/3000 [18:44&lt;03:40,  2.27it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2500/3000:  83%|████████▎ | 2499/3000 [18:44&lt;03:40,  2.27it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2500/3000:  83%|████████▎ | 2500/3000 [18:45&lt;03:59,  2.08it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2500/3000:  83%|████████▎ | 2500/3000 [18:45&lt;03:59,  2.08it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2501/3000:  83%|████████▎ | 2500/3000 [18:45&lt;03:59,  2.08it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2501/3000:  83%|████████▎ | 2501/3000 [18:45&lt;04:13,  1.97it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2501/3000:  83%|████████▎ | 2501/3000 [18:45&lt;04:13,  1.97it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2502/3000:  83%|████████▎ | 2501/3000 [18:45&lt;04:13,  1.97it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2502/3000:  83%|████████▎ | 2502/3000 [18:46&lt;04:09,  1.99it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2502/3000:  83%|████████▎ | 2502/3000 [18:46&lt;04:09,  1.99it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2503/3000:  83%|████████▎ | 2502/3000 [18:46&lt;04:09,  1.99it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2503/3000:  83%|████████▎ | 2503/3000 [18:46&lt;04:13,  1.96it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2503/3000:  83%|████████▎ | 2503/3000 [18:46&lt;04:13,  1.96it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2504/3000:  83%|████████▎ | 2503/3000 [18:46&lt;04:13,  1.96it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2504/3000:  83%|████████▎ | 2504/3000 [18:47&lt;03:54,  2.12it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2504/3000:  83%|████████▎ | 2504/3000 [18:47&lt;03:54,  2.12it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2505/3000:  83%|████████▎ | 2504/3000 [18:47&lt;03:54,  2.12it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2505/3000:  84%|████████▎ | 2505/3000 [18:47&lt;04:02,  2.04it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2505/3000:  84%|████████▎ | 2505/3000 [18:47&lt;04:02,  2.04it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2506/3000:  84%|████████▎ | 2505/3000 [18:47&lt;04:02,  2.04it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2506/3000:  84%|████████▎ | 2506/3000 [18:48&lt;03:30,  2.34it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2506/3000:  84%|████████▎ | 2506/3000 [18:48&lt;03:30,  2.34it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2507/3000:  84%|████████▎ | 2506/3000 [18:48&lt;03:30,  2.34it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2507/3000:  84%|████████▎ | 2507/3000 [18:48&lt;03:19,  2.48it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2507/3000:  84%|████████▎ | 2507/3000 [18:48&lt;03:19,  2.48it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2508/3000:  84%|████████▎ | 2507/3000 [18:48&lt;03:19,  2.48it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2508/3000:  84%|████████▎ | 2508/3000 [18:48&lt;03:29,  2.35it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2508/3000:  84%|████████▎ | 2508/3000 [18:48&lt;03:29,  2.35it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2509/3000:  84%|████████▎ | 2508/3000 [18:48&lt;03:29,  2.35it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2509/3000:  84%|████████▎ | 2509/3000 [18:49&lt;03:34,  2.29it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2509/3000:  84%|████████▎ | 2509/3000 [18:49&lt;03:34,  2.29it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2510/3000:  84%|████████▎ | 2509/3000 [18:49&lt;03:34,  2.29it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2510/3000:  84%|████████▎ | 2510/3000 [18:49&lt;03:06,  2.63it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2510/3000:  84%|████████▎ | 2510/3000 [18:49&lt;03:06,  2.63it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2511/3000:  84%|████████▎ | 2510/3000 [18:49&lt;03:06,  2.63it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2511/3000:  84%|████████▎ | 2511/3000 [18:50&lt;03:23,  2.40it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2511/3000:  84%|████████▎ | 2511/3000 [18:50&lt;03:23,  2.40it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2512/3000:  84%|████████▎ | 2511/3000 [18:50&lt;03:23,  2.40it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2512/3000:  84%|████████▎ | 2512/3000 [18:50&lt;03:13,  2.52it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2512/3000:  84%|████████▎ | 2512/3000 [18:50&lt;03:13,  2.52it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.05e+6]Epoch 2513/3000:  84%|████████▎ | 2512/3000 [18:50&lt;03:13,  2.52it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.05e+6]Epoch 2513/3000:  84%|████████▍ | 2513/3000 [18:51&lt;03:24,  2.38it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.05e+6]Epoch 2513/3000:  84%|████████▍ | 2513/3000 [18:51&lt;03:24,  2.38it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2514/3000:  84%|████████▍ | 2513/3000 [18:51&lt;03:24,  2.38it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2514/3000:  84%|████████▍ | 2514/3000 [18:51&lt;03:16,  2.48it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2514/3000:  84%|████████▍ | 2514/3000 [18:51&lt;03:16,  2.48it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2515/3000:  84%|████████▍ | 2514/3000 [18:51&lt;03:16,  2.48it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2515/3000:  84%|████████▍ | 2515/3000 [18:51&lt;03:40,  2.20it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2515/3000:  84%|████████▍ | 2515/3000 [18:51&lt;03:40,  2.20it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2516/3000:  84%|████████▍ | 2515/3000 [18:51&lt;03:40,  2.20it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2516/3000:  84%|████████▍ | 2516/3000 [18:52&lt;03:14,  2.49it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2516/3000:  84%|████████▍ | 2516/3000 [18:52&lt;03:14,  2.49it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2517/3000:  84%|████████▍ | 2516/3000 [18:52&lt;03:14,  2.49it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2517/3000:  84%|████████▍ | 2517/3000 [18:52&lt;03:23,  2.37it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2517/3000:  84%|████████▍ | 2517/3000 [18:52&lt;03:23,  2.37it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2518/3000:  84%|████████▍ | 2517/3000 [18:52&lt;03:23,  2.37it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2518/3000:  84%|████████▍ | 2518/3000 [18:53&lt;04:03,  1.98it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2518/3000:  84%|████████▍ | 2518/3000 [18:53&lt;04:03,  1.98it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2519/3000:  84%|████████▍ | 2518/3000 [18:53&lt;04:03,  1.98it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2519/3000:  84%|████████▍ | 2519/3000 [18:53&lt;04:07,  1.95it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2519/3000:  84%|████████▍ | 2519/3000 [18:53&lt;04:07,  1.95it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2520/3000:  84%|████████▍ | 2519/3000 [18:53&lt;04:07,  1.95it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2520/3000:  84%|████████▍ | 2520/3000 [18:54&lt;04:07,  1.94it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2520/3000:  84%|████████▍ | 2520/3000 [18:54&lt;04:07,  1.94it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2521/3000:  84%|████████▍ | 2520/3000 [18:54&lt;04:07,  1.94it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2521/3000:  84%|████████▍ | 2521/3000 [18:55&lt;04:18,  1.85it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2521/3000:  84%|████████▍ | 2521/3000 [18:55&lt;04:18,  1.85it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2522/3000:  84%|████████▍ | 2521/3000 [18:55&lt;04:18,  1.85it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2522/3000:  84%|████████▍ | 2522/3000 [18:55&lt;04:02,  1.97it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2522/3000:  84%|████████▍ | 2522/3000 [18:55&lt;04:02,  1.97it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2523/3000:  84%|████████▍ | 2522/3000 [18:55&lt;04:02,  1.97it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2523/3000:  84%|████████▍ | 2523/3000 [18:55&lt;04:05,  1.94it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2523/3000:  84%|████████▍ | 2523/3000 [18:55&lt;04:05,  1.94it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2524/3000:  84%|████████▍ | 2523/3000 [18:56&lt;04:05,  1.94it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2524/3000:  84%|████████▍ | 2524/3000 [18:56&lt;03:54,  2.03it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2524/3000:  84%|████████▍ | 2524/3000 [18:56&lt;03:54,  2.03it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2525/3000:  84%|████████▍ | 2524/3000 [18:56&lt;03:54,  2.03it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2525/3000:  84%|████████▍ | 2525/3000 [18:56&lt;03:53,  2.04it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2525/3000:  84%|████████▍ | 2525/3000 [18:56&lt;03:53,  2.04it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2526/3000:  84%|████████▍ | 2525/3000 [18:56&lt;03:53,  2.04it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2526/3000:  84%|████████▍ | 2526/3000 [18:57&lt;04:06,  1.92it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2526/3000:  84%|████████▍ | 2526/3000 [18:57&lt;04:06,  1.92it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2527/3000:  84%|████████▍ | 2526/3000 [18:57&lt;04:06,  1.92it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2527/3000:  84%|████████▍ | 2527/3000 [18:57&lt;03:56,  2.00it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2527/3000:  84%|████████▍ | 2527/3000 [18:57&lt;03:56,  2.00it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2528/3000:  84%|████████▍ | 2527/3000 [18:57&lt;03:56,  2.00it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2528/3000:  84%|████████▍ | 2528/3000 [18:58&lt;03:51,  2.04it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2528/3000:  84%|████████▍ | 2528/3000 [18:58&lt;03:51,  2.04it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2529/3000:  84%|████████▍ | 2528/3000 [18:58&lt;03:51,  2.04it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2529/3000:  84%|████████▍ | 2529/3000 [18:59&lt;04:04,  1.92it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2529/3000:  84%|████████▍ | 2529/3000 [18:59&lt;04:04,  1.92it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2530/3000:  84%|████████▍ | 2529/3000 [18:59&lt;04:04,  1.92it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2530/3000:  84%|████████▍ | 2530/3000 [18:59&lt;03:40,  2.13it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2530/3000:  84%|████████▍ | 2530/3000 [18:59&lt;03:40,  2.13it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2531/3000:  84%|████████▍ | 2530/3000 [18:59&lt;03:40,  2.13it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2531/3000:  84%|████████▍ | 2531/3000 [18:59&lt;03:43,  2.10it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2531/3000:  84%|████████▍ | 2531/3000 [18:59&lt;03:43,  2.10it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2532/3000:  84%|████████▍ | 2531/3000 [18:59&lt;03:43,  2.10it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2532/3000:  84%|████████▍ | 2532/3000 [19:00&lt;03:27,  2.26it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2532/3000:  84%|████████▍ | 2532/3000 [19:00&lt;03:27,  2.26it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2533/3000:  84%|████████▍ | 2532/3000 [19:00&lt;03:27,  2.26it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2533/3000:  84%|████████▍ | 2533/3000 [19:00&lt;03:28,  2.24it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2533/3000:  84%|████████▍ | 2533/3000 [19:00&lt;03:28,  2.24it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2534/3000:  84%|████████▍ | 2533/3000 [19:00&lt;03:28,  2.24it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2534/3000:  84%|████████▍ | 2534/3000 [19:01&lt;03:17,  2.36it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2534/3000:  84%|████████▍ | 2534/3000 [19:01&lt;03:17,  2.36it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2535/3000:  84%|████████▍ | 2534/3000 [19:01&lt;03:17,  2.36it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2535/3000:  84%|████████▍ | 2535/3000 [19:01&lt;03:11,  2.43it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2535/3000:  84%|████████▍ | 2535/3000 [19:01&lt;03:11,  2.43it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2536/3000:  84%|████████▍ | 2535/3000 [19:01&lt;03:11,  2.43it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2536/3000:  85%|████████▍ | 2536/3000 [19:01&lt;03:08,  2.46it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2536/3000:  85%|████████▍ | 2536/3000 [19:01&lt;03:08,  2.46it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2537/3000:  85%|████████▍ | 2536/3000 [19:01&lt;03:08,  2.46it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2537/3000:  85%|████████▍ | 2537/3000 [19:02&lt;03:10,  2.44it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2537/3000:  85%|████████▍ | 2537/3000 [19:02&lt;03:10,  2.44it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.05e+6]Epoch 2538/3000:  85%|████████▍ | 2537/3000 [19:02&lt;03:10,  2.44it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.05e+6]Epoch 2538/3000:  85%|████████▍ | 2538/3000 [19:02&lt;03:17,  2.34it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.05e+6]Epoch 2538/3000:  85%|████████▍ | 2538/3000 [19:02&lt;03:17,  2.34it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2539/3000:  85%|████████▍ | 2538/3000 [19:02&lt;03:17,  2.34it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2539/3000:  85%|████████▍ | 2539/3000 [19:03&lt;03:10,  2.42it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2539/3000:  85%|████████▍ | 2539/3000 [19:03&lt;03:10,  2.42it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.05e+6]Epoch 2540/3000:  85%|████████▍ | 2539/3000 [19:03&lt;03:10,  2.42it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.05e+6]Epoch 2540/3000:  85%|████████▍ | 2540/3000 [19:03&lt;03:07,  2.45it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.05e+6]Epoch 2540/3000:  85%|████████▍ | 2540/3000 [19:03&lt;03:07,  2.45it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2541/3000:  85%|████████▍ | 2540/3000 [19:03&lt;03:07,  2.45it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2541/3000:  85%|████████▍ | 2541/3000 [19:04&lt;03:35,  2.13it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2541/3000:  85%|████████▍ | 2541/3000 [19:04&lt;03:35,  2.13it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2542/3000:  85%|████████▍ | 2541/3000 [19:04&lt;03:35,  2.13it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2542/3000:  85%|████████▍ | 2542/3000 [19:04&lt;03:32,  2.16it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2542/3000:  85%|████████▍ | 2542/3000 [19:04&lt;03:32,  2.16it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2543/3000:  85%|████████▍ | 2542/3000 [19:04&lt;03:32,  2.16it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2543/3000:  85%|████████▍ | 2543/3000 [19:05&lt;03:43,  2.05it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.05e+6]Epoch 2543/3000:  85%|████████▍ | 2543/3000 [19:05&lt;03:43,  2.05it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2544/3000:  85%|████████▍ | 2543/3000 [19:05&lt;03:43,  2.05it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2544/3000:  85%|████████▍ | 2544/3000 [19:05&lt;03:53,  1.95it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2544/3000:  85%|████████▍ | 2544/3000 [19:05&lt;03:53,  1.95it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2545/3000:  85%|████████▍ | 2544/3000 [19:05&lt;03:53,  1.95it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2545/3000:  85%|████████▍ | 2545/3000 [19:06&lt;03:59,  1.90it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2545/3000:  85%|████████▍ | 2545/3000 [19:06&lt;03:59,  1.90it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2546/3000:  85%|████████▍ | 2545/3000 [19:06&lt;03:59,  1.90it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2546/3000:  85%|████████▍ | 2546/3000 [19:06&lt;03:42,  2.04it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2546/3000:  85%|████████▍ | 2546/3000 [19:06&lt;03:42,  2.04it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2547/3000:  85%|████████▍ | 2546/3000 [19:06&lt;03:42,  2.04it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2547/3000:  85%|████████▍ | 2547/3000 [19:07&lt;03:52,  1.94it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2547/3000:  85%|████████▍ | 2547/3000 [19:07&lt;03:52,  1.94it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2548/3000:  85%|████████▍ | 2547/3000 [19:07&lt;03:52,  1.94it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2548/3000:  85%|████████▍ | 2548/3000 [19:07&lt;03:57,  1.91it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2548/3000:  85%|████████▍ | 2548/3000 [19:07&lt;03:57,  1.91it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2549/3000:  85%|████████▍ | 2548/3000 [19:07&lt;03:57,  1.91it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2549/3000:  85%|████████▍ | 2549/3000 [19:08&lt;03:39,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2549/3000:  85%|████████▍ | 2549/3000 [19:08&lt;03:39,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2550/3000:  85%|████████▍ | 2549/3000 [19:08&lt;03:39,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2550/3000:  85%|████████▌ | 2550/3000 [19:08&lt;03:17,  2.27it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2550/3000:  85%|████████▌ | 2550/3000 [19:08&lt;03:17,  2.27it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2551/3000:  85%|████████▌ | 2550/3000 [19:08&lt;03:17,  2.27it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2551/3000:  85%|████████▌ | 2551/3000 [19:08&lt;02:55,  2.56it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2551/3000:  85%|████████▌ | 2551/3000 [19:08&lt;02:55,  2.56it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2552/3000:  85%|████████▌ | 2551/3000 [19:08&lt;02:55,  2.56it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2552/3000:  85%|████████▌ | 2552/3000 [19:09&lt;02:47,  2.67it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2552/3000:  85%|████████▌ | 2552/3000 [19:09&lt;02:47,  2.67it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2553/3000:  85%|████████▌ | 2552/3000 [19:09&lt;02:47,  2.67it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2553/3000:  85%|████████▌ | 2553/3000 [19:09&lt;02:54,  2.56it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2553/3000:  85%|████████▌ | 2553/3000 [19:09&lt;02:54,  2.56it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2554/3000:  85%|████████▌ | 2553/3000 [19:09&lt;02:54,  2.56it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2554/3000:  85%|████████▌ | 2554/3000 [19:10&lt;03:16,  2.27it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2554/3000:  85%|████████▌ | 2554/3000 [19:10&lt;03:16,  2.27it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.05e+6]Epoch 2555/3000:  85%|████████▌ | 2554/3000 [19:10&lt;03:16,  2.27it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.05e+6]Epoch 2555/3000:  85%|████████▌ | 2555/3000 [19:10&lt;03:25,  2.17it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.05e+6]Epoch 2555/3000:  85%|████████▌ | 2555/3000 [19:10&lt;03:25,  2.17it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2556/3000:  85%|████████▌ | 2555/3000 [19:10&lt;03:25,  2.17it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2556/3000:  85%|████████▌ | 2556/3000 [19:11&lt;03:29,  2.12it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2556/3000:  85%|████████▌ | 2556/3000 [19:11&lt;03:29,  2.12it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2557/3000:  85%|████████▌ | 2556/3000 [19:11&lt;03:29,  2.12it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2557/3000:  85%|████████▌ | 2557/3000 [19:11&lt;03:18,  2.24it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2557/3000:  85%|████████▌ | 2557/3000 [19:11&lt;03:18,  2.24it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2558/3000:  85%|████████▌ | 2557/3000 [19:11&lt;03:18,  2.24it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2558/3000:  85%|████████▌ | 2558/3000 [19:11&lt;03:02,  2.42it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2558/3000:  85%|████████▌ | 2558/3000 [19:11&lt;03:02,  2.42it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.05e+6]Epoch 2559/3000:  85%|████████▌ | 2558/3000 [19:11&lt;03:02,  2.42it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.05e+6]Epoch 2559/3000:  85%|████████▌ | 2559/3000 [19:12&lt;03:28,  2.12it/s, v_num=1, train_loss_step=1.08e+6, train_loss_epoch=1.05e+6]Epoch 2559/3000:  85%|████████▌ | 2559/3000 [19:12&lt;03:28,  2.12it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2560/3000:  85%|████████▌ | 2559/3000 [19:12&lt;03:28,  2.12it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2560/3000:  85%|████████▌ | 2560/3000 [19:12&lt;03:22,  2.17it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2560/3000:  85%|████████▌ | 2560/3000 [19:12&lt;03:22,  2.17it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2561/3000:  85%|████████▌ | 2560/3000 [19:12&lt;03:22,  2.17it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2561/3000:  85%|████████▌ | 2561/3000 [19:13&lt;03:24,  2.15it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2561/3000:  85%|████████▌ | 2561/3000 [19:13&lt;03:24,  2.15it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2562/3000:  85%|████████▌ | 2561/3000 [19:13&lt;03:24,  2.15it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2562/3000:  85%|████████▌ | 2562/3000 [19:13&lt;03:13,  2.26it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2562/3000:  85%|████████▌ | 2562/3000 [19:13&lt;03:13,  2.26it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2563/3000:  85%|████████▌ | 2562/3000 [19:13&lt;03:13,  2.26it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2563/3000:  85%|████████▌ | 2563/3000 [19:14&lt;03:20,  2.18it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2563/3000:  85%|████████▌ | 2563/3000 [19:14&lt;03:20,  2.18it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2564/3000:  85%|████████▌ | 2563/3000 [19:14&lt;03:20,  2.18it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2564/3000:  85%|████████▌ | 2564/3000 [19:14&lt;03:46,  1.93it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2564/3000:  85%|████████▌ | 2564/3000 [19:14&lt;03:46,  1.93it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2565/3000:  85%|████████▌ | 2564/3000 [19:14&lt;03:46,  1.93it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2565/3000:  86%|████████▌ | 2565/3000 [19:15&lt;03:36,  2.01it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2565/3000:  86%|████████▌ | 2565/3000 [19:15&lt;03:36,  2.01it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2566/3000:  86%|████████▌ | 2565/3000 [19:15&lt;03:36,  2.01it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2566/3000:  86%|████████▌ | 2566/3000 [19:15&lt;03:49,  1.89it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2566/3000:  86%|████████▌ | 2566/3000 [19:15&lt;03:49,  1.89it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2567/3000:  86%|████████▌ | 2566/3000 [19:15&lt;03:49,  1.89it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2567/3000:  86%|████████▌ | 2567/3000 [19:16&lt;03:01,  2.39it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2567/3000:  86%|████████▌ | 2567/3000 [19:16&lt;03:01,  2.39it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2568/3000:  86%|████████▌ | 2567/3000 [19:16&lt;03:01,  2.39it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2568/3000:  86%|████████▌ | 2568/3000 [19:16&lt;03:07,  2.30it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2568/3000:  86%|████████▌ | 2568/3000 [19:16&lt;03:07,  2.30it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2569/3000:  86%|████████▌ | 2568/3000 [19:16&lt;03:07,  2.30it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2569/3000:  86%|████████▌ | 2569/3000 [19:17&lt;03:15,  2.21it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2569/3000:  86%|████████▌ | 2569/3000 [19:17&lt;03:15,  2.21it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2570/3000:  86%|████████▌ | 2569/3000 [19:17&lt;03:15,  2.21it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2570/3000:  86%|████████▌ | 2570/3000 [19:17&lt;03:14,  2.22it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2570/3000:  86%|████████▌ | 2570/3000 [19:17&lt;03:14,  2.22it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2571/3000:  86%|████████▌ | 2570/3000 [19:17&lt;03:14,  2.22it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2571/3000:  86%|████████▌ | 2571/3000 [19:18&lt;03:28,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2571/3000:  86%|████████▌ | 2571/3000 [19:18&lt;03:28,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2572/3000:  86%|████████▌ | 2571/3000 [19:18&lt;03:28,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2572/3000:  86%|████████▌ | 2572/3000 [19:18&lt;03:28,  2.05it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2572/3000:  86%|████████▌ | 2572/3000 [19:18&lt;03:28,  2.05it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2573/3000:  86%|████████▌ | 2572/3000 [19:18&lt;03:28,  2.05it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2573/3000:  86%|████████▌ | 2573/3000 [19:19&lt;03:34,  1.99it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.05e+6]Epoch 2573/3000:  86%|████████▌ | 2573/3000 [19:19&lt;03:34,  1.99it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2574/3000:  86%|████████▌ | 2573/3000 [19:19&lt;03:34,  1.99it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2574/3000:  86%|████████▌ | 2574/3000 [19:19&lt;03:41,  1.92it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.05e+6]Epoch 2574/3000:  86%|████████▌ | 2574/3000 [19:19&lt;03:41,  1.92it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2575/3000:  86%|████████▌ | 2574/3000 [19:19&lt;03:41,  1.92it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2575/3000:  86%|████████▌ | 2575/3000 [19:20&lt;03:20,  2.12it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2575/3000:  86%|████████▌ | 2575/3000 [19:20&lt;03:20,  2.12it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2576/3000:  86%|████████▌ | 2575/3000 [19:20&lt;03:20,  2.12it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2576/3000:  86%|████████▌ | 2576/3000 [19:20&lt;03:31,  2.01it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.05e+6]Epoch 2576/3000:  86%|████████▌ | 2576/3000 [19:20&lt;03:31,  2.01it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2577/3000:  86%|████████▌ | 2576/3000 [19:20&lt;03:31,  2.01it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2577/3000:  86%|████████▌ | 2577/3000 [19:21&lt;03:27,  2.04it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.05e+6]Epoch 2577/3000:  86%|████████▌ | 2577/3000 [19:21&lt;03:27,  2.04it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2578/3000:  86%|████████▌ | 2577/3000 [19:21&lt;03:27,  2.04it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2578/3000:  86%|████████▌ | 2578/3000 [19:21&lt;03:20,  2.10it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2578/3000:  86%|████████▌ | 2578/3000 [19:21&lt;03:20,  2.10it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2579/3000:  86%|████████▌ | 2578/3000 [19:21&lt;03:20,  2.10it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2579/3000:  86%|████████▌ | 2579/3000 [19:21&lt;03:06,  2.26it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2579/3000:  86%|████████▌ | 2579/3000 [19:21&lt;03:06,  2.26it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2580/3000:  86%|████████▌ | 2579/3000 [19:21&lt;03:06,  2.26it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2580/3000:  86%|████████▌ | 2580/3000 [19:22&lt;03:03,  2.28it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2580/3000:  86%|████████▌ | 2580/3000 [19:22&lt;03:03,  2.28it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2581/3000:  86%|████████▌ | 2580/3000 [19:22&lt;03:03,  2.28it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2581/3000:  86%|████████▌ | 2581/3000 [19:22&lt;03:22,  2.06it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2581/3000:  86%|████████▌ | 2581/3000 [19:22&lt;03:22,  2.06it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2582/3000:  86%|████████▌ | 2581/3000 [19:22&lt;03:22,  2.06it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2582/3000:  86%|████████▌ | 2582/3000 [19:23&lt;03:24,  2.05it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2582/3000:  86%|████████▌ | 2582/3000 [19:23&lt;03:24,  2.05it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2583/3000:  86%|████████▌ | 2582/3000 [19:23&lt;03:24,  2.05it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2583/3000:  86%|████████▌ | 2583/3000 [19:23&lt;03:10,  2.19it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2583/3000:  86%|████████▌ | 2583/3000 [19:23&lt;03:10,  2.19it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2584/3000:  86%|████████▌ | 2583/3000 [19:23&lt;03:10,  2.19it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2584/3000:  86%|████████▌ | 2584/3000 [19:24&lt;03:21,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2584/3000:  86%|████████▌ | 2584/3000 [19:24&lt;03:21,  2.06it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2585/3000:  86%|████████▌ | 2584/3000 [19:24&lt;03:21,  2.06it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2585/3000:  86%|████████▌ | 2585/3000 [19:24&lt;03:20,  2.07it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2585/3000:  86%|████████▌ | 2585/3000 [19:24&lt;03:20,  2.07it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2586/3000:  86%|████████▌ | 2585/3000 [19:24&lt;03:20,  2.07it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2586/3000:  86%|████████▌ | 2586/3000 [19:25&lt;03:32,  1.94it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2586/3000:  86%|████████▌ | 2586/3000 [19:25&lt;03:32,  1.94it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2587/3000:  86%|████████▌ | 2586/3000 [19:25&lt;03:32,  1.94it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2587/3000:  86%|████████▌ | 2587/3000 [19:25&lt;03:23,  2.03it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2587/3000:  86%|████████▌ | 2587/3000 [19:25&lt;03:23,  2.03it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2588/3000:  86%|████████▌ | 2587/3000 [19:25&lt;03:23,  2.03it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2588/3000:  86%|████████▋ | 2588/3000 [19:26&lt;03:30,  1.96it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2588/3000:  86%|████████▋ | 2588/3000 [19:26&lt;03:30,  1.96it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2589/3000:  86%|████████▋ | 2588/3000 [19:26&lt;03:30,  1.96it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2589/3000:  86%|████████▋ | 2589/3000 [19:26&lt;03:26,  1.99it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2589/3000:  86%|████████▋ | 2589/3000 [19:26&lt;03:26,  1.99it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2590/3000:  86%|████████▋ | 2589/3000 [19:26&lt;03:26,  1.99it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2590/3000:  86%|████████▋ | 2590/3000 [19:27&lt;02:55,  2.34it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2590/3000:  86%|████████▋ | 2590/3000 [19:27&lt;02:55,  2.34it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2591/3000:  86%|████████▋ | 2590/3000 [19:27&lt;02:55,  2.34it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2591/3000:  86%|████████▋ | 2591/3000 [19:27&lt;02:45,  2.47it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2591/3000:  86%|████████▋ | 2591/3000 [19:27&lt;02:45,  2.47it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2592/3000:  86%|████████▋ | 2591/3000 [19:27&lt;02:45,  2.47it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2592/3000:  86%|████████▋ | 2592/3000 [19:27&lt;02:50,  2.39it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2592/3000:  86%|████████▋ | 2592/3000 [19:27&lt;02:50,  2.39it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2593/3000:  86%|████████▋ | 2592/3000 [19:27&lt;02:50,  2.39it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2593/3000:  86%|████████▋ | 2593/3000 [19:28&lt;02:39,  2.55it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2593/3000:  86%|████████▋ | 2593/3000 [19:28&lt;02:39,  2.55it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2594/3000:  86%|████████▋ | 2593/3000 [19:28&lt;02:39,  2.55it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2594/3000:  86%|████████▋ | 2594/3000 [19:28&lt;02:38,  2.56it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2594/3000:  86%|████████▋ | 2594/3000 [19:28&lt;02:38,  2.56it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2595/3000:  86%|████████▋ | 2594/3000 [19:28&lt;02:38,  2.56it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2595/3000:  86%|████████▋ | 2595/3000 [19:29&lt;02:42,  2.49it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2595/3000:  86%|████████▋ | 2595/3000 [19:29&lt;02:42,  2.49it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2596/3000:  86%|████████▋ | 2595/3000 [19:29&lt;02:42,  2.49it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2596/3000:  87%|████████▋ | 2596/3000 [19:29&lt;03:00,  2.24it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2596/3000:  87%|████████▋ | 2596/3000 [19:29&lt;03:00,  2.24it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2597/3000:  87%|████████▋ | 2596/3000 [19:29&lt;03:00,  2.24it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2597/3000:  87%|████████▋ | 2597/3000 [19:29&lt;02:38,  2.54it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2597/3000:  87%|████████▋ | 2597/3000 [19:29&lt;02:38,  2.54it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2598/3000:  87%|████████▋ | 2597/3000 [19:29&lt;02:38,  2.54it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2598/3000:  87%|████████▋ | 2598/3000 [19:30&lt;02:44,  2.45it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2598/3000:  87%|████████▋ | 2598/3000 [19:30&lt;02:44,  2.45it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2599/3000:  87%|████████▋ | 2598/3000 [19:30&lt;02:44,  2.45it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2599/3000:  87%|████████▋ | 2599/3000 [19:30&lt;02:42,  2.47it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2599/3000:  87%|████████▋ | 2599/3000 [19:30&lt;02:42,  2.47it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2600/3000:  87%|████████▋ | 2599/3000 [19:30&lt;02:42,  2.47it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2600/3000:  87%|████████▋ | 2600/3000 [19:31&lt;03:05,  2.16it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2600/3000:  87%|████████▋ | 2600/3000 [19:31&lt;03:05,  2.16it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2601/3000:  87%|████████▋ | 2600/3000 [19:31&lt;03:05,  2.16it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2601/3000:  87%|████████▋ | 2601/3000 [19:31&lt;03:03,  2.17it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2601/3000:  87%|████████▋ | 2601/3000 [19:31&lt;03:03,  2.17it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2602/3000:  87%|████████▋ | 2601/3000 [19:31&lt;03:03,  2.17it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2602/3000:  87%|████████▋ | 2602/3000 [19:32&lt;03:09,  2.10it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2602/3000:  87%|████████▋ | 2602/3000 [19:32&lt;03:09,  2.10it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2603/3000:  87%|████████▋ | 2602/3000 [19:32&lt;03:09,  2.10it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2603/3000:  87%|████████▋ | 2603/3000 [19:32&lt;02:56,  2.25it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2603/3000:  87%|████████▋ | 2603/3000 [19:32&lt;02:56,  2.25it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2604/3000:  87%|████████▋ | 2603/3000 [19:32&lt;02:56,  2.25it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2604/3000:  87%|████████▋ | 2604/3000 [19:33&lt;03:05,  2.13it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2604/3000:  87%|████████▋ | 2604/3000 [19:33&lt;03:05,  2.13it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2605/3000:  87%|████████▋ | 2604/3000 [19:33&lt;03:05,  2.13it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2605/3000:  87%|████████▋ | 2605/3000 [19:33&lt;03:04,  2.14it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2605/3000:  87%|████████▋ | 2605/3000 [19:33&lt;03:04,  2.14it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2606/3000:  87%|████████▋ | 2605/3000 [19:33&lt;03:04,  2.14it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2606/3000:  87%|████████▋ | 2606/3000 [19:33&lt;02:36,  2.52it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2606/3000:  87%|████████▋ | 2606/3000 [19:33&lt;02:36,  2.52it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2607/3000:  87%|████████▋ | 2606/3000 [19:33&lt;02:36,  2.52it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2607/3000:  87%|████████▋ | 2607/3000 [19:34&lt;02:21,  2.77it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2607/3000:  87%|████████▋ | 2607/3000 [19:34&lt;02:21,  2.77it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2608/3000:  87%|████████▋ | 2607/3000 [19:34&lt;02:21,  2.77it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2608/3000:  87%|████████▋ | 2608/3000 [19:34&lt;02:26,  2.68it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2608/3000:  87%|████████▋ | 2608/3000 [19:34&lt;02:26,  2.68it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2609/3000:  87%|████████▋ | 2608/3000 [19:34&lt;02:26,  2.68it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2609/3000:  87%|████████▋ | 2609/3000 [19:35&lt;02:39,  2.46it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2609/3000:  87%|████████▋ | 2609/3000 [19:35&lt;02:39,  2.46it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2610/3000:  87%|████████▋ | 2609/3000 [19:35&lt;02:39,  2.46it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2610/3000:  87%|████████▋ | 2610/3000 [19:35&lt;02:44,  2.37it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2610/3000:  87%|████████▋ | 2610/3000 [19:35&lt;02:44,  2.37it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2611/3000:  87%|████████▋ | 2610/3000 [19:35&lt;02:44,  2.37it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2611/3000:  87%|████████▋ | 2611/3000 [19:36&lt;03:00,  2.16it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2611/3000:  87%|████████▋ | 2611/3000 [19:36&lt;03:00,  2.16it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2612/3000:  87%|████████▋ | 2611/3000 [19:36&lt;03:00,  2.16it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2612/3000:  87%|████████▋ | 2612/3000 [19:36&lt;03:33,  1.81it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2612/3000:  87%|████████▋ | 2612/3000 [19:36&lt;03:33,  1.81it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2613/3000:  87%|████████▋ | 2612/3000 [19:36&lt;03:33,  1.81it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2613/3000:  87%|████████▋ | 2613/3000 [19:37&lt;03:51,  1.67it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2613/3000:  87%|████████▋ | 2613/3000 [19:37&lt;03:51,  1.67it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2614/3000:  87%|████████▋ | 2613/3000 [19:37&lt;03:51,  1.67it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2614/3000:  87%|████████▋ | 2614/3000 [19:38&lt;03:48,  1.69it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2614/3000:  87%|████████▋ | 2614/3000 [19:38&lt;03:48,  1.69it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2615/3000:  87%|████████▋ | 2614/3000 [19:38&lt;03:48,  1.69it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2615/3000:  87%|████████▋ | 2615/3000 [19:38&lt;03:44,  1.72it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2615/3000:  87%|████████▋ | 2615/3000 [19:38&lt;03:44,  1.72it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2616/3000:  87%|████████▋ | 2615/3000 [19:38&lt;03:44,  1.72it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2616/3000:  87%|████████▋ | 2616/3000 [19:39&lt;03:27,  1.85it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2616/3000:  87%|████████▋ | 2616/3000 [19:39&lt;03:27,  1.85it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2617/3000:  87%|████████▋ | 2616/3000 [19:39&lt;03:27,  1.85it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2617/3000:  87%|████████▋ | 2617/3000 [19:39&lt;03:24,  1.88it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2617/3000:  87%|████████▋ | 2617/3000 [19:39&lt;03:24,  1.88it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2618/3000:  87%|████████▋ | 2617/3000 [19:39&lt;03:24,  1.88it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2618/3000:  87%|████████▋ | 2618/3000 [19:40&lt;03:31,  1.80it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2618/3000:  87%|████████▋ | 2618/3000 [19:40&lt;03:31,  1.80it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2619/3000:  87%|████████▋ | 2618/3000 [19:40&lt;03:31,  1.80it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2619/3000:  87%|████████▋ | 2619/3000 [19:40&lt;02:42,  2.35it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2619/3000:  87%|████████▋ | 2619/3000 [19:40&lt;02:42,  2.35it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2620/3000:  87%|████████▋ | 2619/3000 [19:40&lt;02:42,  2.35it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2620/3000:  87%|████████▋ | 2620/3000 [19:40&lt;02:57,  2.15it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2620/3000:  87%|████████▋ | 2620/3000 [19:40&lt;02:57,  2.15it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2621/3000:  87%|████████▋ | 2620/3000 [19:40&lt;02:57,  2.15it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2621/3000:  87%|████████▋ | 2621/3000 [19:41&lt;02:45,  2.29it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2621/3000:  87%|████████▋ | 2621/3000 [19:41&lt;02:45,  2.29it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2622/3000:  87%|████████▋ | 2621/3000 [19:41&lt;02:45,  2.29it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2622/3000:  87%|████████▋ | 2622/3000 [19:41&lt;02:49,  2.24it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2622/3000:  87%|████████▋ | 2622/3000 [19:41&lt;02:49,  2.24it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2623/3000:  87%|████████▋ | 2622/3000 [19:41&lt;02:49,  2.24it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2623/3000:  87%|████████▋ | 2623/3000 [19:42&lt;03:18,  1.90it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2623/3000:  87%|████████▋ | 2623/3000 [19:42&lt;03:18,  1.90it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2624/3000:  87%|████████▋ | 2623/3000 [19:42&lt;03:18,  1.90it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2624/3000:  87%|████████▋ | 2624/3000 [19:43&lt;03:28,  1.80it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2624/3000:  87%|████████▋ | 2624/3000 [19:43&lt;03:28,  1.80it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2625/3000:  87%|████████▋ | 2624/3000 [19:43&lt;03:28,  1.80it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2625/3000:  88%|████████▊ | 2625/3000 [19:43&lt;03:25,  1.82it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2625/3000:  88%|████████▊ | 2625/3000 [19:43&lt;03:25,  1.82it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2626/3000:  88%|████████▊ | 2625/3000 [19:43&lt;03:25,  1.82it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2626/3000:  88%|████████▊ | 2626/3000 [19:44&lt;03:15,  1.92it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2626/3000:  88%|████████▊ | 2626/3000 [19:44&lt;03:15,  1.92it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2627/3000:  88%|████████▊ | 2626/3000 [19:44&lt;03:15,  1.92it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2627/3000:  88%|████████▊ | 2627/3000 [19:44&lt;03:23,  1.83it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2627/3000:  88%|████████▊ | 2627/3000 [19:44&lt;03:23,  1.83it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2628/3000:  88%|████████▊ | 2627/3000 [19:44&lt;03:23,  1.83it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2628/3000:  88%|████████▊ | 2628/3000 [19:45&lt;03:13,  1.92it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2628/3000:  88%|████████▊ | 2628/3000 [19:45&lt;03:13,  1.92it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2629/3000:  88%|████████▊ | 2628/3000 [19:45&lt;03:13,  1.92it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2629/3000:  88%|████████▊ | 2629/3000 [19:45&lt;02:59,  2.07it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2629/3000:  88%|████████▊ | 2629/3000 [19:45&lt;02:59,  2.07it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2630/3000:  88%|████████▊ | 2629/3000 [19:45&lt;02:59,  2.07it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2630/3000:  88%|████████▊ | 2630/3000 [19:46&lt;03:13,  1.91it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2630/3000:  88%|████████▊ | 2630/3000 [19:46&lt;03:13,  1.91it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2631/3000:  88%|████████▊ | 2630/3000 [19:46&lt;03:13,  1.91it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2631/3000:  88%|████████▊ | 2631/3000 [19:46&lt;03:17,  1.87it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2631/3000:  88%|████████▊ | 2631/3000 [19:46&lt;03:17,  1.87it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2632/3000:  88%|████████▊ | 2631/3000 [19:46&lt;03:17,  1.87it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2632/3000:  88%|████████▊ | 2632/3000 [19:47&lt;02:56,  2.09it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2632/3000:  88%|████████▊ | 2632/3000 [19:47&lt;02:56,  2.09it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2633/3000:  88%|████████▊ | 2632/3000 [19:47&lt;02:56,  2.09it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2633/3000:  88%|████████▊ | 2633/3000 [19:47&lt;02:44,  2.23it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2633/3000:  88%|████████▊ | 2633/3000 [19:47&lt;02:44,  2.23it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2634/3000:  88%|████████▊ | 2633/3000 [19:47&lt;02:44,  2.23it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2634/3000:  88%|████████▊ | 2634/3000 [19:47&lt;02:49,  2.16it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2634/3000:  88%|████████▊ | 2634/3000 [19:47&lt;02:49,  2.16it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2635/3000:  88%|████████▊ | 2634/3000 [19:47&lt;02:49,  2.16it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2635/3000:  88%|████████▊ | 2635/3000 [19:48&lt;02:39,  2.29it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2635/3000:  88%|████████▊ | 2635/3000 [19:48&lt;02:39,  2.29it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2636/3000:  88%|████████▊ | 2635/3000 [19:48&lt;02:39,  2.29it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2636/3000:  88%|████████▊ | 2636/3000 [19:48&lt;02:42,  2.24it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2636/3000:  88%|████████▊ | 2636/3000 [19:48&lt;02:42,  2.24it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2637/3000:  88%|████████▊ | 2636/3000 [19:48&lt;02:42,  2.24it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2637/3000:  88%|████████▊ | 2637/3000 [19:49&lt;02:18,  2.63it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2637/3000:  88%|████████▊ | 2637/3000 [19:49&lt;02:18,  2.63it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2638/3000:  88%|████████▊ | 2637/3000 [19:49&lt;02:18,  2.63it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2638/3000:  88%|████████▊ | 2638/3000 [19:49&lt;02:23,  2.52it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2638/3000:  88%|████████▊ | 2638/3000 [19:49&lt;02:23,  2.52it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2639/3000:  88%|████████▊ | 2638/3000 [19:49&lt;02:23,  2.52it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2639/3000:  88%|████████▊ | 2639/3000 [19:50&lt;02:47,  2.16it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2639/3000:  88%|████████▊ | 2639/3000 [19:50&lt;02:47,  2.16it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2640/3000:  88%|████████▊ | 2639/3000 [19:50&lt;02:47,  2.16it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2640/3000:  88%|████████▊ | 2640/3000 [19:50&lt;02:39,  2.25it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2640/3000:  88%|████████▊ | 2640/3000 [19:50&lt;02:39,  2.25it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2641/3000:  88%|████████▊ | 2640/3000 [19:50&lt;02:39,  2.25it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2641/3000:  88%|████████▊ | 2641/3000 [19:50&lt;02:44,  2.18it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2641/3000:  88%|████████▊ | 2641/3000 [19:50&lt;02:44,  2.18it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2642/3000:  88%|████████▊ | 2641/3000 [19:50&lt;02:44,  2.18it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2642/3000:  88%|████████▊ | 2642/3000 [19:51&lt;03:06,  1.92it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2642/3000:  88%|████████▊ | 2642/3000 [19:51&lt;03:06,  1.92it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2643/3000:  88%|████████▊ | 2642/3000 [19:51&lt;03:06,  1.92it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2643/3000:  88%|████████▊ | 2643/3000 [19:52&lt;02:54,  2.04it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2643/3000:  88%|████████▊ | 2643/3000 [19:52&lt;02:54,  2.04it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2644/3000:  88%|████████▊ | 2643/3000 [19:52&lt;02:54,  2.04it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2644/3000:  88%|████████▊ | 2644/3000 [19:52&lt;03:01,  1.96it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2644/3000:  88%|████████▊ | 2644/3000 [19:52&lt;03:01,  1.96it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2645/3000:  88%|████████▊ | 2644/3000 [19:52&lt;03:01,  1.96it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2645/3000:  88%|████████▊ | 2645/3000 [19:53&lt;03:21,  1.76it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2645/3000:  88%|████████▊ | 2645/3000 [19:53&lt;03:21,  1.76it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2646/3000:  88%|████████▊ | 2645/3000 [19:53&lt;03:21,  1.76it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2646/3000:  88%|████████▊ | 2646/3000 [19:53&lt;03:31,  1.68it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2646/3000:  88%|████████▊ | 2646/3000 [19:53&lt;03:31,  1.68it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2647/3000:  88%|████████▊ | 2646/3000 [19:53&lt;03:31,  1.68it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2647/3000:  88%|████████▊ | 2647/3000 [19:54&lt;02:54,  2.02it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2647/3000:  88%|████████▊ | 2647/3000 [19:54&lt;02:54,  2.02it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2648/3000:  88%|████████▊ | 2647/3000 [19:54&lt;02:54,  2.02it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2648/3000:  88%|████████▊ | 2648/3000 [19:54&lt;02:58,  1.97it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2648/3000:  88%|████████▊ | 2648/3000 [19:54&lt;02:58,  1.97it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2649/3000:  88%|████████▊ | 2648/3000 [19:54&lt;02:58,  1.97it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2649/3000:  88%|████████▊ | 2649/3000 [19:55&lt;02:45,  2.12it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2649/3000:  88%|████████▊ | 2649/3000 [19:55&lt;02:45,  2.12it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2650/3000:  88%|████████▊ | 2649/3000 [19:55&lt;02:45,  2.12it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2650/3000:  88%|████████▊ | 2650/3000 [19:55&lt;02:37,  2.23it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2650/3000:  88%|████████▊ | 2650/3000 [19:55&lt;02:37,  2.23it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2651/3000:  88%|████████▊ | 2650/3000 [19:55&lt;02:37,  2.23it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2651/3000:  88%|████████▊ | 2651/3000 [19:55&lt;02:23,  2.43it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2651/3000:  88%|████████▊ | 2651/3000 [19:55&lt;02:23,  2.43it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2652/3000:  88%|████████▊ | 2651/3000 [19:55&lt;02:23,  2.43it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2652/3000:  88%|████████▊ | 2652/3000 [19:56&lt;02:33,  2.27it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2652/3000:  88%|████████▊ | 2652/3000 [19:56&lt;02:33,  2.27it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2653/3000:  88%|████████▊ | 2652/3000 [19:56&lt;02:33,  2.27it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2653/3000:  88%|████████▊ | 2653/3000 [19:56&lt;02:20,  2.47it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2653/3000:  88%|████████▊ | 2653/3000 [19:56&lt;02:20,  2.47it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2654/3000:  88%|████████▊ | 2653/3000 [19:56&lt;02:20,  2.47it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2654/3000:  88%|████████▊ | 2654/3000 [19:57&lt;02:24,  2.39it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2654/3000:  88%|████████▊ | 2654/3000 [19:57&lt;02:24,  2.39it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2655/3000:  88%|████████▊ | 2654/3000 [19:57&lt;02:24,  2.39it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2655/3000:  88%|████████▊ | 2655/3000 [19:57&lt;02:14,  2.57it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2655/3000:  88%|████████▊ | 2655/3000 [19:57&lt;02:14,  2.57it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2656/3000:  88%|████████▊ | 2655/3000 [19:57&lt;02:14,  2.57it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2656/3000:  89%|████████▊ | 2656/3000 [19:57&lt;02:02,  2.81it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2656/3000:  89%|████████▊ | 2656/3000 [19:57&lt;02:02,  2.81it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2657/3000:  89%|████████▊ | 2656/3000 [19:57&lt;02:02,  2.81it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2657/3000:  89%|████████▊ | 2657/3000 [19:57&lt;01:49,  3.13it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2657/3000:  89%|████████▊ | 2657/3000 [19:57&lt;01:49,  3.13it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2658/3000:  89%|████████▊ | 2657/3000 [19:57&lt;01:49,  3.13it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2658/3000:  89%|████████▊ | 2658/3000 [19:58&lt;02:05,  2.73it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2658/3000:  89%|████████▊ | 2658/3000 [19:58&lt;02:05,  2.73it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2659/3000:  89%|████████▊ | 2658/3000 [19:58&lt;02:05,  2.73it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2659/3000:  89%|████████▊ | 2659/3000 [19:58&lt;02:20,  2.42it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2659/3000:  89%|████████▊ | 2659/3000 [19:58&lt;02:20,  2.42it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2660/3000:  89%|████████▊ | 2659/3000 [19:58&lt;02:20,  2.42it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2660/3000:  89%|████████▊ | 2660/3000 [19:59&lt;02:09,  2.64it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2660/3000:  89%|████████▊ | 2660/3000 [19:59&lt;02:09,  2.64it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2661/3000:  89%|████████▊ | 2660/3000 [19:59&lt;02:09,  2.64it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2661/3000:  89%|████████▊ | 2661/3000 [19:59&lt;02:02,  2.77it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2661/3000:  89%|████████▊ | 2661/3000 [19:59&lt;02:02,  2.77it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2662/3000:  89%|████████▊ | 2661/3000 [19:59&lt;02:02,  2.77it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2662/3000:  89%|████████▊ | 2662/3000 [20:00&lt;02:09,  2.61it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2662/3000:  89%|████████▊ | 2662/3000 [20:00&lt;02:09,  2.61it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2663/3000:  89%|████████▊ | 2662/3000 [20:00&lt;02:09,  2.61it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2663/3000:  89%|████████▉ | 2663/3000 [20:00&lt;02:10,  2.58it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2663/3000:  89%|████████▉ | 2663/3000 [20:00&lt;02:10,  2.58it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.04e+6]Epoch 2664/3000:  89%|████████▉ | 2663/3000 [20:00&lt;02:10,  2.58it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.04e+6]Epoch 2664/3000:  89%|████████▉ | 2664/3000 [20:00&lt;02:26,  2.30it/s, v_num=1, train_loss_step=1.07e+6, train_loss_epoch=1.04e+6]Epoch 2664/3000:  89%|████████▉ | 2664/3000 [20:00&lt;02:26,  2.30it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2665/3000:  89%|████████▉ | 2664/3000 [20:00&lt;02:26,  2.30it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2665/3000:  89%|████████▉ | 2665/3000 [20:01&lt;02:30,  2.23it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2665/3000:  89%|████████▉ | 2665/3000 [20:01&lt;02:30,  2.23it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2666/3000:  89%|████████▉ | 2665/3000 [20:01&lt;02:30,  2.23it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2666/3000:  89%|████████▉ | 2666/3000 [20:01&lt;02:31,  2.21it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2666/3000:  89%|████████▉ | 2666/3000 [20:01&lt;02:31,  2.21it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2667/3000:  89%|████████▉ | 2666/3000 [20:01&lt;02:31,  2.21it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2667/3000:  89%|████████▉ | 2667/3000 [20:02&lt;02:42,  2.05it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2667/3000:  89%|████████▉ | 2667/3000 [20:02&lt;02:42,  2.05it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2668/3000:  89%|████████▉ | 2667/3000 [20:02&lt;02:42,  2.05it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2668/3000:  89%|████████▉ | 2668/3000 [20:02&lt;02:41,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2668/3000:  89%|████████▉ | 2668/3000 [20:02&lt;02:41,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2669/3000:  89%|████████▉ | 2668/3000 [20:02&lt;02:41,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2669/3000:  89%|████████▉ | 2669/3000 [20:03&lt;02:20,  2.36it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2669/3000:  89%|████████▉ | 2669/3000 [20:03&lt;02:20,  2.36it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2670/3000:  89%|████████▉ | 2669/3000 [20:03&lt;02:20,  2.36it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2670/3000:  89%|████████▉ | 2670/3000 [20:03&lt;02:02,  2.69it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2670/3000:  89%|████████▉ | 2670/3000 [20:03&lt;02:02,  2.69it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2671/3000:  89%|████████▉ | 2670/3000 [20:03&lt;02:02,  2.69it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2671/3000:  89%|████████▉ | 2671/3000 [20:03&lt;01:56,  2.82it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2671/3000:  89%|████████▉ | 2671/3000 [20:03&lt;01:56,  2.82it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2672/3000:  89%|████████▉ | 2671/3000 [20:03&lt;01:56,  2.82it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2672/3000:  89%|████████▉ | 2672/3000 [20:04&lt;02:22,  2.31it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2672/3000:  89%|████████▉ | 2672/3000 [20:04&lt;02:22,  2.31it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2673/3000:  89%|████████▉ | 2672/3000 [20:04&lt;02:22,  2.31it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2673/3000:  89%|████████▉ | 2673/3000 [20:05&lt;02:41,  2.02it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2673/3000:  89%|████████▉ | 2673/3000 [20:05&lt;02:41,  2.02it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2674/3000:  89%|████████▉ | 2673/3000 [20:05&lt;02:41,  2.02it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2674/3000:  89%|████████▉ | 2674/3000 [20:05&lt;02:36,  2.08it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2674/3000:  89%|████████▉ | 2674/3000 [20:05&lt;02:36,  2.08it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2675/3000:  89%|████████▉ | 2674/3000 [20:05&lt;02:36,  2.08it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2675/3000:  89%|████████▉ | 2675/3000 [20:06&lt;02:43,  1.98it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2675/3000:  89%|████████▉ | 2675/3000 [20:06&lt;02:43,  1.98it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2676/3000:  89%|████████▉ | 2675/3000 [20:06&lt;02:43,  1.98it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2676/3000:  89%|████████▉ | 2676/3000 [20:06&lt;02:53,  1.87it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2676/3000:  89%|████████▉ | 2676/3000 [20:06&lt;02:53,  1.87it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2677/3000:  89%|████████▉ | 2676/3000 [20:06&lt;02:53,  1.87it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2677/3000:  89%|████████▉ | 2677/3000 [20:07&lt;02:45,  1.95it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2677/3000:  89%|████████▉ | 2677/3000 [20:07&lt;02:45,  1.95it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2678/3000:  89%|████████▉ | 2677/3000 [20:07&lt;02:45,  1.95it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2678/3000:  89%|████████▉ | 2678/3000 [20:07&lt;02:36,  2.06it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2678/3000:  89%|████████▉ | 2678/3000 [20:07&lt;02:36,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2679/3000:  89%|████████▉ | 2678/3000 [20:07&lt;02:36,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2679/3000:  89%|████████▉ | 2679/3000 [20:08&lt;02:38,  2.03it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2679/3000:  89%|████████▉ | 2679/3000 [20:08&lt;02:38,  2.03it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2680/3000:  89%|████████▉ | 2679/3000 [20:08&lt;02:38,  2.03it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2680/3000:  89%|████████▉ | 2680/3000 [20:08&lt;02:30,  2.12it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2680/3000:  89%|████████▉ | 2680/3000 [20:08&lt;02:30,  2.12it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2681/3000:  89%|████████▉ | 2680/3000 [20:08&lt;02:30,  2.12it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2681/3000:  89%|████████▉ | 2681/3000 [20:08&lt;02:19,  2.28it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2681/3000:  89%|████████▉ | 2681/3000 [20:08&lt;02:19,  2.28it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2682/3000:  89%|████████▉ | 2681/3000 [20:08&lt;02:19,  2.28it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2682/3000:  89%|████████▉ | 2682/3000 [20:09&lt;02:28,  2.15it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2682/3000:  89%|████████▉ | 2682/3000 [20:09&lt;02:28,  2.15it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2683/3000:  89%|████████▉ | 2682/3000 [20:09&lt;02:28,  2.15it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2683/3000:  89%|████████▉ | 2683/3000 [20:09&lt;02:24,  2.20it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2683/3000:  89%|████████▉ | 2683/3000 [20:09&lt;02:24,  2.20it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2684/3000:  89%|████████▉ | 2683/3000 [20:09&lt;02:24,  2.20it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2684/3000:  89%|████████▉ | 2684/3000 [20:10&lt;02:07,  2.48it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2684/3000:  89%|████████▉ | 2684/3000 [20:10&lt;02:07,  2.48it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2685/3000:  89%|████████▉ | 2684/3000 [20:10&lt;02:07,  2.48it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2685/3000:  90%|████████▉ | 2685/3000 [20:10&lt;02:07,  2.48it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2685/3000:  90%|████████▉ | 2685/3000 [20:10&lt;02:07,  2.48it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2686/3000:  90%|████████▉ | 2685/3000 [20:10&lt;02:07,  2.48it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2686/3000:  90%|████████▉ | 2686/3000 [20:11&lt;02:15,  2.32it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2686/3000:  90%|████████▉ | 2686/3000 [20:11&lt;02:15,  2.32it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2687/3000:  90%|████████▉ | 2686/3000 [20:11&lt;02:15,  2.32it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2687/3000:  90%|████████▉ | 2687/3000 [20:11&lt;01:54,  2.74it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.04e+6]Epoch 2687/3000:  90%|████████▉ | 2687/3000 [20:11&lt;01:54,  2.74it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2688/3000:  90%|████████▉ | 2687/3000 [20:11&lt;01:54,  2.74it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2688/3000:  90%|████████▉ | 2688/3000 [20:11&lt;01:43,  3.00it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2688/3000:  90%|████████▉ | 2688/3000 [20:11&lt;01:43,  3.00it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2689/3000:  90%|████████▉ | 2688/3000 [20:11&lt;01:43,  3.00it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2689/3000:  90%|████████▉ | 2689/3000 [20:11&lt;01:34,  3.28it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2689/3000:  90%|████████▉ | 2689/3000 [20:11&lt;01:34,  3.28it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2690/3000:  90%|████████▉ | 2689/3000 [20:11&lt;01:34,  3.28it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2690/3000:  90%|████████▉ | 2690/3000 [20:12&lt;01:34,  3.28it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2690/3000:  90%|████████▉ | 2690/3000 [20:12&lt;01:34,  3.28it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2691/3000:  90%|████████▉ | 2690/3000 [20:12&lt;01:34,  3.28it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2691/3000:  90%|████████▉ | 2691/3000 [20:12&lt;01:52,  2.73it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2691/3000:  90%|████████▉ | 2691/3000 [20:12&lt;01:52,  2.73it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2692/3000:  90%|████████▉ | 2691/3000 [20:12&lt;01:52,  2.73it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2692/3000:  90%|████████▉ | 2692/3000 [20:12&lt;01:40,  3.06it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2692/3000:  90%|████████▉ | 2692/3000 [20:12&lt;01:40,  3.06it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2693/3000:  90%|████████▉ | 2692/3000 [20:12&lt;01:40,  3.06it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2693/3000:  90%|████████▉ | 2693/3000 [20:13&lt;01:43,  2.96it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2693/3000:  90%|████████▉ | 2693/3000 [20:13&lt;01:43,  2.96it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2694/3000:  90%|████████▉ | 2693/3000 [20:13&lt;01:43,  2.96it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2694/3000:  90%|████████▉ | 2694/3000 [20:13&lt;01:54,  2.68it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2694/3000:  90%|████████▉ | 2694/3000 [20:13&lt;01:54,  2.68it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2695/3000:  90%|████████▉ | 2694/3000 [20:13&lt;01:54,  2.68it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2695/3000:  90%|████████▉ | 2695/3000 [20:14&lt;02:07,  2.39it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2695/3000:  90%|████████▉ | 2695/3000 [20:14&lt;02:07,  2.39it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2696/3000:  90%|████████▉ | 2695/3000 [20:14&lt;02:07,  2.39it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2696/3000:  90%|████████▉ | 2696/3000 [20:14&lt;02:18,  2.20it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2696/3000:  90%|████████▉ | 2696/3000 [20:14&lt;02:18,  2.20it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2697/3000:  90%|████████▉ | 2696/3000 [20:14&lt;02:18,  2.20it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2697/3000:  90%|████████▉ | 2697/3000 [20:15&lt;02:32,  1.99it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2697/3000:  90%|████████▉ | 2697/3000 [20:15&lt;02:32,  1.99it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2698/3000:  90%|████████▉ | 2697/3000 [20:15&lt;02:32,  1.99it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2698/3000:  90%|████████▉ | 2698/3000 [20:15&lt;02:42,  1.86it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2698/3000:  90%|████████▉ | 2698/3000 [20:15&lt;02:42,  1.86it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2699/3000:  90%|████████▉ | 2698/3000 [20:15&lt;02:42,  1.86it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2699/3000:  90%|████████▉ | 2699/3000 [20:16&lt;02:39,  1.89it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2699/3000:  90%|████████▉ | 2699/3000 [20:16&lt;02:39,  1.89it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2700/3000:  90%|████████▉ | 2699/3000 [20:16&lt;02:39,  1.89it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2700/3000:  90%|█████████ | 2700/3000 [20:16&lt;02:30,  2.00it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2700/3000:  90%|█████████ | 2700/3000 [20:16&lt;02:30,  2.00it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2701/3000:  90%|█████████ | 2700/3000 [20:16&lt;02:30,  2.00it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2701/3000:  90%|█████████ | 2701/3000 [20:17&lt;02:16,  2.19it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2701/3000:  90%|█████████ | 2701/3000 [20:17&lt;02:16,  2.19it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2702/3000:  90%|█████████ | 2701/3000 [20:17&lt;02:16,  2.19it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2702/3000:  90%|█████████ | 2702/3000 [20:17&lt;02:08,  2.32it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2702/3000:  90%|█████████ | 2702/3000 [20:17&lt;02:08,  2.32it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.04e+6]Epoch 2703/3000:  90%|█████████ | 2702/3000 [20:17&lt;02:08,  2.32it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.04e+6]Epoch 2703/3000:  90%|█████████ | 2703/3000 [20:18&lt;02:18,  2.15it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.04e+6]Epoch 2703/3000:  90%|█████████ | 2703/3000 [20:18&lt;02:18,  2.15it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2704/3000:  90%|█████████ | 2703/3000 [20:18&lt;02:18,  2.15it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2704/3000:  90%|█████████ | 2704/3000 [20:18&lt;02:06,  2.35it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2704/3000:  90%|█████████ | 2704/3000 [20:18&lt;02:06,  2.35it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2705/3000:  90%|█████████ | 2704/3000 [20:18&lt;02:06,  2.35it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2705/3000:  90%|█████████ | 2705/3000 [20:18&lt;02:14,  2.19it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2705/3000:  90%|█████████ | 2705/3000 [20:18&lt;02:14,  2.19it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2706/3000:  90%|█████████ | 2705/3000 [20:18&lt;02:14,  2.19it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2706/3000:  90%|█████████ | 2706/3000 [20:19&lt;02:10,  2.25it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2706/3000:  90%|█████████ | 2706/3000 [20:19&lt;02:10,  2.25it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2707/3000:  90%|█████████ | 2706/3000 [20:19&lt;02:10,  2.25it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2707/3000:  90%|█████████ | 2707/3000 [20:19&lt;01:56,  2.51it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2707/3000:  90%|█████████ | 2707/3000 [20:19&lt;01:56,  2.51it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2708/3000:  90%|█████████ | 2707/3000 [20:19&lt;01:56,  2.51it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2708/3000:  90%|█████████ | 2708/3000 [20:20&lt;02:09,  2.25it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2708/3000:  90%|█████████ | 2708/3000 [20:20&lt;02:09,  2.25it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2709/3000:  90%|█████████ | 2708/3000 [20:20&lt;02:09,  2.25it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2709/3000:  90%|█████████ | 2709/3000 [20:20&lt;02:04,  2.34it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2709/3000:  90%|█████████ | 2709/3000 [20:20&lt;02:04,  2.34it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2710/3000:  90%|█████████ | 2709/3000 [20:20&lt;02:04,  2.34it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2710/3000:  90%|█████████ | 2710/3000 [20:20&lt;01:46,  2.72it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2710/3000:  90%|█████████ | 2710/3000 [20:20&lt;01:46,  2.72it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2711/3000:  90%|█████████ | 2710/3000 [20:20&lt;01:46,  2.72it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2711/3000:  90%|█████████ | 2711/3000 [20:21&lt;01:56,  2.49it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2711/3000:  90%|█████████ | 2711/3000 [20:21&lt;01:56,  2.49it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2712/3000:  90%|█████████ | 2711/3000 [20:21&lt;01:56,  2.49it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2712/3000:  90%|█████████ | 2712/3000 [20:21&lt;02:04,  2.32it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2712/3000:  90%|█████████ | 2712/3000 [20:21&lt;02:04,  2.32it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2713/3000:  90%|█████████ | 2712/3000 [20:21&lt;02:04,  2.32it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2713/3000:  90%|█████████ | 2713/3000 [20:22&lt;02:05,  2.29it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2713/3000:  90%|█████████ | 2713/3000 [20:22&lt;02:05,  2.29it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2714/3000:  90%|█████████ | 2713/3000 [20:22&lt;02:05,  2.29it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2714/3000:  90%|█████████ | 2714/3000 [20:22&lt;02:02,  2.33it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2714/3000:  90%|█████████ | 2714/3000 [20:22&lt;02:02,  2.33it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2715/3000:  90%|█████████ | 2714/3000 [20:22&lt;02:02,  2.33it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2715/3000:  90%|█████████ | 2715/3000 [20:23&lt;02:10,  2.18it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2715/3000:  90%|█████████ | 2715/3000 [20:23&lt;02:10,  2.18it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2716/3000:  90%|█████████ | 2715/3000 [20:23&lt;02:10,  2.18it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2716/3000:  91%|█████████ | 2716/3000 [20:23&lt;02:10,  2.18it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2716/3000:  91%|█████████ | 2716/3000 [20:23&lt;02:10,  2.18it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2717/3000:  91%|█████████ | 2716/3000 [20:23&lt;02:10,  2.18it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2717/3000:  91%|█████████ | 2717/3000 [20:24&lt;02:00,  2.36it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2717/3000:  91%|█████████ | 2717/3000 [20:24&lt;02:00,  2.36it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2718/3000:  91%|█████████ | 2717/3000 [20:24&lt;02:00,  2.36it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2718/3000:  91%|█████████ | 2718/3000 [20:24&lt;01:39,  2.83it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2718/3000:  91%|█████████ | 2718/3000 [20:24&lt;01:39,  2.83it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2719/3000:  91%|█████████ | 2718/3000 [20:24&lt;01:39,  2.83it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2719/3000:  91%|█████████ | 2719/3000 [20:24&lt;01:44,  2.69it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2719/3000:  91%|█████████ | 2719/3000 [20:24&lt;01:44,  2.69it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2720/3000:  91%|█████████ | 2719/3000 [20:24&lt;01:44,  2.69it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2720/3000:  91%|█████████ | 2720/3000 [20:25&lt;01:58,  2.36it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2720/3000:  91%|█████████ | 2720/3000 [20:25&lt;01:58,  2.36it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2721/3000:  91%|█████████ | 2720/3000 [20:25&lt;01:58,  2.36it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2721/3000:  91%|█████████ | 2721/3000 [20:25&lt;02:09,  2.16it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.04e+6]Epoch 2721/3000:  91%|█████████ | 2721/3000 [20:25&lt;02:09,  2.16it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2722/3000:  91%|█████████ | 2721/3000 [20:25&lt;02:09,  2.16it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2722/3000:  91%|█████████ | 2722/3000 [20:26&lt;02:15,  2.05it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.04e+6]Epoch 2722/3000:  91%|█████████ | 2722/3000 [20:26&lt;02:15,  2.05it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2723/3000:  91%|█████████ | 2722/3000 [20:26&lt;02:15,  2.05it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2723/3000:  91%|█████████ | 2723/3000 [20:26&lt;02:20,  1.98it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.04e+6]Epoch 2723/3000:  91%|█████████ | 2723/3000 [20:26&lt;02:20,  1.98it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2724/3000:  91%|█████████ | 2723/3000 [20:26&lt;02:20,  1.98it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2724/3000:  91%|█████████ | 2724/3000 [20:27&lt;02:20,  1.97it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.04e+6]Epoch 2724/3000:  91%|█████████ | 2724/3000 [20:27&lt;02:20,  1.97it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2725/3000:  91%|█████████ | 2724/3000 [20:27&lt;02:20,  1.97it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2725/3000:  91%|█████████ | 2725/3000 [20:27&lt;02:13,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2725/3000:  91%|█████████ | 2725/3000 [20:27&lt;02:13,  2.06it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2726/3000:  91%|█████████ | 2725/3000 [20:27&lt;02:13,  2.06it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2726/3000:  91%|█████████ | 2726/3000 [20:28&lt;02:17,  2.00it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2726/3000:  91%|█████████ | 2726/3000 [20:28&lt;02:17,  2.00it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2727/3000:  91%|█████████ | 2726/3000 [20:28&lt;02:17,  2.00it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2727/3000:  91%|█████████ | 2727/3000 [20:28&lt;02:23,  1.91it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2727/3000:  91%|█████████ | 2727/3000 [20:28&lt;02:23,  1.91it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2728/3000:  91%|█████████ | 2727/3000 [20:28&lt;02:23,  1.91it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2728/3000:  91%|█████████ | 2728/3000 [20:29&lt;02:11,  2.06it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2728/3000:  91%|█████████ | 2728/3000 [20:29&lt;02:11,  2.06it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2729/3000:  91%|█████████ | 2728/3000 [20:29&lt;02:11,  2.06it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2729/3000:  91%|█████████ | 2729/3000 [20:29&lt;02:04,  2.17it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2729/3000:  91%|█████████ | 2729/3000 [20:29&lt;02:04,  2.17it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2730/3000:  91%|█████████ | 2729/3000 [20:29&lt;02:04,  2.17it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2730/3000:  91%|█████████ | 2730/3000 [20:29&lt;01:43,  2.60it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2730/3000:  91%|█████████ | 2730/3000 [20:29&lt;01:43,  2.60it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2731/3000:  91%|█████████ | 2730/3000 [20:29&lt;01:43,  2.60it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2731/3000:  91%|█████████ | 2731/3000 [20:30&lt;01:40,  2.67it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2731/3000:  91%|█████████ | 2731/3000 [20:30&lt;01:40,  2.67it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2732/3000:  91%|█████████ | 2731/3000 [20:30&lt;01:40,  2.67it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2732/3000:  91%|█████████ | 2732/3000 [20:30&lt;01:42,  2.63it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2732/3000:  91%|█████████ | 2732/3000 [20:30&lt;01:42,  2.63it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2733/3000:  91%|█████████ | 2732/3000 [20:30&lt;01:42,  2.63it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2733/3000:  91%|█████████ | 2733/3000 [20:31&lt;01:58,  2.26it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2733/3000:  91%|█████████ | 2733/3000 [20:31&lt;01:58,  2.26it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2734/3000:  91%|█████████ | 2733/3000 [20:31&lt;01:58,  2.26it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2734/3000:  91%|█████████ | 2734/3000 [20:31&lt;02:04,  2.13it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2734/3000:  91%|█████████ | 2734/3000 [20:31&lt;02:04,  2.13it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2735/3000:  91%|█████████ | 2734/3000 [20:31&lt;02:04,  2.13it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2735/3000:  91%|█████████ | 2735/3000 [20:31&lt;01:48,  2.45it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2735/3000:  91%|█████████ | 2735/3000 [20:31&lt;01:48,  2.45it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2736/3000:  91%|█████████ | 2735/3000 [20:32&lt;01:48,  2.45it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2736/3000:  91%|█████████ | 2736/3000 [20:32&lt;01:48,  2.43it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2736/3000:  91%|█████████ | 2736/3000 [20:32&lt;01:48,  2.43it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2737/3000:  91%|█████████ | 2736/3000 [20:32&lt;01:48,  2.43it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2737/3000:  91%|█████████ | 2737/3000 [20:33&lt;02:02,  2.14it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2737/3000:  91%|█████████ | 2737/3000 [20:33&lt;02:02,  2.14it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2738/3000:  91%|█████████ | 2737/3000 [20:33&lt;02:02,  2.14it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2738/3000:  91%|█████████▏| 2738/3000 [20:33&lt;01:57,  2.24it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2738/3000:  91%|█████████▏| 2738/3000 [20:33&lt;01:57,  2.24it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2739/3000:  91%|█████████▏| 2738/3000 [20:33&lt;01:57,  2.24it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2739/3000:  91%|█████████▏| 2739/3000 [20:33&lt;01:40,  2.61it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2739/3000:  91%|█████████▏| 2739/3000 [20:33&lt;01:40,  2.61it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2740/3000:  91%|█████████▏| 2739/3000 [20:33&lt;01:40,  2.61it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2740/3000:  91%|█████████▏| 2740/3000 [20:34&lt;01:50,  2.35it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2740/3000:  91%|█████████▏| 2740/3000 [20:34&lt;01:50,  2.35it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2741/3000:  91%|█████████▏| 2740/3000 [20:34&lt;01:50,  2.35it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2741/3000:  91%|█████████▏| 2741/3000 [20:34&lt;01:58,  2.19it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2741/3000:  91%|█████████▏| 2741/3000 [20:34&lt;01:58,  2.19it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2742/3000:  91%|█████████▏| 2741/3000 [20:34&lt;01:58,  2.19it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2742/3000:  91%|█████████▏| 2742/3000 [20:35&lt;01:55,  2.22it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2742/3000:  91%|█████████▏| 2742/3000 [20:35&lt;01:55,  2.22it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2743/3000:  91%|█████████▏| 2742/3000 [20:35&lt;01:55,  2.22it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2743/3000:  91%|█████████▏| 2743/3000 [20:35&lt;01:39,  2.58it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2743/3000:  91%|█████████▏| 2743/3000 [20:35&lt;01:39,  2.58it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2744/3000:  91%|█████████▏| 2743/3000 [20:35&lt;01:39,  2.58it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2744/3000:  91%|█████████▏| 2744/3000 [20:35&lt;01:49,  2.34it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2744/3000:  91%|█████████▏| 2744/3000 [20:35&lt;01:49,  2.34it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2745/3000:  91%|█████████▏| 2744/3000 [20:35&lt;01:49,  2.34it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2745/3000:  92%|█████████▏| 2745/3000 [20:36&lt;01:48,  2.36it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2745/3000:  92%|█████████▏| 2745/3000 [20:36&lt;01:48,  2.36it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2746/3000:  92%|█████████▏| 2745/3000 [20:36&lt;01:48,  2.36it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2746/3000:  92%|█████████▏| 2746/3000 [20:36&lt;02:07,  2.00it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2746/3000:  92%|█████████▏| 2746/3000 [20:36&lt;02:07,  2.00it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2747/3000:  92%|█████████▏| 2746/3000 [20:37&lt;02:07,  2.00it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2747/3000:  92%|█████████▏| 2747/3000 [20:37&lt;01:56,  2.17it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2747/3000:  92%|█████████▏| 2747/3000 [20:37&lt;01:56,  2.17it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2748/3000:  92%|█████████▏| 2747/3000 [20:37&lt;01:56,  2.17it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2748/3000:  92%|█████████▏| 2748/3000 [20:37&lt;02:03,  2.03it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2748/3000:  92%|█████████▏| 2748/3000 [20:37&lt;02:03,  2.03it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2749/3000:  92%|█████████▏| 2748/3000 [20:37&lt;02:03,  2.03it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2749/3000:  92%|█████████▏| 2749/3000 [20:38&lt;01:58,  2.11it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2749/3000:  92%|█████████▏| 2749/3000 [20:38&lt;01:58,  2.11it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2750/3000:  92%|█████████▏| 2749/3000 [20:38&lt;01:58,  2.11it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2750/3000:  92%|█████████▏| 2750/3000 [20:38&lt;02:04,  2.00it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2750/3000:  92%|█████████▏| 2750/3000 [20:38&lt;02:04,  2.00it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2751/3000:  92%|█████████▏| 2750/3000 [20:38&lt;02:04,  2.00it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2751/3000:  92%|█████████▏| 2751/3000 [20:39&lt;02:07,  1.95it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2751/3000:  92%|█████████▏| 2751/3000 [20:39&lt;02:07,  1.95it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2752/3000:  92%|█████████▏| 2751/3000 [20:39&lt;02:07,  1.95it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2752/3000:  92%|█████████▏| 2752/3000 [20:40&lt;02:10,  1.91it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2752/3000:  92%|█████████▏| 2752/3000 [20:40&lt;02:10,  1.91it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2753/3000:  92%|█████████▏| 2752/3000 [20:40&lt;02:10,  1.91it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2753/3000:  92%|█████████▏| 2753/3000 [20:40&lt;01:52,  2.20it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2753/3000:  92%|█████████▏| 2753/3000 [20:40&lt;01:52,  2.20it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2754/3000:  92%|█████████▏| 2753/3000 [20:40&lt;01:52,  2.20it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2754/3000:  92%|█████████▏| 2754/3000 [20:40&lt;01:50,  2.23it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2754/3000:  92%|█████████▏| 2754/3000 [20:40&lt;01:50,  2.23it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2755/3000:  92%|█████████▏| 2754/3000 [20:40&lt;01:50,  2.23it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2755/3000:  92%|█████████▏| 2755/3000 [20:41&lt;01:50,  2.21it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2755/3000:  92%|█████████▏| 2755/3000 [20:41&lt;01:50,  2.21it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.03e+6]Epoch 2756/3000:  92%|█████████▏| 2755/3000 [20:41&lt;01:50,  2.21it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.03e+6]Epoch 2756/3000:  92%|█████████▏| 2756/3000 [20:41&lt;01:54,  2.14it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.03e+6]Epoch 2756/3000:  92%|█████████▏| 2756/3000 [20:41&lt;01:54,  2.14it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2757/3000:  92%|█████████▏| 2756/3000 [20:41&lt;01:54,  2.14it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2757/3000:  92%|█████████▏| 2757/3000 [20:42&lt;01:59,  2.03it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2757/3000:  92%|█████████▏| 2757/3000 [20:42&lt;01:59,  2.03it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2758/3000:  92%|█████████▏| 2757/3000 [20:42&lt;01:59,  2.03it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2758/3000:  92%|█████████▏| 2758/3000 [20:42&lt;01:53,  2.14it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2758/3000:  92%|█████████▏| 2758/3000 [20:42&lt;01:53,  2.14it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2759/3000:  92%|█████████▏| 2758/3000 [20:42&lt;01:53,  2.14it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2759/3000:  92%|█████████▏| 2759/3000 [20:43&lt;01:58,  2.03it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2759/3000:  92%|█████████▏| 2759/3000 [20:43&lt;01:58,  2.03it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2760/3000:  92%|█████████▏| 2759/3000 [20:43&lt;01:58,  2.03it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2760/3000:  92%|█████████▏| 2760/3000 [20:43&lt;01:51,  2.15it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2760/3000:  92%|█████████▏| 2760/3000 [20:43&lt;01:51,  2.15it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2761/3000:  92%|█████████▏| 2760/3000 [20:43&lt;01:51,  2.15it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2761/3000:  92%|█████████▏| 2761/3000 [20:44&lt;01:54,  2.09it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2761/3000:  92%|█████████▏| 2761/3000 [20:44&lt;01:54,  2.09it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2762/3000:  92%|█████████▏| 2761/3000 [20:44&lt;01:54,  2.09it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2762/3000:  92%|█████████▏| 2762/3000 [20:44&lt;01:53,  2.09it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2762/3000:  92%|█████████▏| 2762/3000 [20:44&lt;01:53,  2.09it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2763/3000:  92%|█████████▏| 2762/3000 [20:44&lt;01:53,  2.09it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2763/3000:  92%|█████████▏| 2763/3000 [20:45&lt;01:53,  2.10it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2763/3000:  92%|█████████▏| 2763/3000 [20:45&lt;01:53,  2.10it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2764/3000:  92%|█████████▏| 2763/3000 [20:45&lt;01:53,  2.10it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2764/3000:  92%|█████████▏| 2764/3000 [20:45&lt;01:53,  2.08it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2764/3000:  92%|█████████▏| 2764/3000 [20:45&lt;01:53,  2.08it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2765/3000:  92%|█████████▏| 2764/3000 [20:45&lt;01:53,  2.08it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2765/3000:  92%|█████████▏| 2765/3000 [20:45&lt;01:45,  2.22it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2765/3000:  92%|█████████▏| 2765/3000 [20:45&lt;01:45,  2.22it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2766/3000:  92%|█████████▏| 2765/3000 [20:45&lt;01:45,  2.22it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2766/3000:  92%|█████████▏| 2766/3000 [20:46&lt;01:56,  2.01it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2766/3000:  92%|█████████▏| 2766/3000 [20:46&lt;01:56,  2.01it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2767/3000:  92%|█████████▏| 2766/3000 [20:46&lt;01:56,  2.01it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2767/3000:  92%|█████████▏| 2767/3000 [20:46&lt;01:51,  2.08it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2767/3000:  92%|█████████▏| 2767/3000 [20:47&lt;01:51,  2.08it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2768/3000:  92%|█████████▏| 2767/3000 [20:47&lt;01:51,  2.08it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2768/3000:  92%|█████████▏| 2768/3000 [20:47&lt;02:05,  1.85it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2768/3000:  92%|█████████▏| 2768/3000 [20:47&lt;02:05,  1.85it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2769/3000:  92%|█████████▏| 2768/3000 [20:47&lt;02:05,  1.85it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2769/3000:  92%|█████████▏| 2769/3000 [20:48&lt;01:54,  2.02it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2769/3000:  92%|█████████▏| 2769/3000 [20:48&lt;01:54,  2.02it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2770/3000:  92%|█████████▏| 2769/3000 [20:48&lt;01:54,  2.02it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2770/3000:  92%|█████████▏| 2770/3000 [20:48&lt;01:51,  2.06it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2770/3000:  92%|█████████▏| 2770/3000 [20:48&lt;01:51,  2.06it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2771/3000:  92%|█████████▏| 2770/3000 [20:48&lt;01:51,  2.06it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2771/3000:  92%|█████████▏| 2771/3000 [20:48&lt;01:49,  2.09it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2771/3000:  92%|█████████▏| 2771/3000 [20:48&lt;01:49,  2.09it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2772/3000:  92%|█████████▏| 2771/3000 [20:48&lt;01:49,  2.09it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2772/3000:  92%|█████████▏| 2772/3000 [20:49&lt;01:31,  2.49it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2772/3000:  92%|█████████▏| 2772/3000 [20:49&lt;01:31,  2.49it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2773/3000:  92%|█████████▏| 2772/3000 [20:49&lt;01:31,  2.49it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2773/3000:  92%|█████████▏| 2773/3000 [20:49&lt;01:36,  2.35it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2773/3000:  92%|█████████▏| 2773/3000 [20:49&lt;01:36,  2.35it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2774/3000:  92%|█████████▏| 2773/3000 [20:49&lt;01:36,  2.35it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2774/3000:  92%|█████████▏| 2774/3000 [20:50&lt;01:40,  2.26it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2774/3000:  92%|█████████▏| 2774/3000 [20:50&lt;01:40,  2.26it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2775/3000:  92%|█████████▏| 2774/3000 [20:50&lt;01:40,  2.26it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2775/3000:  92%|█████████▎| 2775/3000 [20:50&lt;01:25,  2.64it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2775/3000:  92%|█████████▎| 2775/3000 [20:50&lt;01:25,  2.64it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2776/3000:  92%|█████████▎| 2775/3000 [20:50&lt;01:25,  2.64it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2776/3000:  93%|█████████▎| 2776/3000 [20:50&lt;01:13,  3.04it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2776/3000:  93%|█████████▎| 2776/3000 [20:50&lt;01:13,  3.04it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2777/3000:  93%|█████████▎| 2776/3000 [20:50&lt;01:13,  3.04it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2777/3000:  93%|█████████▎| 2777/3000 [20:51&lt;01:27,  2.54it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2777/3000:  93%|█████████▎| 2777/3000 [20:51&lt;01:27,  2.54it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2778/3000:  93%|█████████▎| 2777/3000 [20:51&lt;01:27,  2.54it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2778/3000:  93%|█████████▎| 2778/3000 [20:51&lt;01:32,  2.41it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2778/3000:  93%|█████████▎| 2778/3000 [20:51&lt;01:32,  2.41it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2779/3000:  93%|█████████▎| 2778/3000 [20:51&lt;01:32,  2.41it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2779/3000:  93%|█████████▎| 2779/3000 [20:52&lt;01:35,  2.30it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2779/3000:  93%|█████████▎| 2779/3000 [20:52&lt;01:35,  2.30it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2780/3000:  93%|█████████▎| 2779/3000 [20:52&lt;01:35,  2.30it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2780/3000:  93%|█████████▎| 2780/3000 [20:52&lt;01:43,  2.12it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2780/3000:  93%|█████████▎| 2780/3000 [20:52&lt;01:43,  2.12it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2781/3000:  93%|█████████▎| 2780/3000 [20:52&lt;01:43,  2.12it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2781/3000:  93%|█████████▎| 2781/3000 [20:53&lt;01:50,  1.99it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2781/3000:  93%|█████████▎| 2781/3000 [20:53&lt;01:50,  1.99it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2782/3000:  93%|█████████▎| 2781/3000 [20:53&lt;01:50,  1.99it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2782/3000:  93%|█████████▎| 2782/3000 [20:53&lt;01:44,  2.08it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2782/3000:  93%|█████████▎| 2782/3000 [20:53&lt;01:44,  2.08it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2783/3000:  93%|█████████▎| 2782/3000 [20:53&lt;01:44,  2.08it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2783/3000:  93%|█████████▎| 2783/3000 [20:54&lt;01:37,  2.24it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2783/3000:  93%|█████████▎| 2783/3000 [20:54&lt;01:37,  2.24it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2784/3000:  93%|█████████▎| 2783/3000 [20:54&lt;01:37,  2.24it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2784/3000:  93%|█████████▎| 2784/3000 [20:54&lt;01:40,  2.16it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2784/3000:  93%|█████████▎| 2784/3000 [20:54&lt;01:40,  2.16it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2785/3000:  93%|█████████▎| 2784/3000 [20:54&lt;01:40,  2.16it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2785/3000:  93%|█████████▎| 2785/3000 [20:54&lt;01:35,  2.25it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2785/3000:  93%|█████████▎| 2785/3000 [20:54&lt;01:35,  2.25it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2786/3000:  93%|█████████▎| 2785/3000 [20:54&lt;01:35,  2.25it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2786/3000:  93%|█████████▎| 2786/3000 [20:55&lt;01:39,  2.16it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2786/3000:  93%|█████████▎| 2786/3000 [20:55&lt;01:39,  2.16it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2787/3000:  93%|█████████▎| 2786/3000 [20:55&lt;01:39,  2.16it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2787/3000:  93%|█████████▎| 2787/3000 [20:55&lt;01:33,  2.29it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2787/3000:  93%|█████████▎| 2787/3000 [20:55&lt;01:33,  2.29it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2788/3000:  93%|█████████▎| 2787/3000 [20:55&lt;01:33,  2.29it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2788/3000:  93%|█████████▎| 2788/3000 [20:56&lt;01:33,  2.27it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2788/3000:  93%|█████████▎| 2788/3000 [20:56&lt;01:33,  2.27it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2789/3000:  93%|█████████▎| 2788/3000 [20:56&lt;01:33,  2.27it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2789/3000:  93%|█████████▎| 2789/3000 [20:56&lt;01:29,  2.36it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2789/3000:  93%|█████████▎| 2789/3000 [20:56&lt;01:29,  2.36it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2790/3000:  93%|█████████▎| 2789/3000 [20:56&lt;01:29,  2.36it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2790/3000:  93%|█████████▎| 2790/3000 [20:57&lt;01:32,  2.28it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2790/3000:  93%|█████████▎| 2790/3000 [20:57&lt;01:32,  2.28it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2791/3000:  93%|█████████▎| 2790/3000 [20:57&lt;01:32,  2.28it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2791/3000:  93%|█████████▎| 2791/3000 [20:57&lt;01:32,  2.26it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2791/3000:  93%|█████████▎| 2791/3000 [20:57&lt;01:32,  2.26it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2792/3000:  93%|█████████▎| 2791/3000 [20:57&lt;01:32,  2.26it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2792/3000:  93%|█████████▎| 2792/3000 [20:58&lt;01:35,  2.19it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2792/3000:  93%|█████████▎| 2792/3000 [20:58&lt;01:35,  2.19it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2793/3000:  93%|█████████▎| 2792/3000 [20:58&lt;01:35,  2.19it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2793/3000:  93%|█████████▎| 2793/3000 [20:58&lt;01:43,  2.01it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2793/3000:  93%|█████████▎| 2793/3000 [20:58&lt;01:43,  2.01it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2794/3000:  93%|█████████▎| 2793/3000 [20:58&lt;01:43,  2.01it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2794/3000:  93%|█████████▎| 2794/3000 [20:59&lt;01:36,  2.14it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2794/3000:  93%|█████████▎| 2794/3000 [20:59&lt;01:36,  2.14it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2795/3000:  93%|█████████▎| 2794/3000 [20:59&lt;01:36,  2.14it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2795/3000:  93%|█████████▎| 2795/3000 [20:59&lt;01:31,  2.23it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2795/3000:  93%|█████████▎| 2795/3000 [20:59&lt;01:31,  2.23it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2796/3000:  93%|█████████▎| 2795/3000 [20:59&lt;01:31,  2.23it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2796/3000:  93%|█████████▎| 2796/3000 [20:59&lt;01:32,  2.21it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2796/3000:  93%|█████████▎| 2796/3000 [20:59&lt;01:32,  2.21it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2797/3000:  93%|█████████▎| 2796/3000 [20:59&lt;01:32,  2.21it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2797/3000:  93%|█████████▎| 2797/3000 [21:00&lt;01:31,  2.22it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2797/3000:  93%|█████████▎| 2797/3000 [21:00&lt;01:31,  2.22it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2798/3000:  93%|█████████▎| 2797/3000 [21:00&lt;01:31,  2.22it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2798/3000:  93%|█████████▎| 2798/3000 [21:00&lt;01:31,  2.20it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2798/3000:  93%|█████████▎| 2798/3000 [21:00&lt;01:31,  2.20it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2799/3000:  93%|█████████▎| 2798/3000 [21:00&lt;01:31,  2.20it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2799/3000:  93%|█████████▎| 2799/3000 [21:01&lt;01:32,  2.17it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2799/3000:  93%|█████████▎| 2799/3000 [21:01&lt;01:32,  2.17it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2800/3000:  93%|█████████▎| 2799/3000 [21:01&lt;01:32,  2.17it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2800/3000:  93%|█████████▎| 2800/3000 [21:01&lt;01:42,  1.96it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2800/3000:  93%|█████████▎| 2800/3000 [21:01&lt;01:42,  1.96it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2801/3000:  93%|█████████▎| 2800/3000 [21:01&lt;01:42,  1.96it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2801/3000:  93%|█████████▎| 2801/3000 [21:02&lt;01:42,  1.94it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2801/3000:  93%|█████████▎| 2801/3000 [21:02&lt;01:42,  1.94it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2802/3000:  93%|█████████▎| 2801/3000 [21:02&lt;01:42,  1.94it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2802/3000:  93%|█████████▎| 2802/3000 [21:02&lt;01:42,  1.93it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2802/3000:  93%|█████████▎| 2802/3000 [21:02&lt;01:42,  1.93it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2803/3000:  93%|█████████▎| 2802/3000 [21:03&lt;01:42,  1.93it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2803/3000:  93%|█████████▎| 2803/3000 [21:03&lt;01:42,  1.91it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2803/3000:  93%|█████████▎| 2803/3000 [21:03&lt;01:42,  1.91it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2804/3000:  93%|█████████▎| 2803/3000 [21:03&lt;01:42,  1.91it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2804/3000:  93%|█████████▎| 2804/3000 [21:04&lt;01:47,  1.82it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2804/3000:  93%|█████████▎| 2804/3000 [21:04&lt;01:47,  1.82it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2805/3000:  93%|█████████▎| 2804/3000 [21:04&lt;01:47,  1.82it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2805/3000:  94%|█████████▎| 2805/3000 [21:04&lt;01:43,  1.88it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2805/3000:  94%|█████████▎| 2805/3000 [21:04&lt;01:43,  1.88it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2806/3000:  94%|█████████▎| 2805/3000 [21:04&lt;01:43,  1.88it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2806/3000:  94%|█████████▎| 2806/3000 [21:05&lt;01:39,  1.95it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2806/3000:  94%|█████████▎| 2806/3000 [21:05&lt;01:39,  1.95it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2807/3000:  94%|█████████▎| 2806/3000 [21:05&lt;01:39,  1.95it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2807/3000:  94%|█████████▎| 2807/3000 [21:05&lt;01:46,  1.82it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2807/3000:  94%|█████████▎| 2807/3000 [21:05&lt;01:46,  1.82it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2808/3000:  94%|█████████▎| 2807/3000 [21:05&lt;01:46,  1.82it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2808/3000:  94%|█████████▎| 2808/3000 [21:06&lt;01:48,  1.77it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2808/3000:  94%|█████████▎| 2808/3000 [21:06&lt;01:48,  1.77it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2809/3000:  94%|█████████▎| 2808/3000 [21:06&lt;01:48,  1.77it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2809/3000:  94%|█████████▎| 2809/3000 [21:06&lt;01:51,  1.71it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2809/3000:  94%|█████████▎| 2809/3000 [21:06&lt;01:51,  1.71it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2810/3000:  94%|█████████▎| 2809/3000 [21:07&lt;01:51,  1.71it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2810/3000:  94%|█████████▎| 2810/3000 [21:07&lt;01:48,  1.75it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2810/3000:  94%|█████████▎| 2810/3000 [21:07&lt;01:48,  1.75it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2811/3000:  94%|█████████▎| 2810/3000 [21:07&lt;01:48,  1.75it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2811/3000:  94%|█████████▎| 2811/3000 [21:07&lt;01:40,  1.89it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2811/3000:  94%|█████████▎| 2811/3000 [21:07&lt;01:40,  1.89it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2812/3000:  94%|█████████▎| 2811/3000 [21:07&lt;01:40,  1.89it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2812/3000:  94%|█████████▎| 2812/3000 [21:08&lt;01:41,  1.85it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2812/3000:  94%|█████████▎| 2812/3000 [21:08&lt;01:41,  1.85it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.03e+6]Epoch 2813/3000:  94%|█████████▎| 2812/3000 [21:08&lt;01:41,  1.85it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.03e+6]Epoch 2813/3000:  94%|█████████▍| 2813/3000 [21:08&lt;01:37,  1.92it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.03e+6]Epoch 2813/3000:  94%|█████████▍| 2813/3000 [21:08&lt;01:37,  1.92it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2814/3000:  94%|█████████▍| 2813/3000 [21:08&lt;01:37,  1.92it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2814/3000:  94%|█████████▍| 2814/3000 [21:09&lt;01:40,  1.85it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2814/3000:  94%|█████████▍| 2814/3000 [21:09&lt;01:40,  1.85it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2815/3000:  94%|█████████▍| 2814/3000 [21:09&lt;01:40,  1.85it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2815/3000:  94%|█████████▍| 2815/3000 [21:10&lt;01:37,  1.90it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2815/3000:  94%|█████████▍| 2815/3000 [21:10&lt;01:37,  1.90it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2816/3000:  94%|█████████▍| 2815/3000 [21:10&lt;01:37,  1.90it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2816/3000:  94%|█████████▍| 2816/3000 [21:10&lt;01:33,  1.98it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2816/3000:  94%|█████████▍| 2816/3000 [21:10&lt;01:33,  1.98it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2817/3000:  94%|█████████▍| 2816/3000 [21:10&lt;01:33,  1.98it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2817/3000:  94%|█████████▍| 2817/3000 [21:10&lt;01:23,  2.18it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2817/3000:  94%|█████████▍| 2817/3000 [21:10&lt;01:23,  2.18it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2818/3000:  94%|█████████▍| 2817/3000 [21:10&lt;01:23,  2.18it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2818/3000:  94%|█████████▍| 2818/3000 [21:11&lt;01:25,  2.12it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2818/3000:  94%|█████████▍| 2818/3000 [21:11&lt;01:25,  2.12it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2819/3000:  94%|█████████▍| 2818/3000 [21:11&lt;01:25,  2.12it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2819/3000:  94%|█████████▍| 2819/3000 [21:11&lt;01:21,  2.23it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2819/3000:  94%|█████████▍| 2819/3000 [21:11&lt;01:21,  2.23it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2820/3000:  94%|█████████▍| 2819/3000 [21:11&lt;01:21,  2.23it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2820/3000:  94%|█████████▍| 2820/3000 [21:12&lt;01:23,  2.14it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2820/3000:  94%|█████████▍| 2820/3000 [21:12&lt;01:23,  2.14it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2821/3000:  94%|█████████▍| 2820/3000 [21:12&lt;01:23,  2.14it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2821/3000:  94%|█████████▍| 2821/3000 [21:12&lt;01:21,  2.20it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2821/3000:  94%|█████████▍| 2821/3000 [21:12&lt;01:21,  2.20it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2822/3000:  94%|█████████▍| 2821/3000 [21:12&lt;01:21,  2.20it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2822/3000:  94%|█████████▍| 2822/3000 [21:12&lt;01:07,  2.65it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2822/3000:  94%|█████████▍| 2822/3000 [21:12&lt;01:07,  2.65it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2823/3000:  94%|█████████▍| 2822/3000 [21:12&lt;01:07,  2.65it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2823/3000:  94%|█████████▍| 2823/3000 [21:13&lt;01:04,  2.73it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2823/3000:  94%|█████████▍| 2823/3000 [21:13&lt;01:04,  2.73it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2824/3000:  94%|█████████▍| 2823/3000 [21:13&lt;01:04,  2.73it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2824/3000:  94%|█████████▍| 2824/3000 [21:13&lt;01:02,  2.80it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2824/3000:  94%|█████████▍| 2824/3000 [21:13&lt;01:02,  2.80it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2825/3000:  94%|█████████▍| 2824/3000 [21:13&lt;01:02,  2.80it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2825/3000:  94%|█████████▍| 2825/3000 [21:14&lt;01:12,  2.43it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2825/3000:  94%|█████████▍| 2825/3000 [21:14&lt;01:12,  2.43it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2826/3000:  94%|█████████▍| 2825/3000 [21:14&lt;01:12,  2.43it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2826/3000:  94%|█████████▍| 2826/3000 [21:14&lt;01:12,  2.41it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2826/3000:  94%|█████████▍| 2826/3000 [21:14&lt;01:12,  2.41it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2827/3000:  94%|█████████▍| 2826/3000 [21:14&lt;01:12,  2.41it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2827/3000:  94%|█████████▍| 2827/3000 [21:15&lt;01:22,  2.10it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2827/3000:  94%|█████████▍| 2827/3000 [21:15&lt;01:22,  2.10it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2828/3000:  94%|█████████▍| 2827/3000 [21:15&lt;01:22,  2.10it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2828/3000:  94%|█████████▍| 2828/3000 [21:15&lt;01:15,  2.28it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2828/3000:  94%|█████████▍| 2828/3000 [21:15&lt;01:15,  2.28it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2829/3000:  94%|█████████▍| 2828/3000 [21:15&lt;01:15,  2.28it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2829/3000:  94%|█████████▍| 2829/3000 [21:15&lt;01:16,  2.24it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2829/3000:  94%|█████████▍| 2829/3000 [21:15&lt;01:16,  2.24it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2830/3000:  94%|█████████▍| 2829/3000 [21:15&lt;01:16,  2.24it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2830/3000:  94%|█████████▍| 2830/3000 [21:16&lt;01:21,  2.08it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2830/3000:  94%|█████████▍| 2830/3000 [21:16&lt;01:21,  2.08it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2831/3000:  94%|█████████▍| 2830/3000 [21:16&lt;01:21,  2.08it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2831/3000:  94%|█████████▍| 2831/3000 [21:16&lt;01:19,  2.12it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2831/3000:  94%|█████████▍| 2831/3000 [21:16&lt;01:19,  2.12it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2832/3000:  94%|█████████▍| 2831/3000 [21:16&lt;01:19,  2.12it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2832/3000:  94%|█████████▍| 2832/3000 [21:17&lt;01:22,  2.05it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2832/3000:  94%|█████████▍| 2832/3000 [21:17&lt;01:22,  2.05it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2833/3000:  94%|█████████▍| 2832/3000 [21:17&lt;01:22,  2.05it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2833/3000:  94%|█████████▍| 2833/3000 [21:17&lt;01:18,  2.13it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2833/3000:  94%|█████████▍| 2833/3000 [21:17&lt;01:18,  2.13it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2834/3000:  94%|█████████▍| 2833/3000 [21:17&lt;01:18,  2.13it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2834/3000:  94%|█████████▍| 2834/3000 [21:18&lt;01:18,  2.12it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2834/3000:  94%|█████████▍| 2834/3000 [21:18&lt;01:18,  2.12it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2835/3000:  94%|█████████▍| 2834/3000 [21:18&lt;01:18,  2.12it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2835/3000:  94%|█████████▍| 2835/3000 [21:18&lt;01:16,  2.15it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2835/3000:  94%|█████████▍| 2835/3000 [21:18&lt;01:16,  2.15it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2836/3000:  94%|█████████▍| 2835/3000 [21:18&lt;01:16,  2.15it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2836/3000:  95%|█████████▍| 2836/3000 [21:19&lt;01:11,  2.29it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2836/3000:  95%|█████████▍| 2836/3000 [21:19&lt;01:11,  2.29it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2837/3000:  95%|█████████▍| 2836/3000 [21:19&lt;01:11,  2.29it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2837/3000:  95%|█████████▍| 2837/3000 [21:19&lt;01:15,  2.16it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2837/3000:  95%|█████████▍| 2837/3000 [21:19&lt;01:15,  2.16it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2838/3000:  95%|█████████▍| 2837/3000 [21:19&lt;01:15,  2.16it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2838/3000:  95%|█████████▍| 2838/3000 [21:20&lt;01:15,  2.14it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2838/3000:  95%|█████████▍| 2838/3000 [21:20&lt;01:15,  2.14it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2839/3000:  95%|█████████▍| 2838/3000 [21:20&lt;01:15,  2.14it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2839/3000:  95%|█████████▍| 2839/3000 [21:20&lt;01:19,  2.02it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2839/3000:  95%|█████████▍| 2839/3000 [21:20&lt;01:19,  2.02it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2840/3000:  95%|█████████▍| 2839/3000 [21:20&lt;01:19,  2.02it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2840/3000:  95%|█████████▍| 2840/3000 [21:21&lt;01:19,  2.01it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2840/3000:  95%|█████████▍| 2840/3000 [21:21&lt;01:19,  2.01it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2841/3000:  95%|█████████▍| 2840/3000 [21:21&lt;01:19,  2.01it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2841/3000:  95%|█████████▍| 2841/3000 [21:21&lt;01:12,  2.18it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2841/3000:  95%|█████████▍| 2841/3000 [21:21&lt;01:12,  2.18it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2842/3000:  95%|█████████▍| 2841/3000 [21:21&lt;01:12,  2.18it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2842/3000:  95%|█████████▍| 2842/3000 [21:22&lt;01:15,  2.10it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2842/3000:  95%|█████████▍| 2842/3000 [21:22&lt;01:15,  2.10it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2843/3000:  95%|█████████▍| 2842/3000 [21:22&lt;01:15,  2.10it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2843/3000:  95%|█████████▍| 2843/3000 [21:22&lt;01:13,  2.15it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2843/3000:  95%|█████████▍| 2843/3000 [21:22&lt;01:13,  2.15it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2844/3000:  95%|█████████▍| 2843/3000 [21:22&lt;01:13,  2.15it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2844/3000:  95%|█████████▍| 2844/3000 [21:22&lt;01:08,  2.27it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2844/3000:  95%|█████████▍| 2844/3000 [21:23&lt;01:08,  2.27it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2845/3000:  95%|█████████▍| 2844/3000 [21:23&lt;01:08,  2.27it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2845/3000:  95%|█████████▍| 2845/3000 [21:23&lt;01:12,  2.15it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2845/3000:  95%|█████████▍| 2845/3000 [21:23&lt;01:12,  2.15it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2846/3000:  95%|█████████▍| 2845/3000 [21:23&lt;01:12,  2.15it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2846/3000:  95%|█████████▍| 2846/3000 [21:24&lt;01:12,  2.12it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2846/3000:  95%|█████████▍| 2846/3000 [21:24&lt;01:12,  2.12it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2847/3000:  95%|█████████▍| 2846/3000 [21:24&lt;01:12,  2.12it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2847/3000:  95%|█████████▍| 2847/3000 [21:24&lt;01:08,  2.23it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2847/3000:  95%|█████████▍| 2847/3000 [21:24&lt;01:08,  2.23it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2848/3000:  95%|█████████▍| 2847/3000 [21:24&lt;01:08,  2.23it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2848/3000:  95%|█████████▍| 2848/3000 [21:24&lt;01:04,  2.35it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2848/3000:  95%|█████████▍| 2848/3000 [21:24&lt;01:04,  2.35it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2849/3000:  95%|█████████▍| 2848/3000 [21:24&lt;01:04,  2.35it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2849/3000:  95%|█████████▍| 2849/3000 [21:25&lt;00:59,  2.52it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2849/3000:  95%|█████████▍| 2849/3000 [21:25&lt;00:59,  2.52it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2850/3000:  95%|█████████▍| 2849/3000 [21:25&lt;00:59,  2.52it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2850/3000:  95%|█████████▌| 2850/3000 [21:25&lt;00:55,  2.73it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2850/3000:  95%|█████████▌| 2850/3000 [21:25&lt;00:55,  2.73it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2851/3000:  95%|█████████▌| 2850/3000 [21:25&lt;00:55,  2.73it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2851/3000:  95%|█████████▌| 2851/3000 [21:25&lt;00:53,  2.79it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2851/3000:  95%|█████████▌| 2851/3000 [21:25&lt;00:53,  2.79it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2852/3000:  95%|█████████▌| 2851/3000 [21:25&lt;00:53,  2.79it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2852/3000:  95%|█████████▌| 2852/3000 [21:26&lt;00:57,  2.56it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2852/3000:  95%|█████████▌| 2852/3000 [21:26&lt;00:57,  2.56it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2853/3000:  95%|█████████▌| 2852/3000 [21:26&lt;00:57,  2.56it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2853/3000:  95%|█████████▌| 2853/3000 [21:26&lt;00:48,  3.02it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2853/3000:  95%|█████████▌| 2853/3000 [21:26&lt;00:48,  3.02it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2854/3000:  95%|█████████▌| 2853/3000 [21:26&lt;00:48,  3.02it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2854/3000:  95%|█████████▌| 2854/3000 [21:26&lt;00:51,  2.84it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2854/3000:  95%|█████████▌| 2854/3000 [21:26&lt;00:51,  2.84it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2855/3000:  95%|█████████▌| 2854/3000 [21:26&lt;00:51,  2.84it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2855/3000:  95%|█████████▌| 2855/3000 [21:27&lt;00:58,  2.47it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2855/3000:  95%|█████████▌| 2855/3000 [21:27&lt;00:58,  2.47it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2856/3000:  95%|█████████▌| 2855/3000 [21:27&lt;00:58,  2.47it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2856/3000:  95%|█████████▌| 2856/3000 [21:27&lt;00:58,  2.45it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2856/3000:  95%|█████████▌| 2856/3000 [21:27&lt;00:58,  2.45it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2857/3000:  95%|█████████▌| 2856/3000 [21:27&lt;00:58,  2.45it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2857/3000:  95%|█████████▌| 2857/3000 [21:28&lt;01:05,  2.18it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2857/3000:  95%|█████████▌| 2857/3000 [21:28&lt;01:05,  2.18it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2858/3000:  95%|█████████▌| 2857/3000 [21:28&lt;01:05,  2.18it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2858/3000:  95%|█████████▌| 2858/3000 [21:28&lt;01:07,  2.09it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2858/3000:  95%|█████████▌| 2858/3000 [21:28&lt;01:07,  2.09it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2859/3000:  95%|█████████▌| 2858/3000 [21:28&lt;01:07,  2.09it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2859/3000:  95%|█████████▌| 2859/3000 [21:29&lt;01:05,  2.15it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2859/3000:  95%|█████████▌| 2859/3000 [21:29&lt;01:05,  2.15it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2860/3000:  95%|█████████▌| 2859/3000 [21:29&lt;01:05,  2.15it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2860/3000:  95%|█████████▌| 2860/3000 [21:29&lt;01:02,  2.25it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2860/3000:  95%|█████████▌| 2860/3000 [21:29&lt;01:02,  2.25it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2861/3000:  95%|█████████▌| 2860/3000 [21:29&lt;01:02,  2.25it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2861/3000:  95%|█████████▌| 2861/3000 [21:30&lt;01:01,  2.27it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2861/3000:  95%|█████████▌| 2861/3000 [21:30&lt;01:01,  2.27it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2862/3000:  95%|█████████▌| 2861/3000 [21:30&lt;01:01,  2.27it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2862/3000:  95%|█████████▌| 2862/3000 [21:30&lt;00:58,  2.36it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2862/3000:  95%|█████████▌| 2862/3000 [21:30&lt;00:58,  2.36it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2863/3000:  95%|█████████▌| 2862/3000 [21:30&lt;00:58,  2.36it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2863/3000:  95%|█████████▌| 2863/3000 [21:30&lt;00:46,  2.94it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2863/3000:  95%|█████████▌| 2863/3000 [21:30&lt;00:46,  2.94it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2864/3000:  95%|█████████▌| 2863/3000 [21:30&lt;00:46,  2.94it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2864/3000:  95%|█████████▌| 2864/3000 [21:30&lt;00:45,  2.97it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2864/3000:  95%|█████████▌| 2864/3000 [21:30&lt;00:45,  2.97it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2865/3000:  95%|█████████▌| 2864/3000 [21:30&lt;00:45,  2.97it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2865/3000:  96%|█████████▌| 2865/3000 [21:31&lt;00:56,  2.38it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2865/3000:  96%|█████████▌| 2865/3000 [21:31&lt;00:56,  2.38it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2866/3000:  96%|█████████▌| 2865/3000 [21:31&lt;00:56,  2.38it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2866/3000:  96%|█████████▌| 2866/3000 [21:31&lt;00:52,  2.54it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2866/3000:  96%|█████████▌| 2866/3000 [21:31&lt;00:52,  2.54it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2867/3000:  96%|█████████▌| 2866/3000 [21:31&lt;00:52,  2.54it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2867/3000:  96%|█████████▌| 2867/3000 [21:32&lt;00:50,  2.63it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2867/3000:  96%|█████████▌| 2867/3000 [21:32&lt;00:50,  2.63it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2868/3000:  96%|█████████▌| 2867/3000 [21:32&lt;00:50,  2.63it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2868/3000:  96%|█████████▌| 2868/3000 [21:32&lt;00:54,  2.41it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2868/3000:  96%|█████████▌| 2868/3000 [21:32&lt;00:54,  2.41it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2869/3000:  96%|█████████▌| 2868/3000 [21:32&lt;00:54,  2.41it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2869/3000:  96%|█████████▌| 2869/3000 [21:33&lt;01:00,  2.16it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2869/3000:  96%|█████████▌| 2869/3000 [21:33&lt;01:00,  2.16it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2870/3000:  96%|█████████▌| 2869/3000 [21:33&lt;01:00,  2.16it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2870/3000:  96%|█████████▌| 2870/3000 [21:33&lt;00:57,  2.28it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2870/3000:  96%|█████████▌| 2870/3000 [21:33&lt;00:57,  2.28it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2871/3000:  96%|█████████▌| 2870/3000 [21:33&lt;00:57,  2.28it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2871/3000:  96%|█████████▌| 2871/3000 [21:34&lt;00:57,  2.24it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2871/3000:  96%|█████████▌| 2871/3000 [21:34&lt;00:57,  2.24it/s, v_num=1, train_loss_step=1e+6, train_loss_epoch=1.03e+6]   Epoch 2872/3000:  96%|█████████▌| 2871/3000 [21:34&lt;00:57,  2.24it/s, v_num=1, train_loss_step=1e+6, train_loss_epoch=1.03e+6]Epoch 2872/3000:  96%|█████████▌| 2872/3000 [21:34&lt;00:55,  2.29it/s, v_num=1, train_loss_step=1e+6, train_loss_epoch=1.03e+6]Epoch 2872/3000:  96%|█████████▌| 2872/3000 [21:34&lt;00:55,  2.29it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2873/3000:  96%|█████████▌| 2872/3000 [21:34&lt;00:55,  2.29it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2873/3000:  96%|█████████▌| 2873/3000 [21:35&lt;00:54,  2.33it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2873/3000:  96%|█████████▌| 2873/3000 [21:35&lt;00:54,  2.33it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2874/3000:  96%|█████████▌| 2873/3000 [21:35&lt;00:54,  2.33it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2874/3000:  96%|█████████▌| 2874/3000 [21:35&lt;00:52,  2.39it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2874/3000:  96%|█████████▌| 2874/3000 [21:35&lt;00:52,  2.39it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2875/3000:  96%|█████████▌| 2874/3000 [21:35&lt;00:52,  2.39it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2875/3000:  96%|█████████▌| 2875/3000 [21:35&lt;00:55,  2.26it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2875/3000:  96%|█████████▌| 2875/3000 [21:35&lt;00:55,  2.26it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2876/3000:  96%|█████████▌| 2875/3000 [21:35&lt;00:55,  2.26it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2876/3000:  96%|█████████▌| 2876/3000 [21:36&lt;00:47,  2.63it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2876/3000:  96%|█████████▌| 2876/3000 [21:36&lt;00:47,  2.63it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2877/3000:  96%|█████████▌| 2876/3000 [21:36&lt;00:47,  2.63it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2877/3000:  96%|█████████▌| 2877/3000 [21:36&lt;00:37,  3.24it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2877/3000:  96%|█████████▌| 2877/3000 [21:36&lt;00:37,  3.24it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2878/3000:  96%|█████████▌| 2877/3000 [21:36&lt;00:37,  3.24it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2878/3000:  96%|█████████▌| 2878/3000 [21:36&lt;00:45,  2.68it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2878/3000:  96%|█████████▌| 2878/3000 [21:36&lt;00:45,  2.68it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2879/3000:  96%|█████████▌| 2878/3000 [21:36&lt;00:45,  2.68it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2879/3000:  96%|█████████▌| 2879/3000 [21:37&lt;00:56,  2.16it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2879/3000:  96%|█████████▌| 2879/3000 [21:37&lt;00:56,  2.16it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2880/3000:  96%|█████████▌| 2879/3000 [21:37&lt;00:56,  2.16it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2880/3000:  96%|█████████▌| 2880/3000 [21:37&lt;00:56,  2.11it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2880/3000:  96%|█████████▌| 2880/3000 [21:37&lt;00:56,  2.11it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2881/3000:  96%|█████████▌| 2880/3000 [21:37&lt;00:56,  2.11it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2881/3000:  96%|█████████▌| 2881/3000 [21:38&lt;00:57,  2.06it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2881/3000:  96%|█████████▌| 2881/3000 [21:38&lt;00:57,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2882/3000:  96%|█████████▌| 2881/3000 [21:38&lt;00:57,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2882/3000:  96%|█████████▌| 2882/3000 [21:38&lt;00:55,  2.12it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2882/3000:  96%|█████████▌| 2882/3000 [21:38&lt;00:55,  2.12it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2883/3000:  96%|█████████▌| 2882/3000 [21:38&lt;00:55,  2.12it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2883/3000:  96%|█████████▌| 2883/3000 [21:39&lt;00:55,  2.11it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2883/3000:  96%|█████████▌| 2883/3000 [21:39&lt;00:55,  2.11it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2884/3000:  96%|█████████▌| 2883/3000 [21:39&lt;00:55,  2.11it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2884/3000:  96%|█████████▌| 2884/3000 [21:39&lt;00:55,  2.11it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2884/3000:  96%|█████████▌| 2884/3000 [21:39&lt;00:55,  2.11it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2885/3000:  96%|█████████▌| 2884/3000 [21:39&lt;00:55,  2.11it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2885/3000:  96%|█████████▌| 2885/3000 [21:40&lt;00:49,  2.33it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2885/3000:  96%|█████████▌| 2885/3000 [21:40&lt;00:49,  2.33it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2886/3000:  96%|█████████▌| 2885/3000 [21:40&lt;00:49,  2.33it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2886/3000:  96%|█████████▌| 2886/3000 [21:40&lt;00:46,  2.46it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2886/3000:  96%|█████████▌| 2886/3000 [21:40&lt;00:46,  2.46it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2887/3000:  96%|█████████▌| 2886/3000 [21:40&lt;00:46,  2.46it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2887/3000:  96%|█████████▌| 2887/3000 [21:41&lt;00:51,  2.19it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2887/3000:  96%|█████████▌| 2887/3000 [21:41&lt;00:51,  2.19it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2888/3000:  96%|█████████▌| 2887/3000 [21:41&lt;00:51,  2.19it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2888/3000:  96%|█████████▋| 2888/3000 [21:41&lt;00:56,  1.99it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.03e+6]Epoch 2888/3000:  96%|█████████▋| 2888/3000 [21:41&lt;00:56,  1.99it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2889/3000:  96%|█████████▋| 2888/3000 [21:41&lt;00:56,  1.99it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2889/3000:  96%|█████████▋| 2889/3000 [21:42&lt;00:48,  2.28it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2889/3000:  96%|█████████▋| 2889/3000 [21:42&lt;00:48,  2.28it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2890/3000:  96%|█████████▋| 2889/3000 [21:42&lt;00:48,  2.28it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2890/3000:  96%|█████████▋| 2890/3000 [21:42&lt;00:49,  2.21it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2890/3000:  96%|█████████▋| 2890/3000 [21:42&lt;00:49,  2.21it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2891/3000:  96%|█████████▋| 2890/3000 [21:42&lt;00:49,  2.21it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2891/3000:  96%|█████████▋| 2891/3000 [21:43&lt;00:52,  2.06it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2891/3000:  96%|█████████▋| 2891/3000 [21:43&lt;00:52,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2892/3000:  96%|█████████▋| 2891/3000 [21:43&lt;00:52,  2.06it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2892/3000:  96%|█████████▋| 2892/3000 [21:43&lt;00:51,  2.12it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2892/3000:  96%|█████████▋| 2892/3000 [21:43&lt;00:51,  2.12it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2893/3000:  96%|█████████▋| 2892/3000 [21:43&lt;00:51,  2.12it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2893/3000:  96%|█████████▋| 2893/3000 [21:43&lt;00:47,  2.24it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2893/3000:  96%|█████████▋| 2893/3000 [21:43&lt;00:47,  2.24it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2894/3000:  96%|█████████▋| 2893/3000 [21:43&lt;00:47,  2.24it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2894/3000:  96%|█████████▋| 2894/3000 [21:44&lt;00:48,  2.18it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2894/3000:  96%|█████████▋| 2894/3000 [21:44&lt;00:48,  2.18it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2895/3000:  96%|█████████▋| 2894/3000 [21:44&lt;00:48,  2.18it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2895/3000:  96%|█████████▋| 2895/3000 [21:44&lt;00:47,  2.21it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2895/3000:  96%|█████████▋| 2895/3000 [21:44&lt;00:47,  2.21it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2896/3000:  96%|█████████▋| 2895/3000 [21:44&lt;00:47,  2.21it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2896/3000:  97%|█████████▋| 2896/3000 [21:45&lt;00:49,  2.10it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2896/3000:  97%|█████████▋| 2896/3000 [21:45&lt;00:49,  2.10it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2897/3000:  97%|█████████▋| 2896/3000 [21:45&lt;00:49,  2.10it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2897/3000:  97%|█████████▋| 2897/3000 [21:45&lt;00:51,  2.00it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2897/3000:  97%|█████████▋| 2897/3000 [21:45&lt;00:51,  2.00it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2898/3000:  97%|█████████▋| 2897/3000 [21:45&lt;00:51,  2.00it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2898/3000:  97%|█████████▋| 2898/3000 [21:46&lt;00:52,  1.93it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.03e+6]Epoch 2898/3000:  97%|█████████▋| 2898/3000 [21:46&lt;00:52,  1.93it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2899/3000:  97%|█████████▋| 2898/3000 [21:46&lt;00:52,  1.93it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2899/3000:  97%|█████████▋| 2899/3000 [21:46&lt;00:48,  2.10it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.03e+6]Epoch 2899/3000:  97%|█████████▋| 2899/3000 [21:46&lt;00:48,  2.10it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2900/3000:  97%|█████████▋| 2899/3000 [21:46&lt;00:48,  2.10it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2900/3000:  97%|█████████▋| 2900/3000 [21:47&lt;00:48,  2.05it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2900/3000:  97%|█████████▋| 2900/3000 [21:47&lt;00:48,  2.05it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2901/3000:  97%|█████████▋| 2900/3000 [21:47&lt;00:48,  2.05it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2901/3000:  97%|█████████▋| 2901/3000 [21:48&lt;00:53,  1.86it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2901/3000:  97%|█████████▋| 2901/3000 [21:48&lt;00:53,  1.86it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2902/3000:  97%|█████████▋| 2901/3000 [21:48&lt;00:53,  1.86it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2902/3000:  97%|█████████▋| 2902/3000 [21:48&lt;00:51,  1.92it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2902/3000:  97%|█████████▋| 2902/3000 [21:48&lt;00:51,  1.92it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2903/3000:  97%|█████████▋| 2902/3000 [21:48&lt;00:51,  1.92it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2903/3000:  97%|█████████▋| 2903/3000 [21:48&lt;00:44,  2.17it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.03e+6]Epoch 2903/3000:  97%|█████████▋| 2903/3000 [21:48&lt;00:44,  2.17it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2904/3000:  97%|█████████▋| 2903/3000 [21:48&lt;00:44,  2.17it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2904/3000:  97%|█████████▋| 2904/3000 [21:49&lt;00:39,  2.41it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.03e+6]Epoch 2904/3000:  97%|█████████▋| 2904/3000 [21:49&lt;00:39,  2.41it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2905/3000:  97%|█████████▋| 2904/3000 [21:49&lt;00:39,  2.41it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2905/3000:  97%|█████████▋| 2905/3000 [21:49&lt;00:33,  2.82it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2905/3000:  97%|█████████▋| 2905/3000 [21:49&lt;00:33,  2.82it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2906/3000:  97%|█████████▋| 2905/3000 [21:49&lt;00:33,  2.82it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2906/3000:  97%|█████████▋| 2906/3000 [21:49&lt;00:30,  3.07it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2906/3000:  97%|█████████▋| 2906/3000 [21:49&lt;00:30,  3.07it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2907/3000:  97%|█████████▋| 2906/3000 [21:49&lt;00:30,  3.07it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2907/3000:  97%|█████████▋| 2907/3000 [21:50&lt;00:36,  2.53it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2907/3000:  97%|█████████▋| 2907/3000 [21:50&lt;00:36,  2.53it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2908/3000:  97%|█████████▋| 2907/3000 [21:50&lt;00:36,  2.53it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2908/3000:  97%|█████████▋| 2908/3000 [21:50&lt;00:38,  2.37it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2908/3000:  97%|█████████▋| 2908/3000 [21:50&lt;00:38,  2.37it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2909/3000:  97%|█████████▋| 2908/3000 [21:50&lt;00:38,  2.37it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2909/3000:  97%|█████████▋| 2909/3000 [21:51&lt;00:44,  2.04it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2909/3000:  97%|█████████▋| 2909/3000 [21:51&lt;00:44,  2.04it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2910/3000:  97%|█████████▋| 2909/3000 [21:51&lt;00:44,  2.04it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2910/3000:  97%|█████████▋| 2910/3000 [21:51&lt;00:40,  2.21it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2910/3000:  97%|█████████▋| 2910/3000 [21:51&lt;00:40,  2.21it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2911/3000:  97%|█████████▋| 2910/3000 [21:51&lt;00:40,  2.21it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2911/3000:  97%|█████████▋| 2911/3000 [21:52&lt;00:41,  2.14it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2911/3000:  97%|█████████▋| 2911/3000 [21:52&lt;00:41,  2.14it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2912/3000:  97%|█████████▋| 2911/3000 [21:52&lt;00:41,  2.14it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2912/3000:  97%|█████████▋| 2912/3000 [21:52&lt;00:38,  2.27it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2912/3000:  97%|█████████▋| 2912/3000 [21:52&lt;00:38,  2.27it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2913/3000:  97%|█████████▋| 2912/3000 [21:52&lt;00:38,  2.27it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2913/3000:  97%|█████████▋| 2913/3000 [21:53&lt;00:39,  2.22it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2913/3000:  97%|█████████▋| 2913/3000 [21:53&lt;00:39,  2.22it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2914/3000:  97%|█████████▋| 2913/3000 [21:53&lt;00:39,  2.22it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2914/3000:  97%|█████████▋| 2914/3000 [21:53&lt;00:39,  2.18it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2914/3000:  97%|█████████▋| 2914/3000 [21:53&lt;00:39,  2.18it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.02e+6]Epoch 2915/3000:  97%|█████████▋| 2914/3000 [21:53&lt;00:39,  2.18it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.02e+6]Epoch 2915/3000:  97%|█████████▋| 2915/3000 [21:54&lt;00:41,  2.07it/s, v_num=1, train_loss_step=1.05e+6, train_loss_epoch=1.02e+6]Epoch 2915/3000:  97%|█████████▋| 2915/3000 [21:54&lt;00:41,  2.07it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2916/3000:  97%|█████████▋| 2915/3000 [21:54&lt;00:41,  2.07it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2916/3000:  97%|█████████▋| 2916/3000 [21:54&lt;00:36,  2.28it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2916/3000:  97%|█████████▋| 2916/3000 [21:54&lt;00:36,  2.28it/s, v_num=1, train_loss_step=1e+6, train_loss_epoch=1.02e+6]   Epoch 2917/3000:  97%|█████████▋| 2916/3000 [21:54&lt;00:36,  2.28it/s, v_num=1, train_loss_step=1e+6, train_loss_epoch=1.02e+6]Epoch 2917/3000:  97%|█████████▋| 2917/3000 [21:54&lt;00:38,  2.16it/s, v_num=1, train_loss_step=1e+6, train_loss_epoch=1.02e+6]Epoch 2917/3000:  97%|█████████▋| 2917/3000 [21:54&lt;00:38,  2.16it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2918/3000:  97%|█████████▋| 2917/3000 [21:54&lt;00:38,  2.16it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2918/3000:  97%|█████████▋| 2918/3000 [21:55&lt;00:36,  2.27it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2918/3000:  97%|█████████▋| 2918/3000 [21:55&lt;00:36,  2.27it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2919/3000:  97%|█████████▋| 2918/3000 [21:55&lt;00:36,  2.27it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2919/3000:  97%|█████████▋| 2919/3000 [21:55&lt;00:38,  2.10it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2919/3000:  97%|█████████▋| 2919/3000 [21:55&lt;00:38,  2.10it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2920/3000:  97%|█████████▋| 2919/3000 [21:55&lt;00:38,  2.10it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2920/3000:  97%|█████████▋| 2920/3000 [21:56&lt;00:40,  1.99it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2920/3000:  97%|█████████▋| 2920/3000 [21:56&lt;00:40,  1.99it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2921/3000:  97%|█████████▋| 2920/3000 [21:56&lt;00:40,  1.99it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2921/3000:  97%|█████████▋| 2921/3000 [21:56&lt;00:37,  2.10it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2921/3000:  97%|█████████▋| 2921/3000 [21:56&lt;00:37,  2.10it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2922/3000:  97%|█████████▋| 2921/3000 [21:56&lt;00:37,  2.10it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2922/3000:  97%|█████████▋| 2922/3000 [21:57&lt;00:39,  1.97it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2922/3000:  97%|█████████▋| 2922/3000 [21:57&lt;00:39,  1.97it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2923/3000:  97%|█████████▋| 2922/3000 [21:57&lt;00:39,  1.97it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2923/3000:  97%|█████████▋| 2923/3000 [21:57&lt;00:39,  1.95it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2923/3000:  97%|█████████▋| 2923/3000 [21:57&lt;00:39,  1.95it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2924/3000:  97%|█████████▋| 2923/3000 [21:57&lt;00:39,  1.95it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2924/3000:  97%|█████████▋| 2924/3000 [21:58&lt;00:35,  2.14it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2924/3000:  97%|█████████▋| 2924/3000 [21:58&lt;00:35,  2.14it/s, v_num=1, train_loss_step=1e+6, train_loss_epoch=1.02e+6]   Epoch 2925/3000:  97%|█████████▋| 2924/3000 [21:58&lt;00:35,  2.14it/s, v_num=1, train_loss_step=1e+6, train_loss_epoch=1.02e+6]Epoch 2925/3000:  98%|█████████▊| 2925/3000 [21:58&lt;00:38,  1.92it/s, v_num=1, train_loss_step=1e+6, train_loss_epoch=1.02e+6]Epoch 2925/3000:  98%|█████████▊| 2925/3000 [21:58&lt;00:38,  1.92it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2926/3000:  98%|█████████▊| 2925/3000 [21:58&lt;00:38,  1.92it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2926/3000:  98%|█████████▊| 2926/3000 [21:59&lt;00:35,  2.10it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2926/3000:  98%|█████████▊| 2926/3000 [21:59&lt;00:35,  2.10it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2927/3000:  98%|█████████▊| 2926/3000 [21:59&lt;00:35,  2.10it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2927/3000:  98%|█████████▊| 2927/3000 [21:59&lt;00:32,  2.28it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2927/3000:  98%|█████████▊| 2927/3000 [21:59&lt;00:32,  2.28it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2928/3000:  98%|█████████▊| 2927/3000 [21:59&lt;00:32,  2.28it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2928/3000:  98%|█████████▊| 2928/3000 [22:00&lt;00:30,  2.39it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2928/3000:  98%|█████████▊| 2928/3000 [22:00&lt;00:30,  2.39it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2929/3000:  98%|█████████▊| 2928/3000 [22:00&lt;00:30,  2.39it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2929/3000:  98%|█████████▊| 2929/3000 [22:00&lt;00:28,  2.47it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2929/3000:  98%|█████████▊| 2929/3000 [22:00&lt;00:28,  2.47it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2930/3000:  98%|█████████▊| 2929/3000 [22:00&lt;00:28,  2.47it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2930/3000:  98%|█████████▊| 2930/3000 [22:00&lt;00:32,  2.17it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2930/3000:  98%|█████████▊| 2930/3000 [22:00&lt;00:32,  2.17it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2931/3000:  98%|█████████▊| 2930/3000 [22:01&lt;00:32,  2.17it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2931/3000:  98%|█████████▊| 2931/3000 [22:01&lt;00:30,  2.25it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2931/3000:  98%|█████████▊| 2931/3000 [22:01&lt;00:30,  2.25it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2932/3000:  98%|█████████▊| 2931/3000 [22:01&lt;00:30,  2.25it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2932/3000:  98%|█████████▊| 2932/3000 [22:01&lt;00:28,  2.40it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2932/3000:  98%|█████████▊| 2932/3000 [22:01&lt;00:28,  2.40it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2933/3000:  98%|█████████▊| 2932/3000 [22:01&lt;00:28,  2.40it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2933/3000:  98%|█████████▊| 2933/3000 [22:01&lt;00:24,  2.74it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2933/3000:  98%|█████████▊| 2933/3000 [22:02&lt;00:24,  2.74it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2934/3000:  98%|█████████▊| 2933/3000 [22:02&lt;00:24,  2.74it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2934/3000:  98%|█████████▊| 2934/3000 [22:02&lt;00:23,  2.85it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2934/3000:  98%|█████████▊| 2934/3000 [22:02&lt;00:23,  2.85it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2935/3000:  98%|█████████▊| 2934/3000 [22:02&lt;00:23,  2.85it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2935/3000:  98%|█████████▊| 2935/3000 [22:02&lt;00:22,  2.88it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2935/3000:  98%|█████████▊| 2935/3000 [22:02&lt;00:22,  2.88it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2936/3000:  98%|█████████▊| 2935/3000 [22:02&lt;00:22,  2.88it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2936/3000:  98%|█████████▊| 2936/3000 [22:03&lt;00:27,  2.31it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2936/3000:  98%|█████████▊| 2936/3000 [22:03&lt;00:27,  2.31it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.02e+6]Epoch 2937/3000:  98%|█████████▊| 2936/3000 [22:03&lt;00:27,  2.31it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.02e+6]Epoch 2937/3000:  98%|█████████▊| 2937/3000 [22:03&lt;00:26,  2.35it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.02e+6]Epoch 2937/3000:  98%|█████████▊| 2937/3000 [22:03&lt;00:26,  2.35it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2938/3000:  98%|█████████▊| 2937/3000 [22:03&lt;00:26,  2.35it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2938/3000:  98%|█████████▊| 2938/3000 [22:04&lt;00:28,  2.17it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2938/3000:  98%|█████████▊| 2938/3000 [22:04&lt;00:28,  2.17it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2939/3000:  98%|█████████▊| 2938/3000 [22:04&lt;00:28,  2.17it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2939/3000:  98%|█████████▊| 2939/3000 [22:04&lt;00:28,  2.12it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2939/3000:  98%|█████████▊| 2939/3000 [22:04&lt;00:28,  2.12it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2940/3000:  98%|█████████▊| 2939/3000 [22:04&lt;00:28,  2.12it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2940/3000:  98%|█████████▊| 2940/3000 [22:05&lt;00:30,  1.94it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2940/3000:  98%|█████████▊| 2940/3000 [22:05&lt;00:30,  1.94it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2941/3000:  98%|█████████▊| 2940/3000 [22:05&lt;00:30,  1.94it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2941/3000:  98%|█████████▊| 2941/3000 [22:05&lt;00:26,  2.20it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2941/3000:  98%|█████████▊| 2941/3000 [22:05&lt;00:26,  2.20it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2942/3000:  98%|█████████▊| 2941/3000 [22:05&lt;00:26,  2.20it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2942/3000:  98%|█████████▊| 2942/3000 [22:05&lt;00:21,  2.74it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2942/3000:  98%|█████████▊| 2942/3000 [22:05&lt;00:21,  2.74it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2943/3000:  98%|█████████▊| 2942/3000 [22:05&lt;00:21,  2.74it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2943/3000:  98%|█████████▊| 2943/3000 [22:06&lt;00:18,  3.14it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2943/3000:  98%|█████████▊| 2943/3000 [22:06&lt;00:18,  3.14it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2944/3000:  98%|█████████▊| 2943/3000 [22:06&lt;00:18,  3.14it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2944/3000:  98%|█████████▊| 2944/3000 [22:06&lt;00:18,  3.06it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2944/3000:  98%|█████████▊| 2944/3000 [22:06&lt;00:18,  3.06it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2945/3000:  98%|█████████▊| 2944/3000 [22:06&lt;00:18,  3.06it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2945/3000:  98%|█████████▊| 2945/3000 [22:06&lt;00:21,  2.55it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2945/3000:  98%|█████████▊| 2945/3000 [22:06&lt;00:21,  2.55it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2946/3000:  98%|█████████▊| 2945/3000 [22:06&lt;00:21,  2.55it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2946/3000:  98%|█████████▊| 2946/3000 [22:07&lt;00:20,  2.68it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2946/3000:  98%|█████████▊| 2946/3000 [22:07&lt;00:20,  2.68it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2947/3000:  98%|█████████▊| 2946/3000 [22:07&lt;00:20,  2.68it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2947/3000:  98%|█████████▊| 2947/3000 [22:07&lt;00:20,  2.55it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2947/3000:  98%|█████████▊| 2947/3000 [22:07&lt;00:20,  2.55it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2948/3000:  98%|█████████▊| 2947/3000 [22:07&lt;00:20,  2.55it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2948/3000:  98%|█████████▊| 2948/3000 [22:08&lt;00:20,  2.57it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2948/3000:  98%|█████████▊| 2948/3000 [22:08&lt;00:20,  2.57it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2949/3000:  98%|█████████▊| 2948/3000 [22:08&lt;00:20,  2.57it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2949/3000:  98%|█████████▊| 2949/3000 [22:08&lt;00:20,  2.48it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2949/3000:  98%|█████████▊| 2949/3000 [22:08&lt;00:20,  2.48it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2950/3000:  98%|█████████▊| 2949/3000 [22:08&lt;00:20,  2.48it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2950/3000:  98%|█████████▊| 2950/3000 [22:08&lt;00:17,  2.80it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2950/3000:  98%|█████████▊| 2950/3000 [22:08&lt;00:17,  2.80it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2951/3000:  98%|█████████▊| 2950/3000 [22:08&lt;00:17,  2.80it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2951/3000:  98%|█████████▊| 2951/3000 [22:09&lt;00:18,  2.70it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2951/3000:  98%|█████████▊| 2951/3000 [22:09&lt;00:18,  2.70it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2952/3000:  98%|█████████▊| 2951/3000 [22:09&lt;00:18,  2.70it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2952/3000:  98%|█████████▊| 2952/3000 [22:09&lt;00:21,  2.27it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2952/3000:  98%|█████████▊| 2952/3000 [22:09&lt;00:21,  2.27it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2953/3000:  98%|█████████▊| 2952/3000 [22:09&lt;00:21,  2.27it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2953/3000:  98%|█████████▊| 2953/3000 [22:10&lt;00:20,  2.29it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2953/3000:  98%|█████████▊| 2953/3000 [22:10&lt;00:20,  2.29it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2954/3000:  98%|█████████▊| 2953/3000 [22:10&lt;00:20,  2.29it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2954/3000:  98%|█████████▊| 2954/3000 [22:10&lt;00:21,  2.10it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2954/3000:  98%|█████████▊| 2954/3000 [22:10&lt;00:21,  2.10it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2955/3000:  98%|█████████▊| 2954/3000 [22:10&lt;00:21,  2.10it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2955/3000:  98%|█████████▊| 2955/3000 [22:11&lt;00:20,  2.21it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2955/3000:  98%|█████████▊| 2955/3000 [22:11&lt;00:20,  2.21it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2956/3000:  98%|█████████▊| 2955/3000 [22:11&lt;00:20,  2.21it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2956/3000:  99%|█████████▊| 2956/3000 [22:11&lt;00:20,  2.17it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2956/3000:  99%|█████████▊| 2956/3000 [22:11&lt;00:20,  2.17it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2957/3000:  99%|█████████▊| 2956/3000 [22:11&lt;00:20,  2.17it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2957/3000:  99%|█████████▊| 2957/3000 [22:11&lt;00:17,  2.50it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2957/3000:  99%|█████████▊| 2957/3000 [22:11&lt;00:17,  2.50it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2958/3000:  99%|█████████▊| 2957/3000 [22:11&lt;00:17,  2.50it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2958/3000:  99%|█████████▊| 2958/3000 [22:12&lt;00:18,  2.31it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2958/3000:  99%|█████████▊| 2958/3000 [22:12&lt;00:18,  2.31it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2959/3000:  99%|█████████▊| 2958/3000 [22:12&lt;00:18,  2.31it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2959/3000:  99%|█████████▊| 2959/3000 [22:12&lt;00:19,  2.08it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2959/3000:  99%|█████████▊| 2959/3000 [22:13&lt;00:19,  2.08it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2960/3000:  99%|█████████▊| 2959/3000 [22:13&lt;00:19,  2.08it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2960/3000:  99%|█████████▊| 2960/3000 [22:13&lt;00:17,  2.27it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2960/3000:  99%|█████████▊| 2960/3000 [22:13&lt;00:17,  2.27it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2961/3000:  99%|█████████▊| 2960/3000 [22:13&lt;00:17,  2.27it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2961/3000:  99%|█████████▊| 2961/3000 [22:13&lt;00:16,  2.36it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2961/3000:  99%|█████████▊| 2961/3000 [22:13&lt;00:16,  2.36it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2962/3000:  99%|█████████▊| 2961/3000 [22:13&lt;00:16,  2.36it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2962/3000:  99%|█████████▊| 2962/3000 [22:14&lt;00:16,  2.37it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2962/3000:  99%|█████████▊| 2962/3000 [22:14&lt;00:16,  2.37it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2963/3000:  99%|█████████▊| 2962/3000 [22:14&lt;00:16,  2.37it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2963/3000:  99%|█████████▉| 2963/3000 [22:14&lt;00:15,  2.40it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2963/3000:  99%|█████████▉| 2963/3000 [22:14&lt;00:15,  2.40it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2964/3000:  99%|█████████▉| 2963/3000 [22:14&lt;00:15,  2.40it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2964/3000:  99%|█████████▉| 2964/3000 [22:15&lt;00:18,  2.00it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2964/3000:  99%|█████████▉| 2964/3000 [22:15&lt;00:18,  2.00it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2965/3000:  99%|█████████▉| 2964/3000 [22:15&lt;00:18,  2.00it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2965/3000:  99%|█████████▉| 2965/3000 [22:15&lt;00:18,  1.85it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2965/3000:  99%|█████████▉| 2965/3000 [22:15&lt;00:18,  1.85it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2966/3000:  99%|█████████▉| 2965/3000 [22:15&lt;00:18,  1.85it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2966/3000:  99%|█████████▉| 2966/3000 [22:16&lt;00:17,  1.98it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2966/3000:  99%|█████████▉| 2966/3000 [22:16&lt;00:17,  1.98it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2967/3000:  99%|█████████▉| 2966/3000 [22:16&lt;00:17,  1.98it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2967/3000:  99%|█████████▉| 2967/3000 [22:16&lt;00:14,  2.22it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2967/3000:  99%|█████████▉| 2967/3000 [22:16&lt;00:14,  2.22it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2968/3000:  99%|█████████▉| 2967/3000 [22:16&lt;00:14,  2.22it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2968/3000:  99%|█████████▉| 2968/3000 [22:17&lt;00:15,  2.03it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2968/3000:  99%|█████████▉| 2968/3000 [22:17&lt;00:15,  2.03it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2969/3000:  99%|█████████▉| 2968/3000 [22:17&lt;00:15,  2.03it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2969/3000:  99%|█████████▉| 2969/3000 [22:17&lt;00:15,  1.97it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2969/3000:  99%|█████████▉| 2969/3000 [22:17&lt;00:15,  1.97it/s, v_num=1, train_loss_step=9.95e+5, train_loss_epoch=1.02e+6]Epoch 2970/3000:  99%|█████████▉| 2969/3000 [22:17&lt;00:15,  1.97it/s, v_num=1, train_loss_step=9.95e+5, train_loss_epoch=1.02e+6]Epoch 2970/3000:  99%|█████████▉| 2970/3000 [22:18&lt;00:16,  1.86it/s, v_num=1, train_loss_step=9.95e+5, train_loss_epoch=1.02e+6]Epoch 2970/3000:  99%|█████████▉| 2970/3000 [22:18&lt;00:16,  1.86it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2971/3000:  99%|█████████▉| 2970/3000 [22:18&lt;00:16,  1.86it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2971/3000:  99%|█████████▉| 2971/3000 [22:18&lt;00:14,  2.02it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2971/3000:  99%|█████████▉| 2971/3000 [22:18&lt;00:14,  2.02it/s, v_num=1, train_loss_step=9.93e+5, train_loss_epoch=1.02e+6]Epoch 2972/3000:  99%|█████████▉| 2971/3000 [22:18&lt;00:14,  2.02it/s, v_num=1, train_loss_step=9.93e+5, train_loss_epoch=1.02e+6]Epoch 2972/3000:  99%|█████████▉| 2972/3000 [22:19&lt;00:14,  1.93it/s, v_num=1, train_loss_step=9.93e+5, train_loss_epoch=1.02e+6]Epoch 2972/3000:  99%|█████████▉| 2972/3000 [22:19&lt;00:14,  1.93it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2973/3000:  99%|█████████▉| 2972/3000 [22:19&lt;00:14,  1.93it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2973/3000:  99%|█████████▉| 2973/3000 [22:19&lt;00:13,  2.04it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2973/3000:  99%|█████████▉| 2973/3000 [22:19&lt;00:13,  2.04it/s, v_num=1, train_loss_step=9.95e+5, train_loss_epoch=1.02e+6]Epoch 2974/3000:  99%|█████████▉| 2973/3000 [22:19&lt;00:13,  2.04it/s, v_num=1, train_loss_step=9.95e+5, train_loss_epoch=1.02e+6]Epoch 2974/3000:  99%|█████████▉| 2974/3000 [22:20&lt;00:12,  2.06it/s, v_num=1, train_loss_step=9.95e+5, train_loss_epoch=1.02e+6]Epoch 2974/3000:  99%|█████████▉| 2974/3000 [22:20&lt;00:12,  2.06it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2975/3000:  99%|█████████▉| 2974/3000 [22:20&lt;00:12,  2.06it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2975/3000:  99%|█████████▉| 2975/3000 [22:20&lt;00:13,  1.91it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2975/3000:  99%|█████████▉| 2975/3000 [22:20&lt;00:13,  1.91it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2976/3000:  99%|█████████▉| 2975/3000 [22:20&lt;00:13,  1.91it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2976/3000:  99%|█████████▉| 2976/3000 [22:21&lt;00:11,  2.08it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2976/3000:  99%|█████████▉| 2976/3000 [22:21&lt;00:11,  2.08it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2977/3000:  99%|█████████▉| 2976/3000 [22:21&lt;00:11,  2.08it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2977/3000:  99%|█████████▉| 2977/3000 [22:21&lt;00:11,  2.06it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2977/3000:  99%|█████████▉| 2977/3000 [22:21&lt;00:11,  2.06it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2978/3000:  99%|█████████▉| 2977/3000 [22:21&lt;00:11,  2.06it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2978/3000:  99%|█████████▉| 2978/3000 [22:21&lt;00:09,  2.38it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2978/3000:  99%|█████████▉| 2978/3000 [22:21&lt;00:09,  2.38it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2979/3000:  99%|█████████▉| 2978/3000 [22:22&lt;00:09,  2.38it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2979/3000:  99%|█████████▉| 2979/3000 [22:22&lt;00:09,  2.29it/s, v_num=1, train_loss_step=1.04e+6, train_loss_epoch=1.02e+6]Epoch 2979/3000:  99%|█████████▉| 2979/3000 [22:22&lt;00:09,  2.29it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2980/3000:  99%|█████████▉| 2979/3000 [22:22&lt;00:09,  2.29it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2980/3000:  99%|█████████▉| 2980/3000 [22:23&lt;00:09,  2.04it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2980/3000:  99%|█████████▉| 2980/3000 [22:23&lt;00:09,  2.04it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2981/3000:  99%|█████████▉| 2980/3000 [22:23&lt;00:09,  2.04it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2981/3000:  99%|█████████▉| 2981/3000 [22:23&lt;00:07,  2.38it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2981/3000:  99%|█████████▉| 2981/3000 [22:23&lt;00:07,  2.38it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2982/3000:  99%|█████████▉| 2981/3000 [22:23&lt;00:07,  2.38it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2982/3000:  99%|█████████▉| 2982/3000 [22:23&lt;00:07,  2.31it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2982/3000:  99%|█████████▉| 2982/3000 [22:23&lt;00:07,  2.31it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2983/3000:  99%|█████████▉| 2982/3000 [22:23&lt;00:07,  2.31it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2983/3000:  99%|█████████▉| 2983/3000 [22:24&lt;00:07,  2.30it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2983/3000:  99%|█████████▉| 2983/3000 [22:24&lt;00:07,  2.30it/s, v_num=1, train_loss_step=1e+6, train_loss_epoch=1.02e+6]   Epoch 2984/3000:  99%|█████████▉| 2983/3000 [22:24&lt;00:07,  2.30it/s, v_num=1, train_loss_step=1e+6, train_loss_epoch=1.02e+6]Epoch 2984/3000:  99%|█████████▉| 2984/3000 [22:24&lt;00:07,  2.04it/s, v_num=1, train_loss_step=1e+6, train_loss_epoch=1.02e+6]Epoch 2984/3000:  99%|█████████▉| 2984/3000 [22:24&lt;00:07,  2.04it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2985/3000:  99%|█████████▉| 2984/3000 [22:24&lt;00:07,  2.04it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2985/3000: 100%|█████████▉| 2985/3000 [22:25&lt;00:07,  2.01it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2985/3000: 100%|█████████▉| 2985/3000 [22:25&lt;00:07,  2.01it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2986/3000: 100%|█████████▉| 2985/3000 [22:25&lt;00:07,  2.01it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2986/3000: 100%|█████████▉| 2986/3000 [22:25&lt;00:07,  1.96it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2986/3000: 100%|█████████▉| 2986/3000 [22:25&lt;00:07,  1.96it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2987/3000: 100%|█████████▉| 2986/3000 [22:25&lt;00:07,  1.96it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2987/3000: 100%|█████████▉| 2987/3000 [22:26&lt;00:06,  1.92it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2987/3000: 100%|█████████▉| 2987/3000 [22:26&lt;00:06,  1.92it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.02e+6]Epoch 2988/3000: 100%|█████████▉| 2987/3000 [22:26&lt;00:06,  1.92it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.02e+6]Epoch 2988/3000: 100%|█████████▉| 2988/3000 [22:27&lt;00:06,  1.89it/s, v_num=1, train_loss_step=1.06e+6, train_loss_epoch=1.02e+6]Epoch 2988/3000: 100%|█████████▉| 2988/3000 [22:27&lt;00:06,  1.89it/s, v_num=1, train_loss_step=9.99e+5, train_loss_epoch=1.02e+6]Epoch 2989/3000: 100%|█████████▉| 2988/3000 [22:27&lt;00:06,  1.89it/s, v_num=1, train_loss_step=9.99e+5, train_loss_epoch=1.02e+6]Epoch 2989/3000: 100%|█████████▉| 2989/3000 [22:27&lt;00:05,  2.09it/s, v_num=1, train_loss_step=9.99e+5, train_loss_epoch=1.02e+6]Epoch 2989/3000: 100%|█████████▉| 2989/3000 [22:27&lt;00:05,  2.09it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2990/3000: 100%|█████████▉| 2989/3000 [22:27&lt;00:05,  2.09it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2990/3000: 100%|█████████▉| 2990/3000 [22:27&lt;00:04,  2.13it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2990/3000: 100%|█████████▉| 2990/3000 [22:27&lt;00:04,  2.13it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2991/3000: 100%|█████████▉| 2990/3000 [22:27&lt;00:04,  2.13it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2991/3000: 100%|█████████▉| 2991/3000 [22:28&lt;00:04,  1.97it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2991/3000: 100%|█████████▉| 2991/3000 [22:28&lt;00:04,  1.97it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2992/3000: 100%|█████████▉| 2991/3000 [22:28&lt;00:04,  1.97it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2992/3000: 100%|█████████▉| 2992/3000 [22:28&lt;00:04,  1.91it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2992/3000: 100%|█████████▉| 2992/3000 [22:28&lt;00:04,  1.91it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2993/3000: 100%|█████████▉| 2992/3000 [22:28&lt;00:04,  1.91it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2993/3000: 100%|█████████▉| 2993/3000 [22:29&lt;00:03,  1.99it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 2993/3000: 100%|█████████▉| 2993/3000 [22:29&lt;00:03,  1.99it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2994/3000: 100%|█████████▉| 2993/3000 [22:29&lt;00:03,  1.99it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2994/3000: 100%|█████████▉| 2994/3000 [22:29&lt;00:02,  2.17it/s, v_num=1, train_loss_step=1.02e+6, train_loss_epoch=1.02e+6]Epoch 2994/3000: 100%|█████████▉| 2994/3000 [22:29&lt;00:02,  2.17it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2995/3000: 100%|█████████▉| 2994/3000 [22:29&lt;00:02,  2.17it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2995/3000: 100%|█████████▉| 2995/3000 [22:30&lt;00:02,  2.08it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2995/3000: 100%|█████████▉| 2995/3000 [22:30&lt;00:02,  2.08it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2996/3000: 100%|█████████▉| 2995/3000 [22:30&lt;00:02,  2.08it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2996/3000: 100%|█████████▉| 2996/3000 [22:30&lt;00:01,  2.23it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2996/3000: 100%|█████████▉| 2996/3000 [22:30&lt;00:01,  2.23it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2997/3000: 100%|█████████▉| 2996/3000 [22:30&lt;00:01,  2.23it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2997/3000: 100%|█████████▉| 2997/3000 [22:31&lt;00:01,  2.14it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2997/3000: 100%|█████████▉| 2997/3000 [22:31&lt;00:01,  2.14it/s, v_num=1, train_loss_step=1e+6, train_loss_epoch=1.02e+6]   Epoch 2998/3000: 100%|█████████▉| 2997/3000 [22:31&lt;00:01,  2.14it/s, v_num=1, train_loss_step=1e+6, train_loss_epoch=1.02e+6]Epoch 2998/3000: 100%|█████████▉| 2998/3000 [22:31&lt;00:00,  2.13it/s, v_num=1, train_loss_step=1e+6, train_loss_epoch=1.02e+6]Epoch 2998/3000: 100%|█████████▉| 2998/3000 [22:31&lt;00:00,  2.13it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2999/3000: 100%|█████████▉| 2998/3000 [22:31&lt;00:00,  2.13it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2999/3000: 100%|█████████▉| 2999/3000 [22:32&lt;00:00,  1.98it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 2999/3000: 100%|█████████▉| 2999/3000 [22:32&lt;00:00,  1.98it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 3000/3000: 100%|█████████▉| 2999/3000 [22:32&lt;00:00,  1.98it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 3000/3000: 100%|██████████| 3000/3000 [22:32&lt;00:00,  2.22it/s, v_num=1, train_loss_step=1.01e+6, train_loss_epoch=1.02e+6]Epoch 3000/3000: 100%|██████████| 3000/3000 [22:32&lt;00:00,  2.22it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]Epoch 3000/3000: 100%|██████████| 3000/3000 [22:32&lt;00:00,  2.22it/s, v_num=1, train_loss_step=1.03e+6, train_loss_epoch=1.02e+6]\n\n\n\n\n\n\n\n\n\nGet the results from the model, also put them in the .obs slot.\n\nst_adata.obsm[\"deconvolution\"] = spatial_model.get_proportions()\n\n# also copy to the obsm data frame\nfor ct in st_adata.obsm[\"deconvolution\"].columns:\n    st_adata.obs[ct] = st_adata.obsm[\"deconvolution\"][ct]\n\nWe are then able to explore how cell types in the scRNA-seq dataset are predicted onto the visium dataset. Let’s first visualize the neurons cortical layers.\n\nsc.pl.spatial(\n    st_adata,\n    img_key=\"hires\",\n    color=[\"L2/3 IT\", \"L4\", \"L5 PT\", \"L6 CT\"],\n    library_id=lib_a,\n    size=1.5,\n    ncols=2\n)\n\n\n\n\n\n\n\n\nWe can go ahead an visualize astrocytes and oligodendrocytes as well.\n\nsc.pl.spatial(\n    st_adata, img_key=\"hires\", color=[\"Oligo\", \"Astro\"], size=1.5, library_id=lib_a\n)\n\n\n\n\n\n\n\n\nKeep in mind that the deconvolution results are just predictions, depending on how well your scRNAseq data covers the celltypes that are present in the ST data and on how parameters, gene selection etc. are tuned you may get different results.\n\nsc.pl.violin(st_adata, [\"L2/3 IT\", \"L6 CT\",\"Oligo\",\"Astro\"],\n            jitter=0.4, groupby = 'clusters', rotation= 45)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nSubset for another region that does not contain cortex cells and check what you get from the label transfer. Suggested region is the right end of the posterial section that you can select like this:\n\nlib_p = \"V1_Mouse_Brain_Sagittal_Posterior\"\n\nadata_subregion = adata[\n    (adata.obs.library_id == lib_p)\n    & (adata.obsm[\"spatial\"][:, 0] &gt; 6500),\n    :,\n].copy()\n\nsc.pl.spatial(\n    adata_subregion,\n    img_key=\"hires\",\n    library_id=lib_p,\n    color=['n_genes_by_counts'],\n    size=1.5\n)"
  },
  {
    "objectID": "labs/scanpy/scanpy_08_spatial.html#meta-session",
    "href": "labs/scanpy/scanpy_08_spatial.html#meta-session",
    "title": " Spatial Transcriptomics",
    "section": "7 Session info",
    "text": "7 Session info\n\n\nClick here\n\n\nsc.logging.print_versions()\n\n-----\nanndata     0.10.3\nscanpy      1.9.6\n-----\nPIL                 10.0.0\nabsl                NA\nannotated_types     0.6.0\nannoy               NA\nanyio               NA\nasttokens           NA\nattr                23.1.0\nbabel               2.12.1\nbackcall            0.2.0\nbackoff             2.2.1\nbs4                 4.12.2\ncertifi             2023.11.17\ncffi                1.15.1\ncharset_normalizer  3.1.0\nchex                0.1.83\nclick               8.1.7\ncolorama            0.4.6\ncomm                0.1.3\ncontextlib2         NA\ncroniter            NA\ncycler              0.12.1\ncython_runtime      NA\ndateutil            2.8.2\ndebugpy             1.6.7\ndecorator           5.1.1\ndeepdiff            6.7.1\ndefusedxml          0.7.1\ndocrep              0.3.2\nexceptiongroup      1.2.0\nexecuting           1.2.0\nfastapi             0.103.0\nfastjsonschema      NA\nfbpca               NA\nflax                0.6.1\nfsspec              2023.12.2\ngmpy2               2.1.2\nh5py                3.9.0\nidna                3.4\nigraph              0.10.8\nintervaltree        NA\nipykernel           6.23.1\nipython_genutils    0.2.0\njax                 0.4.13\njaxlib              0.4.12\njedi                0.18.2\njinja2              3.1.2\njoblib              1.3.2\njson5               NA\njsonpointer         2.0\njsonschema          4.17.3\njupyter_events      0.6.3\njupyter_server      2.6.0\njupyterlab_server   2.22.1\nkiwisolver          1.4.5\nleidenalg           0.10.1\nlightning           2.0.9.post0\nlightning_cloud     0.5.57\nlightning_utilities 0.10.0\nllvmlite            0.41.1\nlouvain             0.8.1\nmarkupsafe          2.1.2\nmatplotlib          3.8.0\nmatplotlib_inline   0.1.6\nml_collections      NA\nml_dtypes           0.3.1\nmpl_toolkits        NA\nmpmath              1.3.0\nmsgpack             1.0.7\nmudata              0.2.3\nmultipart           0.0.6\nmultipledispatch    0.6.0\nnatsort             8.4.0\nnbformat            5.8.0\nnumba               0.58.1\nnumpy               1.26.2\nnumpyro             0.13.2\nopt_einsum          v3.3.0\noptax               0.1.7\nordered_set         4.1.0\norjson              3.9.10\noverrides           NA\npackaging           23.1\npandas              2.1.4\nparso               0.8.3\npatsy               0.5.5\npexpect             4.8.0\npickleshare         0.7.5\npkg_resources       NA\nplatformdirs        3.5.1\nprometheus_client   NA\nprompt_toolkit      3.0.38\npsutil              5.9.5\nptyprocess          0.7.0\npure_eval           0.2.2\npvectorc            NA\npycparser           2.21\npydantic            2.1.1\npydantic_core       2.4.0\npydev_ipython       NA\npydevconsole        NA\npydevd              2.9.5\npydevd_file_utils   NA\npydevd_plugins      NA\npydevd_tracing      NA\npygments            2.15.1\npynndescent         0.5.11\npyparsing           3.1.1\npyro                1.8.6+4be5c2e\npyrsistent          NA\npythonjsonlogger    NA\npytz                2023.3\nrequests            2.31.0\nrfc3339_validator   0.1.4\nrfc3986_validator   0.1.1\nrich                NA\nscanorama           1.7.4\nscipy               1.11.4\nscvi                1.0.4\nseaborn             0.12.2\nsend2trash          NA\nsession_info        1.0.0\nsix                 1.16.0\nsklearn             1.3.2\nsniffio             1.3.0\nsocks               1.7.1\nsortedcontainers    2.4.0\nsoupsieve           2.3.2.post1\nsparse              0.14.0\nstack_data          0.6.2\nstarlette           0.27.0\nstatsmodels         0.14.1\nsympy               1.12\ntexttable           1.7.0\nthreadpoolctl       3.2.0\ntomli               2.0.1\ntoolz               0.12.0\ntorch               2.0.0\ntorchmetrics        1.2.1\ntornado             6.3.2\ntqdm                4.65.0\ntraitlets           5.9.0\ntyping_extensions   NA\numap                0.5.5\nurllib3             2.0.2\nuvicorn             0.25.0\nwcwidth             0.2.6\nwebsocket           1.5.2\nwebsockets          12.0\nxarray              2023.12.0\nyaml                6.0\nzmq                 25.0.2\nzoneinfo            NA\nzstandard           0.19.0\n-----\nIPython             8.13.2\njupyter_client      8.2.0\njupyter_core        5.3.0\njupyterlab          4.0.1\nnotebook            6.5.4\n-----\nPython 3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) [GCC 11.3.0]\nLinux-5.15.0-92-generic-x86_64-with-glibc2.35\n-----\nSession information updated at 2024-02-06 01:15"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Warning\n\n\n\nCourse content is presently being updated. As a result, there’s a possibility that certain lab exercises might not function correctly. This alert will be taken down once the update process is complete."
  },
  {
    "objectID": "index.html#single-cell-rna-seq-analysis",
    "href": "index.html#single-cell-rna-seq-analysis",
    "title": "",
    "section": "Single Cell RNA-seq Analysis",
    "text": "Single Cell RNA-seq Analysis\n\n\nOverview of scRNAseq technologies\nQC, normalization and transformation\nDimensionality reduction and clustering\nDifferential gene expression\nCelltype prediction\nTrajectory analysis\nSeurat, Bioconductor and Scanpy toolkits\n\n\n\nUpdated: 06-02-2024 at 01:15:17."
  },
  {
    "objectID": "home_contents.html",
    "href": "home_contents.html",
    "title": "Contents",
    "section": "",
    "text": "You can run the labs either in a Singularity container on Uppmax (recommended during course) or using Docker locally on your system. Instructions on running the labs are provided here\nA short description of the data used in the tutorials is provided here."
  },
  {
    "objectID": "home_contents.html#useful-resources",
    "href": "home_contents.html#useful-resources",
    "title": "Contents",
    "section": "Useful resources",
    "text": "Useful resources\n\nThe github repository for this course \nPre-recorded videos of lectures (from 2022) are available on Youtube \nSingle Cell Glossary \nSingle cell RNA-seq course at from Hemberg lab \nSingle cell RNA-seq course in Python \nSingle cell RNA-seq course at Broad \nRepository listing many scRNA-seq tools \nSingleCellExperiment objects for many datasets \nConquer datasets - many different datasets based on a salmon pipeline \nThe Human Cell Atlas project"
  },
  {
    "objectID": "home_info.html",
    "href": "home_info.html",
    "title": "Practical Info",
    "section": "",
    "text": "Uppsala\n\n\n\n\n\n\n\n\nRoom E10:1309 Entrance C11 Biomedicinskt centrum Uppsala University / ScilifeLab Husargatan 3 75237 Uppsala Sweden\nFew selected hotels are listed below ranked by distance from the venue.\n\nHotel von Kraemer (900 m, 11 min walk)\nAkademihotellet (1.7 Km, 21 min walk)\nCityStay Hotell (1.8 Km, 21 min walk)\nGrand Hotel Hörnan (1.9 Km, 23 min walk)\nHotell Centralstation (2.1 Km, 25 min walk)\nBest Western Svava (2.2 Km, 26 min walk)\n\nThe venue and hotels are also marked on the map.\nUse the UL website or the UL app for bus and train services around Uppsala. For buses from the Centralstation (Train/Bus), take Bus 4 (towards Gottsunda Centrum) or 8 (towards Sunnersta) and get off at the stop Uppsala Science Park. Bus tickets can be purchased in the app or directly from the driver using a credit card."
  },
  {
    "objectID": "home_info.html#location",
    "href": "home_info.html#location",
    "title": "Practical Info",
    "section": "",
    "text": "Uppsala\n\n\n\n\n\n\n\n\nRoom E10:1309 Entrance C11 Biomedicinskt centrum Uppsala University / ScilifeLab Husargatan 3 75237 Uppsala Sweden\nFew selected hotels are listed below ranked by distance from the venue.\n\nHotel von Kraemer (900 m, 11 min walk)\nAkademihotellet (1.7 Km, 21 min walk)\nCityStay Hotell (1.8 Km, 21 min walk)\nGrand Hotel Hörnan (1.9 Km, 23 min walk)\nHotell Centralstation (2.1 Km, 25 min walk)\nBest Western Svava (2.2 Km, 26 min walk)\n\nThe venue and hotels are also marked on the map.\nUse the UL website or the UL app for bus and train services around Uppsala. For buses from the Centralstation (Train/Bus), take Bus 4 (towards Gottsunda Centrum) or 8 (towards Sunnersta) and get off at the stop Uppsala Science Park. Bus tickets can be purchased in the app or directly from the driver using a credit card."
  },
  {
    "objectID": "home_info.html#contact",
    "href": "home_info.html#contact",
    "title": "Practical Info",
    "section": "Contact",
    "text": "Contact\nThis workshop is run by the National Bioinformatics Infrastructure Sweden (NBIS). NBIS is a platform at SciLifeLab (Science For Life Laboratory) and the Swedish node of Elixir.\nIf you would like to get in touch with us regarding this workshop, please contact us at edu.sc [at] nbis.se."
  },
  {
    "objectID": "home_precourse.html",
    "href": "home_precourse.html",
    "title": "Precourse",
    "section": "",
    "text": "We strongly recommend for those not yet familiar with UNIX and/or R/Python to take this opportunity and take these online tutorials, since those are requirements for the workshop. This will help you to develop your programming skills and we can always learn a few tricks here and there, even if you are already experienced.\n\nUNIX: Part 1, Part 2\nR: Part 1, Part 2\nPython: Part 1, Part 2\n\nAfter taking those courses (or any other equivalent course in programming in bash and R or Python), you should be familiar with\n\nFile structure and manipulation in bash\nLoading, handling and manipulating vectors, matrices, factors and lists\nCreating for-loops\nUsing Rmarkdown/Jupyter for reports\nEditing and writing files in the command line\nAnd much more …"
  },
  {
    "objectID": "home_precourse.html#fa-book-coding",
    "href": "home_precourse.html#fa-book-coding",
    "title": "Precourse",
    "section": "",
    "text": "We strongly recommend for those not yet familiar with UNIX and/or R/Python to take this opportunity and take these online tutorials, since those are requirements for the workshop. This will help you to develop your programming skills and we can always learn a few tricks here and there, even if you are already experienced.\n\nUNIX: Part 1, Part 2\nR: Part 1, Part 2\nPython: Part 1, Part 2\n\nAfter taking those courses (or any other equivalent course in programming in bash and R or Python), you should be familiar with\n\nFile structure and manipulation in bash\nLoading, handling and manipulating vectors, matrices, factors and lists\nCreating for-loops\nUsing Rmarkdown/Jupyter for reports\nEditing and writing files in the command line\nAnd much more …"
  },
  {
    "objectID": "home_precourse.html#fa-brands-slack-slack",
    "href": "home_precourse.html#fa-brands-slack-slack",
    "title": "Precourse",
    "section": " Slack",
    "text": "Slack\nWe will use Slack for communication, troubleshooting and group discussions. Please install Slack. All accepted students will receive an invitation link via email to join the course workspace. Please add this workspace to your Slack application on your desktop rather than using it in the browser.\nOnce you are in the workspace, please join the following channels:\n\n#general for general questions about the workshop\n#precourse for questions related to precourse preparation\n\n\n\n\n\n\n\nNote\n\n\n\nPlease post your question in the channel and NOT directly to the teacher. Any participant that knows the answer to any problem is encouraged to help too."
  },
  {
    "objectID": "home_precourse.html#fa-server-uppmax",
    "href": "home_precourse.html#fa-server-uppmax",
    "title": "Precourse",
    "section": " Uppmax",
    "text": "Uppmax\nWe will use the high performance computing cluster (HPC) UPPMAX for the workshop. You will need to create accounts if you don’t already have one. And you will need to join the course projects. See instructions here."
  },
  {
    "objectID": "home_precourse.html#fa-brands-docker-docker",
    "href": "home_precourse.html#fa-brands-docker-docker",
    "title": "Precourse",
    "section": " Docker",
    "text": "Docker\nIf you use Uppmax, you do not need any local installation or setup on your system. If you use Docker, you will need to set up and run Docker yourself. Instructions are here."
  },
  {
    "objectID": "home_precourse.html#fa-circle-question-faq",
    "href": "home_precourse.html#fa-circle-question-faq",
    "title": "Precourse",
    "section": " FAQ",
    "text": "FAQ\nPlease refer to the FAQ for troubleshooting common issues."
  },
  {
    "objectID": "home_syllabus.html",
    "href": "home_syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "SyllabusWho can apply?Entry requirements\n\n\nThis workshop will cover the basic steps in single cell RNAseq (scRNAseq) processing and data analysis, with lectures and practical exercises.\nTopics covered will include:\n\nOverview of the current scRNAseq technologies\nRaw reads into expression values\nQuality control of scRNAseq data\nDimensionality reduction and clustering techniques\nData normalization\nDifferential gene expression for scRNAseq data\nCelltype prediction\nTrajectory analysis\nComparison of sc analysis toolkits: Seurat, Bioconductor and Scanpy\n\n\n\nThis is a national workshop open for PhD students, postdocs, group leaders and core facility staff within all Swedish universities. We do accept application from other countries, but priority is given to applicants from Swedish universities prior to applicants from industry and academics from other countries.\nPlease note that NBIS training events do not provide any formal university credits. The training content is estimated to correspond to a certain number of credits, however the estimated credits are just guidelines. If formal credits are crucial, the student needs to confer with the home department before submitting a workshop application in order to establish whether the workshop is valid for formal credits or not.\nWe cannot invoice private individuals, therefore an affiliation is required.\n\n\nPractical exercises can be performed using R or Python, so we only accept students with previous experience in one of those programming languages.\n\nBasic knowledge in R/Python and command line (bash).\nBe able to use your own computer with R or Python installed for the practical computational exercises. Instructions on installation will be sent by email to accepted participants.\nProgramming/scripting experience is required (in R or python).\nBasic understanding of NGS technologies and RNA-sequencing data.\nDesirable: Previous experience with RNA-seq analysis and/or participation in NGS/RNA-seq workshop is an advantage.\n\nDue to limited space the workshop can accommodate maximum of 25 participants. If we receive more applications, participants will be selected based on several criteria. Selection criteria include correct entry requirements, motivation to attend the workshop as well as gender and geographical balance."
  },
  {
    "objectID": "other/uppmax.html",
    "href": "other/uppmax.html",
    "title": "UPPMAX Account Guide",
    "section": "",
    "text": "Caution\n\n\n\nDo these steps well in advance as it can take up to 2 weeks for UPPMAX accounts to be approved. If this is incomplete, you may not be able to follow the labs during the workshop.\nThese are the basic steps in this process:"
  },
  {
    "objectID": "other/uppmax.html#create-an-account-in-supr.",
    "href": "other/uppmax.html#create-an-account-in-supr.",
    "title": "UPPMAX Account Guide",
    "section": "1 ​Create an account in SUPR.",
    "text": "1 ​Create an account in SUPR.\n​If you already have a SUPR account, please continue to the next step.\n​Go to​ https://supr.naiss.se/​ and click Register New Person at the bottom of the first page. Complete the registration process, preferably using SWAMID, and login. If you for some reason can’t use SWAMID to login, you will have to send physical (not electronic) copy of your ID to a place in Gothenburg for manual approval. Do this as ​soon as possible​, as this process can take ​more than 2 weeks.\n\n\n\nSUPR login screen"
  },
  {
    "objectID": "other/uppmax.html#apply-for-membership",
    "href": "other/uppmax.html#apply-for-membership",
    "title": "UPPMAX Account Guide",
    "section": "2 ​Apply for membership",
    "text": "2 ​Apply for membership\n​Log in using your SUPR account. ​Under the Projects heading, go to Request Membership in Project. ​Search for the following project IDs:\n\n\nnaiss2023-22-1345, naiss2023-23-648\n\n\nRequest membership to both projects. The first project is to run computations and the second project is for storage.\n\n\n\nRequest to join a project in SUPR"
  },
  {
    "objectID": "other/uppmax.html#accept-naiss-user-agreement",
    "href": "other/uppmax.html#accept-naiss-user-agreement",
    "title": "UPPMAX Account Guide",
    "section": "3 ​Accept NAISS User Agreement",
    "text": "3 ​Accept NAISS User Agreement\n​In SUPR, click on the link Personal Information in the left sidebar. You will have to accept the NAISS User Agreement to be able to get an UPPMAX account."
  },
  {
    "objectID": "other/uppmax.html#apply-for-uppmax-account",
    "href": "other/uppmax.html#apply-for-uppmax-account",
    "title": "UPPMAX Account Guide",
    "section": "4 Apply for UPPMAX account",
    "text": "4 Apply for UPPMAX account\n​In SUPR, click on the link Accounts in the left sidebar and apply for an UPPMAX account under the heading Account Requests."
  },
  {
    "objectID": "other/uppmax.html#uppmax-account-details",
    "href": "other/uppmax.html#uppmax-account-details",
    "title": "UPPMAX Account Guide",
    "section": "5 UPPMAX account details",
    "text": "5 UPPMAX account details\n​Within about 2 working days you should get an email with instructions. ​Please, follow these instructions carefully. ​A while later you will get an email with your user name, and another email with a link to your password.\n\n\n\n\n\n\nCaution\n\n\n\nThe link is only valid for ​1​ visit or 7 days​, so if you click the link you better save the password, because you will not be able to use the link again. Do this before 7 days have passed, otherwise the link will no longer be valid."
  },
  {
    "objectID": "other/uppmax.html#login-with-new-uppmax-account",
    "href": "other/uppmax.html#login-with-new-uppmax-account",
    "title": "UPPMAX Account Guide",
    "section": "6 Login with new UPPMAX account",
    "text": "6 Login with new UPPMAX account\n​Open your terminal program (Terminal in OSX and Linux, otherwise download MobaXterm​ (portable edition) if you have Windows).\n​Type this command in your terminal program: ssh username@rackham.uppmax.uu.se ​You will be asked for your password now, and you will not see any response in the terminal while typing your password. This is to hide the length of your password, i.e. normal. Just press enter when you have typed it in and you should log in.\n​If it is the first time you log in, it will ask you to change your LDAP password (the password you just typed). It will directly ask you for your password again, so type it once more. After that it will ask you for your new password, so make up a new one and press enter. After that it will ask you to confirm the new password. When the password change is completed you will be disconnected and you will have to connect again, using your new password to log in this time."
  },
  {
    "objectID": "other/uppmax.html#create-a-folder",
    "href": "other/uppmax.html#create-a-folder",
    "title": "UPPMAX Account Guide",
    "section": "7 ​Create a folder",
    "text": "7 ​Create a folder\n\n\n\n\n\n\nCaution\n\n\n\n​After having received information that your membership is approved, ​wait 24 h before continuing, as it takes up to 24 h for SUPR to sync with UPPMAX. Else, you might get the message Permission denied when writing files or folders.\n\n\nCreate a directory for you to work in. Replace &lt;username&gt; with your actual user name.\n\n\n\n\nbash\n\nmkdir /proj/​naiss2023-23-648/nobackup/&lt;username&gt;\n\n\n\n​Unless you got some kind of error message. you should now be finished. To make sure the folder was created you can type\n\n\n\n\nbash\n\nls /proj/​naiss2023-23-648/nobackup/\n\n\n\n​It should list all directories along with the one you created. ​If you get an error message, contact us in Slack."
  },
  {
    "objectID": "other/docker.html",
    "href": "other/docker.html",
    "title": "Docker Set Up",
    "section": "",
    "text": "Ensure that you install Docker or Docker Desktop before the course starts. If you do not have admin rights to install software on your laptop, talk to your local IT for help.\n\n\nFollow the installation instructions for your OS:\n\nUbuntu\nDebian\nFedora\n\nAfter Docker Desktop starts, open the Dashboard and go to Settings ( in the top-right) &gt; General. Follow the instructions in the General section.\n\n\n\nVisit this page to ensure you have the requirements necessary.\nDownload Docker Desktop for Mac with Apple Silicon or Intel Chip and follow the installation instructions.\nAfter Docker Desktop starts, open the Dashboard and go to Settings ( in the top-right) &gt; General. Follow the instructions in the General section.\n\n\n\n\n\n\nImportant\n\n\n\nOn Mac M1/M2/M3, in Docker Settings &gt; General, check Use Rosetta for x86/amd64 emulation on Apple Silicon.\n\n\n\n\n\nVisit this page to ensure you have the requirements necessary.\nDownload Docker Desktop for Windows and follow the installation instructions.\nAfter Docker Desktop starts, open the Dashboard and go to Settings ( in the top-right) &gt; General. Follow the instructions in the General section.\nAfter installation, open a PowerShell terminal and try to run docker --version. If that works, Docker is set up."
  },
  {
    "objectID": "other/docker.html#install-docker",
    "href": "other/docker.html#install-docker",
    "title": "Docker Set Up",
    "section": "",
    "text": "Ensure that you install Docker or Docker Desktop before the course starts. If you do not have admin rights to install software on your laptop, talk to your local IT for help.\n\n\nFollow the installation instructions for your OS:\n\nUbuntu\nDebian\nFedora\n\nAfter Docker Desktop starts, open the Dashboard and go to Settings ( in the top-right) &gt; General. Follow the instructions in the General section.\n\n\n\nVisit this page to ensure you have the requirements necessary.\nDownload Docker Desktop for Mac with Apple Silicon or Intel Chip and follow the installation instructions.\nAfter Docker Desktop starts, open the Dashboard and go to Settings ( in the top-right) &gt; General. Follow the instructions in the General section.\n\n\n\n\n\n\nImportant\n\n\n\nOn Mac M1/M2/M3, in Docker Settings &gt; General, check Use Rosetta for x86/amd64 emulation on Apple Silicon.\n\n\n\n\n\nVisit this page to ensure you have the requirements necessary.\nDownload Docker Desktop for Windows and follow the installation instructions.\nAfter Docker Desktop starts, open the Dashboard and go to Settings ( in the top-right) &gt; General. Follow the instructions in the General section.\nAfter installation, open a PowerShell terminal and try to run docker --version. If that works, Docker is set up."
  },
  {
    "objectID": "other/docker.html#test-installation",
    "href": "other/docker.html#test-installation",
    "title": "Docker Set Up",
    "section": "2 Test installation",
    "text": "2 Test installation\nFrom the terminal, type:\ndocker --version\nand then run your first image by typing:\ndocker run hello-world\nIf both work as expected, you successfully installed Docker Desktop!"
  },
  {
    "objectID": "other/docker.html#allocate-resources",
    "href": "other/docker.html#allocate-resources",
    "title": "Docker Set Up",
    "section": "3 Allocate resources",
    "text": "3 Allocate resources\nOpen the Docker Dashboard when Docker Desktop starts and go to Settings ( in the top-right) &gt; Resources to allocate the following resources:\n\nCPU limit: 8\n\nMemory limit: 12 GB\n\nSwap: 2 GB\n\nOn Windows, if WSL engine is used, you might not be able to change resources directly."
  },
  {
    "objectID": "other/containers.html",
    "href": "other/containers.html",
    "title": "Run labs in container",
    "section": "",
    "text": "Note\n\n\n\nThree different toolkits, namely Seurat (R/RStudio), Bioconductor (R/RStudio) and Scanpy (Python/Jupyter) are available to perform the scRNAseq analysis. The labs can be run on Uppmax using Singularity (Apptainer) or on your local machine using Docker. Both options provide the necessary environment to run the analysis.\nIf you use Uppmax, you do not need any local installation or setup on your system but you need an Uppmax account and become a member of the Uppmax projects. If you use Docker, you will need to set up and run Docker yourself."
  },
  {
    "objectID": "other/containers.html#option-a-run-singularity-on-uppmax-recommended",
    "href": "other/containers.html#option-a-run-singularity-on-uppmax-recommended",
    "title": "Run labs in container",
    "section": "1 Option A: Run Singularity on Uppmax (Recommended)",
    "text": "1 Option A: Run Singularity on Uppmax (Recommended)\n\n1.1 Configure Uppmax\n\n1.1.1 Storage\nThis setup needs to be run only once at the beginning of the workshop. It will connect you to Uppmax and create a folder with your username in the workshop’s directory.\nFirst connect to Uppmax. Replace &lt;username&gt; with your actual user name.\nssh -Y &lt;username&gt;@rackham.uppmax.uu.se\nOnce you are connected, change into the working directory that you created in the Precourse instructions.\n\n\n# change to project directory\ncd /proj/naiss2023-23-648/nobackup/&lt;username&gt;\n\n\nNow we can fetch the scripts for the labs.\ngit clone --depth 1 --single-branch --branch master https://github.com/nbisweden/workshop-scRNAseq.git\ncd workshop-scRNAseq/compiled/labs\nNavigate to compiled/labs/&lt;topic&gt; directory to work on labs. And &lt;topic&gt; is either seurat, bioc or scanpy.\n\n\n1.1.2 Compute\nTo run the labs you need to first create an interactive session.\nConnect to Uppmax and change into your working directory (if not already done):\n\n\nssh -Y &lt;username&gt;@rackham.uppmax.uu.se\ncd /proj/naiss2023-23-648/nobackup/&lt;username&gt;/workshop-scRNAseq/compiled/labs\n\n\nThen start an interactive session:\n\n\ninteractive -A naiss2023-22-1345 -p core -n 4 -t 08:00:00\n\n\n\n\n\n\n\n\nImportant\n\n\n\nRun the interactive command above only ONCE per day at the beginning of the practical session.\n\n\nThis command will connect you to a node with 5 cores for a duration of 8 hours. You can check the node name under the NODELIST column in the output of the following command:\nsqueue -u &lt;username&gt;\nIf you are disconnected from the interactive session, you can re-connect to your node with the following command:\nssh -Y &lt;nodename&gt;\nIf you get disconnected from Uppmax, reconnect to Uppmax as shown in the very first step, then check your job is running using squeue and login to the compute node as shown in the above step.\n\n\n\n1.2 Launch RStudio\n\n\n\n\n\n\nTip\n\n\n\nTo avoid running out of memory, restart R (Session &gt; Restart R) after each lab.\n\n\n\n1.2.1 Seurat\nTo launch RStudio server and run the Seurat labs perform the following steps:\n\n\ncd /proj/naiss2023-23-648/nobackup/&lt;username&gt;/workshop-scRNAseq/compiled/labs/seurat\n/sw/courses/scrnaseq/singularity/launch_rstudio.sh /sw/courses/scrnaseq/singularity/2024-seurat-r4.3.0.sif\n\n\n\n\n1.2.2 Bioconductor\nTo launch RStudio server and run the Bioconductor labs perform the following steps:\n\n\ncd /proj/naiss2023-23-648/nobackup/&lt;username&gt;/workshop-scRNAseq/compiled/labs/bioc\n/sw/courses/scrnaseq/singularity/launch_rstudio.sh /sw/courses/scrnaseq/singularity/2024-bioconductor-r4.3.0.sif\n\n\n\n\n\n1.3 Connect to RStudio\nAfter executing the launch_rstudio.sh script, a message with your login credentials will be printed to your screen, and it looks similar to the one below.\n\n\n\n\n\n\nImportant\n\n\n\nDo not close this terminal!\n\n\n        *************************************************\n        *                                               *\n        *  IMPORTANT: Do not close or exit this shell!  *\n        *                                               *\n        *************************************************\n\n1. SSH tunnel from your workstation using the following command:\n\n   ssh -N -L 8787:r483.uppmax.uu.se:58359 susanner@rackham3.uppmax.uu.se\n\n   and point your web browser to http://localhost:8787\n\n2. log in to RStudio Server using the following credentials:\n\n   user: susanner\n   password: scrnaseq\n\nWhen done using RStudio Server, terminate the job by:\n\n1. Exit the RStudio Session (\"power\" button in the top right corner of the RStudio window).\n2. Issue the following command in both shells:\n\n      CTRL-C\nFollow the instructions printed on the screen to launch the RStudio Server. Open a shell locally and run your ssh command from step 1. Then open localhost:8787 in your web browser and log in to the RStudio Server using your username and password provided in step 2.\nIn RStudio, make sure you are in the correct working directory else set it.\n\n\ngetwd()\nsetwd('/crex/proj/naiss2023-23-648/nobackup/&lt;username&gt;/workshop-scRNAseq/compiled/labs/&lt;topic&gt;')\n\n\n\n\n1.4 Launch JupyterLab\n\n1.4.1 Scanpy\nTo launch JupyterLab and run the Scanpy labs using Jupyter notebooks perform the following steps:\n\n\n\n\n\n\nTip\n\n\n\nTo avoid running out of memory, restart the kernel (Kernel &gt; Restart Kernel) after each lab.\n\n\n\n\ncd /proj/naiss2023-23-648/nobackup/&lt;username&gt;/workshop-scRNAseq/compiled/labs/scanpy\n/sw/courses/scrnaseq/singularity/launch_jupyter.sh /sw/courses/scrnaseq/singularity/2024-scanpy-py3.10.sif\n\n\n\n\n\n1.5 Connect to JupyterLab\nAfter executing the launch_jupyter.sh script, a message with your login credentials will be printed to your screen, and it looks similar to the one below.\n\n\n\n\n\n\nImportant\n\n\n\nDo not close this terminal!\n\n\n        *************************************************\n        *                                               *\n        *  IMPORTANT: Do not close or exit this shell!  *\n        *                                               *\n        *************************************************\n\n1. SSH tunnel from your workstation using the following command:\n\n   ssh -N -L 8888:r483.uppmax.uu.se:34968 susanner@rackham1.uppmax.uu.se\n\n   point your web browser to http://localhost:8888/lab\n\n2. Log in to JupyterLab using the password:\n\n   scrnaseq\n\nWhen done using JupyterLab, terminate the job by:\n\n1. Shut down all kernels.\n2. Issue the following command in both shells:\n\n   CTRL-C\nFollow the instructions to launch the JupyterLab. Open a shell locally and run your ssh command from step 1. Then open localhost:8888/lab in your web browser and log in to the JupyterLab using the password provided in step 2."
  },
  {
    "objectID": "other/containers.html#option-b-run-docker-locally",
    "href": "other/containers.html#option-b-run-docker-locally",
    "title": "Run labs in container",
    "section": "2 Option B: Run Docker Locally",
    "text": "2 Option B: Run Docker Locally\n\n\n\n\n\n\nImportant\n\n\n\nThe docker containers are not tested on Microsoft Windows OS.\n\n\n\n2.1 Local Setup\nCreate a new directory at a suitable location. Now you can fetch the scripts for the labs. You can either download individual .qmd or .ipynb files from the Contents page or clone the whole repo. If you clone the repo, navigate to compiled/labs to work on labs.\ngit clone --depth 1 --single-branch --branch master https://github.com/nbisweden/workshop-scRNAseq.git\ncd workshop-scRNAseq/compiled/labs\nIf the git command is not available, you can simply go to https://github.com/NBISweden/workshop-scRNAseq and download the repo as a zip file and unzip it in a suitable location.\n\n\n2.2 Images\nSeparate Docker images are made available for Seurat, Bioconductor and Scanpy toolkits. An additional set of images are available for spatial analyses. All images follow the registry/username/image:tag convention. The image is always ghcr.io/nbisweden/workshop-scrnaseq. Add the appropriate tag based on the lab you are running.\nAn overview of the available docker images. Note the space requirements.\n\n\n\nTopic\nImage\nSize (GB)\n\n\n\n\nSeurat\n2024-seurat-r4.3.0\n8.87\n\n\nBioconductor\n2024-bioconductor-r4.3.0\n7.89\n\n\nScanpy\n2024-scanpy-py3.10\n3.68\n\n\n\n\n\n\nOptional Topic\nImage\nSize (GB)\n\n\n\n\nSeurat spatial\n2024-seurat_spatial-r4.3.0\n6.85\n\n\nBioconductor spatial\n2024-bioconductor_spatial-r4.3.0\n6.47\n\n\nScanpy spatial\n2024-scanpy_spatial-py3.10\n3.68\n\n\n\n\n\n2.3 Seurat\n\n\n\n\n\n\nTip\n\n\n\nTo avoid running out of memory, restart R (Session &gt; Restart R) after each lab.\n\n\ncd /path/to/labs  # replace this with the full path to the workshop compiled lab folder\ndocker pull --platform=linux/amd64 ghcr.io/nbisweden/workshop-scrnaseq:2024-seurat-r4.3.0\ndocker run --platform=linux/amd64 --rm -p 8788:8787 -e PASSWORD=scrnaseq -v ${PWD}:/home/rstudio/workdir ghcr.io/nbisweden/workshop-scrnaseq:2024-seurat-r4.3.0\nDo not close the terminal. In the browser, go to localhost:8788.\nUse the following credentials to log in to the RStudio Server:\n\nUser: rstudio\nPassword: scrnaseq\n\nNavigate to /home/rstudio/workdir/ and open qmd files\n\n\n\n\n\nRStudio login screen\n\n\n\n\n\nRStudio preview\n\n\n\n\n\n\n2.4 Bioconductor\n\n\n\n\n\n\nTip\n\n\n\nTo avoid running out of memory, restart R (Session &gt; Restart R) after each lab.\n\n\ncd /path/to/labs  # replace this with the full path to the workshop compiled lab folder\ndocker pull --platform=linux/amd64 ghcr.io/nbisweden/workshop-scrnaseq:2024-bioconductor-r4.3.0\ndocker run --platform=linux/amd64 --rm -p 8789:8787 -e PASSWORD=scrnaseq -v ${PWD}:/home/rstudio/workdir ghcr.io/nbisweden/workshop-scrnaseq:2024-bioconductor-r4.3.0\nDo not close the terminal. In the browser, go to localhost:8789. Use the following credentials to log in to the RStudio Server:\n\nUser: rstudio\nPassword: scrnaseq\n\nNavigate to /home/rstudio/workdir/ and open qmd files\n\n\n2.5 Scanpy\n\n\n\n\n\n\nTip\n\n\n\nTo avoid running out of memory, restart the kernel (Kernel &gt; Restart Kernel) after each lab.\n\n\ncd /path/to/labs  # replace this with the full path to the workshop compiled lab folder\ndocker pull --platform=linux/amd64 ghcr.io/nbisweden/workshop-scrnaseq:2024-scanpy-py3.10\ndocker run --platform=linux/amd64 --rm -p 8888:8888 -v ${PWD}:/home/jovyan/workdir ghcr.io/nbisweden/workshop-scrnaseq:2024-scanpy-py3.10\nDo not close the terminal. At the end of the prompt, you will see a URL that starts with http://127.0.0.1, similar to the one below:\nhttp://127.0.0.1:8888/lab?token=0a1d9ec51b91528a1d1fe2ad2c74f59ecb94c47070c2911d\nNote that your token value will be different. Copy the entire URL (with the token) and paste it in your browser.\n\n\n\n\n\nJupyterLab home\n\n\n\n\n\nJupyterLab preview"
  },
  {
    "objectID": "other/faq.html",
    "href": "other/faq.html",
    "title": "FAQ",
    "section": "",
    "text": "If you don’t yet have Mac OSX command line developer tools, please install it using:\nxcode-select --install"
  },
  {
    "objectID": "other/faq.html#command-line-developer-tools-not-found",
    "href": "other/faq.html#command-line-developer-tools-not-found",
    "title": "FAQ",
    "section": "",
    "text": "If you don’t yet have Mac OSX command line developer tools, please install it using:\nxcode-select --install"
  },
  {
    "objectID": "other/faq.html#error-umap-learn-not-found-or-other-python-packages",
    "href": "other/faq.html#error-umap-learn-not-found-or-other-python-packages",
    "title": "FAQ",
    "section": "2 Error: umap-learn not found, or other python packages",
    "text": "2 Error: umap-learn not found, or other python packages\n\nIf your R does not find the correct python version, it will complain that umap-learn is not installed and ask you to install it. Here are some tips on how to find the correct python version that was installed in the conda environment.\nTry selecting the correct conda env in R. In this example the conda environment is named myenv.\nlibrary(reticulate)\nreticulate::use_conda(\"myenv\")\nThen check what python you have in R:\nreticulate::py_config()\n# should read at top:\npython:         /Users/asbj/miniconda3/envs/myenv/bin/python\nIf that still is not right, you may have an r-reticulate python installation as well and need to perform the steps below.\n\nRestart R and select python version\nFirst, find out what path you have to your conda python (in TERMINAL):\n\nwhich python\n/Users/asbj/miniconda3/envs/scRNAseq2021/bin/python\n\nThen in R (after restarting):\n\nreticulate::use_python(\"/Users/asbj/miniconda3/envs/scRNAseq2021/bin/python\", required=T)\n\nThen check again with py_config if correct version of python is used:\n\nreticulate::py_config()\n\nIf you have the correct version now, you should be able to run UMAP without issues."
  },
  {
    "objectID": "other/faq.html#unable-to-load-stringi.so",
    "href": "other/faq.html#unable-to-load-stringi.so",
    "title": "FAQ",
    "section": "3 Unable to load stringi.so",
    "text": "3 Unable to load stringi.so\n \nYou can install stringi in R using:\ninstall.packages('stringi')"
  },
  {
    "objectID": "other/faq.html#error-failed-building-wheel-for-gevent-macosx10.9.sdk-missing",
    "href": "other/faq.html#error-failed-building-wheel-for-gevent-macosx10.9.sdk-missing",
    "title": "FAQ",
    "section": "4 ERROR: Failed building wheel for gevent / MacOSX10.9.sdk missing",
    "text": "4 ERROR: Failed building wheel for gevent / MacOSX10.9.sdk missing\n\nThis is a problem with the MacOSX compiler, in which conda is unable to find it.\n#Download MacOSX10.9.sdk from Github\ncurl -o MacOSX10.9.sdk.tar.gz \"https://github.com/phracker/MacOSX-SDKs/releases/download/11.3/MacOSX10.9.sdk.tar.xz\"\n\n#extract\nsudo tar -xzf MacOSX10.9.sdk.tar.xz\n\n#copy\nsudo cp -r MacOSX10.9.sdk /opt/\n\n#give executable permissions\nsudo chmod -R a+rX /opt\n\n#Link the path where conda looks to where the file is\nln -s /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk /opt/MacOSX10.9.sdk"
  },
  {
    "objectID": "other/faq.html#error-option-error-has-null-value",
    "href": "other/faq.html#error-option-error-has-null-value",
    "title": "FAQ",
    "section": "5 ERROR: option error has NULL value",
    "text": "5 ERROR: option error has NULL value\n\nThis error happens when running code inline. One possible solution is to restart Rstudio and type.\nif(interactive()) { options(error = utils::recover)}\nPlease try other solutions listed here. If none of those work, you can click on the wheel engine symbol and check Chunk output in console."
  },
  {
    "objectID": "other/faq.html#r-crashes-due-to-memory-issues",
    "href": "other/faq.html#r-crashes-due-to-memory-issues",
    "title": "FAQ",
    "section": "6 R crashes due to memory issues",
    "text": "6 R crashes due to memory issues\n\nIf R crashes due to memory issues, it may be a good idea to increase the vector size R_MAX_VSIZE. Put in the file .Renviron either in your home directory or the folder you are launching Rstudio from:\nR_MAX_VSIZE=70Gb\nOr to whatever value matches your computer, the default size is 16Gb."
  },
  {
    "objectID": "other/faq.html#docker-run-fails-on-mac-apple-silicon",
    "href": "other/faq.html#docker-run-fails-on-mac-apple-silicon",
    "title": "FAQ",
    "section": "7 Docker run fails on Mac apple silicon",
    "text": "7 Docker run fails on Mac apple silicon\n\nDocker run on Apple Mac M1/M2/M3 processors experience this error when running docker run ... on an image not built on Apple silicon.\n[s6-init] making user provided files available at /var/run/s6/etc...exited 0.\n[s6-init] ensuring user provided files have correct perms...exited 0.\n[fix-attrs.d] applying ownership & permissions fixes...\n[fix-attrs.d] done.\n[cont-init.d] executing container initialization scripts...\n[cont-init.d] 01_set_env: executing...\nskipping /var/run/s6/container_environment/HOME\nskipping /var/run/s6/container_environment/PASSWORD\nskipping /var/run/s6/container_environment/RSTUDIO_VERSION\n[cont-init.d] 01_set_env: exited 0.\n[cont-init.d] 02_userconf: executing...\n[cont-init.d] 02_userconf: exited 0.\n[cont-init.d] done.\n[services.d] starting services\n[services.d] done.\nTTY detected. Printing informational message about logging configuration. Logging configuration loaded from '/etc/rstudio/logging.conf'. Logging to 'syslog'.\nrserver[1195]: ERROR system error 1 (Operation not permitted); OCCURRED AT rstudio::core::Error rstudio::core::system::posix::{anonymous}::restorePrivilegesImpl(uid_t) src/cpp/shared_core/system/PosixSystem.cpp:97; LOGGED FROM: void rstudio::server::pam_auth::{anonymous}::assumeRootPriv() src/cpp/server/ServerPAMAuth.cpp:59\n\n2023-11-28T14:31:03.943703Z [rserver] ERROR system error 1 (Operation not permitted); OCCURRED AT rstudio::core::Error rstudio::core::system::posix::{anonymous}::restorePrivilegesImpl(uid_t) src/cpp/shared_core/system/PosixSystem.cpp:97; LOGGED FROM: void rstudio::server::pam_auth::{anonymous}::assumeRootPriv() src/cpp/server/ServerPAMAuth.cpp:59\nrserver[1199]: ERROR system error 1 (Operation not permitted); OCCURRED AT rstudio::core::Error rstudio::core::system::posix::{anonymous}::restorePrivilegesImpl(uid_t) src/cpp/shared_core/system/PosixSystem.cpp:97; LOGGED FROM: rstudio::core::Error rstudio::core::system::launchChildProcess(std::string, std::string, rstudio::core::system::ProcessConfig, rstudio::core::system::ProcessConfigFilter, PidType*) src/cpp/core/system/PosixSystem.cpp:2195\nIn Docker Settings &gt; General, check Use Rosetta for x86/amd64 emulation on Apple Silicon."
  },
  {
    "objectID": "other/faq.html#open-multiple-files-simultaneously-in-rstudio",
    "href": "other/faq.html#open-multiple-files-simultaneously-in-rstudio",
    "title": "FAQ",
    "section": "8 Open multiple files simultaneously in RStudio",
    "text": "8 Open multiple files simultaneously in RStudio\n\nOpen all qmd files in the current working directory.\n\n\n\nR\n\nlapply(list.files(pattern = \"\\\\.qmd$\"), rstudioapi::documentOpen)"
  },
  {
    "objectID": "labs/index.html",
    "href": "labs/index.html",
    "title": "Labs",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "labs/index.html#fa-brands-r-project-seurat",
    "href": "labs/index.html#fa-brands-r-project-seurat",
    "title": "Labs",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "labs/index.html#fa-brands-r-project-bioconductor",
    "href": "labs/index.html#fa-brands-r-project-bioconductor",
    "title": "Labs",
    "section": " Bioconductor",
    "text": "Bioconductor\n\n\n\n\n\n\n\n\n\n\n Quality Control\n\n\n\n\n\n\n\n\n\n\n\n\n\n Dimensionality Reduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Integration\n\n\n\n\n\n\n\n\n\n\n\n\n\n Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n Differential gene expression\n\n\n\n\n\n\n\n\n\n\n\n\n\n Celltype prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n Spatial Transcriptomics\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "labs/index.html#fa-brands-python-scanpy",
    "href": "labs/index.html#fa-brands-python-scanpy",
    "title": "Labs",
    "section": " Scanpy",
    "text": "Scanpy\n\n\n\n\n\n\n\n\n\n\n Quality Control\n\n\n\n\n\n\n\n\n\n\n\n\n\n Dimensionality Reduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n Data Integration\n\n\n\n\n\n\n\n\n\n\n\n\n\n Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n Differential gene expression\n\n\n\n\n\n\n\n\n\n\n\n\n\n Celltype prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n Trajectory inference using PAGA\n\n\n\n\n\n\n\n\n\n\n\n\n\n Spatial Transcriptomics\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "home_schedule.html",
    "href": "home_schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Time\nTopic\nInstructor\n\n\n\n\n12-Feb-2024MonUppsala\n\n\n09:00 - 09:30\nWelcome and General introduction \nÅsa Björklund\n\n\n09:30 - 10:30\nLecture: Quality control \nÅsa Björklund\n\n\n10:30 - 11:00\nBreak\n\n\n\n11:00 - 12:00\nLecture: scRNAseq methods and ESCG platform \nHenrik Gezelius\n\n\n11:45 - 13:00\nLunch\n\n\n\n13:00 - 13:30\nIntro to Exercises \nÅsa Björklund\n\n\n13:30 - 15:00\nLab: Quality control \nÅsa Björklund\n\n\n15:00 - 15:30\nBreak\n\n\n\n15:30 - 16:30\nLecture: Data normalization \nÅsa Björklund\n\n\n16:30 - 17:00\nWrap Up\nÅsa Björklund\n\n\n13-Feb-2024TueUppsala\n\n\n09:00 - 10:00\nLecture: Dimensionality reduction \nNikolay Oskolkov\n\n\n10:00 - 10:30\nBreak\n\n\n\n10:30 - 12:00\nLab: Dimensionality reduction \nNikolay Oskolkov\n\n\n11:45 - 13:00\nLunch\n\n\n\n13:00 - 14:00\nLecture: Batch correction + Integration \nNikolay Oskolkov\n\n\n14:00 - 15:00\nLab: Data integration \nNikolay Oskolkov\n\n\n15:00 - 15:30\nBreak\n\n\n\n15:30 - 16:30\nLecture: Clustering \nÅsa Björklund\n\n\n16:30 - 17:00\nWrap Up\nÅsa Björklund\n\n\n18:00 - 21:00\nCourse Dinner\n\n\n\n14-Feb-2024WedUppsala\n\n\n09:00 - 10:00\nLab: Clustering \nÅsa Björklund\n\n\n10:00 - 10:30\nBreak\n\n\n\n10:30 - 12:00\nLecture: Single Cell Epigenetics \nJakub Westholm\n\n\n12:00 - 13:00\nLunch\n\n\n\n13:00 - 14:00\nLecture: Differential Gene Expression \nRoy Francis\n\n\n14:00 - 14:30\nLecture: Gene set analysis \nRoy Francis\n\n\n14:30 - 15:00\nBreak\n\n\n\n15:00 - 16:30\nLab: Differential expression \nRoy Francis\n\n\n16:30 - 17:00\nWrap Up\nÅsa Björklund\n\n\n15-Feb-2024ThuUppsala\n\n\n09:00 - 10:00\nLecture: Celltype prediction \nÅsa Björklund\n\n\n10:00 - 10:30\nBreak\n\n\n\n10:30 - 12:00\nLab: Celltype prediction \nÅsa Björklund\n\n\n12:00 - 13:00\nLunch\n\n\n\n13:00 - 14:00\nLecture: Trajectory inference \nPaulo Czarnewski\n\n\n14:00 - 15:00\nLab: Trajectory inference \nPaulo Czarnewski\n\n\n15:00 - 15:30\nBreak\n\n\n\n16:30 - 17:00\nWrap Up\nÅsa Björklund\n\n\n16-Feb-2024FriUppsala\n\n\n09:00 - 09:30\nIntro to BYOD\nÅsa Björklund\n\n\n09:30 - 12:00\nBYOD\nÅsa Björklund\n\n\n12:00 - 13:00\nLunch\n\n\n\n13:00 - 14:00\nWorkshop summary \nÅsa Björklund\n\n\n\n\n\n\n\n   Date    Venue    Slides    Lab    Form    Video"
  },
  {
    "objectID": "labs/seurat/seurat_06_celltyping.html#meta-ct_azimuth",
    "href": "labs/seurat/seurat_06_celltyping.html#meta-ct_azimuth",
    "title": " Celltype prediction",
    "section": "5 Azimuth",
    "text": "5 Azimuth\nThere are multiple online resources with large curated datasets with methods to integrate and do label transfer. One such resource is Azimuth another one is Disco.\nHere we will use the PBMC reference provided with Azimuth. Which in principle runs label transfer of your dataset onto a large curated reference. The first time you run the command, the pbmcref dataset will be downloaded to your local computer.\n\noptions(future.globals.maxSize = 1e9)\n\n# will install the pbmcref dataset first time you run it.\nctrl &lt;- RunAzimuth(ctrl, reference = \"pbmcref\")\n\nThis dataset has predictions at 3 different levels of annotation wiht l1 being the more broad celltypes and l3 more detailed annotation.\n\nDimPlot(ctrl, group.by = \"predicted.celltype.l1\", label = T, repel = T) + NoAxes()\n\n\n\n\n\n\n\nDimPlot(ctrl, group.by = \"predicted.celltype.l2\", label = T, repel = T) + NoAxes()\n\n\n\n\n\n\n\nDimPlot(ctrl, group.by = \"predicted.celltype.l3\", label = T, repel = T) + NoAxes()"
  },
  {
    "objectID": "labs/seurat/seurat_06_celltyping.html#meta-ct_singler",
    "href": "labs/seurat/seurat_06_celltyping.html#meta-ct_singler",
    "title": " Celltype prediction",
    "section": "4 SinlgeR",
    "text": "4 SinlgeR\nSingleR is performs unbiased cell type recognition from single-cell RNA sequencing data, by leveraging reference transcriptomic datasets of pure cell types to infer the cell of origin of each single cell independently.\nWe first have to convert the Seurat object to a SingleCellExperiment object.\n\nsce = as.SingleCellExperiment(ctrl)\n\nThere are multiple datasets included in the celldex package that can be used for celltype prediction, here we will test two different ones, the DatabaseImmuneCellExpressionData and the HumanPrimaryCellAtlasData. In addition we will use the same reference dataset that we used for label transfer above but using SingleR instead.\n\n4.1 Immune cell reference\n\nimmune = celldex::DatabaseImmuneCellExpressionData()\nsingler.immune &lt;- SingleR(test = sce, ref = immune, assay.type.test=1,\n    labels = immune$label.main)\n\nhead(singler.immune)\n\nDataFrame with 6 rows and 4 columns\n                                                       scores        labels\n                                                     &lt;matrix&gt;   &lt;character&gt;\nctrl_13_AGGTCATGTGCGAACA-13  0.0679680:0.0760411:0.186694:... T cells, CD4+\nctrl_13_CCTATCGGTCCCTCAT-13  0.0027079:0.0960641:0.386088:...      NK cells\nctrl_13_TCCTCCCTCGTTCATT-13  0.0361115:0.1067465:0.394579:...      NK cells\nctrl_13_CAACCAATCATCTATC-13  0.0342030:0.1345967:0.402377:...      NK cells\nctrl_13_TACGGTATCGGATTAC-13 -0.0131813:0.0717678:0.283882:...      NK cells\nctrl_13_AATAGAGAGTTCGGTT-13  0.0841091:0.1367749:0.273738:... T cells, CD4+\n                            delta.next pruned.labels\n                             &lt;numeric&gt;   &lt;character&gt;\nctrl_13_AGGTCATGTGCGAACA-13  0.1375513 T cells, CD4+\nctrl_13_CCTATCGGTCCCTCAT-13  0.1490740      NK cells\nctrl_13_TCCTCCCTCGTTCATT-13  0.1220681      NK cells\nctrl_13_CAACCAATCATCTATC-13  0.1513308      NK cells\nctrl_13_TACGGTATCGGATTAC-13  0.0620657      NK cells\nctrl_13_AATAGAGAGTTCGGTT-13  0.0660296 T cells, CD4+\n\n\n\n\n4.2 HPCA reference\n\nhpca &lt;- HumanPrimaryCellAtlasData()\nsingler.hpca &lt;- SingleR(test = sce, ref = hpca, assay.type.test=1,\n    labels = hpca$label.main)\n\nhead(singler.hpca)\n\nDataFrame with 6 rows and 4 columns\n                                                    scores      labels\n                                                  &lt;matrix&gt; &lt;character&gt;\nctrl_13_AGGTCATGTGCGAACA-13 0.141378:0.310009:0.275987:...     T_cells\nctrl_13_CCTATCGGTCCCTCAT-13 0.145926:0.300045:0.277827:...     NK_cell\nctrl_13_TCCTCCCTCGTTCATT-13 0.132119:0.311754:0.274127:...     NK_cell\nctrl_13_CAACCAATCATCTATC-13 0.157184:0.302219:0.284496:...     NK_cell\nctrl_13_TACGGTATCGGATTAC-13 0.125120:0.283118:0.250322:...     T_cells\nctrl_13_AATAGAGAGTTCGGTT-13 0.191441:0.374422:0.329988:...     T_cells\n                            delta.next pruned.labels\n                             &lt;numeric&gt;   &lt;character&gt;\nctrl_13_AGGTCATGTGCGAACA-13  0.4918992       T_cells\nctrl_13_CCTATCGGTCCCTCAT-13  0.3241970       NK_cell\nctrl_13_TCCTCCCTCGTTCATT-13  0.0640608       NK_cell\nctrl_13_CAACCAATCATCTATC-13  0.2012408       NK_cell\nctrl_13_TACGGTATCGGATTAC-13  0.1545913       T_cells\nctrl_13_AATAGAGAGTTCGGTT-13  0.5063484       T_cells\n\n\n\n\n4.3 With own reference data\n\nsce.ref = as.SingleCellExperiment(reference)\nsingler.ref &lt;- SingleR(test=sce, ref=sce.ref, labels=sce.ref$cell_type, de.method=\"wilcox\")\nhead(singler.ref)\n\nDataFrame with 6 rows and 4 columns\n                                                    scores      labels\n                                                  &lt;matrix&gt; &lt;character&gt;\nctrl_13_AGGTCATGTGCGAACA-13 0.740975:0.838990:0.805340:...  CD4 T cell\nctrl_13_CCTATCGGTCCCTCAT-13 0.647281:0.735248:0.815289:...     NK cell\nctrl_13_TCCTCCCTCGTTCATT-13 0.668322:0.732251:0.822483:...     NK cell\nctrl_13_CAACCAATCATCTATC-13 0.645869:0.719972:0.799716:...  CD8 T cell\nctrl_13_TACGGTATCGGATTAC-13 0.701719:0.772023:0.802647:...  CD8 T cell\nctrl_13_AATAGAGAGTTCGGTT-13 0.724356:0.846323:0.815530:...  CD4 T cell\n                            delta.next pruned.labels\n                             &lt;numeric&gt;   &lt;character&gt;\nctrl_13_AGGTCATGTGCGAACA-13  0.0383797    CD4 T cell\nctrl_13_CCTATCGGTCCCTCAT-13  0.0743326       NK cell\nctrl_13_TCCTCCCTCGTTCATT-13  0.0318152       NK cell\nctrl_13_CAACCAATCATCTATC-13  0.0394666    CD8 T cell\nctrl_13_TACGGTATCGGATTAC-13  0.0743317    CD8 T cell\nctrl_13_AATAGAGAGTTCGGTT-13  0.0377684    CD4 T cell\n\n\nCompare results:\n\nctrl$singler.immune = singler.immune$pruned.labels\nctrl$singler.hpca = singler.hpca$pruned.labels\nctrl$singler.ref = singler.ref$pruned.labels\n\nDimPlot(ctrl, group.by = c(\"singler.hpca\", \"singler.immune\", \"singler.ref\"), ncol = 2)"
  },
  {
    "objectID": "labs/seurat/seurat_04_clustering.html#meta-clust_sub",
    "href": "labs/seurat/seurat_04_clustering.html#meta-clust_sub",
    "title": " Clustering",
    "section": "5 Subclustering of T and NK-cells",
    "text": "5 Subclustering of T and NK-cells\nIt is common that the subtypes of cells within a cluster is not so well separated when you have a heterogeneous dataset. In such a case it could be a good idea to run subclustering of individual celltypes. The main reason for subclustering is that the variable genes and the first principal components in the full analysis are mainly driven by differences between celltypes, while with subclustering we may detect smaller differences between subtypes within celltypes.\nSo first, lets find out where our T-cell and NK-cell clusters are. We know that T-cells express CD3E, and the main subtypes are CD4 and CD8, while NK-cells express GNLY.\n\n# check with the lowest resolution\np1 = DimPlot(alldata, reduction = \"umap_cca\", group.by = \"RNA_snn_res.0.1\", label = T) + ggtitle(\"louvain_0.1\")\np2 = FeaturePlot(alldata, features = \"CD3E\", reduction = \"umap_cca\", order = T) \np3 = FeaturePlot(alldata, features = \"CD4\", reduction = \"umap_cca\", order = T) \np4 = FeaturePlot(alldata, features = \"CD8A\", reduction = \"umap_cca\", order = T) \np5 = FeaturePlot(alldata, features = \"GNLY\", reduction = \"umap_cca\", order = T) \n\n\nwrap_plots(p1,p2,p3,p4,p5, ncol=3) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nWe can clearly see what clusters are T-cell clusters, so lets subset the data for those cells\n\ntcells = alldata[,alldata$RNA_snn_res.0.1 %in% c(0,3)]\n\ntable(tcells$orig.ident)\n\n\n covid_1 covid_15 covid_16 covid_17  ctrl_13  ctrl_14  ctrl_19   ctrl_5 \n     281      241      130      292      633      525      561      705 \n\n\nIdeally we should rerun all steps of integration with that subset of cells instead of just taking the joint embedding. If you have too few cells per sample in the celltype that you want to cluster it may not be possible. We will start with selecting a new set of genes that better reflecs the variability within this celltype\n\ntcells = FindVariableFeatures(tcells, verbose = FALSE)\n\n# check overlap with the variable genes using all the data\nlength(intersect(VariableFeatures(alldata), VariableFeatures(tcells)))\n\n[1] 1213\n\n\nWe clearly have a very different geneset now, so hopefully it should better capture the variability within T-cells.\nNow we have to run the full pipeline with scaling, pca, integration and clustering on this subset of cells, using the new set of variable genes\n\n# run all the steps from before:\ntcells = ScaleData(tcells, vars.to.regress = c(\"percent_mito\", \"nFeature_RNA\"), assay = \"RNA\")\ntcells = RunPCA(tcells, npcs = 50, verbose = F)\n\ntcells &lt;- IntegrateLayers(object = tcells, \n                           method = CCAIntegration, orig.reduction = \"pca\", \n                           new.reduction = \"integrated_tcells\", verbose = FALSE)\n\ntcells &lt;- RunUMAP(tcells, reduction = \"integrated_tcells\", dims = 1:30, reduction.name = \"umap_tcells\")\n\ntcells &lt;- FindNeighbors(tcells, reduction = \"integrated_tcells\", dims = 1:30)\ntcells &lt;- FindClusters(tcells, graph.name = \"RNA_snn\", resolution = 0.5, algorithm = 1, cluster.name = \"tcell_0.5\")\n\nModularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck\n\nNumber of nodes: 3368\nNumber of edges: 202471\n\nRunning Louvain algorithm...\nMaximum modularity in 10 random starts: 0.8505\nNumber of communities: 9\nElapsed time: 0 seconds\n\n\n\nwrap_plots(\n  DimPlot(tcells, reduction = \"umap_cca\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"Full umap\"),\n  DimPlot(tcells, reduction = \"umap_cca\", group.by = \"RNA_snn_res.0.5\", label = T)+NoAxes()+ggtitle(\"Full umap, full clust\"),\n  DimPlot(tcells, reduction = \"umap_cca\", group.by = \"tcell_0.5\", label = T)+NoAxes()+ggtitle(\"Full umap, T-cell clust\"),\n  DimPlot(tcells, reduction = \"umap_tcells\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"T-cell umap, T-cell clust\"),\n  DimPlot(tcells, reduction = \"umap_tcells\", group.by = \"RNA_snn_res.0.5\")+NoAxes()+ggtitle(\"T-cell umap, full clust\"),\n  DimPlot(tcells, reduction = \"umap_tcells\", group.by = \"tcell_0.5\", label = T)+NoAxes()+ggtitle(\"T-cell umap\"),  \n  ncol = 3\n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nAs you can see, we do have some new clusters that did not stand out before (clusters 6,7). But in general the separation looks very similar.\nLets also have a look at some genes in the new umap:\n\nwrap_plots(\n  FeaturePlot(tcells, features = \"CD3E\", reduction = \"umap_tcells\", order = T), \n  FeaturePlot(tcells, features = \"CD4\", reduction = \"umap_tcells\", order = T), \n  FeaturePlot(tcells, features = \"CD8A\", reduction = \"umap_tcells\", order = T), \n  FeaturePlot(tcells, features = \"GNLY\", reduction = \"umap_tcells\", order = T), \n  ncol = 2\n) + plot_layout(guides = \"collect\")"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#section",
    "href": "labs/scanpy/scanpy_01_qc.html#section",
    "title": " Quality Control",
    "section": "1 Get data",
    "text": "1 Get data\nIn this tutorial, we will run all tutorials with a set of 8 PBMC 10x datasets from 4 covid-19 patients and 4 healthy controls, the samples have been subsampled to 1500 cells per sample. We can start by defining our paths.\n\nimport os\n\n# download pre-computed annotation\nfetch_annotation = False\n\npath_data = \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\n\npath_covid = \"./data/covid\"\nif not os.path.exists(path_covid):\n    os.makedirs(path_covid, exist_ok=True)\n\npath_results = \"data/covid/results\"\nif not os.path.exists(path_results):\n    os.makedirs(path_results, exist_ok=True)\n\n\nimport urllib.request\n\nfile_list = [\n    \"normal_pbmc_13.h5\", \"normal_pbmc_14.h5\", \"normal_pbmc_19.h5\", \"normal_pbmc_5.h5\",\n    \"ncov_pbmc_15.h5\", \"ncov_pbmc_16.h5\", \"ncov_pbmc_17.h5\", \"ncov_pbmc_1.h5\"\n]\n\nfor i in file_list:\n    path_file = os.path.join(path_covid, i)\n    if not os.path.exists(path_file):\n        file_url = os.path.join(path_data, \"covid\", i)\n        urllib.request.urlretrieve(file_url, path_file)\n\nWith data in place, now we can start loading libraries we will use in this tutorial.\n\nimport numpy as np\nimport pandas as pd\nimport scanpy as sc\nimport warnings\n\nwarnings.simplefilter(action='ignore', category=Warning)\n\n# verbosity: errors (0), warnings (1), info (2), hints (3)\nsc.settings.verbosity = 3\nsc.settings.set_figure_params(dpi=80)\n\nWe can first load the data individually by reading directly from HDF5 file format (.h5).\nIn Scanpy we read them into an Anndata object with the the function read_10x_h5\n\ndata_cov1 = sc.read_10x_h5(os.path.join(path_covid,'ncov_pbmc_1.h5'))\ndata_cov1.var_names_make_unique()\ndata_cov15 = sc.read_10x_h5(os.path.join(path_covid,'ncov_pbmc_15.h5'))\ndata_cov15.var_names_make_unique()\ndata_cov16 = sc.read_10x_h5(os.path.join(path_covid,'ncov_pbmc_16.h5'))\ndata_cov16.var_names_make_unique()\ndata_cov17 = sc.read_10x_h5(os.path.join(path_covid,'ncov_pbmc_17.h5'))\ndata_cov17.var_names_make_unique()\ndata_ctrl5 = sc.read_10x_h5(os.path.join(path_covid,'normal_pbmc_5.h5'))\ndata_ctrl5.var_names_make_unique()\ndata_ctrl13 = sc.read_10x_h5(os.path.join(path_covid,'normal_pbmc_13.h5'))\ndata_ctrl13.var_names_make_unique()\ndata_ctrl14 = sc.read_10x_h5(os.path.join(path_covid,'normal_pbmc_14.h5'))\ndata_ctrl14.var_names_make_unique()\ndata_ctrl19 = sc.read_10x_h5(os.path.join(path_covid,'normal_pbmc_19.h5'))\ndata_ctrl19.var_names_make_unique()\n\nreading ./data/covid/ncov_pbmc_1.h5\n (0:00:00)\nreading ./data/covid/ncov_pbmc_15.h5\n (0:00:00)\nreading ./data/covid/ncov_pbmc_16.h5\n (0:00:00)\nreading ./data/covid/ncov_pbmc_17.h5\n (0:00:00)\nreading ./data/covid/normal_pbmc_5.h5\n (0:00:00)\nreading ./data/covid/normal_pbmc_13.h5\n (0:00:00)\nreading ./data/covid/normal_pbmc_14.h5\n (0:00:00)\nreading ./data/covid/normal_pbmc_19.h5\n (0:00:00)"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#section-1",
    "href": "labs/scanpy/scanpy_01_qc.html#section-1",
    "title": " Quality Control",
    "section": "2 Collate",
    "text": "2 Collate\nWe can now merge them objects into a single object. Each analysis workflow (Seurat, Scater, Scanpy, etc) has its own way of storing data. We will add dataset labels as cell.ids just in case you have overlapping barcodes between the datasets. After that we add a column type in the metadata to define covid and ctrl samples.\n\n# add some metadata\ndata_cov1.obs['type']=\"Covid\"\ndata_cov1.obs['sample']=\"covid_1\"\ndata_cov15.obs['type']=\"Covid\"\ndata_cov15.obs['sample']=\"covid_15\"\ndata_cov16.obs['type']=\"Covid\"\ndata_cov16.obs['sample']=\"covid_16\"\ndata_cov17.obs['type']=\"Covid\"\ndata_cov17.obs['sample']=\"covid_17\"\ndata_ctrl5.obs['type']=\"Ctrl\"\ndata_ctrl5.obs['sample']=\"ctrl_5\"\ndata_ctrl13.obs['type']=\"Ctrl\"\ndata_ctrl13.obs['sample']=\"ctrl_13\"\ndata_ctrl14.obs['type']=\"Ctrl\"\ndata_ctrl14.obs['sample']=\"ctrl_14\"\ndata_ctrl19.obs['type']=\"Ctrl\"\ndata_ctrl19.obs['sample']=\"ctrl_19\"\n\n# merge into one object.\nadata = data_cov1.concatenate(data_cov15, data_cov16, data_cov17, data_ctrl5, data_ctrl13, data_ctrl14, data_ctrl19)\n\n# and delete individual datasets to save space\ndel(data_cov1, data_cov15, data_cov16, data_cov17)\ndel(data_ctrl5, data_ctrl13, data_ctrl14, data_ctrl19)\n\nYou can print a summary of the datasets in the Scanpy object, or a summary of the whole object.\n\nprint(adata.obs['sample'].value_counts())\nadata\n\ncovid_1     1500\ncovid_15    1500\ncovid_16    1500\ncovid_17    1500\nctrl_5      1500\nctrl_13     1500\nctrl_14     1500\nctrl_19     1500\nName: sample, dtype: int64\n\n\nAnnData object with n_obs × n_vars = 12000 × 33538\n    obs: 'type', 'sample', 'batch'\n    var: 'gene_ids', 'feature_types', 'genome'"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#section-2",
    "href": "labs/scanpy/scanpy_01_qc.html#section-2",
    "title": " Quality Control",
    "section": "3 Calculate QC",
    "text": "3 Calculate QC\nHaving the data in a suitable format, we can start calculating some quality metrics. We can for example calculate the percentage of mitochondrial and ribosomal genes per cell and add to the metadata. The proportion of hemoglobin genes can give an indication of red blood cell contamination, but in some tissues it can also be the case that some celltypes have higher content of hemoglobin. This will be helpful to visualize them across different metadata parameters (i.e. datasetID and chemistry version). There are several ways of doing this. The QC metrics are finally added to the metadata table.\nCiting from Simple Single Cell workflows (Lun, McCarthy & Marioni, 2017): High proportions are indicative of poor-quality cells (Islam et al. 2014; Ilicic et al. 2016), possibly because of loss of cytoplasmic RNA from perforated cells. The reasoning is that mitochondria are larger than individual transcript molecules and less likely to escape through tears in the cell membrane.\nFirst, let Scanpy calculate some general qc-stats for genes and cells with the function sc.pp.calculate_qc_metrics, similar to calculateQCmetrics() in Scater. It can also calculate proportion of counts for specific gene populations, so first we need to define which genes are mitochondrial, ribosomal and hemoglobin.\n\n# mitochondrial genes\nadata.var['mt'] = adata.var_names.str.startswith('MT-') \n# ribosomal genes\nadata.var['ribo'] = adata.var_names.str.startswith((\"RPS\",\"RPL\"))\n# hemoglobin genes.\nadata.var['hb'] = adata.var_names.str.contains((\"^HB[^(P|E|S)]\"))\n\nadata.var\n\n\n\n\n\n\n\n\ngene_ids\nfeature_types\ngenome\nmt\nribo\nhb\n\n\n\n\nMIR1302-2HG\nENSG00000243485\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\nFAM138A\nENSG00000237613\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\nOR4F5\nENSG00000186092\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\nAL627309.1\nENSG00000238009\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\nAL627309.3\nENSG00000239945\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\nAC233755.2\nENSG00000277856\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\nAC233755.1\nENSG00000275063\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\nAC240274.1\nENSG00000271254\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\nAC213203.1\nENSG00000277475\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\nFAM231C\nENSG00000268674\nGene Expression\nGRCh38\nFalse\nFalse\nFalse\n\n\n\n\n33538 rows × 6 columns\n\n\n\n\nsc.pp.calculate_qc_metrics(adata, qc_vars=['mt','ribo','hb'], percent_top=None, log1p=False, inplace=True)\n\nNow you can see that we have additional data in the metadata slot.\nAnother opition to using the calculate_qc_metrics function is to calculate the values on your own and add to a metadata slot. An example for mito genes can be found below:\n\nmito_genes = adata.var_names.str.startswith('MT-')\n# for each cell compute fraction of counts in mito genes vs. all genes\n# the `.A1` is only necessary as X is sparse (to transform to a dense array after summing)\nadata.obs['percent_mt2'] = np.sum(\n    adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1\n# add the total counts per cell as observations-annotation to adata\nadata.obs['n_counts'] = adata.X.sum(axis=1).A1\n\nadata\n\nAnnData object with n_obs × n_vars = 12000 × 33538\n    obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts'\n    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts'"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#section-3",
    "href": "labs/scanpy/scanpy_01_qc.html#section-3",
    "title": " Quality Control",
    "section": "4 Plot QC",
    "text": "4 Plot QC\nNow we can plot some of the QC variables as violin plots.\n\nsc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt', 'pct_counts_ribo', 'pct_counts_hb'], jitter=0.4, groupby = 'sample', rotation= 45)\n\n\n\n\n\n\n\n\nAs you can see, there is quite some difference in quality for these samples, with for instance the covid_15 and covid_16 samples having cells with fewer detected genes and more mitochondrial content. As the ribosomal proteins are highly expressed they will make up a larger proportion of the transcriptional landscape when fewer of the lowly expressed genes are detected. We can also plot the different QC-measures as scatter plots.\n\nsc.pl.scatter(adata, x='total_counts', y='pct_counts_mt', color=\"sample\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nPlot additional QC stats that we have calculated as scatter plots. How are the different measures correlated? Can you explain why?"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#section-4",
    "href": "labs/scanpy/scanpy_01_qc.html#section-4",
    "title": " Quality Control",
    "section": "5 Filtering",
    "text": "5 Filtering\n\n5.1 Detection-based filtering\nA standard approach is to filter cells with low number of reads as well as genes that are present in at least a given number of cells. Here we will only consider cells with at least 200 detected genes and genes need to be expressed in at least 3 cells. Please note that those values are highly dependent on the library preparation method used.\n\nsc.pp.filter_cells(adata, min_genes=200)\nsc.pp.filter_genes(adata, min_cells=3)\n\nprint(adata.n_obs, adata.n_vars)\n\nfiltered out 1336 cells that have less than 200 genes expressed\nfiltered out 14047 genes that are detected in less than 3 cells\n10664 19491\n\n\nExtremely high number of detected genes could indicate doublets. However, depending on the cell type composition in your sample, you may have cells with higher number of genes (and also higher counts) from one cell type. In this case, we will run doublet prediction further down, so we will skip this step now, but the code below is an example of how it can be run:\n\n# skip for now as we are doing doublet prediction\n#keep_v2 = (adata.obs['n_genes_by_counts'] &lt; 2000) & (adata.obs['n_genes_by_counts'] &gt; 500) & (adata.obs['lib_prep'] == 'v2')\n#print(sum(keep_v2))\n\n# filter for gene detection for v3\n#keep_v3 = (adata.obs['n_genes_by_counts'] &lt; 4100) & (adata.obs['n_genes_by_counts'] &gt; 1000) & (adata.obs['lib_prep'] != 'v2')\n#print(sum(keep_v3))\n\n# keep both sets of cells\n#keep = (keep_v2) | (keep_v3)\n#print(sum(keep))\n#adata = adata[keep, :]\n\n#print(\"Remaining cells %d\"%adata.n_obs)\n\nAdditionally, we can also see which genes contribute the most to such reads. We can for instance plot the percentage of counts per gene.\n\nsc.pl.highest_expr_genes(adata, n_top=20)\n\nnormalizing counts per cell\n    finished (0:00:00)\n\n\n\n\n\n\n\n\n\nAs you can see, MALAT1 constitutes up to 30% of the UMIs from a single cell and the other top genes are mitochondrial and ribosomal genes. It is quite common that nuclear lincRNAs have correlation with quality and mitochondrial reads, so high detection of MALAT1 may be a technical issue. Let us assemble some information about such genes, which are important for quality control and downstream filtering.\n\n\n5.2 Mito/Ribo filtering\nWe also have quite a lot of cells with high proportion of mitochondrial and low proportion of ribosomal reads. It would be wise to remove those cells, if we have enough cells left after filtering. Another option would be to either remove all mitochondrial reads from the dataset and hope that the remaining genes still have enough biological signal. A third option would be to just regress out the percent_mito variable during scaling. In this case we had as much as 99.7% mitochondrial reads in some of the cells, so it is quite unlikely that there is much cell type signature left in those. Looking at the plots, make reasonable decisions on where to draw the cutoff. In this case, the bulk of the cells are below 20% mitochondrial reads and that will be used as a cutoff. We will also remove cells with less than 5% ribosomal reads.\n\n# filter for percent mito\nadata = adata[adata.obs['pct_counts_mt'] &lt; 20, :]\n\n# filter for percent ribo &gt; 0.05\nadata = adata[adata.obs['pct_counts_ribo'] &gt; 5, :]\n\nprint(\"Remaining cells %d\"%adata.n_obs)\n\nRemaining cells 7431\n\n\nAs you can see, a large proportion of sample covid_15 is filtered out. Also, there is still quite a lot of variation in percent_mito, so it will have to be dealt with in the data analysis step. We can also notice that the percent_ribo are also highly variable, but that is expected since different cell types have different proportions of ribosomal content, according to their function.\n\n\n5.3 Plot filtered QC\nLets plot the same QC-stats once more.\n\nsc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt','pct_counts_ribo', 'pct_counts_hb'], jitter=0.4, groupby = 'sample', rotation = 45)\n\n\n\n\n\n\n\n\n\n\n5.4 Filter genes\nAs the level of expression of mitochondrial and MALAT1 genes are judged as mainly technical, it can be wise to remove them from the dataset before any further analysis. In this case we will also remove the HB genes.\n\nmalat1 = adata.var_names.str.startswith('MALAT1')\n# we need to redefine the mito_genes since they were first \n# calculated on the full object before removing low expressed genes.\nmito_genes = adata.var_names.str.startswith('MT-')\nhb_genes = adata.var_names.str.contains('^HB[^(P|E|S)]')\n\nremove = np.add(mito_genes, malat1)\nremove = np.add(remove, hb_genes)\nkeep = np.invert(remove)\n\nadata = adata[:,keep]\n\nprint(adata.n_obs, adata.n_vars)\n\n7431 19468"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#section-9",
    "href": "labs/scanpy/scanpy_01_qc.html#section-9",
    "title": " Quality Control",
    "section": "6 Sample sex",
    "text": "6 Sample sex\nWhen working with human or animal samples, you should ideally constrain your experiments to a single sex to avoid including sex bias in the conclusions. However this may not always be possible. By looking at reads from chromosomeY (males) and XIST (X-inactive specific transcript) expression (mainly female) it is quite easy to determine per sample which sex it is. It can also be a good way to detect if there has been any mislabelling in which case, the sample metadata sex does not agree with the computational predictions.\nTo get choromosome information for all genes, you should ideally parse the information from the gtf file that you used in the mapping pipeline as it has the exact same annotation version/gene naming. However, it may not always be available, as in this case where we have downloaded public data. Hence, we will use biomart to fetch chromosome information.\n\n# requires pybiomart\nif not fetch_annotation:\n    annot = sc.queries.biomart_annotations(\"hsapiens\", [\"ensembl_gene_id\", \"external_gene_name\", \"start_position\", \"end_position\", \"chromosome_name\"], ).set_index(\"external_gene_name\")\n    # adata.var[annot.columns] = annot\n\nNow that we have the chromosome information, we can calculate the proportion of reads that comes from chromosome Y per cell.But first we have to remove all genes in the pseudoautosmal regions of chrY that are: * chromosome:GRCh38:Y:10001 - 2781479 is shared with X: 10001 - 2781479 (PAR1) * chromosome:GRCh38:Y:56887903 - 57217415 is shared with X: 155701383 - 156030895 (PAR2)\n\nchrY_genes = adata.var_names.intersection(annot.index[annot.chromosome_name == \"Y\"])\nchrY_genes\n\npar1 = [10001, 2781479]\npar2 = [56887903, 57217415]\n\npar1_genes = annot.index[(annot.chromosome_name == \"Y\") & (annot.start_position &gt; par1[0]) & (annot.start_position &lt; par1[1]) ]\n\npar2_genes = annot.index[(annot.chromosome_name == \"Y\") & (annot.start_position &gt; par2[0]) & (annot.start_position &lt; par2[1]) ]\n\nchrY_genes = chrY_genes.difference(par1_genes)\nchrY_genes = chrY_genes.difference(par2_genes)\n\nadata.obs['percent_chrY'] = np.sum(\n    adata[:, chrY_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1 * 100\n\nThen plot XIST expression vs chrY proportion. As you can see, the samples are clearly on either side, even if some cells do not have detection of either.\n\n# color inputs must be from either .obs or .var, so add in XIST expression to obs.\nadata.obs[\"XIST-counts\"] = adata.X[:,adata.var_names.str.match('XIST')].toarray()\n\nsc.pl.scatter(adata, x='XIST-counts', y='percent_chrY', color=\"sample\")\n\n\n\n\n\n\n\n\nPlot as violins.\n\nsc.pl.violin(adata, [\"XIST-counts\", \"percent_chrY\"], jitter=0.4, groupby = 'sample', rotation= 45)\n\n\n\n\n\n\n\n\nHere, we can see clearly that we have three males and five females, can you see which samples they are? Do you think this will cause any problems for downstream analysis? Discuss with your group: what would be the best way to deal with this type of sex bias?"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#section-10",
    "href": "labs/scanpy/scanpy_01_qc.html#section-10",
    "title": " Quality Control",
    "section": "7 Cell cycle state",
    "text": "7 Cell cycle state\nWe here perform cell cycle scoring. To score a gene list, the algorithm calculates the difference of mean expression of the given list and the mean expression of reference genes. To build the reference, the function randomly chooses a bunch of genes matching the distribution of the expression of the given list. Cell cycle scoring adds three slots in the metadata, a score for S phase, a score for G2M phase and the predicted cell cycle phase.\nFirst read the file with cell cycle genes, from Regev lab and split into S and G2M phase genes. We first download the file.\n\npath_file = os.path.join(path_results, 'regev_lab_cell_cycle_genes.txt')\nif not os.path.exists(path_file):\n    urllib.request.urlretrieve(os.path.join(path_data, 'regev_lab_cell_cycle_genes.txt'), path_file)\n\n\ncell_cycle_genes = [x.strip() for x in open('./data/covid/results/regev_lab_cell_cycle_genes.txt')]\nprint(len(cell_cycle_genes))\n\n# Split into 2 lists\ns_genes = cell_cycle_genes[:43]\ng2m_genes = cell_cycle_genes[43:]\n\ncell_cycle_genes = [x for x in cell_cycle_genes if x in adata.var_names]\nprint(len(cell_cycle_genes))\n\n97\n94\n\n\nBefore running cell cycle we have to normalize the data. In the scanpy object, the data slot will be overwritten with the normalized data. So first, save the raw data into the slot raw. Then run normalization, log transformation and scale the data.\n\n# save raw counts in raw slot.\nadata.raw = adata\n\n# normalize to depth 10 000\nsc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n\n# logaritmize\nsc.pp.log1p(adata)\n\n# scale\nsc.pp.scale(adata)\n\nnormalizing by total count per cell\n    finished (0:00:00): normalized adata.X and added\n    'n_counts', counts per cell before normalization (adata.obs)\n... as `zero_center=True`, sparse input is densified and may lead to large memory consumption\n\n\nWe here perform cell cycle scoring. The function is actually a wrapper to sc.tl.score_gene_list, which is launched twice, to score separately S and G2M phases. Both sc.tl.score_gene_list and sc.tl.score_cell_cycle_genes are a port from Seurat and are supposed to work in a very similar way. To score a gene list, the algorithm calculates the difference of mean expression of the given list and the mean expression of reference genes. To build the reference, the function randomly chooses a bunch of genes matching the distribution of the expression of the given list. Cell cycle scoring adds three slots in data, a score for S phase, a score for G2M phase and the predicted cell cycle phase.\n\nsc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes)\n\ncalculating cell cycle phase\ncomputing score 'S_score'\nWARNING: genes are not in var_names and ignored: Index(['MLF1IP'], dtype='object')\n    finished: added\n    'S_score', score of gene set (adata.obs).\n    774 total control genes are used. (0:00:00)\ncomputing score 'G2M_score'\nWARNING: genes are not in var_names and ignored: Index(['FAM64A', 'HN1'], dtype='object')\n    finished: added\n    'G2M_score', score of gene set (adata.obs).\n    772 total control genes are used. (0:00:00)\n--&gt;     'phase', cell cycle phase (adata.obs)\n\n\nWe can now create a violin plot for the cell cycle scores as well.\n\nsc.pl.violin(adata, ['S_score', 'G2M_score'], jitter=0.4, groupby = 'sample', rotation=45)\n\n\n\n\n\n\n\n\nIn this case it looks like we only have a few cycling cells in these datasets.\nScanpy does an automatic prediction of cell cycle phase with a default cutoff of the scores at zero. As you can see this does not fit this data very well, so be cautios with using these predictions. Instead we suggest that you look at the scores.\n\nsc.pl.scatter(adata, x='S_score', y='G2M_score', color=\"phase\")"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#section-11",
    "href": "labs/scanpy/scanpy_01_qc.html#section-11",
    "title": " Quality Control",
    "section": "8 Predict doublets",
    "text": "8 Predict doublets\nDoublets/Multiples of cells in the same well/droplet is a common issue in scRNAseq protocols. Especially in droplet-based methods with overloading of cells. In a typical 10x experiment the proportion of doublets is linearly dependent on the amount of loaded cells. As indicated from the Chromium user guide, doublet rates are about as follows:\n\nMost doublet detectors simulates doublets by merging cell counts and predicts doublets as cells that have similar embeddings as the simulated doublets. Most such packages need an assumption about the number/proportion of expected doublets in the dataset. The data you are using is subsampled, but the original datasets contained about 5 000 cells per sample, hence we can assume that they loaded about 9 000 cells and should have a doublet rate at about 4%.\nFor doublet detection, we will use the package Scrublet, so first we need to get the raw counts from adata.raw.X and run scrublet with that matrix. Then we add in the doublet prediction info into our anndata object.\nDoublet prediction should be run for each dataset separately, so first we need to split the adata object into 6 separate objects, one per sample and then run scrublet on each of them.\n\nimport scrublet as scr\n\n# split per batch into new objects.\nbatches = adata.obs['sample'].cat.categories.tolist()\nalldata = {}\nfor batch in batches:\n    tmp = adata[adata.obs['sample'] == batch,]\n    print(batch, \":\", tmp.shape[0], \" cells\")\n    scrub = scr.Scrublet(tmp.raw.X)\n    out = scrub.scrub_doublets(verbose=False, n_prin_comps = 20)\n    alldata[batch] = pd.DataFrame({'doublet_score':out[0],'predicted_doublets':out[1]},index = tmp.obs.index)\n    print(alldata[batch].predicted_doublets.sum(), \" predicted_doublets\")\n\ncovid_1 : 900  cells\n24  predicted_doublets\ncovid_15 : 599  cells\n8  predicted_doublets\ncovid_16 : 373  cells\n3  predicted_doublets\ncovid_17 : 1101  cells\n17  predicted_doublets\nctrl_5 : 1052  cells\n35  predicted_doublets\nctrl_13 : 1173  cells\n52  predicted_doublets\nctrl_14 : 1063  cells\n33  predicted_doublets\nctrl_19 : 1170  cells\n37  predicted_doublets\n\n\n\n# add predictions to the adata object.\nscrub_pred = pd.concat(alldata.values())\nadata.obs['doublet_scores'] = scrub_pred['doublet_score'] \nadata.obs['predicted_doublets'] = scrub_pred['predicted_doublets'] \n\nsum(adata.obs['predicted_doublets'])\n\n209\n\n\nWe should expect that two cells have more detected genes than a single cell, lets check if our predicted doublets also have more detected genes in general.\n\n# add in column with singlet/doublet instead of True/Fals\n%matplotlib inline\n\nadata.obs['doublet_info'] = adata.obs[\"predicted_doublets\"].astype(str)\nsc.pl.violin(adata, 'n_genes_by_counts', jitter=0.4, groupby = 'doublet_info', rotation=45)\n\n\n\n\n\n\n\n\nNow, lets run PCA and UMAP and plot doublet scores onto UMAP to check the doublet predictions. We will go through these steps in more detail in the later exercises.\n\nsc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\nadata = adata[:, adata.var.highly_variable]\nsc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt'])\nsc.pp.scale(adata, max_value=10)\nsc.tl.pca(adata, svd_solver='arpack')\nsc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)\nsc.tl.umap(adata)\nsc.pl.umap(adata, color=['doublet_scores','doublet_info','sample'])\n\nextracting highly variable genes\n    finished (0:00:02)\n--&gt; added\n    'highly_variable', boolean vector (adata.var)\n    'means', float vector (adata.var)\n    'dispersions', float vector (adata.var)\n    'dispersions_norm', float vector (adata.var)\nregressing out ['total_counts', 'pct_counts_mt']\n    finished (0:00:31)\ncomputing PCA\n    with n_comps=50\n    finished (0:00:02)\ncomputing neighbors\n    using 'X_pca' with n_pcs = 40\n    finished: added to `.uns['neighbors']`\n    `.obsp['distances']`, distances for each pair of neighbors\n    `.obsp['connectivities']`, weighted adjacency matrix (0:00:05)\ncomputing UMAP\n    finished: added\n    'X_umap', UMAP coordinates (adata.obsm)\n    'umap', UMAP parameters (adata.uns) (0:00:09)\n\n\n\n\n\n\n\n\n\nNow, lets remove all predicted doublets from our data.\n\n# also revert back to the raw counts as the main matrix in adata\nadata = adata.raw.to_adata() \n\nadata = adata[adata.obs['doublet_info'] == 'False',:]\nprint(adata.shape)\n\n(7222, 19468)\n\n\nTo summarize, lets check how many cells we have removed per sample, we started with 1500 cells per sample. Looking back at the intitial QC plots does it make sense that some samples have much fewer cells now?\nadata.obs[\"sample\"].value_counts()"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#section-12",
    "href": "labs/scanpy/scanpy_01_qc.html#section-12",
    "title": " Quality Control",
    "section": "9 Save data",
    "text": "9 Save data\nFinally, lets save the QC-filtered data for further analysis. Create output directory data/covid/results and save data to that folder. This will be used in downstream labs.\n\nadata.write_h5ad('data/covid/results/scanpy_covid_qc.h5ad')"
  },
  {
    "objectID": "labs/scanpy/scanpy_01_qc.html#section-13",
    "href": "labs/scanpy/scanpy_01_qc.html#section-13",
    "title": " Quality Control",
    "section": "10 Session info",
    "text": "10 Session info\n\n\nClick here\n\n\nsc.logging.print_versions()\n\n-----\nanndata     0.10.8\nscanpy      1.10.3\n-----\nCoreFoundation              NA\nFoundation                  NA\nPIL                         10.4.0\nPyObjCTools                 NA\nannoy                       NA\nanyio                       NA\nappnope                     0.1.4\narrow                       1.3.0\nasciitree                   NA\nasttokens                   NA\nattr                        24.2.0\nattrs                       24.2.0\nbabel                       2.14.0\nbrotli                      1.1.0\ncertifi                     2024.08.30\ncffi                        1.17.1\ncharset_normalizer          3.3.2\ncloudpickle                 3.0.0\ncolorama                    0.4.6\ncomm                        0.2.2\ncycler                      0.12.1\ncython_runtime              NA\ncytoolz                     0.12.3\ndask                        2024.9.0\ndateutil                    2.9.0\ndebugpy                     1.8.5\ndecorator                   5.1.1\ndefusedxml                  0.7.1\nexceptiongroup              1.2.2\nexecuting                   2.1.0\nfastjsonschema              NA\nfqdn                        NA\nfuture                      1.0.0\ngoogle                      NA\nh5py                        3.11.0\nidna                        3.10\nigraph                      0.11.6\nipykernel                   6.29.5\nisoduration                 NA\njedi                        0.19.1\njinja2                      3.1.4\njoblib                      1.4.2\njson5                       0.9.25\njsonpointer                 3.0.0\njsonschema                  4.23.0\njsonschema_specifications   NA\njupyter_events              0.10.0\njupyter_server              2.14.2\njupyterlab_server           2.27.3\nkiwisolver                  1.4.7\nlazy_loader                 0.4\nlegacy_api_wrap             NA\nleidenalg                   0.10.2\nllvmlite                    0.43.0\nmarkupsafe                  2.1.5\nmatplotlib                  3.9.2\nmatplotlib_inline           0.1.7\nmpl_toolkits                NA\nmsgpack                     1.1.0\nnatsort                     8.4.0\nnbformat                    5.10.4\nnumba                       0.60.0\nnumcodecs                   0.13.0\nnumpy                       1.26.4\nnumpydoc                    1.8.0\nobjc                        10.3.1\noverrides                   NA\npackaging                   24.1\npandas                      1.5.3\nparso                       0.8.4\npatsy                       0.5.6\npickleshare                 0.7.5\nplatformdirs                4.3.6\npooch                       v1.8.2\nprometheus_client           NA\nprompt_toolkit              3.0.47\npsutil                      6.0.0\npure_eval                   0.2.3\npybiomart                   0.2.0\npycparser                   2.22\npydev_ipython               NA\npydevconsole                NA\npydevd                      2.9.5\npydevd_file_utils           NA\npydevd_plugins              NA\npydevd_tracing              NA\npygments                    2.18.0\npynndescent                 0.5.13\npyparsing                   3.1.4\npythonjsonlogger            NA\npytz                        2024.2\nreferencing                 NA\nrequests                    2.32.3\nrequests_cache              0.4.13\nrfc3339_validator           0.1.4\nrfc3986_validator           0.1.1\nrpds                        NA\nscipy                       1.14.1\nscrublet                    NA\nseaborn                     0.13.2\nsend2trash                  NA\nsession_info                1.0.0\nsix                         1.16.0\nskimage                     0.24.0\nsklearn                     1.5.2\nsniffio                     1.3.1\nsocks                       1.7.1\nsparse                      0.15.4\nsphinxcontrib               NA\nstack_data                  0.6.2\nstatsmodels                 0.14.3\ntexttable                   1.7.0\nthreadpoolctl               3.5.0\ntlz                         0.12.3\ntoolz                       0.12.1\ntorch                       2.4.0.post101\ntorchgen                    NA\ntornado                     6.4.1\ntqdm                        4.66.5\ntraitlets                   5.14.3\ntyping_extensions           NA\numap                        0.5.6\nuri_template                NA\nurllib3                     2.2.3\nwcwidth                     0.2.13\nwebcolors                   24.8.0\nwebsocket                   1.8.0\nyaml                        6.0.2\nzarr                        2.18.3\nzipp                        NA\nzmq                         26.2.0\nzoneinfo                    NA\nzstandard                   0.23.0\n-----\nIPython             8.27.0\njupyter_client      8.6.3\njupyter_core        5.7.2\njupyterlab          4.2.5\nnotebook            7.2.2\n-----\nPython 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:17:34) [Clang 14.0.6 ]\nmacOS-14.6.1-x86_64-i386-64bit\n-----\nSession information updated at 2024-10-04 11:42"
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#section",
    "href": "labs/scanpy/scanpy_02_dimred.html#section",
    "title": " Dimensionality Reduction",
    "section": "1 Data preparation",
    "text": "1 Data preparation\nFirst, let’s load all necessary libraries and the QC-filtered dataset from the previous step.\n\nimport numpy as np\nimport pandas as pd\nimport scanpy as sc\nimport matplotlib.pyplot as plt\nimport warnings\nimport os\nimport urllib.request\n\nwarnings.simplefilter(action=\"ignore\", category=Warning)\n\n# verbosity: errors (0), warnings (1), info (2), hints (3)\nsc.settings.verbosity = 3\n# sc.logging.print_versions()\n\nsc.settings.set_figure_params(dpi=80)\n\n\n# download pre-computed data if missing or long compute\nfetch_data = True\n\n# url for source and intermediate data\npath_data = \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\n\npath_results = \"data/covid/results\"\nif not os.path.exists(path_results):\n    os.makedirs(path_results, exist_ok=True)\n\npath_file = \"data/covid/results/scanpy_covid_qc.h5ad\"\n# if fetch_data is false and path_file doesn't exist\n\nif fetch_data and not os.path.exists(path_file):\n    urllib.request.urlretrieve(os.path.join(\n        path_data, 'covid/results/scanpy_covid_qc.h5ad'), path_file)\n\nadata = sc.read_h5ad(path_file)\nadata\n\nAnnData object with n_obs × n_vars = 7222 × 19468\n    obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info'\n    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells'\n    uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'umap'\n    obsm: 'X_pca', 'X_umap'\n    obsp: 'connectivities', 'distances'\n\n\nBefore variable gene selection we need to normalize and log transform the data. Then store the full matrix in the raw slot before doing variable gene selection.\n\n# normalize to depth 10 000\nsc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n\n# log transform\nsc.pp.log1p(adata)\n\n# store normalized counts in the raw slot, \n# we will subset adata.X for variable genes, but want to keep all genes matrix as well.\nadata.raw = adata\n\nadata\n\nnormalizing by total count per cell\n    finished (0:00:00): normalized adata.X and added\n    'n_counts', counts per cell before normalization (adata.obs)\nWARNING: adata.X seems to be already log-transformed.\n\n\nAnnData object with n_obs × n_vars = 7222 × 19468\n    obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info'\n    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells'\n    uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'umap'\n    obsm: 'X_pca', 'X_umap'\n    obsp: 'connectivities', 'distances'"
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#section-1",
    "href": "labs/scanpy/scanpy_02_dimred.html#section-1",
    "title": " Dimensionality Reduction",
    "section": "2 Feature selection",
    "text": "2 Feature selection\nWe first need to define which features/genes are important in our dataset to distinguish cell types. For this purpose, we need to find genes that are highly variable across cells, which in turn will also provide a good separation of the cell clusters.\n\n# compute variable genes\nsc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\nprint(\"Highly variable genes: %d\"%sum(adata.var.highly_variable))\n\n#plot variable genes\nsc.pl.highly_variable_genes(adata)\n\n# subset for variable genes in the dataset\nadata = adata[:, adata.var['highly_variable']]\n\nextracting highly variable genes\n    finished (0:00:00)\n--&gt; added\n    'highly_variable', boolean vector (adata.var)\n    'means', float vector (adata.var)\n    'dispersions', float vector (adata.var)\n    'dispersions_norm', float vector (adata.var)\nHighly variable genes: 2626"
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#section-2",
    "href": "labs/scanpy/scanpy_02_dimred.html#section-2",
    "title": " Dimensionality Reduction",
    "section": "3 Z-score transformation",
    "text": "3 Z-score transformation\nNow that the genes have been selected, we now proceed with PCA. Since each gene has a different expression level, it means that genes with higher expression values will naturally have higher variation that will be captured by PCA. This means that we need to somehow give each gene a similar weight when performing PCA (see below). The common practice is to center and scale each gene before performing PCA. This exact scaling called Z-score normalization is very useful for PCA, clustering and plotting heatmaps. Additionally, we can use regression to remove any unwanted sources of variation from the dataset, such as cell cycle, sequencing depth, percent mitochondria etc. This is achieved by doing a generalized linear regression using these parameters as co-variates in the model. Then the residuals of the model are taken as the regressed data. Although perhaps not in the best way, batch effect regression can also be done here. By default, variables are scaled in the PCA step and is not done separately. But it could be achieved by running the commands below:\n\n#run this line if you get the \"AttributeError: swapaxes not found\" \n# adata = adata.copy()\n\n# regress out unwanted variables\nsc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt'])\n\n# scale data, clip values exceeding standard deviation 10.\nsc.pp.scale(adata, max_value=10)\n\nregressing out ['total_counts', 'pct_counts_mt']\n    sparse input is densified and may lead to high memory use\n    finished (0:00:39)"
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#section-3",
    "href": "labs/scanpy/scanpy_02_dimred.html#section-3",
    "title": " Dimensionality Reduction",
    "section": "4 PCA",
    "text": "4 PCA\nPerforming PCA has many useful applications and interpretations, which much depends on the data used. In the case of single-cell data, we want to segregate samples based on gene expression patterns in the data.\nTo run PCA, you can use the function pca().\n\nsc.tl.pca(adata, svd_solver='arpack')\n\ncomputing PCA\n    with n_comps=50\n    finished (0:00:02)\n\n\nWe then plot the first principal components.\n\n# plot more PCS\nsc.pl.pca(adata, color='sample', components = ['1,2','3,4','5,6','7,8'], ncols=2)\n\n\n\n\n\n\n\n\nTo identify genes that contribute most to each PC, one can retrieve the loading matrix information.\n\n#Plot loadings\nsc.pl.pca_loadings(adata, components=[1,2,3,4,5,6,7,8])\n\n# OBS! only plots the positive axes genes from each PC!!\n\n\n\n\n\n\n\n\nThe function to plot loading genes only plots genes on the positive axes. Instead plot as a heatmaps, with genes on both positive and negative side, one per pc, and plot their expression amongst cells ordered by their position along the pc.\n\n# adata.obsm[\"X_pca\"] is the embeddings\n# adata.uns[\"pca\"] is pc variance\n# adata.varm['PCs'] is the loadings\n\ngenes = adata.var['gene_ids']\n\nfor pc in [1,2,3,4]:\n    g = adata.varm['PCs'][:,pc-1]\n    o = np.argsort(g)\n    sel = np.concatenate((o[:10],o[-10:])).tolist()\n    emb = adata.obsm['X_pca'][:,pc-1]\n    # order by position on that pc\n    tempdata = adata[np.argsort(emb),]\n    sc.pl.heatmap(tempdata, var_names = genes[sel].index.tolist(), groupby='predicted_doublets', swap_axes = True, use_raw=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also plot the amount of variance explained by each PC.\n\nsc.pl.pca_variance_ratio(adata, log=True, n_pcs = 50)\n\n\n\n\n\n\n\n\nBased on this plot, we can see that the top 8 PCs retain a lot of information, while other PCs contain progressively less. However, it is still advisable to use more PCs since they might contain information about rare cell types (such as platelets and DCs in this dataset)"
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#section-4",
    "href": "labs/scanpy/scanpy_02_dimred.html#section-4",
    "title": " Dimensionality Reduction",
    "section": "5 tSNE",
    "text": "5 tSNE\nWe will now run BH-tSNE.\n\nsc.tl.tsne(adata, n_pcs = 30)\n\ncomputing tSNE\n    using 'X_pca' with n_pcs = 30\n    using sklearn.manifold.TSNE\n    finished: added\n    'X_tsne', tSNE coordinates (adata.obsm)\n    'tsne', tSNE parameters (adata.uns) (0:00:14)\n\n\nWe plot the tSNE scatterplot colored by dataset. We can clearly see the effect of batches present in the dataset.\n\nsc.pl.tsne(adata, color='sample')"
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#section-5",
    "href": "labs/scanpy/scanpy_02_dimred.html#section-5",
    "title": " Dimensionality Reduction",
    "section": "6 UMAP",
    "text": "6 UMAP\nThe UMAP implementation in SCANPY uses a neighborhood graph as the distance matrix, so we need to first calculate the graph.\n\nsc.pp.neighbors(adata, n_pcs = 30, n_neighbors = 20)\n\ncomputing neighbors\n    using 'X_pca' with n_pcs = 30\n    finished: added to `.uns['neighbors']`\n    `.obsp['distances']`, distances for each pair of neighbors\n    `.obsp['connectivities']`, weighted adjacency matrix (0:00:04)\n\n\nWe can now run UMAP for cell embeddings.\n\nsc.tl.umap(adata)\nsc.pl.umap(adata, color='sample')\n\ncomputing UMAP\n    finished: added\n    'X_umap', UMAP coordinates (adata.obsm)\n    'umap', UMAP parameters (adata.uns) (0:00:11)\n\n\n\n\n\n\n\n\n\nUMAP is plotted colored per dataset. Although less distinct as in the tSNE, we still see quite an effect of the different batches in the data.\n\n# run with 10 components, save to a new object so that the umap with 2D is not overwritten.\numap10 = sc.tl.umap(adata, n_components=10, copy=True)\nfig, axs = plt.subplots(1, 3, figsize=(10, 4), constrained_layout=True)\n\nsc.pl.umap(adata, color='sample',  title=\"UMAP\",\n           show=False, ax=axs[0], legend_loc=None)\nsc.pl.umap(umap10, color='sample', title=\"UMAP10\", show=False,\n           ax=axs[1], components=['1,2'], legend_loc=None)\nsc.pl.umap(umap10, color='sample', title=\"UMAP10\",\n           show=False, ax=axs[2], components=['3,4'], legend_loc=None)\n\n# we can also plot the umap with neighbor edges\nsc.pl.umap(adata, color='sample', title=\"UMAP\", edges=True)\n\ncomputing UMAP\n    finished: added\n    'X_umap', UMAP coordinates (adata.obsm)\n    'umap', UMAP parameters (adata.uns) (0:00:13)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can now plot PCA, UMAP and tSNE side by side for comparison. Have a look at the UMAP and tSNE. What similarities/differences do you see? Can you explain the differences based on what you learned during the lecture? Also, we can conclude from the dimensionality reductions that our dataset contains a batch effect that needs to be corrected before proceeding to clustering and differential gene expression analysis.\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 8), constrained_layout=True)\nsc.pl.pca(adata, color='sample', components=['1,2'], ax=axs[0, 0], show=False)\nsc.pl.tsne(adata, color='sample', components=['1,2'], ax=axs[0, 1], show=False)\nsc.pl.umap(adata, color='sample', components=['1,2'], ax=axs[1, 0], show=False)\n\n\n\n\n\n\n\n\nFinally, we can compare the PCA, tSNE and UMAP.\n\n\n\n\n\n\nDiscuss\n\n\n\nWe have now done Variable gene selection, PCA and UMAP with the settings we selected for you. Test a few different ways of selecting variable genes, number of PCs for UMAP and check how it influences your embedding."
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#section-6",
    "href": "labs/scanpy/scanpy_02_dimred.html#section-6",
    "title": " Dimensionality Reduction",
    "section": "7 Genes of interest",
    "text": "7 Genes of interest\nLet’s plot some marker genes for different cell types onto the embedding.\n\n\n\nMarkers\nCell Type\n\n\n\n\nCD3E\nT cells\n\n\nCD3E CD4\nCD4+ T cells\n\n\nCD3E CD8A\nCD8+ T cells\n\n\nGNLY, NKG7\nNK cells\n\n\nMS4A1\nB cells\n\n\nCD14, LYZ, CST3, MS4A7\nCD14+ Monocytes\n\n\nFCGR3A, LYZ, CST3, MS4A7\nFCGR3A+ Monocytes\n\n\nFCER1A, CST3\nDCs\n\n\n\n\nsc.pl.umap(adata, color=[\"CD3E\", \"CD4\", \"CD8A\", \"GNLY\",\"NKG7\", \"MS4A1\",\"CD14\",\"LYZ\",\"CST3\",\"MS4A7\",\"FCGR3A\"])\n\n\n\n\n\n\n\n\nThe default is to plot gene expression in the normalized and log-transformed data. You can also plot it on the scaled and corrected data by using use_raw=False. However, not all of these genes are included in the variable gene set, and hence are not included in the scaled adata.X, so we first need to filter them.\n\ngenes  = [\"CD3E\", \"CD4\", \"CD8A\", \"GNLY\",\"NKG7\", \"MS4A1\",\"CD14\",\"LYZ\",\"CST3\",\"MS4A7\",\"FCGR3A\"]\nvar_genes = adata.var.highly_variable\nvar_genes.index[var_genes]\nvarg = [x for x in genes if x in var_genes.index[var_genes]]\nsc.pl.umap(adata, color=varg, use_raw=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nSelect some of your dimensionality reductions and plot some of the QC stats that were calculated in the previous lab. Can you see if some of the separation in your data is driven by quality of the cells?\n\n\n\nsc.pl.umap(adata, color=['n_genes_by_counts', 'total_counts', 'pct_counts_mt','pct_counts_ribo', 'pct_counts_hb'], ncols=3,use_raw=False)"
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#section-7",
    "href": "labs/scanpy/scanpy_02_dimred.html#section-7",
    "title": " Dimensionality Reduction",
    "section": "8 Save data",
    "text": "8 Save data\nWe can finally save the object for use in future steps.\n\nadata.write_h5ad('data/covid/results/scanpy_covid_qc_dr.h5ad')"
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#note",
    "href": "labs/scanpy/scanpy_02_dimred.html#note",
    "title": " Dimensionality Reduction",
    "section": "9 Note",
    "text": "9 Note\nJust as a reminder, you need to keep in mind what you have in the X matrix. After these operations you have an X matrix with only variable genes, that are normalized, logtransformed and scaled.\nWe stored the expression of all genes in raw.X after doing lognormalization so that matrix is a sparse matrix with logtransformed values.\n\nprint(adata.X.shape)\nprint(adata.raw.X.shape)\n\nprint(adata.X[:3,:3])\nprint(adata.raw.X[:10,:10])\n\n(7222, 2626)\n(7222, 19468)\n[[-0.16998859 -0.06050171 -0.08070081]\n [-0.19315341 -0.09975121 -0.31379319]\n [-0.2051203  -0.11680799 -0.43194618]]\n&lt;Compressed Sparse Row sparse matrix of dtype 'float64'\n    with 2 stored elements and shape (10, 10)&gt;\n  Coords    Values\n  (1, 4)    0.7825693876867097\n  (8, 7)    1.1311041336746985"
  },
  {
    "objectID": "labs/scanpy/scanpy_02_dimred.html#section-8",
    "href": "labs/scanpy/scanpy_02_dimred.html#section-8",
    "title": " Dimensionality Reduction",
    "section": "10 Session info",
    "text": "10 Session info\n\n\nClick here\n\n\nsc.logging.print_versions()\n\n-----\nanndata     0.10.8\nscanpy      1.10.3\n-----\nCoreFoundation              NA\nFoundation                  NA\nPIL                         10.4.0\nPyObjCTools                 NA\nanyio                       NA\nappnope                     0.1.4\narrow                       1.3.0\nasciitree                   NA\nasttokens                   NA\nattr                        24.2.0\nattrs                       24.2.0\nbabel                       2.14.0\nbrotli                      1.1.0\ncertifi                     2024.08.30\ncffi                        1.17.1\ncharset_normalizer          3.3.2\ncloudpickle                 3.0.0\ncolorama                    0.4.6\ncomm                        0.2.2\ncycler                      0.12.1\ncython_runtime              NA\ncytoolz                     0.12.3\ndask                        2024.9.0\ndateutil                    2.9.0\ndebugpy                     1.8.5\ndecorator                   5.1.1\ndefusedxml                  0.7.1\nexceptiongroup              1.2.2\nexecuting                   2.1.0\nfastjsonschema              NA\nfqdn                        NA\ngoogle                      NA\nh5py                        3.11.0\nidna                        3.10\nigraph                      0.11.6\nipykernel                   6.29.5\nisoduration                 NA\njedi                        0.19.1\njinja2                      3.1.4\njoblib                      1.4.2\njson5                       0.9.25\njsonpointer                 3.0.0\njsonschema                  4.23.0\njsonschema_specifications   NA\njupyter_events              0.10.0\njupyter_server              2.14.2\njupyterlab_server           2.27.3\nkiwisolver                  1.4.7\nlegacy_api_wrap             NA\nleidenalg                   0.10.2\nllvmlite                    0.43.0\nmarkupsafe                  2.1.5\nmatplotlib                  3.9.2\nmatplotlib_inline           0.1.7\nmpl_toolkits                NA\nmsgpack                     1.1.0\nnatsort                     8.4.0\nnbformat                    5.10.4\nnetworkx                    3.3\nnumba                       0.60.0\nnumcodecs                   0.13.0\nnumpy                       1.26.4\nobjc                        10.3.1\noverrides                   NA\npackaging                   24.1\npandas                      1.5.3\nparso                       0.8.4\npatsy                       0.5.6\npickleshare                 0.7.5\nplatformdirs                4.3.6\nprometheus_client           NA\nprompt_toolkit              3.0.47\npsutil                      6.0.0\npure_eval                   0.2.3\npycparser                   2.22\npydev_ipython               NA\npydevconsole                NA\npydevd                      2.9.5\npydevd_file_utils           NA\npydevd_plugins              NA\npydevd_tracing              NA\npygments                    2.18.0\npynndescent                 0.5.13\npyparsing                   3.1.4\npythonjsonlogger            NA\npytz                        2024.2\nreferencing                 NA\nrequests                    2.32.3\nrfc3339_validator           0.1.4\nrfc3986_validator           0.1.1\nrpds                        NA\nscipy                       1.14.1\nsend2trash                  NA\nsession_info                1.0.0\nsix                         1.16.0\nsklearn                     1.5.2\nsniffio                     1.3.1\nsocks                       1.7.1\nsparse                      0.15.4\nsphinxcontrib               NA\nstack_data                  0.6.2\nstatsmodels                 0.14.3\ntexttable                   1.7.0\nthreadpoolctl               3.5.0\ntlz                         0.12.3\ntoolz                       0.12.1\ntorch                       2.4.0.post101\ntorchgen                    NA\ntornado                     6.4.1\ntqdm                        4.66.5\ntraitlets                   5.14.3\ntyping_extensions           NA\numap                        0.5.6\nuri_template                NA\nurllib3                     2.2.3\nwcwidth                     0.2.13\nwebcolors                   24.8.0\nwebsocket                   1.8.0\nyaml                        6.0.2\nzarr                        2.18.3\nzipp                        NA\nzmq                         26.2.0\nzoneinfo                    NA\nzstandard                   0.23.0\n-----\nIPython             8.27.0\njupyter_client      8.6.3\njupyter_core        5.7.2\njupyterlab          4.2.5\nnotebook            7.2.2\n-----\nPython 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:17:34) [Clang 14.0.6 ]\nmacOS-14.6.1-x86_64-i386-64bit\n-----\nSession information updated at 2024-10-04 11:37"
  },
  {
    "objectID": "labs/scanpy/scanpy_03_integration.html#section",
    "href": "labs/scanpy/scanpy_03_integration.html#section",
    "title": " Data Integration",
    "section": "1 Data preparation",
    "text": "1 Data preparation\nLet’s first load necessary libraries and the data saved in the previous lab.\n\nimport numpy as np\nimport pandas as pd\nimport scanpy as sc\nimport matplotlib.pyplot as plt\nimport warnings\nimport os\nimport urllib.request\n\nwarnings.simplefilter(action='ignore', category=Warning)\n\n# verbosity: errors (0), warnings (1), info (2), hints (3)\nsc.settings.verbosity = 3             \n\nsc.settings.set_figure_params(dpi=80)\n%matplotlib inline\n\nCreate individual adata objects per batch.\n\n# download pre-computed data if missing or long compute\nfetch_data = True\n\n# url for source and intermediate data\npath_data = \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\n\npath_results = \"data/covid/results\"\nif not os.path.exists(path_results):\n    os.makedirs(path_results, exist_ok=True)\n\npath_file = \"data/covid/results/scanpy_covid_qc_dr.h5ad\"\nif fetch_data and not os.path.exists(path_file):\n    urllib.request.urlretrieve(os.path.join(\n        path_data, 'covid/results/scanpy_covid_qc_dr.h5ad'), path_file)\n\nadata = sc.read_h5ad(path_file)\nadata\n\nAnnData object with n_obs × n_vars = 7222 × 2626\n    obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info'\n    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'\n    uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap'\n    obsm: 'X_pca', 'X_tsne', 'X_umap'\n    varm: 'PCs'\n    obsp: 'connectivities', 'distances'\n\n\n\nprint(adata.X.shape)\n\n(7222, 2626)\n\n\nAs the stored AnnData object contains scaled data based on variable genes, we need to make a new object with the logtransformed normalized counts. The new variable gene selection should not be performed on the scaled data matrix.\n\nadata2 = adata.raw.to_adata() \n\n# in some versions of Anndata there is an issue with information on the logtransformation in the slot log1p.base so we set it to None to not get errors.\nadata2.uns['log1p']['base']=None\n\n# check that the matrix looks like normalized counts\nprint(adata2.X[1:10,1:10])\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'float64'\n    with 2 stored elements and shape (9, 9)&gt;\n  Coords    Values\n  (0, 3)    0.7825693876867097\n  (7, 6)    1.1311041336746985"
  },
  {
    "objectID": "labs/scanpy/scanpy_03_integration.html#harmony",
    "href": "labs/scanpy/scanpy_03_integration.html#harmony",
    "title": " Data Integration",
    "section": "4 Harmony",
    "text": "4 Harmony\nAn alternative method for integration is Harmony, for more details on the method, please se their paper Nat. Methods. This method runs the integration on a dimensionality reduction, in most applications the PCA.\n\nimport scanpy.external as sce \nimport harmonypy as hm \n\nsce.pp.harmony_integrate(adata2, 'sample')\n\n# Then we calculate a new umap and tsne.\nsc.pp.neighbors(adata2, n_neighbors=10, n_pcs=30, use_rep='X_pca_harmony')\nsc.tl.umap(adata2)\nsc.tl.tsne(adata2, use_rep='X_pca_harmony')\nsc.tl.leiden(adata2, resolution=0.5)\n\ncomputing neighbors\n    finished: added to `.uns['neighbors']`\n    `.obsp['distances']`, distances for each pair of neighbors\n    `.obsp['connectivities']`, weighted adjacency matrix (0:00:00)\ncomputing UMAP\n    finished: added\n    'X_umap', UMAP coordinates (adata.obsm)\n    'umap', UMAP parameters (adata.uns) (0:00:09)\ncomputing tSNE\n    using sklearn.manifold.TSNE\n    finished: added\n    'X_tsne', tSNE coordinates (adata.obsm)\n    'tsne', tSNE parameters (adata.uns) (0:00:15)\nrunning Leiden clustering\n    finished: found 10 clusters and added\n    'leiden', the cluster labels (adata.obs, categorical) (0:00:00)\n\n\n\nfig, axs = plt.subplots(2, 2, figsize=(10,8),constrained_layout=True)\nsc.pl.embedding(adata2, 'X_tsne_bbknn', color=\"sample\", title=\"BBKNN tsne\", ax=axs[0,0], show=False)\nsc.pl.tsne(adata2, color=\"sample\", title=\"Harmony tsne\", ax=axs[0,1], show=False)\nsc.pl.embedding(adata2, 'X_umap_bbknn', color=\"sample\", title=\"BBKNN umap\", ax=axs[1,0], show=False)\nsc.pl.umap(adata2, color=\"sample\", title=\"Harmony umap\", ax=axs[1,1], show=False)\n\n\n\n\n\n\n\n\nLet’s save the integrated data for further analysis.\n\n# Store this umap and tsne with a new name.\nadata2.obsm['X_umap_harmony'] = adata2.obsm['X_umap']\nadata2.obsm['X_tsne_harmony'] = adata2.obsm['X_tsne']\n\n#save to file\nsave_file = './data/covid/results/scanpy_covid_qc_dr_harmony.h5ad'\nadata2.write_h5ad(save_file)"
  },
  {
    "objectID": "labs/scanpy/scanpy_03_integration.html#section-1",
    "href": "labs/scanpy/scanpy_03_integration.html#section-1",
    "title": " Data Integration",
    "section": "6 Scanorama",
    "text": "6 Scanorama\nTry out Scanorama for data integration as well. First we need to create individual AnnData objects from each of the datasets.\n\n# split per batch into new objects.\nbatches = adata2.obs['sample'].cat.categories.tolist()\nalldata = {}\nfor batch in batches:\n    alldata[batch] = adata2[adata2.obs['sample'] == batch,]\n\nalldata   \n\n{'covid_1': View of AnnData object with n_obs × n_vars = 876 × 19468\n     obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info', 'leiden'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n     uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap', 'leiden'\n     obsm: 'X_pca', 'X_tsne', 'X_umap', 'X_umap_uncorr', 'X_tsne_uncorr', 'X_umap_bbknn', 'X_tsne_bbknn', 'X_pca_harmony', 'X_umap_harmony', 'X_tsne_harmony'\n     obsp: 'connectivities', 'distances',\n 'covid_15': View of AnnData object with n_obs × n_vars = 591 × 19468\n     obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info', 'leiden'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n     uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap', 'leiden'\n     obsm: 'X_pca', 'X_tsne', 'X_umap', 'X_umap_uncorr', 'X_tsne_uncorr', 'X_umap_bbknn', 'X_tsne_bbknn', 'X_pca_harmony', 'X_umap_harmony', 'X_tsne_harmony'\n     obsp: 'connectivities', 'distances',\n 'covid_16': View of AnnData object with n_obs × n_vars = 370 × 19468\n     obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info', 'leiden'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n     uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap', 'leiden'\n     obsm: 'X_pca', 'X_tsne', 'X_umap', 'X_umap_uncorr', 'X_tsne_uncorr', 'X_umap_bbknn', 'X_tsne_bbknn', 'X_pca_harmony', 'X_umap_harmony', 'X_tsne_harmony'\n     obsp: 'connectivities', 'distances',\n 'covid_17': View of AnnData object with n_obs × n_vars = 1084 × 19468\n     obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info', 'leiden'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n     uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap', 'leiden'\n     obsm: 'X_pca', 'X_tsne', 'X_umap', 'X_umap_uncorr', 'X_tsne_uncorr', 'X_umap_bbknn', 'X_tsne_bbknn', 'X_pca_harmony', 'X_umap_harmony', 'X_tsne_harmony'\n     obsp: 'connectivities', 'distances',\n 'ctrl_5': View of AnnData object with n_obs × n_vars = 1017 × 19468\n     obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info', 'leiden'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n     uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap', 'leiden'\n     obsm: 'X_pca', 'X_tsne', 'X_umap', 'X_umap_uncorr', 'X_tsne_uncorr', 'X_umap_bbknn', 'X_tsne_bbknn', 'X_pca_harmony', 'X_umap_harmony', 'X_tsne_harmony'\n     obsp: 'connectivities', 'distances',\n 'ctrl_13': View of AnnData object with n_obs × n_vars = 1121 × 19468\n     obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info', 'leiden'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n     uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap', 'leiden'\n     obsm: 'X_pca', 'X_tsne', 'X_umap', 'X_umap_uncorr', 'X_tsne_uncorr', 'X_umap_bbknn', 'X_tsne_bbknn', 'X_pca_harmony', 'X_umap_harmony', 'X_tsne_harmony'\n     obsp: 'connectivities', 'distances',\n 'ctrl_14': View of AnnData object with n_obs × n_vars = 1030 × 19468\n     obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info', 'leiden'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n     uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap', 'leiden'\n     obsm: 'X_pca', 'X_tsne', 'X_umap', 'X_umap_uncorr', 'X_tsne_uncorr', 'X_umap_bbknn', 'X_tsne_bbknn', 'X_pca_harmony', 'X_umap_harmony', 'X_tsne_harmony'\n     obsp: 'connectivities', 'distances',\n 'ctrl_19': View of AnnData object with n_obs × n_vars = 1133 × 19468\n     obs: 'type', 'sample', 'batch', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'n_genes', 'percent_chrY', 'XIST-counts', 'S_score', 'G2M_score', 'phase', 'doublet_scores', 'predicted_doublets', 'doublet_info', 'leiden'\n     var: 'gene_ids', 'feature_types', 'genome', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'highly_variable_nbatches', 'highly_variable_intersection'\n     uns: 'doublet_info_colors', 'hvg', 'log1p', 'neighbors', 'pca', 'phase_colors', 'sample_colors', 'tsne', 'umap', 'leiden'\n     obsm: 'X_pca', 'X_tsne', 'X_umap', 'X_umap_uncorr', 'X_tsne_uncorr', 'X_umap_bbknn', 'X_tsne_bbknn', 'X_pca_harmony', 'X_umap_harmony', 'X_tsne_harmony'\n     obsp: 'connectivities', 'distances'}\n\n\n\nimport scanorama\n\n#subset the individual dataset to the variable genes we defined at the beginning\nalldata2 = dict()\nfor ds in alldata.keys():\n    print(ds)\n    alldata2[ds] = alldata[ds][:,var_genes]\n\n#convert to list of AnnData objects\nadatas = list(alldata2.values())\n\n# run scanorama.integrate\nscanorama.integrate_scanpy(adatas, dimred = 50)\n\ncovid_1\ncovid_15\ncovid_16\ncovid_17\nctrl_5\nctrl_13\nctrl_14\nctrl_19\nFound 4268 genes among all datasets\n[[0.         0.50761421 0.52972973 0.26845018 0.59488692 0.48401826\n  0.36757991 0.09973522]\n [0.         0.         0.81891892 0.33840948 0.43362832 0.23181049\n  0.29949239 0.17597293]\n [0.         0.         0.         0.22702703 0.49459459 0.52972973\n  0.42702703 0.3       ]\n [0.         0.         0.         0.         0.27138643 0.09132841\n  0.1300738  0.17387467]\n [0.         0.         0.         0.         0.         0.8446411\n  0.73647984 0.25419241]\n [0.         0.         0.         0.         0.         0.\n  0.82815534 0.44836717]\n [0.         0.         0.         0.         0.         0.\n  0.         0.78022948]\n [0.         0.         0.         0.         0.         0.\n  0.         0.        ]]\nProcessing datasets (4, 5)\nProcessing datasets (5, 6)\nProcessing datasets (1, 2)\nProcessing datasets (6, 7)\nProcessing datasets (4, 6)\nProcessing datasets (0, 4)\nProcessing datasets (2, 5)\nProcessing datasets (0, 2)\nProcessing datasets (0, 1)\nProcessing datasets (2, 4)\nProcessing datasets (0, 5)\nProcessing datasets (5, 7)\nProcessing datasets (1, 4)\nProcessing datasets (2, 6)\nProcessing datasets (0, 6)\nProcessing datasets (1, 3)\nProcessing datasets (2, 7)\nProcessing datasets (1, 6)\nProcessing datasets (3, 4)\nProcessing datasets (0, 3)\nProcessing datasets (4, 7)\nProcessing datasets (1, 5)\nProcessing datasets (2, 3)\nProcessing datasets (1, 7)\nProcessing datasets (3, 7)\nProcessing datasets (3, 6)\n\n\n\n#scanorama adds the corrected matrix to adata.obsm in each of the datasets in adatas.\nadatas[0].obsm['X_scanorama'].shape\n\n(876, 50)\n\n\n\n# Get all the integrated matrices.\nscanorama_int = [ad.obsm['X_scanorama'] for ad in adatas]\n\n# make into one matrix.\nall_s = np.concatenate(scanorama_int)\nprint(all_s.shape)\n\n# add to the AnnData object, create a new object first\nadata2.obsm[\"Scanorama\"] = all_s\n\n(7222, 50)\n\n\n\n# tsne and umap\nsc.pp.neighbors(adata2, n_pcs =30, use_rep = \"Scanorama\")\nsc.tl.umap(adata2)\nsc.tl.tsne(adata2, n_pcs = 30, use_rep = \"Scanorama\")\n\ncomputing neighbors\n    finished: added to `.uns['neighbors']`\n    `.obsp['distances']`, distances for each pair of neighbors\n    `.obsp['connectivities']`, weighted adjacency matrix (0:00:00)\ncomputing UMAP\n    finished: added\n    'X_umap', UMAP coordinates (adata.obsm)\n    'umap', UMAP parameters (adata.uns) (0:00:10)\ncomputing tSNE\n    using sklearn.manifold.TSNE\n    finished: added\n    'X_tsne', tSNE coordinates (adata.obsm)\n    'tsne', tSNE parameters (adata.uns) (0:00:14)\n\n\nWe can now plot the unintegrated and the integrated space reduced dimensions.\n\nfig, axs = plt.subplots(2, 2, figsize=(10,8),constrained_layout=True)\nsc.pl.embedding(adata2, 'X_tsne_harmony', color=\"sample\", title=\"Harmony tsne\", ax=axs[0,0], show=False)\nsc.pl.tsne(adata2, color=\"sample\", title=\"Scanorama tsne\", ax=axs[0,1], show=False)\nsc.pl.embedding(adata2, 'X_umap_harmony', color=\"sample\", title=\"Harmony umap\", ax=axs[1,0], show=False)\nsc.pl.umap(adata2, color=\"sample\", title=\"Scanorama umap\", ax=axs[1,1], show=False)\n\n\n\n\n\n\n\n\nLet’s save the integrated data for further analysis.\n\n# Store this umap and tsne with a new name.\nadata2.obsm['X_umap_scanorama'] = adata2.obsm['X_umap']\nadata2.obsm['X_tsne_scanorama'] = adata2.obsm['X_tsne']\n\n#save to file, now contains all integrations except the combat one.\nsave_file = './data/covid/results/scanpy_covid_qc_dr_int.h5ad'\nadata2.write_h5ad(save_file)"
  },
  {
    "objectID": "labs/scanpy/scanpy_03_integration.html#section-2",
    "href": "labs/scanpy/scanpy_03_integration.html#section-2",
    "title": " Data Integration",
    "section": "9 Session info",
    "text": "9 Session info\n\n\nClick here\n\n\nsc.logging.print_versions()\n\n-----\nanndata     0.10.8\nscanpy      1.10.3\n-----\nCoreFoundation              NA\nFoundation                  NA\nPIL                         10.4.0\nPyObjCTools                 NA\nannoy                       NA\nanyio                       NA\nappnope                     0.1.4\narrow                       1.3.0\nasciitree                   NA\nasttokens                   NA\nattr                        24.2.0\nattrs                       24.2.0\nbabel                       2.14.0\nbbknn                       1.6.0\nbrotli                      1.1.0\ncertifi                     2024.08.30\ncffi                        1.17.1\ncharset_normalizer          3.3.2\ncloudpickle                 3.0.0\ncolorama                    0.4.6\ncomm                        0.2.2\ncycler                      0.12.1\ncython_runtime              NA\ncytoolz                     0.12.3\ndask                        2024.9.0\ndateutil                    2.9.0\ndebugpy                     1.8.5\ndecorator                   5.1.1\ndefusedxml                  0.7.1\nexceptiongroup              1.2.2\nexecuting                   2.1.0\nfastjsonschema              NA\nfbpca                       NA\nfqdn                        NA\ngoogle                      NA\nh5py                        3.11.0\nharmonypy                   0.0.10\nidna                        3.10\nigraph                      0.11.6\nintervaltree                NA\nipykernel                   6.29.5\nisoduration                 NA\njedi                        0.19.1\njinja2                      3.1.4\njoblib                      1.4.2\njson5                       0.9.25\njsonpointer                 3.0.0\njsonschema                  4.23.0\njsonschema_specifications   NA\njupyter_events              0.10.0\njupyter_server              2.14.2\njupyterlab_server           2.27.3\nkiwisolver                  1.4.7\nlegacy_api_wrap             NA\nleidenalg                   0.10.2\nllvmlite                    0.43.0\nmarkupsafe                  2.1.5\nmatplotlib                  3.9.2\nmatplotlib_inline           0.1.7\nmpl_toolkits                NA\nmsgpack                     1.1.0\nnatsort                     8.4.0\nnbformat                    5.10.4\nnumba                       0.60.0\nnumcodecs                   0.13.0\nnumpy                       1.26.4\nobjc                        10.3.1\noverrides                   NA\npackaging                   24.1\npandas                      1.5.3\nparso                       0.8.4\npatsy                       0.5.6\npickleshare                 0.7.5\nplatformdirs                4.3.6\nprometheus_client           NA\nprompt_toolkit              3.0.47\npsutil                      6.0.0\npure_eval                   0.2.3\npycparser                   2.22\npydev_ipython               NA\npydevconsole                NA\npydevd                      2.9.5\npydevd_file_utils           NA\npydevd_plugins              NA\npydevd_tracing              NA\npygments                    2.18.0\npynndescent                 0.5.13\npyparsing                   3.1.4\npythonjsonlogger            NA\npytz                        2024.2\nreferencing                 NA\nrequests                    2.32.3\nrfc3339_validator           0.1.4\nrfc3986_validator           0.1.1\nrpds                        NA\nscanorama                   1.7.4\nscipy                       1.14.1\nsend2trash                  NA\nsession_info                1.0.0\nsix                         1.16.0\nsklearn                     1.5.2\nsniffio                     1.3.1\nsocks                       1.7.1\nsortedcontainers            2.4.0\nsparse                      0.15.4\nsphinxcontrib               NA\nstack_data                  0.6.2\ntexttable                   1.7.0\nthreadpoolctl               3.5.0\ntlz                         0.12.3\ntoolz                       0.12.1\ntorch                       2.4.0.post101\ntorchgen                    NA\ntornado                     6.4.1\ntqdm                        4.66.5\ntraitlets                   5.14.3\ntyping_extensions           NA\numap                        0.5.6\nuri_template                NA\nurllib3                     2.2.3\nwcwidth                     0.2.13\nwebcolors                   24.8.0\nwebsocket                   1.8.0\nyaml                        6.0.2\nzarr                        2.18.3\nzipp                        NA\nzmq                         26.2.0\nzoneinfo                    NA\nzstandard                   0.23.0\n-----\nIPython             8.27.0\njupyter_client      8.6.3\njupyter_core        5.7.2\njupyterlab          4.2.5\nnotebook            7.2.2\n-----\nPython 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:17:34) [Clang 14.0.6 ]\nmacOS-14.6.1-x86_64-i386-64bit\n-----\nSession information updated at 2024-10-07 08:45"
  },
  {
    "objectID": "labs/scanpy/scanpy_04_clustering.html#section",
    "href": "labs/scanpy/scanpy_04_clustering.html#section",
    "title": " Clustering",
    "section": "1 Graph clustering",
    "text": "1 Graph clustering\nThe procedure of clustering on a Graph can be generalized as 3 main steps:\n- Build a kNN graph from the data.\n- Prune spurious connections from kNN graph (optional step). This is a SNN graph.\n- Find groups of cells that maximizes the connections within the group compared other groups.\nIn Scanpy we do not build an SNN graph, instead the community detection is done on the KNN graph which we construct using the command sc.pp.neighbors().\nThe main options to consider are:\n\nn_pcs - the number of dimensions from the initial reduction to include when calculating distances between cells.\nn_neighbors - the number of neighbors per cell to include in the KNN graph.\n\nIn this case, we will use the integrated data using Harmony. If you recall, we stored the harmony reduction in X_pca_harmony in the previous lab.\n\nsc.pp.neighbors(adata, n_neighbors=20, n_pcs=30, use_rep='X_pca_harmony')\n\n# We will also set the default umap to the one created with harmony\n# so that sc.pl.umap selects that embedding.\nadata.obsm[\"X_umap\"] = adata.obsm[\"X_umap_harmony\"]\n\ncomputing neighbors\n    finished: added to `.uns['neighbors']`\n    `.obsp['distances']`, distances for each pair of neighbors\n    `.obsp['connectivities']`, weighted adjacency matrix (0:00:06)\n\n\nThe modularity optimization algoritm in Scanpy is Leiden. Previously ther was also Louvain, but since the Louvain algorithm is no longer maintained, using Leiden is recommended by the Scanpy community.\n\n1.1 Leiden\n\n# default resolution is 1.0, but we will try a few different values.\nsc.tl.leiden(adata, resolution = 0.4, key_added = \"leiden_0.4\")\nsc.tl.leiden(adata, resolution = 0.6, key_added = \"leiden_0.6\")\nsc.tl.leiden(adata, resolution = 1.0, key_added = \"leiden_1.0\")\nsc.tl.leiden(adata, resolution = 1.4, key_added = \"leiden_1.4\")\n\nrunning Leiden clustering\n    finished: found 9 clusters and added\n    'leiden_0.4', the cluster labels (adata.obs, categorical) (0:00:00)\nrunning Leiden clustering\n    finished: found 11 clusters and added\n    'leiden_0.6', the cluster labels (adata.obs, categorical) (0:00:00)\nrunning Leiden clustering\n    finished: found 16 clusters and added\n    'leiden_1.0', the cluster labels (adata.obs, categorical) (0:00:00)\nrunning Leiden clustering\n    finished: found 18 clusters and added\n    'leiden_1.4', the cluster labels (adata.obs, categorical) (0:00:00)\n\n\nPlot the clusters, as you can see, with increased resolution, we get higher granularity in the clustering.\n\nsc.pl.umap(adata, color=['leiden_0.4', 'leiden_0.6', 'leiden_1.0','leiden_1.4'], legend_fontsize=8)\n\n\n\n\n\n\n\n\nOnce we have done clustering, the relationships between clusters can be calculated as correlation in PCA space and we also visualize some of the marker genes that we used in the Dim Reduction lab onto the clusters. If we set dendrogram=True the clusters are ordered by the dendrogram in the dotplot.\n\nsc.tl.dendrogram(adata, groupby = \"leiden_0.6\")\nsc.pl.dendrogram(adata, groupby = \"leiden_0.6\")\n\ngenes  = [\"CD3E\", \"CD4\", \"CD8A\", \"GNLY\",\"NKG7\", \"MS4A1\",\"FCGR3A\",\"CD14\",\"LYZ\",\"CST3\",\"MS4A7\",\"FCGR1A\"]\nsc.pl.dotplot(adata, genes, groupby='leiden_0.6', dendrogram=True)\n\n    using 'X_pca' with n_pcs = 50\nStoring dendrogram info using `.uns['dendrogram_leiden_0.6']`"
  },
  {
    "objectID": "labs/scanpy/scanpy_04_clustering.html#section-1",
    "href": "labs/scanpy/scanpy_04_clustering.html#section-1",
    "title": " Clustering",
    "section": "2 K-means clustering",
    "text": "2 K-means clustering\nK-means is a generic clustering algorithm that has been used in many application areas. In R, it can be applied via the kmeans() function. Typically, it is applied to a reduced dimension representation of the expression data (most often PCA, because of the interpretability of the low-dimensional distances). We need to define the number of clusters in advance. Since the results depend on the initialization of the cluster centers, it is typically recommended to run K-means with multiple starting configurations (via the nstart argument).\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\n# extract pca coordinates\nX_pca = adata.obsm['X_pca_harmony'] \n\n# kmeans with k=5\nkmeans = KMeans(n_clusters=5, random_state=0).fit(X_pca) \nadata.obs['kmeans5'] = kmeans.labels_.astype(str)\n\n# kmeans with k=10\nkmeans = KMeans(n_clusters=10, random_state=0).fit(X_pca) \nadata.obs['kmeans10'] = kmeans.labels_.astype(str)\n\n# kmeans with k=15\nkmeans = KMeans(n_clusters=15, random_state=0).fit(X_pca)\nadata.obs['kmeans15'] = kmeans.labels_.astype(str)\n\nsc.pl.umap(adata, color=['kmeans5', 'kmeans10', 'kmeans15'])\n\nadata.obsm\n\n\n\n\n\n\n\n\nAxisArrays with keys: Scanorama, X_pca, X_pca_harmony, X_tsne, X_tsne_bbknn, X_tsne_harmony, X_tsne_scanorama, X_tsne_uncorr, X_umap, X_umap_bbknn, X_umap_harmony, X_umap_scanorama, X_umap_uncorr"
  },
  {
    "objectID": "labs/scanpy/scanpy_04_clustering.html#section-2",
    "href": "labs/scanpy/scanpy_04_clustering.html#section-2",
    "title": " Clustering",
    "section": "3 Hierarchical clustering",
    "text": "3 Hierarchical clustering\nHierarchical clustering is another generic form of clustering that can be applied also to scRNA-seq data. As K-means, it is typically applied to a reduced dimension representation of the data. Hierarchical clustering returns an entire hierarchy of partitionings (a dendrogram) that can be cut at different levels. Hierarchical clustering is done in these steps:\n\nDefine the distances between samples. The most common are Euclidean distance (a.k.a. straight line between two points) or correlation coefficients.\nDefine a measure of distances between clusters, called linkage criteria. It can for example be average distances between clusters. Commonly used methods are single, complete, average, median, centroid and ward.\nDefine the dendrogram among all samples using Bottom-up or Top-down approach. Bottom-up is where samples start with their own cluster which end up merged pair-by-pair until only one cluster is left. Top-down is where samples start all in the same cluster that end up being split by 2 until each sample has its own cluster.\n\nAs you might have realized, correlation is not a method implemented in the dist() function. However, we can create our own distances and transform them to a distance object. We can first compute sample correlations using the cor function.\nAs you already know, correlation range from -1 to 1, where 1 indicates that two samples are closest, -1 indicates that two samples are the furthest and 0 is somewhat in between. This, however, creates a problem in defining distances because a distance of 0 indicates that two samples are closest, 1 indicates that two samples are the furthest and distance of -1 is not meaningful. We thus need to transform the correlations to a positive scale (a.k.a. adjacency):\n\\[adj = \\frac{1- cor}{2}\\]\nOnce we transformed the correlations to a 0-1 scale, we can simply convert it to a distance object using as.dist() function. The transformation does not need to have a maximum of 1, but it is more intuitive to have it at 1, rather than at any other number.\nThe function AgglomerativeClustering has the option of running with disntance metrics “euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or “precomputed”. However, with ward linkage only euklidean distances works. Here we will try out euclidean distance and ward linkage calculated in PCA space.\n\nfrom sklearn.cluster import AgglomerativeClustering\n\ncluster = AgglomerativeClustering(n_clusters=5, linkage='ward')\nadata.obs['hclust_5'] = cluster.fit_predict(X_pca).astype(str)\n\ncluster = AgglomerativeClustering(n_clusters=10, linkage='ward')\nadata.obs['hclust_10'] = cluster.fit_predict(X_pca).astype(str)\n\ncluster = AgglomerativeClustering(n_clusters=15, linkage='ward')\nadata.obs['hclust_15'] = cluster.fit_predict(X_pca).astype(str)\n\nsc.pl.umap(adata, color=['hclust_5', 'hclust_10', 'hclust_15'])\n\n\n\n\n\n\n\n\nFinally, lets save the clustered data for further analysis.\n\nadata.write_h5ad('./data/covid/results/scanpy_covid_qc_dr_scanorama_cl.h5ad')"
  },
  {
    "objectID": "labs/scanpy/scanpy_04_clustering.html#section-3",
    "href": "labs/scanpy/scanpy_04_clustering.html#section-3",
    "title": " Clustering",
    "section": "4 Distribution of clusters",
    "text": "4 Distribution of clusters\nNow, we can select one of our clustering methods and compare the proportion of samples across the clusters.\nSelect the “leiden_0.6” and plot proportion of samples per cluster and also proportion covid vs ctrl.\nPlot proportion of cells from each condition per cluster.\n\ntmp = pd.crosstab(adata.obs['leiden_0.6'],adata.obs['type'], normalize='index')\ntmp.plot.bar(stacked=True).legend(bbox_to_anchor=(1.4, 1), loc='upper right')\n\ntmp = pd.crosstab(adata.obs['leiden_0.6'],adata.obs['sample'], normalize='index')\ntmp.plot.bar(stacked=True).legend(bbox_to_anchor=(1.4, 1),loc='upper right')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this case we have quite good representation of each sample in each cluster. But there are clearly some biases with more cells from one sample in some clusters and also more covid cells in some of the clusters.\nWe can also plot it in the other direction, the proportion of each cluster per sample.\n\ntmp = pd.crosstab(adata.obs['sample'],adata.obs['leiden_0.6'], normalize='index')\ntmp.plot.bar(stacked=True).legend(bbox_to_anchor=(1.4, 1), loc='upper right')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nBy now you should know how to plot different features onto your data. Take the QC metrics that were calculated in the first exercise, that should be stored in your data object, and plot it as violin plots per cluster using the clustering method of your choice. For example, plot number of UMIS, detected genes, percent mitochondrial reads. Then, check carefully if there is any bias in how your data is separated by quality metrics. Could it be explained biologically, or could there be a technical bias there?\n\n\n\nsc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt'], jitter=0.4, groupby = 'leiden_0.6', rotation= 45)\n\n\n\n\n\n\n\n\nSome clusters that are clearly defined by higher number of genes and counts. These are either doublets or a larger celltype. And some clusters with low values on these metrics that are either low quality cells or a smaller celltype. You will have to explore these clusters in more detail to judge what you believe them to be."
  },
  {
    "objectID": "labs/scanpy/scanpy_04_clustering.html#section-4",
    "href": "labs/scanpy/scanpy_04_clustering.html#section-4",
    "title": " Clustering",
    "section": "5 Subclustering of T and NK-cells",
    "text": "5 Subclustering of T and NK-cells\nIt is common that the subtypes of cells within a cluster is not so well separated when you have a heterogeneous dataset. In such a case it could be a good idea to run subclustering of individual celltypes. The main reason for subclustering is that the variable genes and the first principal components in the full analysis are mainly driven by differences between celltypes, while with subclustering we may detect smaller differences between subtypes within celltypes.\nSo first, lets find out where our T-cell and NK-cell clusters are. We know that T-cells express CD3E, and the main subtypes are CD4 and CD8, while NK-cells express GNLY.\n\n# check with the lowest resolution\nfig, axs = plt.subplots(2, 3, figsize=(10,8),constrained_layout=True)\nsc.pl.umap(adata, color=\"leiden_0.4\", ax=axs[0,0], show=False, legend_loc = \"on data\")\nsc.pl.umap(adata, color=\"CD3E\", ax=axs[0,1], show=False)\nsc.pl.umap(adata, color=\"CD4\", ax=axs[0,2], show=False)\nsc.pl.umap(adata, color=\"CD8A\", ax=axs[1,0], show=False)\nsc.pl.umap(adata, color=\"GNLY\", ax=axs[1,1], show=False)\n\n\n\n\n\n\n\n\nWe can clearly see what clusters are T-cell clusters, so lets subset the data for those cells\n\ntcells = adata[adata.obs[\"leiden_0.4\"].isin(['1','2','4']),:]\ntcells.obs[\"sample\"].value_counts()\n\nctrl_5      684\nctrl_13     605\nctrl_19     548\nctrl_14     497\ncovid_17    278\ncovid_1     258\ncovid_15    243\ncovid_16    124\nName: sample, dtype: int64\n\n\nIdeally we should rerun all steps of integration with that subset of cells instead of just taking the joint embedding. If you have too few cells per sample in the celltype that you want to cluster it may not be possible. We will start with selecting a new set of genes that better reflecs the variability within this celltype\n\nsc.pp.highly_variable_genes(tcells, min_mean=0.0125, max_mean=3, min_disp=0.5)\n\n\nprint(\"Full data:\" , sum(adata.var.highly_variable ))\nprint(\"Tcells:\" , sum(tcells.var.highly_variable))\nprint(\"Intersection:\" , sum(tcells.var.highly_variable & adata.var.highly_variable))\n\nextracting highly variable genes\n    finished (0:00:00)\n--&gt; added\n    'highly_variable', boolean vector (adata.var)\n    'means', float vector (adata.var)\n    'dispersions', float vector (adata.var)\n    'dispersions_norm', float vector (adata.var)\nFull data: 2388\nTcells: 2893\nIntersection: 1167\n\n\nWe clearly have a very different geneset now, so hopefully it should better capture the variability within T-cells.\nNow we have to run the full pipeline with scaling, pca, integration and clustering on this subset of cells, using the new set of variable genes\n\nimport scanpy.external as sce \nimport harmonypy as hm \n\ntcells.raw = tcells\ntcells = tcells[:, tcells.var.highly_variable]\nsc.pp.regress_out(tcells, ['total_counts', 'pct_counts_mt'])\nsc.pp.scale(tcells, max_value=10)\nsc.tl.pca(tcells, svd_solver='arpack')\nsce.pp.harmony_integrate(tcells, 'sample')\nsc.pp.neighbors(tcells, n_neighbors=10, n_pcs=30, use_rep='X_pca_harmony')\nsc.tl.leiden(tcells, resolution = 0.6, key_added = \"tcells_0.6\")\nsc.tl.umap(tcells)\n\nregressing out ['total_counts', 'pct_counts_mt']\n    sparse input is densified and may lead to high memory use\n    finished (0:00:20)\ncomputing PCA\n    with n_comps=50\n    finished (0:00:01)\n\n\ncomputing neighbors\n    finished: added to `.uns['neighbors']`\n    `.obsp['distances']`, distances for each pair of neighbors\n    `.obsp['connectivities']`, weighted adjacency matrix (0:00:00)\nrunning Leiden clustering\n    finished: found 12 clusters and added\n    'tcells_0.6', the cluster labels (adata.obs, categorical) (0:00:00)\ncomputing UMAP\n    finished: added\n    'X_umap', UMAP coordinates (adata.obsm)\n    'umap', UMAP parameters (adata.uns) (0:00:04)\n\n\n\nfig, axs = plt.subplots(2, 3, figsize=(10,8),constrained_layout=True)\nsc.pl.umap(tcells, color=\"sample\", title=\"Tcell umap\", ax=axs[0,0], show=False)\nsc.pl.embedding(tcells, 'X_umap_harmony', color=\"sample\", title=\"Full umap\", ax=axs[1,0], show=False)\nsc.pl.umap(tcells, color=\"leiden_0.6\", title=\"Tcell umap, full clust\", ax=axs[0,1], show=False)\nsc.pl.embedding(tcells, 'X_umap_harmony', color=\"leiden_0.6\", title=\"Full umap, full clust\", ax=axs[1,1], show=False)\nsc.pl.umap(tcells, color=\"tcells_0.6\", title=\"Tcell umap, tcell clust\", ax=axs[0,2], show=False)\nsc.pl.embedding(tcells, 'X_umap_harmony', color=\"tcells_0.6\", title=\"Full umap, tcell clust\", ax=axs[1,2], show=False)\n\n\n\n\n\n\n\n\nAs you can see, we do have some new clusters that did not stand out before. But in general the separation looks very similar.\nWe can plot the subtype genes again. If you try plotting the genes with use_raw=False you will notice that some of the genes are not in the adata.X matrix. Since they are no longer included in the variable genes. So now we have to plot with use_raw=True.\n\nfig, axs = plt.subplots(2, 2, figsize=(10,8),constrained_layout=True)\nsc.pl.umap(tcells, color=\"CD3E\", ax=axs[0,0], show=False, use_raw=True)\nsc.pl.umap(tcells, color=\"CD4\", ax=axs[0,1], show=False, use_raw=True)\nsc.pl.umap(tcells, color=\"CD8A\", ax=axs[1,0], show=False, use_raw=True)\nsc.pl.umap(tcells, color=\"GNLY\", ax=axs[1,1], show=False, use_raw=True)"
  },
  {
    "objectID": "labs/scanpy/scanpy_04_clustering.html#section-5",
    "href": "labs/scanpy/scanpy_04_clustering.html#section-5",
    "title": " Clustering",
    "section": "6 Session info",
    "text": "6 Session info\n\n\nClick here\n\n\nsc.logging.print_versions()\n\n-----\nanndata     0.10.8\nscanpy      1.10.3\n-----\nCoreFoundation              NA\nFoundation                  NA\nPIL                         10.4.0\nPyObjCTools                 NA\nanyio                       NA\nappnope                     0.1.4\narrow                       1.3.0\nasciitree                   NA\nasttokens                   NA\nattr                        24.2.0\nattrs                       24.2.0\nbabel                       2.14.0\nbrotli                      1.1.0\ncertifi                     2024.08.30\ncffi                        1.17.1\ncharset_normalizer          3.3.2\ncloudpickle                 3.0.0\ncolorama                    0.4.6\ncomm                        0.2.2\ncycler                      0.12.1\ncython_runtime              NA\ncytoolz                     0.12.3\ndask                        2024.9.0\ndateutil                    2.9.0\ndebugpy                     1.8.5\ndecorator                   5.1.1\ndefusedxml                  0.7.1\nexceptiongroup              1.2.2\nexecuting                   2.1.0\nfastjsonschema              NA\nfqdn                        NA\ngoogle                      NA\nh5py                        3.11.0\nharmonypy                   0.0.10\nidna                        3.10\nigraph                      0.11.6\nipykernel                   6.29.5\nisoduration                 NA\njedi                        0.19.1\njinja2                      3.1.4\njoblib                      1.4.2\njson5                       0.9.25\njsonpointer                 3.0.0\njsonschema                  4.23.0\njsonschema_specifications   NA\njupyter_events              0.10.0\njupyter_server              2.14.2\njupyterlab_server           2.27.3\nkiwisolver                  1.4.7\nlegacy_api_wrap             NA\nleidenalg                   0.10.2\nllvmlite                    0.43.0\nmarkupsafe                  2.1.5\nmatplotlib                  3.9.2\nmatplotlib_inline           0.1.7\nmpl_toolkits                NA\nmsgpack                     1.1.0\nnatsort                     8.4.0\nnbformat                    5.10.4\nnumba                       0.60.0\nnumcodecs                   0.13.0\nnumpy                       1.26.4\nobjc                        10.3.1\noverrides                   NA\npackaging                   24.1\npandas                      1.5.3\nparso                       0.8.4\npatsy                       0.5.6\npickleshare                 0.7.5\nplatformdirs                4.3.6\nprometheus_client           NA\nprompt_toolkit              3.0.47\npsutil                      6.0.0\npure_eval                   0.2.3\npycparser                   2.22\npydev_ipython               NA\npydevconsole                NA\npydevd                      2.9.5\npydevd_file_utils           NA\npydevd_plugins              NA\npydevd_tracing              NA\npygments                    2.18.0\npynndescent                 0.5.13\npyparsing                   3.1.4\npythonjsonlogger            NA\npytz                        2024.2\nreferencing                 NA\nrequests                    2.32.3\nrfc3339_validator           0.1.4\nrfc3986_validator           0.1.1\nrpds                        NA\nscipy                       1.14.1\nseaborn                     0.13.2\nsend2trash                  NA\nsession_info                1.0.0\nsix                         1.16.0\nsklearn                     1.5.2\nsniffio                     1.3.1\nsocks                       1.7.1\nsparse                      0.15.4\nsphinxcontrib               NA\nstack_data                  0.6.2\nstatsmodels                 0.14.3\ntexttable                   1.7.0\nthreadpoolctl               3.5.0\ntlz                         0.12.3\ntoolz                       0.12.1\ntorch                       2.4.0.post101\ntorchgen                    NA\ntornado                     6.4.1\ntqdm                        4.66.5\ntraitlets                   5.14.3\ntyping_extensions           NA\numap                        0.5.6\nuri_template                NA\nurllib3                     2.2.3\nwcwidth                     0.2.13\nwebcolors                   24.8.0\nwebsocket                   1.8.0\nyaml                        6.0.2\nzarr                        2.18.3\nzipp                        NA\nzmq                         26.2.0\nzoneinfo                    NA\nzstandard                   0.23.0\n-----\nIPython             8.27.0\njupyter_client      8.6.3\njupyter_core        5.7.2\njupyterlab          4.2.5\nnotebook            7.2.2\n-----\nPython 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:17:34) [Clang 14.0.6 ]\nmacOS-14.6.1-x86_64-i386-64bit\n-----\nSession information updated at 2024-09-24 09:01"
  },
  {
    "objectID": "labs/scanpy/scanpy_05_dge.html#section",
    "href": "labs/scanpy/scanpy_05_dge.html#section",
    "title": " Differential gene expression",
    "section": "8 DGE across conditions",
    "text": "8 DGE across conditions\nThe second way of computing differential expression is to answer which genes are differentially expressed within a cluster. For example, in our case we have libraries comming from patients and controls and we would like to know which genes are influenced the most in a particular cell type. For this end, we will first subset our data for the desired cell cluster, then change the cell identities to the variable of comparison (which now in our case is the type, e.g. Covid/Ctrl).\n\ncl1 = adata[adata.obs['leiden_0.6'] == '4',:]\ncl1.obs['type'].value_counts()\n\nsc.tl.rank_genes_groups(cl1, 'type', method='wilcoxon', key_added = \"wilcoxon\")\nsc.pl.rank_genes_groups(cl1, n_genes=25, sharey=False, key=\"wilcoxon\")\n\nranking genes\n    finished (0:00:00)\n\n\n\n\n\n\n\n\n\n\nsc.pl.rank_genes_groups_violin(cl1, n_genes=10, key=\"wilcoxon\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also plot these genes across all clusters, but split by “type”, to check if the genes are also up/downregulated in other celltypes.\n\nimport seaborn as sns\n\ngenes1 = sc.get.rank_genes_groups_df(cl1, group='Covid', key='wilcoxon')['names'][:5]\ngenes2 = sc.get.rank_genes_groups_df(cl1, group='Ctrl', key='wilcoxon')['names'][:5]\ngenes = genes1.tolist() +  genes2.tolist() \ndf = sc.get.obs_df(adata, genes + ['leiden_0.6','type'], use_raw=False)\ndf2 = df.melt(id_vars=[\"leiden_0.6\",'type'], value_vars=genes)\n\nsns.catplot(x = \"leiden_0.6\", y = \"value\", hue = \"type\", kind = 'violin', col = \"variable\", data = df2, col_wrap=4, inner=None)\n\n\n\n\n\n\n\n\nAs you can see, we have many sex chromosome related genes among the top DE genes. And if you remember from the QC lab, we have inbalanced sex distribution among our subjects, so this is probably not related to covid at all.\n\n8.1 Remove sex chromosome genes\nTo remove some of the bias due to inbalanced sex in the subjects we can remove the sex chromosome related genes.\n\nannot = sc.queries.biomart_annotations(\n        \"hsapiens\",\n        [\"ensembl_gene_id\", \"external_gene_name\", \"start_position\", \"end_position\", \"chromosome_name\"],\n    ).set_index(\"external_gene_name\")\n\nchrY_genes = adata.var_names.intersection(annot.index[annot.chromosome_name == \"Y\"])\nchrX_genes = adata.var_names.intersection(annot.index[annot.chromosome_name == \"X\"])\n\nsex_genes = chrY_genes.union(chrX_genes)\nprint(len(sex_genes))\nall_genes = cl1.var.index.tolist()\nprint(len(all_genes))\n\nkeep_genes = [x for x in all_genes if x not in sex_genes]\nprint(len(keep_genes))\n\ncl1 = cl1[:,keep_genes]\n\n550\n19468\n18918\n\n\nRerun differential expression.\n\nsc.tl.rank_genes_groups(cl1, 'type', method='wilcoxon', key_added = \"wilcoxon\")\nsc.pl.rank_genes_groups(cl1, n_genes=25, sharey=False, key=\"wilcoxon\")\n\nranking genes\n    finished (0:00:00)\n\n\n\n\n\n\n\n\n\nNow at least we do not have the sex chromosome genes as DE but still, some of the differences between patient and control could still be related to sex.\n\n\n8.2 Patient batch effects\nWhen we are testing for Covid vs Control we are running a DGE test for 4 vs 4 individuals. That will be very sensitive to sample differences unless we find a way to control for it. So first, lets check how the top DGEs are expressed in that cluster, across the individuals:\n\ngenes1 = sc.get.rank_genes_groups_df(cl1, group='Covid', key='wilcoxon')['names'][:5]\ngenes2 = sc.get.rank_genes_groups_df(cl1, group='Ctrl', key='wilcoxon')['names'][:5]\ngenes = genes1.tolist() +  genes2.tolist() \n\nsc.pl.violin(cl1, genes1, groupby='sample', rotation=45)\nsc.pl.violin(cl1, genes2, groupby='sample', rotation=45)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, many of the genes detected as DGE in Covid are unique to one or 2 patients.\nWe can also plot the top Covid and top Ctrl genes as a dotplot:\n\ngenes1 = sc.get.rank_genes_groups_df(cl1, group='Covid', key='wilcoxon')['names'][:20]\ngenes2 = sc.get.rank_genes_groups_df(cl1, group='Ctrl', key='wilcoxon')['names'][:20]\ngenes = genes1.tolist() +  genes2.tolist() \n\nsc.pl.dotplot(cl1,genes, groupby='sample')\n\n\n\n\n\n\n\n\nClearly many of the top Covid genes are only high in the covid_17 sample, and not a general feature of covid patients.\nThis is also the patient with the highest number of cells in this cluster:\n\ncl1.obs['sample'].value_counts()\n\ncovid_17    178\nctrl_5      150\ncovid_1      92\nctrl_13      63\nctrl_14      63\nctrl_19      46\ncovid_16     38\ncovid_15     32\nName: sample, dtype: int64\n\n\n\n\n8.3 Subsample\nSo one obvious thing to consider is an equal amount of cells per individual so that the DGE results are not dominated by a single sample.\nSo we will downsample to an equal number of cells per sample, in this case 34 cells per sample as it is the lowest number among all samples\n\ntarget_cells = 37\n\ntmp = [cl1[cl1.obs['sample'] == s] for s in cl1.obs['sample'].cat.categories]\n\nfor dat in tmp:\n    if dat.n_obs &gt; target_cells:\n            sc.pp.subsample(dat, n_obs=target_cells)\n\ncl1_sub = tmp[0].concatenate(*tmp[1:])\n\ncl1_sub.obs['sample'].value_counts()\n\ncovid_1     37\ncovid_16    37\ncovid_17    37\nctrl_5      37\nctrl_13     37\nctrl_14     37\nctrl_19     37\ncovid_15    32\nName: sample, dtype: int64\n\n\n\nsc.tl.rank_genes_groups(cl1_sub, 'type', method='wilcoxon', key_added = \"wilcoxon\")\nsc.pl.rank_genes_groups(cl1_sub, n_genes=25, sharey=False, key=\"wilcoxon\")\n\nranking genes\n    finished (0:00:00)\n\n\n\n\n\n\n\n\n\n\ngenes1 = sc.get.rank_genes_groups_df(cl1_sub, group='Covid', key='wilcoxon')['names'][:20]\ngenes2 = sc.get.rank_genes_groups_df(cl1_sub, group='Ctrl', key='wilcoxon')['names'][:20]\ngenes = genes1.tolist() +  genes2.tolist() \n\nsc.pl.dotplot(cl1,genes, groupby='sample')\n\n\n\n\n\n\n\n\nIt looks much better now. But if we look per subject you can see that we still have some genes that are dominated by a single patient. Still, it is often a good idea to control the number of cells from each sample when doing differential expression.\nThere are many different ways to try and resolve the issue of patient batch effects, however most of them require R packages. These can be run via rpy2 as is demonstraded in this compendium: https://www.sc-best-practices.org/conditions/differential_gene_expression.html\nHowever, we have not included it here as of now. So please have a look at the patient batch effect section in the seurat DGE tutorial where we run EdgeR on pseudobulk and MAST with random effect."
  },
  {
    "objectID": "labs/scanpy/scanpy_05_dge.html#section-1",
    "href": "labs/scanpy/scanpy_05_dge.html#section-1",
    "title": " Differential gene expression",
    "section": "9 Gene Set Analysis (GSA)",
    "text": "9 Gene Set Analysis (GSA)\n\n9.1 Hypergeometric enrichment test\nHaving a defined list of differentially expressed genes, you can now look for their combined function using hypergeometric test.\n\n#Available databases : ‘Human’, ‘Mouse’, ‘Yeast’, ‘Fly’, ‘Fish’, ‘Worm’ \ngene_set_names = gseapy.get_library_name(organism='Human')\nprint(gene_set_names)\n\n['ARCHS4_Cell-lines', 'ARCHS4_IDG_Coexp', 'ARCHS4_Kinases_Coexp', 'ARCHS4_TFs_Coexp', 'ARCHS4_Tissues', 'Achilles_fitness_decrease', 'Achilles_fitness_increase', 'Aging_Perturbations_from_GEO_down', 'Aging_Perturbations_from_GEO_up', 'Allen_Brain_Atlas_10x_scRNA_2021', 'Allen_Brain_Atlas_down', 'Allen_Brain_Atlas_up', 'Azimuth_2023', 'Azimuth_Cell_Types_2021', 'BioCarta_2013', 'BioCarta_2015', 'BioCarta_2016', 'BioPlanet_2019', 'BioPlex_2017', 'CCLE_Proteomics_2020', 'CORUM', 'COVID-19_Related_Gene_Sets', 'COVID-19_Related_Gene_Sets_2021', 'Cancer_Cell_Line_Encyclopedia', 'CellMarker_2024', 'CellMarker_Augmented_2021', 'ChEA_2013', 'ChEA_2015', 'ChEA_2016', 'ChEA_2022', 'Chromosome_Location', 'Chromosome_Location_hg19', 'ClinVar_2019', 'DGIdb_Drug_Targets_2024', 'DSigDB', 'Data_Acquisition_Method_Most_Popular_Genes', 'DepMap_CRISPR_GeneDependency_CellLines_2023', 'DepMap_WG_CRISPR_Screens_Broad_CellLines_2019', 'DepMap_WG_CRISPR_Screens_Sanger_CellLines_2019', 'Descartes_Cell_Types_and_Tissue_2021', 'Diabetes_Perturbations_GEO_2022', 'DisGeNET', 'Disease_Perturbations_from_GEO_down', 'Disease_Perturbations_from_GEO_up', 'Disease_Signatures_from_GEO_down_2014', 'Disease_Signatures_from_GEO_up_2014', 'DrugMatrix', 'Drug_Perturbations_from_GEO_2014', 'Drug_Perturbations_from_GEO_down', 'Drug_Perturbations_from_GEO_up', 'ENCODE_Histone_Modifications_2013', 'ENCODE_Histone_Modifications_2015', 'ENCODE_TF_ChIP-seq_2014', 'ENCODE_TF_ChIP-seq_2015', 'ENCODE_and_ChEA_Consensus_TFs_from_ChIP-X', 'ESCAPE', 'Elsevier_Pathway_Collection', 'Enrichr_Libraries_Most_Popular_Genes', 'Enrichr_Submissions_TF-Gene_Coocurrence', 'Enrichr_Users_Contributed_Lists_2020', 'Epigenomics_Roadmap_HM_ChIP-seq', 'FANTOM6_lncRNA_KD_DEGs', 'GO_Biological_Process_2013', 'GO_Biological_Process_2015', 'GO_Biological_Process_2017', 'GO_Biological_Process_2017b', 'GO_Biological_Process_2018', 'GO_Biological_Process_2021', 'GO_Biological_Process_2023', 'GO_Cellular_Component_2013', 'GO_Cellular_Component_2015', 'GO_Cellular_Component_2017', 'GO_Cellular_Component_2017b', 'GO_Cellular_Component_2018', 'GO_Cellular_Component_2021', 'GO_Cellular_Component_2023', 'GO_Molecular_Function_2013', 'GO_Molecular_Function_2015', 'GO_Molecular_Function_2017', 'GO_Molecular_Function_2017b', 'GO_Molecular_Function_2018', 'GO_Molecular_Function_2021', 'GO_Molecular_Function_2023', 'GTEx_Aging_Signatures_2021', 'GTEx_Tissue_Expression_Down', 'GTEx_Tissue_Expression_Up', 'GTEx_Tissues_V8_2023', 'GWAS_Catalog_2019', 'GWAS_Catalog_2023', 'GeDiPNet_2023', 'GeneSigDB', 'Gene_Perturbations_from_GEO_down', 'Gene_Perturbations_from_GEO_up', 'Genes_Associated_with_NIH_Grants', 'Genome_Browser_PWMs', 'GlyGen_Glycosylated_Proteins_2022', 'HDSigDB_Human_2021', 'HDSigDB_Mouse_2021', 'HMDB_Metabolites', 'HMS_LINCS_KinomeScan', 'HomoloGene', 'HuBMAP_ASCT_plus_B_augmented_w_RNAseq_Coexpression', 'HuBMAP_ASCTplusB_augmented_2022', 'HumanCyc_2015', 'HumanCyc_2016', 'Human_Gene_Atlas', 'Human_Phenotype_Ontology', 'IDG_Drug_Targets_2022', 'InterPro_Domains_2019', 'Jensen_COMPARTMENTS', 'Jensen_DISEASES', 'Jensen_TISSUES', 'KEA_2013', 'KEA_2015', 'KEGG_2013', 'KEGG_2015', 'KEGG_2016', 'KEGG_2019_Human', 'KEGG_2019_Mouse', 'KEGG_2021_Human', 'KOMP2_Mouse_Phenotypes_2022', 'Kinase_Perturbations_from_GEO_down', 'Kinase_Perturbations_from_GEO_up', 'L1000_Kinase_and_GPCR_Perturbations_down', 'L1000_Kinase_and_GPCR_Perturbations_up', 'LINCS_L1000_CRISPR_KO_Consensus_Sigs', 'LINCS_L1000_Chem_Pert_Consensus_Sigs', 'LINCS_L1000_Chem_Pert_down', 'LINCS_L1000_Chem_Pert_up', 'LINCS_L1000_Ligand_Perturbations_down', 'LINCS_L1000_Ligand_Perturbations_up', 'Ligand_Perturbations_from_GEO_down', 'Ligand_Perturbations_from_GEO_up', 'MAGMA_Drugs_and_Diseases', 'MAGNET_2023', 'MCF7_Perturbations_from_GEO_down', 'MCF7_Perturbations_from_GEO_up', 'MGI_Mammalian_Phenotype_2013', 'MGI_Mammalian_Phenotype_2017', 'MGI_Mammalian_Phenotype_Level_3', 'MGI_Mammalian_Phenotype_Level_4', 'MGI_Mammalian_Phenotype_Level_4_2019', 'MGI_Mammalian_Phenotype_Level_4_2021', 'MGI_Mammalian_Phenotype_Level_4_2024', 'MSigDB_Computational', 'MSigDB_Hallmark_2020', 'MSigDB_Oncogenic_Signatures', 'Metabolomics_Workbench_Metabolites_2022', 'Microbe_Perturbations_from_GEO_down', 'Microbe_Perturbations_from_GEO_up', 'MoTrPAC_2023', 'Mouse_Gene_Atlas', 'NCI-60_Cancer_Cell_Lines', 'NCI-Nature_2016', 'NIH_Funded_PIs_2017_AutoRIF_ARCHS4_Predictions', 'NIH_Funded_PIs_2017_GeneRIF_ARCHS4_Predictions', 'NIH_Funded_PIs_2017_Human_AutoRIF', 'NIH_Funded_PIs_2017_Human_GeneRIF', 'NURSA_Human_Endogenous_Complexome', 'OMIM_Disease', 'OMIM_Expanded', 'Old_CMAP_down', 'Old_CMAP_up', 'Orphanet_Augmented_2021', 'PFOCR_Pathways', 'PFOCR_Pathways_2023', 'PPI_Hub_Proteins', 'PanglaoDB_Augmented_2021', 'Panther_2015', 'Panther_2016', 'Pfam_Domains_2019', 'Pfam_InterPro_Domains', 'PheWeb_2019', 'PhenGenI_Association_2021', 'Phosphatase_Substrates_from_DEPOD', 'ProteomicsDB_2020', 'Proteomics_Drug_Atlas_2023', 'RNA-Seq_Disease_Gene_and_Drug_Signatures_from_GEO', 'RNAseq_Automatic_GEO_Signatures_Human_Down', 'RNAseq_Automatic_GEO_Signatures_Human_Up', 'RNAseq_Automatic_GEO_Signatures_Mouse_Down', 'RNAseq_Automatic_GEO_Signatures_Mouse_Up', 'Rare_Diseases_AutoRIF_ARCHS4_Predictions', 'Rare_Diseases_AutoRIF_Gene_Lists', 'Rare_Diseases_GeneRIF_ARCHS4_Predictions', 'Rare_Diseases_GeneRIF_Gene_Lists', 'Reactome_2013', 'Reactome_2015', 'Reactome_2016', 'Reactome_2022', 'Rummagene_kinases', 'Rummagene_signatures', 'Rummagene_transcription_factors', 'SILAC_Phosphoproteomics', 'SubCell_BarCode', 'SynGO_2022', 'SynGO_2024', 'SysMyo_Muscle_Gene_Sets', 'TF-LOF_Expression_from_GEO', 'TF_Perturbations_Followed_by_Expression', 'TG_GATES_2020', 'TRANSFAC_and_JASPAR_PWMs', 'TRRUST_Transcription_Factors_2019', 'Table_Mining_of_CRISPR_Studies', 'Tabula_Muris', 'Tabula_Sapiens', 'TargetScan_microRNA', 'TargetScan_microRNA_2017', 'The_Kinase_Library_2023', 'Tissue_Protein_Expression_from_Human_Proteome_Map', 'Tissue_Protein_Expression_from_ProteomicsDB', 'Transcription_Factor_PPIs', 'UK_Biobank_GWAS_v1', 'Virus-Host_PPI_P-HIPSTer_2020', 'VirusMINT', 'Virus_Perturbations_from_GEO_down', 'Virus_Perturbations_from_GEO_up', 'WikiPathway_2021_Human', 'WikiPathway_2023_Human', 'WikiPathways_2013', 'WikiPathways_2015', 'WikiPathways_2016', 'WikiPathways_2019_Human', 'WikiPathways_2019_Mouse', 'dbGaP', 'huMAP', 'lncHUB_lncRNA_Co-Expression', 'miRTarBase_2017']\n\n\nGet the significant DEGs for the Covid patients.\n\n#?gseapy.enrichr\nglist = sc.get.rank_genes_groups_df(cl1_sub, group='Covid', key='wilcoxon', log2fc_min=0.25, pval_cutoff=0.05)['names'].squeeze().str.strip().tolist()\nprint(len(glist))\n\n18\n\n\n\nenr_res = gseapy.enrichr(gene_list=glist, organism='Human', gene_sets='GO_Biological_Process_2018', cutoff = 0.5)\nenr_res.results.head()\n\n\n\n\n\n\n\n\nGene_set\nTerm\nOverlap\nP-value\nAdjusted P-value\nOld P-value\nOld Adjusted P-value\nOdds Ratio\nCombined Score\nGenes\n\n\n\n\n0\nGO_Biological_Process_2018\nSRP-dependent cotranslational protein targetin...\n4/89\n0.000001\n0.000119\n0\n0\n66.880672\n919.542482\nRPL31;RPLP1;RPL37A;RPS21\n\n\n1\nGO_Biological_Process_2018\ncotranslational protein targeting to membrane ...\n4/93\n0.000001\n0.000119\n0\n0\n63.861958\n866.761029\nRPL31;RPLP1;RPL37A;RPS21\n\n\n2\nGO_Biological_Process_2018\nprotein targeting to ER (GO:0045047)\n4/97\n0.000002\n0.000119\n0\n0\n61.102919\n818.992020\nRPL31;RPLP1;RPL37A;RPS21\n\n\n3\nGO_Biological_Process_2018\nviral gene expression (GO:0019080)\n4/110\n0.000002\n0.000119\n0\n0\n53.574124\n691.117842\nRPL31;RPLP1;RPL37A;RPS21\n\n\n4\nGO_Biological_Process_2018\nnuclear-transcribed mRNA catabolic process, no...\n4/112\n0.000003\n0.000119\n0\n0\n52.576720\n674.468361\nRPL31;RPLP1;RPL37A;RPS21\n\n\n\n\n\n\n\nSome databases of interest:\nGO_Biological_Process_2017bKEGG_2019_HumanKEGG_2019_MouseWikiPathways_2019_HumanWikiPathways_2019_Mouse\nYou visualize your results using a simple barplot, for example:\n\ngseapy.barplot(enr_res.res2d,title='GO_Biological_Process_2018')"
  },
  {
    "objectID": "labs/scanpy/scanpy_05_dge.html#section-3",
    "href": "labs/scanpy/scanpy_05_dge.html#section-3",
    "title": " Differential gene expression",
    "section": "10 Gene Set Enrichment Analysis (GSEA)",
    "text": "10 Gene Set Enrichment Analysis (GSEA)\nBesides the enrichment using hypergeometric test, we can also perform gene set enrichment analysis (GSEA), which scores ranked genes list (usually based on fold changes) and computes permutation test to check if a particular gene set is more present in the Up-regulated genes, among the DOWN_regulated genes or not differentially regulated.\nWe need a table with all DEGs and their log foldchanges. However, many lowly expressed genes will have high foldchanges and just contribue noise, so also filter for expression in enough cells.\n\ngene_rank = sc.get.rank_genes_groups_df(cl1_sub, group='Covid', key='wilcoxon')[['names','logfoldchanges']]\ngene_rank.sort_values(by=['logfoldchanges'], inplace=True, ascending=False)\n\n# calculate_qc_metrics will calculate number of cells per gene\nsc.pp.calculate_qc_metrics(cl1, percent_top=None, log1p=False, inplace=True)\n\n# filter for genes expressed in at least 30 cells.\ngene_rank = gene_rank[gene_rank['names'].isin(cl1.var_names[cl1.var.n_cells_by_counts&gt;30])]\n\ngene_rank\n\n\n\n\n\n\n\n\nnames\nlogfoldchanges\n\n\n\n\n246\nPF4\n27.179865\n\n\n429\nCXCL8\n26.754713\n\n\n526\nG0S2\n26.559198\n\n\n525\nCCL3\n26.557562\n\n\n956\nIFIT1\n25.773088\n\n\n...\n...\n...\n\n\n18711\nOSBPL10-AS1\n-2.679634\n\n\n18542\nSERTAD3\n-2.968311\n\n\n18399\nZNF266\n-3.012577\n\n\n17570\nFAM217B\n-3.357616\n\n\n18332\nZNF385A\n-26.434734\n\n\n\n\n6991 rows × 2 columns\n\n\n\nOnce our list of genes are sorted, we can proceed with the enrichment itself. We can use the package to get gene set from the Molecular Signature Database (MSigDB) and select KEGG pathways as an example.\n\n#Available databases : ‘Human’, ‘Mouse’, ‘Yeast’, ‘Fly’, ‘Fish’, ‘Worm’ \ngene_set_names = gseapy.get_library_name(organism='Human')\nprint(gene_set_names)\n\n['ARCHS4_Cell-lines', 'ARCHS4_IDG_Coexp', 'ARCHS4_Kinases_Coexp', 'ARCHS4_TFs_Coexp', 'ARCHS4_Tissues', 'Achilles_fitness_decrease', 'Achilles_fitness_increase', 'Aging_Perturbations_from_GEO_down', 'Aging_Perturbations_from_GEO_up', 'Allen_Brain_Atlas_10x_scRNA_2021', 'Allen_Brain_Atlas_down', 'Allen_Brain_Atlas_up', 'Azimuth_2023', 'Azimuth_Cell_Types_2021', 'BioCarta_2013', 'BioCarta_2015', 'BioCarta_2016', 'BioPlanet_2019', 'BioPlex_2017', 'CCLE_Proteomics_2020', 'CORUM', 'COVID-19_Related_Gene_Sets', 'COVID-19_Related_Gene_Sets_2021', 'Cancer_Cell_Line_Encyclopedia', 'CellMarker_2024', 'CellMarker_Augmented_2021', 'ChEA_2013', 'ChEA_2015', 'ChEA_2016', 'ChEA_2022', 'Chromosome_Location', 'Chromosome_Location_hg19', 'ClinVar_2019', 'DGIdb_Drug_Targets_2024', 'DSigDB', 'Data_Acquisition_Method_Most_Popular_Genes', 'DepMap_CRISPR_GeneDependency_CellLines_2023', 'DepMap_WG_CRISPR_Screens_Broad_CellLines_2019', 'DepMap_WG_CRISPR_Screens_Sanger_CellLines_2019', 'Descartes_Cell_Types_and_Tissue_2021', 'Diabetes_Perturbations_GEO_2022', 'DisGeNET', 'Disease_Perturbations_from_GEO_down', 'Disease_Perturbations_from_GEO_up', 'Disease_Signatures_from_GEO_down_2014', 'Disease_Signatures_from_GEO_up_2014', 'DrugMatrix', 'Drug_Perturbations_from_GEO_2014', 'Drug_Perturbations_from_GEO_down', 'Drug_Perturbations_from_GEO_up', 'ENCODE_Histone_Modifications_2013', 'ENCODE_Histone_Modifications_2015', 'ENCODE_TF_ChIP-seq_2014', 'ENCODE_TF_ChIP-seq_2015', 'ENCODE_and_ChEA_Consensus_TFs_from_ChIP-X', 'ESCAPE', 'Elsevier_Pathway_Collection', 'Enrichr_Libraries_Most_Popular_Genes', 'Enrichr_Submissions_TF-Gene_Coocurrence', 'Enrichr_Users_Contributed_Lists_2020', 'Epigenomics_Roadmap_HM_ChIP-seq', 'FANTOM6_lncRNA_KD_DEGs', 'GO_Biological_Process_2013', 'GO_Biological_Process_2015', 'GO_Biological_Process_2017', 'GO_Biological_Process_2017b', 'GO_Biological_Process_2018', 'GO_Biological_Process_2021', 'GO_Biological_Process_2023', 'GO_Cellular_Component_2013', 'GO_Cellular_Component_2015', 'GO_Cellular_Component_2017', 'GO_Cellular_Component_2017b', 'GO_Cellular_Component_2018', 'GO_Cellular_Component_2021', 'GO_Cellular_Component_2023', 'GO_Molecular_Function_2013', 'GO_Molecular_Function_2015', 'GO_Molecular_Function_2017', 'GO_Molecular_Function_2017b', 'GO_Molecular_Function_2018', 'GO_Molecular_Function_2021', 'GO_Molecular_Function_2023', 'GTEx_Aging_Signatures_2021', 'GTEx_Tissue_Expression_Down', 'GTEx_Tissue_Expression_Up', 'GTEx_Tissues_V8_2023', 'GWAS_Catalog_2019', 'GWAS_Catalog_2023', 'GeDiPNet_2023', 'GeneSigDB', 'Gene_Perturbations_from_GEO_down', 'Gene_Perturbations_from_GEO_up', 'Genes_Associated_with_NIH_Grants', 'Genome_Browser_PWMs', 'GlyGen_Glycosylated_Proteins_2022', 'HDSigDB_Human_2021', 'HDSigDB_Mouse_2021', 'HMDB_Metabolites', 'HMS_LINCS_KinomeScan', 'HomoloGene', 'HuBMAP_ASCT_plus_B_augmented_w_RNAseq_Coexpression', 'HuBMAP_ASCTplusB_augmented_2022', 'HumanCyc_2015', 'HumanCyc_2016', 'Human_Gene_Atlas', 'Human_Phenotype_Ontology', 'IDG_Drug_Targets_2022', 'InterPro_Domains_2019', 'Jensen_COMPARTMENTS', 'Jensen_DISEASES', 'Jensen_TISSUES', 'KEA_2013', 'KEA_2015', 'KEGG_2013', 'KEGG_2015', 'KEGG_2016', 'KEGG_2019_Human', 'KEGG_2019_Mouse', 'KEGG_2021_Human', 'KOMP2_Mouse_Phenotypes_2022', 'Kinase_Perturbations_from_GEO_down', 'Kinase_Perturbations_from_GEO_up', 'L1000_Kinase_and_GPCR_Perturbations_down', 'L1000_Kinase_and_GPCR_Perturbations_up', 'LINCS_L1000_CRISPR_KO_Consensus_Sigs', 'LINCS_L1000_Chem_Pert_Consensus_Sigs', 'LINCS_L1000_Chem_Pert_down', 'LINCS_L1000_Chem_Pert_up', 'LINCS_L1000_Ligand_Perturbations_down', 'LINCS_L1000_Ligand_Perturbations_up', 'Ligand_Perturbations_from_GEO_down', 'Ligand_Perturbations_from_GEO_up', 'MAGMA_Drugs_and_Diseases', 'MAGNET_2023', 'MCF7_Perturbations_from_GEO_down', 'MCF7_Perturbations_from_GEO_up', 'MGI_Mammalian_Phenotype_2013', 'MGI_Mammalian_Phenotype_2017', 'MGI_Mammalian_Phenotype_Level_3', 'MGI_Mammalian_Phenotype_Level_4', 'MGI_Mammalian_Phenotype_Level_4_2019', 'MGI_Mammalian_Phenotype_Level_4_2021', 'MGI_Mammalian_Phenotype_Level_4_2024', 'MSigDB_Computational', 'MSigDB_Hallmark_2020', 'MSigDB_Oncogenic_Signatures', 'Metabolomics_Workbench_Metabolites_2022', 'Microbe_Perturbations_from_GEO_down', 'Microbe_Perturbations_from_GEO_up', 'MoTrPAC_2023', 'Mouse_Gene_Atlas', 'NCI-60_Cancer_Cell_Lines', 'NCI-Nature_2016', 'NIH_Funded_PIs_2017_AutoRIF_ARCHS4_Predictions', 'NIH_Funded_PIs_2017_GeneRIF_ARCHS4_Predictions', 'NIH_Funded_PIs_2017_Human_AutoRIF', 'NIH_Funded_PIs_2017_Human_GeneRIF', 'NURSA_Human_Endogenous_Complexome', 'OMIM_Disease', 'OMIM_Expanded', 'Old_CMAP_down', 'Old_CMAP_up', 'Orphanet_Augmented_2021', 'PFOCR_Pathways', 'PFOCR_Pathways_2023', 'PPI_Hub_Proteins', 'PanglaoDB_Augmented_2021', 'Panther_2015', 'Panther_2016', 'Pfam_Domains_2019', 'Pfam_InterPro_Domains', 'PheWeb_2019', 'PhenGenI_Association_2021', 'Phosphatase_Substrates_from_DEPOD', 'ProteomicsDB_2020', 'Proteomics_Drug_Atlas_2023', 'RNA-Seq_Disease_Gene_and_Drug_Signatures_from_GEO', 'RNAseq_Automatic_GEO_Signatures_Human_Down', 'RNAseq_Automatic_GEO_Signatures_Human_Up', 'RNAseq_Automatic_GEO_Signatures_Mouse_Down', 'RNAseq_Automatic_GEO_Signatures_Mouse_Up', 'Rare_Diseases_AutoRIF_ARCHS4_Predictions', 'Rare_Diseases_AutoRIF_Gene_Lists', 'Rare_Diseases_GeneRIF_ARCHS4_Predictions', 'Rare_Diseases_GeneRIF_Gene_Lists', 'Reactome_2013', 'Reactome_2015', 'Reactome_2016', 'Reactome_2022', 'Rummagene_kinases', 'Rummagene_signatures', 'Rummagene_transcription_factors', 'SILAC_Phosphoproteomics', 'SubCell_BarCode', 'SynGO_2022', 'SynGO_2024', 'SysMyo_Muscle_Gene_Sets', 'TF-LOF_Expression_from_GEO', 'TF_Perturbations_Followed_by_Expression', 'TG_GATES_2020', 'TRANSFAC_and_JASPAR_PWMs', 'TRRUST_Transcription_Factors_2019', 'Table_Mining_of_CRISPR_Studies', 'Tabula_Muris', 'Tabula_Sapiens', 'TargetScan_microRNA', 'TargetScan_microRNA_2017', 'The_Kinase_Library_2023', 'Tissue_Protein_Expression_from_Human_Proteome_Map', 'Tissue_Protein_Expression_from_ProteomicsDB', 'Transcription_Factor_PPIs', 'UK_Biobank_GWAS_v1', 'Virus-Host_PPI_P-HIPSTer_2020', 'VirusMINT', 'Virus_Perturbations_from_GEO_down', 'Virus_Perturbations_from_GEO_up', 'WikiPathway_2021_Human', 'WikiPathway_2023_Human', 'WikiPathways_2013', 'WikiPathways_2015', 'WikiPathways_2016', 'WikiPathways_2019_Human', 'WikiPathways_2019_Mouse', 'dbGaP', 'huMAP', 'lncHUB_lncRNA_Co-Expression', 'miRTarBase_2017']\n\n\nNext, we will run GSEA. This will result in a table containing information for several pathways. We can then sort and filter those pathways to visualize only the top ones. You can select/filter them by either p-value or normalized enrichment score (NES).\n\nres = gseapy.prerank(rnk=gene_rank, gene_sets='KEGG_2021_Human')\n\nterms = res.res2d.Term\nterms[:10]\n\n0               Cytokine-cytokine receptor interaction\n1                                 Rheumatoid arthritis\n2    Viral protein interaction with cytokine and cy...\n3                 Toll-like receptor signaling pathway\n4                          Chemokine signaling pathway\n5                RIG-I-like receptor signaling pathway\n6                                       Chagas disease\n7                                       Bladder cancer\n8                              IL-17 signaling pathway\n9    AGE-RAGE signaling pathway in diabetic complic...\nName: Term, dtype: object\n\n\n\ngseapy.gseaplot(rank_metric=res.ranking, term=terms[0], **res.results[terms[0]])\n\n[&lt;Axes: xlabel='Gene Rank', ylabel='Ranked metric'&gt;,\n &lt;Axes: &gt;,\n &lt;Axes: &gt;,\n &lt;Axes: ylabel='Enrichment Score'&gt;]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nWhich KEGG pathways are upregulated in this cluster? Which KEGG pathways are dowregulated in this cluster? Change the pathway source to another gene set (e.g. CP:WIKIPATHWAYS or CP:REACTOME or CP:BIOCARTA or GO:BP) and check the if you get similar results?\n\n\nFinally, let’s save the integrated data for further analysis.\n\nadata.write_h5ad('./data/covid/results/scanpy_covid_qc_dr_scanorama_cl_dge.h5ad')"
  },
  {
    "objectID": "labs/scanpy/scanpy_05_dge.html#section-4",
    "href": "labs/scanpy/scanpy_05_dge.html#section-4",
    "title": " Differential gene expression",
    "section": "11 Session info",
    "text": "11 Session info\n\n\nClick here\n\n\nsc.logging.print_versions()\n\n-----\nanndata     0.10.8\nscanpy      1.10.3\n-----\nCoreFoundation              NA\nFoundation                  NA\nPIL                         10.4.0\nPyObjCTools                 NA\nanyio                       NA\nappnope                     0.1.4\narrow                       1.3.0\nasciitree                   NA\nasttokens                   NA\nattr                        24.2.0\nattrs                       24.2.0\nbabel                       2.14.0\nbrotli                      1.1.0\ncertifi                     2024.08.30\ncffi                        1.17.1\ncharset_normalizer          3.3.2\ncloudpickle                 3.0.0\ncolorama                    0.4.6\ncomm                        0.2.2\ncycler                      0.12.1\ncython_runtime              NA\ncytoolz                     0.12.3\ndask                        2024.9.0\ndateutil                    2.9.0\ndebugpy                     1.8.5\ndecorator                   5.1.1\ndefusedxml                  0.7.1\nexceptiongroup              1.2.2\nexecuting                   2.1.0\nfastjsonschema              NA\nfqdn                        NA\nfuture                      1.0.0\ngoogle                      NA\ngseapy                      1.1.3\nh5py                        3.11.0\nidna                        3.10\nigraph                      0.11.6\nipykernel                   6.29.5\nisoduration                 NA\njedi                        0.19.1\njinja2                      3.1.4\njoblib                      1.4.2\njson5                       0.9.25\njsonpointer                 3.0.0\njsonschema                  4.23.0\njsonschema_specifications   NA\njupyter_events              0.10.0\njupyter_server              2.14.2\njupyterlab_server           2.27.3\nkiwisolver                  1.4.7\nlegacy_api_wrap             NA\nleidenalg                   0.10.2\nllvmlite                    0.43.0\nmarkupsafe                  2.1.5\nmatplotlib                  3.9.2\nmatplotlib_inline           0.1.7\nmatplotlib_venn             1.1.1\nmpl_toolkits                NA\nmsgpack                     1.1.0\nnatsort                     8.4.0\nnbformat                    5.10.4\nnumba                       0.60.0\nnumcodecs                   0.13.0\nnumpy                       1.26.4\nobjc                        10.3.1\noverrides                   NA\npackaging                   24.1\npandas                      1.5.3\nparso                       0.8.4\npatsy                       0.5.6\npickleshare                 0.7.5\nplatformdirs                4.3.6\nprometheus_client           NA\nprompt_toolkit              3.0.47\npsutil                      6.0.0\npure_eval                   0.2.3\npybiomart                   0.2.0\npycparser                   2.22\npydev_ipython               NA\npydevconsole                NA\npydevd                      2.9.5\npydevd_file_utils           NA\npydevd_plugins              NA\npydevd_tracing              NA\npygments                    2.18.0\npyparsing                   3.1.4\npythonjsonlogger            NA\npytz                        2024.2\nreferencing                 NA\nrequests                    2.32.3\nrequests_cache              0.4.13\nrfc3339_validator           0.1.4\nrfc3986_validator           0.1.1\nrpds                        NA\nscipy                       1.14.1\nseaborn                     0.13.2\nsend2trash                  NA\nsession_info                1.0.0\nsix                         1.16.0\nsklearn                     1.5.2\nsniffio                     1.3.1\nsocks                       1.7.1\nsparse                      0.15.4\nsphinxcontrib               NA\nstack_data                  0.6.2\nstatsmodels                 0.14.3\ntexttable                   1.7.0\nthreadpoolctl               3.5.0\ntlz                         0.12.3\ntoolz                       0.12.1\ntorch                       2.4.0.post101\ntorchgen                    NA\ntornado                     6.4.1\ntqdm                        4.66.5\ntraitlets                   5.14.3\ntyping_extensions           NA\nuri_template                NA\nurllib3                     2.2.3\nwcwidth                     0.2.13\nwebcolors                   24.8.0\nwebsocket                   1.8.0\nyaml                        6.0.2\nzarr                        2.18.3\nzipp                        NA\nzmq                         26.2.0\nzoneinfo                    NA\nzstandard                   0.23.0\n-----\nIPython             8.27.0\njupyter_client      8.6.3\njupyter_core        5.7.2\njupyterlab          4.2.5\nnotebook            7.2.2\n-----\nPython 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:17:34) [Clang 14.0.6 ]\nmacOS-14.6.1-x86_64-i386-64bit\n-----\nSession information updated at 2024-09-24 13:41"
  },
  {
    "objectID": "labs/scanpy/scanpy_06_celltyping.html#section",
    "href": "labs/scanpy/scanpy_06_celltyping.html#section",
    "title": " Celltype prediction",
    "section": "1 Reference data",
    "text": "1 Reference data\nLoad the reference data from scanpy.datasets. It is the annotated and processed pbmc3k dataset from 10x.\n\nadata_ref = sc.datasets.pbmc3k_processed() \n\nadata_ref.obs['sample']='pbmc3k'\n\nprint(adata_ref.shape)\nadata_ref.obs\n\n(2638, 1838)\n\n\n\n\n\n\n\n\n\nn_genes\npercent_mito\nn_counts\nlouvain\nsample\n\n\nindex\n\n\n\n\n\n\n\n\n\nAAACATACAACCAC-1\n781\n0.030178\n2419.0\nCD4 T cells\npbmc3k\n\n\nAAACATTGAGCTAC-1\n1352\n0.037936\n4903.0\nB cells\npbmc3k\n\n\nAAACATTGATCAGC-1\n1131\n0.008897\n3147.0\nCD4 T cells\npbmc3k\n\n\nAAACCGTGCTTCCG-1\n960\n0.017431\n2639.0\nCD14+ Monocytes\npbmc3k\n\n\nAAACCGTGTATGCG-1\n522\n0.012245\n980.0\nNK cells\npbmc3k\n\n\n...\n...\n...\n...\n...\n...\n\n\nTTTCGAACTCTCAT-1\n1155\n0.021104\n3459.0\nCD14+ Monocytes\npbmc3k\n\n\nTTTCTACTGAGGCA-1\n1227\n0.009294\n3443.0\nB cells\npbmc3k\n\n\nTTTCTACTTCCTCG-1\n622\n0.021971\n1684.0\nB cells\npbmc3k\n\n\nTTTGCATGAGAGGC-1\n454\n0.020548\n1022.0\nB cells\npbmc3k\n\n\nTTTGCATGCCTCAC-1\n724\n0.008065\n1984.0\nCD4 T cells\npbmc3k\n\n\n\n\n2638 rows × 5 columns\n\n\n\nAs you can see, the celltype annotation is in the metadata column louvain, so that is the column we will have to use for classification.\n\nsc.pl.umap(adata_ref, color='louvain')\n\n\n\n\n\n\n\n\nMake sure we have the same genes in both datset by taking the intersection\n\n# before filtering genes, store the full matrix in raw.\nadata.raw = adata\n# also store the umap in a new slot as it will get overwritten\nadata.obsm[\"X_umap_uncorr\"] = adata.obsm[\"X_umap\"]\n\nprint(adata_ref.shape[1])\nprint(adata.shape[1])\nvar_names = adata_ref.var_names.intersection(adata.var_names)\nprint(len(var_names))\n\nadata_ref = adata_ref[:, var_names]\nadata = adata[:, var_names]\n\n1838\n19468\n1676\n\n\nFirst we need to rerun pca and umap with the same gene set for both datasets.\n\nsc.pp.pca(adata_ref)\nsc.pp.neighbors(adata_ref)\nsc.tl.umap(adata_ref)\nsc.pl.umap(adata_ref, color='louvain')\n\ncomputing PCA\n    with n_comps=50\n    finished (0:00:02)\ncomputing neighbors\n    using 'X_pca' with n_pcs = 50\n    finished (0:00:06)\ncomputing UMAP\n    finished (0:00:04)\n\n\n\n\n\n\n\n\n\n\nsc.pp.pca(adata)\nsc.pp.neighbors(adata)\nsc.tl.umap(adata)\nsc.pl.umap(adata, color='leiden_0.6')\n\ncomputing PCA\n    with n_comps=50\n    finished (0:00:00)\ncomputing neighbors\n    using 'X_pca' with n_pcs = 50\n    finished (0:00:00)\ncomputing UMAP\n    finished (0:00:01)"
  },
  {
    "objectID": "labs/scanpy/scanpy_06_celltyping.html#celltypist",
    "href": "labs/scanpy/scanpy_06_celltyping.html#celltypist",
    "title": " Celltype prediction",
    "section": "4 Celltypist",
    "text": "4 Celltypist\nCelltypist provides pretrained models for classification for many different human tissues and celltypes. Here, we are following the steps of this tutorial, with some adaptations for this dataset. So please check out the tutorial for more detail.\n\nimport celltypist\nfrom celltypist import models\n\n# there are many different models, we will only download 2 of them for now.\nmodels.download_models(force_update = False, model = 'Immune_All_Low.pkl')\nmodels.download_models(force_update = False, model = 'Immune_All_High.pkl')\n\nNow select the model you want to use and show the info:\n\nmodel = models.Model.load(model = 'Immune_All_High.pkl')\n\nmodel\n\nCellTypist model with 32 cell types and 6639 features\n    date: 2022-07-16 08:53:00.959521\n    details: immune populations combined from 20 tissues of 18 studies\n    source: https://doi.org/10.1126/science.abl5197\n    version: v2\n    cell types: B cells, B-cell lineage, ..., pDC precursor\n    features: A1BG, A2M, ..., ZYX\n\n\nTo infer celltype labels to our cells, we first need to convert back to the full matrix. OBS! For celltypist we want to have log1p normalised expression to 10,000 counts per cell. Which we already have in adata.raw.X, check by summing up the data, it should sum to 10K.\n\nadata = adata.raw.to_adata() \nadata.X.expm1().sum(axis = 1)[:10]\n\nmatrix([[10000.],\n        [10000.],\n        [10000.],\n        [10000.],\n        [10000.],\n        [10000.],\n        [10000.],\n        [10000.],\n        [10000.],\n        [10000.]])\n\n\n\npredictions = celltypist.annotate(adata, model = 'Immune_All_High.pkl', majority_voting = True)\n\npredictions.predicted_labels\n\nrunning Leiden clustering\n    finished (0:00:00)\n\n\n\n\n\n\n\n\n\npredicted_labels\nover_clustering\nmajority_voting\n\n\n\n\nAGGTCATGTGCGAACA-13-5\nT cells\n25\nT cells\n\n\nCCTATCGGTCCCTCAT-13-5\nILC\n19\nILC\n\n\nTCCTCCCTCGTTCATT-13-5\nILC\n7\nILC\n\n\nCAACCAATCATCTATC-13-5\nILC\n8\nILC\n\n\nTACGGTATCGGATTAC-13-5\nT cells\n18\nT cells\n\n\n...\n...\n...\n...\n\n\nTCCACCATCATAGCAC-13-5\nT cells\n0\nT cells\n\n\nGAGTTACAGTGAGTGC-13-5\nT cells\n5\nT cells\n\n\nATCATTCAGGCTCACC-13-5\nMonocytes\n14\nMonocytes\n\n\nAGCCACGCAACCCTAA-13-5\nT cells\n4\nT cells\n\n\nCTACCTGGTCAGGAGT-13-5\nILC\n19\nILC\n\n\n\n\n1121 rows × 3 columns\n\n\n\nThe first column predicted_labels is the predictions made for each individual cell, while majority_voting is done for local subclusters, the clustering identities are in column over_clustering.\nNow we convert the predictions to an anndata object.\n\nadata = predictions.to_adata()\n\nsc.pl.umap(adata, color = ['leiden_0.6', 'predicted_labels', 'majority_voting'], legend_loc = 'on data')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nRerun predictions with Celltypist, but use another model, for instance Immune_All_High.pkl, or any other model you find relevant, you can find a list of models here. How do the results differ for you?\n\n\n\n4.1 Celltypist custom model\nWe can also train our own model on any reference data that we want to use. In this case we will use the pbmc data in adata_ref to train a model.\nCelltypist requires the data to be in the format of log1p normalised expression to 10,000 counts per cell, we can check if that is the case for the object we have:\n\nadata_ref.raw.X.expm1().sum(axis = 1)[:10]\n\nmatrix([[2419.],\n        [4903.],\n        [3147.],\n        [2639.],\n        [ 980.],\n        [2163.],\n        [2175.],\n        [2260.],\n        [1275.],\n        [1103.]], dtype=float32)\n\n\nThese should all sum up to 10K, which is not the case, probably since some genes were removed after normalizing. Wo we will have to start from the raw counts of that dataset instead. Before we selected the data pbmc3k_processed, but now we will instead use pbmc3k.\n\nadata_ref2 = sc.datasets.pbmc3k() \nadata_ref2\n\nAnnData object with n_obs × n_vars = 2700 × 32738\n    var: 'gene_ids'\n\n\nThis data is not annotated, so we will have to match the indices from the filtered and processed object. And add in the metadata with annotations.\n\nadata_ref2 = adata_ref2[adata_ref.obs_names,:]\nadata_ref2.obs = adata_ref.obs\nadata_ref2\n\nAnnData object with n_obs × n_vars = 2638 × 32738\n    obs: 'n_genes', 'percent_mito', 'n_counts', 'louvain', 'sample'\n    var: 'gene_ids'\n\n\nNow we can normalize the matrix:\n\nsc.pp.normalize_total(adata_ref2, target_sum = 1e4)\nsc.pp.log1p(adata_ref2)\n\n# check the sums again\nadata_ref2.X.expm1().sum(axis = 1)[:10]\n\nnormalizing counts per cell\n    finished (0:00:00)\n\n\nmatrix([[10000.   ],\n        [ 9999.999],\n        [10000.   ],\n        [ 9999.998],\n        [ 9999.998],\n        [10000.   ],\n        [ 9999.999],\n        [10000.   ],\n        [10000.001],\n        [10000.   ]], dtype=float32)\n\n\nAnd finally train the model.\n\nnew_model = celltypist.train(adata_ref2, labels = 'louvain', n_jobs = 10, feature_selection = True)\n\nNow we can run predictions on our data\n\npredictions2 = celltypist.annotate(adata, model = new_model, majority_voting = True)\n\nrunning Leiden clustering\n    finished (0:00:00)\n\n\nInstead of converting the predictions to anndata we will just add another column in the adata.obs with these new predictions since the column names from the previous celltypist runs with clash.\n\nadata.obs[\"predicted_labels_ref\"] = predictions2.predicted_labels[\"predicted_labels\"]\nadata.obs[\"majority_voting_ref\"] = predictions2.predicted_labels[\"majority_voting\"]\n\n\nsc.pl.umap(adata, color = ['predicted_labels', 'majority_voting','predicted_labels_ref', 'majority_voting_ref'], legend_loc = 'on data', ncols=2)"
  },
  {
    "objectID": "labs/scanpy/scanpy_06_celltyping.html#section-1",
    "href": "labs/scanpy/scanpy_06_celltyping.html#section-1",
    "title": " Celltype prediction",
    "section": "7 Save data",
    "text": "7 Save data\nWe can finally save the object for use in future steps.\n\nadata.write_h5ad('data/covid/results/scanpy_covid_qc_dr_int_cl_ct-ctrl13.h5ad')"
  },
  {
    "objectID": "labs/bioc/bioc_04_clustering.html#meta-clust_sub",
    "href": "labs/bioc/bioc_04_clustering.html#meta-clust_sub",
    "title": " Clustering",
    "section": "5 Subclustering of T and NK-cells",
    "text": "5 Subclustering of T and NK-cells\nIt is common that the subtypes of cells within a cluster is not so well separated when you have a heterogeneous dataset. In such a case it could be a good idea to run subclustering of individual celltypes. The main reason for subclustering is that the variable genes and the first principal components in the full analysis are mainly driven by differences between celltypes, while with subclustering we may detect smaller differences between subtypes within celltypes.\nSo first, lets find out where our T-cell and NK-cell clusters are. We know that T-cells express CD3E, and the main subtypes are CD4 and CD8, while NK-cells express GNLY.\n\nwrap_plots(\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"leiden_k30\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"CD3E\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"CD4\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"CD8A\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_Harmony\", colour_by = \"GNLY\"),\n    ncol = 3\n)\n\n\n\n\n\n\n\n\nWe can clearly see what clusters are T-cell clusters, so lets subset the data for those cells\n\ntcells = sce[,sce$leiden_k30 %in% c(3,4)]\n\ntable(tcells$sample)\n\n\n  cov.1  cov.15  cov.16  cov.17 ctrl.13 ctrl.14 ctrl.19  ctrl.5 \n    221     154     109     245     604     432     498     632 \n\n\nIdeally we should rerun all steps of integration with that subset of cells instead of just taking the joint embedding. If you have too few cells per sample in the celltype that you want to cluster it may not be possible. We will start with selecting a new set of genes that better reflecs the variability within this celltype\n\nvar.out &lt;- modelGeneVar(tcells, block = tcells$sample)\nhvgs.tcell &lt;- getTopHVGs(var.out, n = 2000)\n\n# check overlap with the variable genes using all the data\nlength(intersect(metadata(sce)$hvgs, hvgs.tcell))\n\n[1] 915\n\n\nWe clearly have a very different geneset now, so hopefully it should better capture the variability within T-cells.\nNow we have to run the full pipeline with scaling, pca, integration and clustering on this subset of cells, using the new set of variable genes\n\ntcells = runPCA(tcells, exprs_values = \"logcounts\", ncomponents = 30, subset_row = hvgs.tcell, scale = TRUE)\n\n\nlibrary(harmony)\ntcells &lt;- RunHarmony(\n    tcells,\n    group.by.vars = \"sample\",\n    reduction.save = \"harmony\",\n    reduction = \"PCA\"\n)\n\n# Here we use all PCs computed from Harmony for UMAP calculation\ntcells &lt;- runUMAP(tcells, dimred = \"harmony\", n_dimred = 30, ncomponents = 2, name = \"UMAP_tcell\")\ntcells$leiden_tcell_k20 &lt;- clusterCells(tcells, use.dimred = \"harmony\", BLUSPARAM=SNNGraphParam(k=20, cluster.fun=\"leiden\",  cluster.args = list(resolution_parameter=0.3)))\n\n\nwrap_plots(\n    plotReducedDim(tcells, dimred = \"UMAP_on_Harmony\", colour_by = \"sample\") +ggtitle(\"Full umap\"),\n    plotReducedDim(tcells, dimred = \"UMAP_on_Harmony\", colour_by = \"leiden_k20\") +ggtitle(\"Full umap, full clust\"),\n    plotReducedDim(tcells, dimred = \"UMAP_on_Harmony\", colour_by = \"leiden_tcell_k20\") +ggtitle(\"Full umap, T-cell clust\"),\n    plotReducedDim(tcells, dimred = \"UMAP_tcell\", colour_by = \"sample\") +ggtitle(\"T-cell umap\"),\n    plotReducedDim(tcells, dimred = \"UMAP_tcell\", colour_by = \"leiden_k20\") +ggtitle(\"T-cell umap, full clust\"),\n    plotReducedDim(tcells, dimred = \"UMAP_tcell\", colour_by = \"leiden_tcell_k20\") +ggtitle(\"T-cell umap, T-cell clust\"),\n    ncol = 3\n)+ plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nAs you can see, we do have some new clusters that did not stand out before. But in general the separation looks very similar.\nLets also have a look at the same genes in the new umap:\n\nwrap_plots(\n    plotReducedDim(tcells, dimred = \"UMAP_tcell\", colour_by = \"CD3E\"),\n    plotReducedDim(tcells, dimred = \"UMAP_tcell\", colour_by = \"CD4\"),\n    plotReducedDim(tcells, dimred = \"UMAP_tcell\", colour_by = \"CD8A\"),\n    plotReducedDim(tcells, dimred = \"UMAP_tcell\", colour_by = \"GNLY\"),\n    ncol = 2\n)"
  },
  {
    "objectID": "labs/bioc/bioc_05_dge.html#meta-dge_numbers",
    "href": "labs/bioc/bioc_05_dge.html#meta-dge_numbers",
    "title": " Differential gene expression",
    "section": "1 Number of cells per cluster",
    "text": "1 Number of cells per cluster\nWhen doing differential expression of one cluster vs all it is a good idea to have equal aomunt of cells from each cluster. Otherwise, the largest clusters will have the highest effect on the vs all comparison.\nHowever, findMarker in Scran is implemented so that the tests are run in a pariwise manner, e.g. each cluster is tested agains all the others individually. Then a combined p-value is calculated across all the tests using combineMarkers. So for this method, one large cluster will not influence the results in the same way as FindMarkers in Seurat or rank_genes_groups in Scanpy."
  },
  {
    "objectID": "labs/bioc/bioc_06_celltyping.html#meta-ct_singler",
    "href": "labs/bioc/bioc_06_celltyping.html#meta-ct_singler",
    "title": " Celltype prediction",
    "section": "5 SinlgeR",
    "text": "5 SinlgeR\nSingleR is performs unbiased cell type recognition from single-cell RNA sequencing data, by leveraging reference transcriptomic datasets of pure cell types to infer the cell of origin of each single cell independently.\nThere are multiple datasets included in the celldex package that can be used for celltype prediction, here we will test two different ones, the DatabaseImmuneCellExpressionData and the HumanPrimaryCellAtlasData. In addition we will use the same reference dataset that we used for label transfer above but using SingleR instead.\n\n5.1 Immune cell reference\n\nimmune = celldex::DatabaseImmuneCellExpressionData()\nsingler.immune &lt;- SingleR(test = ctrl.sce, ref = immune, assay.type.test=1,\n    labels = immune$label.main)\n\nhead(singler.immune)\n\nDataFrame with 6 rows and 4 columns\n                                               scores        labels delta.next\n                                             &lt;matrix&gt;   &lt;character&gt;  &lt;numeric&gt;\nAGGTCATGTGCGAACA-13  0.0679680:0.0760411:0.186694:... T cells, CD4+  0.1375513\nCCTATCGGTCCCTCAT-13  0.0027079:0.0960641:0.386088:...      NK cells  0.1490740\nTCCTCCCTCGTTCATT-13  0.0361115:0.1067465:0.394579:...      NK cells  0.1220681\nCAACCAATCATCTATC-13  0.0342030:0.1345967:0.402377:...      NK cells  0.1513308\nTACGGTATCGGATTAC-13 -0.0131813:0.0717678:0.283882:...      NK cells  0.0620657\nAATAGAGAGTTCGGTT-13  0.0841091:0.1367749:0.273738:... T cells, CD4+  0.0660296\n                    pruned.labels\n                      &lt;character&gt;\nAGGTCATGTGCGAACA-13 T cells, CD4+\nCCTATCGGTCCCTCAT-13      NK cells\nTCCTCCCTCGTTCATT-13      NK cells\nCAACCAATCATCTATC-13      NK cells\nTACGGTATCGGATTAC-13      NK cells\nAATAGAGAGTTCGGTT-13 T cells, CD4+\n\n\n\n\n5.2 HPCA reference\n\nhpca &lt;- HumanPrimaryCellAtlasData()\nsingler.hpca &lt;- SingleR(test = ctrl.sce, ref = hpca, assay.type.test=1,\n    labels = hpca$label.main)\n\nhead(singler.hpca)\n\nDataFrame with 6 rows and 4 columns\n                                            scores      labels delta.next\n                                          &lt;matrix&gt; &lt;character&gt;  &lt;numeric&gt;\nAGGTCATGTGCGAACA-13 0.141378:0.310009:0.275987:...     T_cells  0.4918992\nCCTATCGGTCCCTCAT-13 0.145926:0.300045:0.277827:...     NK_cell  0.3241970\nTCCTCCCTCGTTCATT-13 0.132119:0.311754:0.274127:...     NK_cell  0.0640608\nCAACCAATCATCTATC-13 0.157184:0.302219:0.284496:...     NK_cell  0.2012408\nTACGGTATCGGATTAC-13 0.125120:0.283118:0.250322:...     T_cells  0.1545913\nAATAGAGAGTTCGGTT-13 0.191441:0.374422:0.329988:...     T_cells  0.5063484\n                    pruned.labels\n                      &lt;character&gt;\nAGGTCATGTGCGAACA-13       T_cells\nCCTATCGGTCCCTCAT-13       NK_cell\nTCCTCCCTCGTTCATT-13       NK_cell\nCAACCAATCATCTATC-13       NK_cell\nTACGGTATCGGATTAC-13       T_cells\nAATAGAGAGTTCGGTT-13       T_cells\n\n\n\n\n5.3 With own reference data\n\nsingler.ref &lt;- SingleR(test=ctrl.sce, ref=ref.sce, labels=ref.sce$cell_type, de.method=\"wilcox\")\nhead(singler.ref)\n\nDataFrame with 6 rows and 4 columns\n                                            scores      labels delta.next\n                                          &lt;matrix&gt; &lt;character&gt;  &lt;numeric&gt;\nAGGTCATGTGCGAACA-13 0.741719:0.840093:0.805977:...  CD4 T cell  0.0423204\nCCTATCGGTCCCTCAT-13 0.649491:0.736753:0.815987:...     NK cell  0.0451715\nTCCTCCCTCGTTCATT-13 0.669603:0.731356:0.823308:...     NK cell  0.0865526\nCAACCAATCATCTATC-13 0.649948:0.721639:0.801202:...  CD8 T cell  0.0740195\nTACGGTATCGGATTAC-13 0.708827:0.776244:0.808044:...  CD8 T cell  0.0905218\nAATAGAGAGTTCGGTT-13 0.729010:0.847462:0.816299:...  CD4 T cell  0.0409309\n                    pruned.labels\n                      &lt;character&gt;\nAGGTCATGTGCGAACA-13    CD4 T cell\nCCTATCGGTCCCTCAT-13       NK cell\nTCCTCCCTCGTTCATT-13       NK cell\nCAACCAATCATCTATC-13    CD8 T cell\nTACGGTATCGGATTAC-13    CD8 T cell\nAATAGAGAGTTCGGTT-13    CD4 T cell\n\n\nCompare results:\n\nctrl.sce$singler.immune = singler.immune$pruned.labels\nctrl.sce$singler.hpca = singler.hpca$pruned.labels\nctrl.sce$singler.ref = singler.ref$pruned.labels\n\nwrap_plots(\n    plotReducedDim(ctrl.sce, dimred = \"UMAP\", colour_by = \"singler.immune\"),\n    plotReducedDim(ctrl.sce, dimred = \"UMAP\", colour_by = \"singler.hpca\"),\n    plotReducedDim(ctrl.sce, dimred = \"UMAP\", colour_by = \"singler.ref\"),\n    ncol = 3\n)"
  },
  {
    "objectID": "labs/comparison/comparison_pipelines.html",
    "href": "labs/comparison/comparison_pipelines.html",
    "title": "Comparison of all pipelines",
    "section": "",
    "text": "With data in place, now we can start loading libraries we will use in this tutorial.\nsuppressPackageStartupMessages({\n    library(Seurat)\n    library(zellkonverter)\n    library(Matrix)\n    library(ggplot2)\n    library(patchwork)\n    library(scran)\n    library(ComplexHeatmap)\n})"
  },
  {
    "objectID": "labs/comparison/comparison_pipelines.html#filtering-overview",
    "href": "labs/comparison/comparison_pipelines.html#filtering-overview",
    "title": "Comparison of all pipelines",
    "section": "1 Filtering overview",
    "text": "1 Filtering overview\nIn all 3 pipelines we run the filtering of cells in the exact same way, the only differences between them are in:\n\nDoublet detection, DoubletFinder in Seurat, scDblFinder in Bioconductor, Scrublet in Scanpy.\nCell cycle predictions are done in a similar way for Seurat and Scanpy using module scores, while in Bioc we are using Cyclone.\n\nOBS! Zellkonverter installs conda env with basilisk! Takes a while to run first time!!\n\npath_results &lt;- \"data/covid/results\"\nif (!dir.exists(path_results)) dir.create(path_results, recursive = T)\n\npath_seurat = \"../seurat/data/covid/results/\"\npath_bioc = \"../bioc/data/covid/results/\"\npath_scanpy = \"../scanpy/data/covid/results/\"\n\n# fetch the files with qc and dimred for each \n\n# seurat\nsobj = readRDS(file.path(path_seurat,\"seurat_covid_qc_dr_int_cl.rds\"))\n\n# bioc\nsce = readRDS(file.path(path_bioc,\"bioc_covid_qc_dr_int_cl.rds\"))\nbioc = as.Seurat(sce)\n\n# scanpy\nscanpy.sce = readH5AD(file.path(path_scanpy, \"scanpy_covid_qc_dr_scanorama_cl.h5ad\"))\nscanpy = as.Seurat(scanpy.sce, counts = NULL, data = \"X\") # only have the var.genes data that is scaled.\n\n\nwrap_plots(\n    DimPlot(sobj, group.by = \"orig.ident\") + NoAxes() + ggtitle(\"Seurat\"),\n    DimPlot(bioc, group.by = \"sample\") + NoAxes() + ggtitle(\"Bioc\"),\n    DimPlot(scanpy, group.by = \"sample\", reduction = \"X_umap_uncorr\") + NoAxes() + ggtitle(\"Scanpy\"),\n    ncol = 3\n)\n\n\n\n\n\n\n\n\n\ncat(\"Seurat: \", dim(sobj),\"\\n\")\n\nSeurat:  18854 7134 \n\ncat(\"Bioc: \", dim(bioc),\"\\n\")\n\nBioc:  18854 6741 \n\ncat(\"Scanpy: \", dim(scanpy),\"\\n\")\n\nScanpy:  19468 7222 \n\n\nHighest number of cells filtered with Bioc. Lowest with scanpy.\nCell names are different in all 3, have to make a conversion to match them.\n\nSeurat has the sample name merged to cell name.\nScanpy has sample number after cell name.\nBioc has just cell name.\n\n\nmeta.seurat = sobj@meta.data\nmeta.scanpy = scanpy@meta.data\nmeta.bioc = bioc@meta.data\n\nmeta.bioc$cell = rownames(meta.bioc)\nmeta.scanpy$cell = sapply(rownames(meta.scanpy), function(x) substr(x,1,nchar(x)-2))\nmeta.seurat$cell = unlist(lapply(strsplit(rownames(meta.seurat),\"_\"), function(x) x[3]))\n\nVisualize overlap of cells using an UpSet plot in the ComplexHeatmap package.\n\nl = list(seurat = meta.seurat$cell, bioc = meta.bioc$cell, scanpy = meta.scanpy$cell)\n\ncmat = make_comb_mat(l)\nprint(cmat)\n\nA combination matrix with 3 sets and 7 combinations.\n  ranges of combination set size: c(10, 6540).\n  mode for the combination size: distinct.\n  sets are on rows.\n\nCombination sets are:\n  seurat bioc scanpy code size\n       x    x      x  111 6540\n       x    x         110   36\n       x           x  101  484\n            x      x  011  155\n       x              100   74\n            x         010   10\n                   x  001   43\n\nSets are:\n     set size\n  seurat 7134\n    bioc 6741\n  scanpy 7222\n\nUpSet(cmat)"
  },
  {
    "objectID": "labs/comparison/comparison_pipelines.html#doublet-predictions",
    "href": "labs/comparison/comparison_pipelines.html#doublet-predictions",
    "title": "Comparison of all pipelines",
    "section": "3 Doublet predictions",
    "text": "3 Doublet predictions\nIn all 3 pipelines we run the filtering of cells in the exact same way, but doublet detection is different.\n\nIn Seurat - DoubletFinder with predefined 4% doublets.\nIn Scanpy - Scrublet, done per batch. No predefined cutoff.\nIn Bioc - scDblFinder - default cutoffs, no batch separation.\n\n\ncat(\"Seurat: \", dim(sobj),\"\\n\")\n\nSeurat:  18854 7134 \n\ncat(\"Bioc: \", dim(bioc),\"\\n\")\n\nBioc:  18854 6741 \n\ncat(\"Scanpy: \", dim(scanpy),\"\\n\")\n\nScanpy:  19468 7222 \n\n\nHighest number of cells filtered with Bioc. Lowest with scanpy.\nCell names are different in all 3, have to make a conversion to match them.\n\nSeurat has the sample name merged to cell name.\nScanpy has sample number after cell name.\nBioc has just cell name.\n\n\nmeta.seurat = sobj@meta.data\nmeta.scanpy = scanpy@meta.data\nmeta.bioc = bioc@meta.data\n\nmeta.bioc$cell = rownames(meta.bioc)\nmeta.scanpy$cell = sapply(rownames(meta.scanpy), function(x) substr(x,1,nchar(x)-2))\nmeta.seurat$cell = unlist(lapply(strsplit(rownames(meta.seurat),\"_\"), function(x) x[3]))\n\nVisualize overlap of cells using an UpSet plot in the ComplexHeatmap package.\n\nl = list(seurat = meta.seurat$cell, bioc = meta.bioc$cell, scanpy = meta.scanpy$cell)\n\ncmat = make_comb_mat(l)\nprint(cmat)\n\nA combination matrix with 3 sets and 7 combinations.\n  ranges of combination set size: c(10, 6540).\n  mode for the combination size: distinct.\n  sets are on rows.\n\nCombination sets are:\n  seurat bioc scanpy code size\n       x    x      x  111 6540\n       x    x         110   36\n       x           x  101  484\n            x      x  011  155\n       x              100   74\n            x         010   10\n                   x  001   43\n\nSets are:\n     set size\n  seurat 7134\n    bioc 6741\n  scanpy 7222\n\nUpSet(cmat)\n\n\n\n\n\n\n\n\n\n3.1 Doublet scores\nCreate one dataset with the cells that are present in all samples. Also add in umap from all 3 pipelines.\n\nin.all = intersect(intersect(meta.scanpy$cell, meta.seurat$cell), meta.bioc$cell)\n\ntmp1 = meta.bioc[match(in.all, meta.bioc$cell),]\ncolnames(tmp1) = paste0(colnames(tmp1),\"_bioc\")\ntmp2 = meta.scanpy[match(in.all, meta.scanpy$cell),]\ncolnames(tmp2) = paste0(colnames(tmp2),\"_scpy\")\n\nall = sobj[,match(in.all, meta.seurat$cell)]\n\nmeta.all = cbind(all@meta.data, tmp1,tmp2)\nall@meta.data = meta.all\n\nReductions(all)\n\n [1] \"pca\"               \"umap\"              \"tsne\"             \n [4] \"UMAP10_on_PCA\"     \"UMAP_on_ScaleData\" \"integrated_cca\"   \n [7] \"umap_cca\"          \"tsne_cca\"          \"harmony\"          \n[10] \"umap_harmony\"      \"scanorama\"         \"scanoramaC\"       \n[13] \"umap_scanorama\"    \"umap_scanoramaC\"  \n\ntmp = bioc@reductions$UMAP_on_PCA@cell.embeddings[match(in.all, meta.bioc$cell),]\nrownames(tmp) = colnames(all)\nall[[\"umap_bioc\"]] = CreateDimReducObject(tmp, key = \"umapbioc_\", assay = \"RNA\")\ntmp = scanpy@reductions$X_umap_uncorr@cell.embeddings[match(in.all, meta.scanpy$cell),]\nrownames(tmp) = colnames(all)\nall[[\"umap_scpy\"]] = CreateDimReducObject(tmp, key = \"umapscpy_\", assay = \"RNA\")\n\nReductions(all)\n\n [1] \"pca\"               \"umap\"              \"tsne\"             \n [4] \"UMAP10_on_PCA\"     \"UMAP_on_ScaleData\" \"integrated_cca\"   \n [7] \"umap_cca\"          \"tsne_cca\"          \"harmony\"          \n[10] \"umap_harmony\"      \"scanorama\"         \"scanoramaC\"       \n[13] \"umap_scanorama\"    \"umap_scanoramaC\"   \"umap_bioc\"        \n[16] \"umap_scpy\"        \n\n\n\nwrap_plots(\n  FeatureScatter(all, \"pANN_0.25_0.09_297\",\"scDblFinder.score_bioc\"),\n  FeatureScatter(all, \"pANN_0.25_0.09_297\",\"doublet_scores_scpy\"),\n FeatureScatter(all, \"scDblFinder.score_bioc\",\"doublet_scores_scpy\"),\n  ncol = 3 \n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nHighest correlation for the doublet scores in Scrublet and scDblFinder, but still only 0.27 in correlation.\n\nwrap_plots(\n  FeaturePlot(all, reduction = \"umap\", features = \"pANN_0.25_0.09_297\") + NoAxes(),\n  FeaturePlot(all, reduction = \"umap\", features = \"scDblFinder.score_bioc\") + NoAxes(),\n  FeaturePlot(all, reduction = \"umap\", features = \"doublet_scores_scpy\") + NoAxes(),\n  FeaturePlot(all, reduction = \"umap_bioc\", features = \"pANN_0.25_0.09_297\") + NoAxes(),\n  FeaturePlot(all, reduction = \"umap_bioc\", features = \"scDblFinder.score_bioc\") + NoAxes(),\n  FeaturePlot(all, reduction = \"umap_bioc\", features = \"doublet_scores_scpy\") + NoAxes(),  \n  FeaturePlot(all, reduction = \"umap_scpy\", features = \"pANN_0.25_0.09_297\") + NoAxes(),\n  FeaturePlot(all, reduction = \"umap_scpy\", features = \"scDblFinder.score_bioc\") + NoAxes(),\n  FeaturePlot(all, reduction = \"umap_scpy\", features = \"doublet_scores_scpy\") + NoAxes(),  \n  ncol = 3\n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nIt seems like the DoubletFinder scores are mainly high in the monocyte populations, while it is more mixed in the other 2 methods."
  },
  {
    "objectID": "labs/comparison/comparison_pipelines.html#meta-session",
    "href": "labs/comparison/comparison_pipelines.html#meta-session",
    "title": "Comparison of all pipelines",
    "section": "10 Session info",
    "text": "10 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] grid      stats4    stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] ComplexHeatmap_2.18.0       scran_1.30.0               \n [3] scuttle_1.12.0              SingleCellExperiment_1.24.0\n [5] SummarizedExperiment_1.32.0 Biobase_2.62.0             \n [7] GenomicRanges_1.54.1        GenomeInfoDb_1.38.1        \n [9] IRanges_2.36.0              S4Vectors_0.40.2           \n[11] BiocGenerics_0.48.1         MatrixGenerics_1.14.0      \n[13] matrixStats_1.4.1           patchwork_1.2.0            \n[15] ggplot2_3.5.1               Matrix_1.6-5               \n[17] zellkonverter_1.12.1        Seurat_5.1.0               \n[19] SeuratObject_5.0.2          sp_2.1-4                   \n\nloaded via a namespace (and not attached):\n  [1] RcppAnnoy_0.0.22          splines_4.3.3            \n  [3] later_1.3.2               bitops_1.0-8             \n  [5] filelock_1.0.3            tibble_3.2.1             \n  [7] polyclip_1.10-7           basilisk.utils_1.14.1    \n  [9] fastDummies_1.7.4         lifecycle_1.0.4          \n [11] doParallel_1.0.17         edgeR_4.0.16             \n [13] globals_0.16.3            lattice_0.22-6           \n [15] MASS_7.3-60.0.1           magrittr_2.0.3           \n [17] limma_3.58.1              plotly_4.10.4            \n [19] rmarkdown_2.28            yaml_2.3.10              \n [21] metapod_1.10.0            httpuv_1.6.15            \n [23] sctransform_0.4.1         spam_2.10-0              \n [25] spatstat.sparse_3.1-0     reticulate_1.39.0        \n [27] cowplot_1.1.3             pbapply_1.7-2            \n [29] RColorBrewer_1.1-3        abind_1.4-5              \n [31] zlibbioc_1.48.0           Rtsne_0.17               \n [33] purrr_1.0.2               RCurl_1.98-1.16          \n [35] circlize_0.4.16           GenomeInfoDbData_1.2.11  \n [37] ggrepel_0.9.6             irlba_2.3.5.1            \n [39] listenv_0.9.1             spatstat.utils_3.1-0     \n [41] pheatmap_1.0.12           goftest_1.2-3            \n [43] RSpectra_0.16-2           spatstat.random_3.2-3    \n [45] dqrng_0.3.2               fitdistrplus_1.2-1       \n [47] parallelly_1.38.0         DelayedMatrixStats_1.24.0\n [49] leiden_0.4.3.1            codetools_0.2-20         \n [51] DelayedArray_0.28.0       shape_1.4.6.1            \n [53] tidyselect_1.2.1          farver_2.1.2             \n [55] ScaledMatrix_1.10.0       spatstat.explore_3.2-6   \n [57] jsonlite_1.8.8            GetoptLong_1.0.5         \n [59] BiocNeighbors_1.20.0      progressr_0.14.0         \n [61] iterators_1.0.14          ggridges_0.5.6           \n [63] survival_3.7-0            foreach_1.5.2            \n [65] tools_4.3.3               ica_1.0-3                \n [67] Rcpp_1.0.13               glue_1.7.0               \n [69] gridExtra_2.3             SparseArray_1.2.2        \n [71] xfun_0.47                 dplyr_1.1.4              \n [73] withr_3.0.1               fastmap_1.2.0            \n [75] basilisk_1.14.1           bluster_1.12.0           \n [77] fansi_1.0.6               digest_0.6.37            \n [79] rsvd_1.0.5                R6_2.5.1                 \n [81] mime_0.12                 colorspace_2.1-1         \n [83] Cairo_1.6-2               scattermore_1.2          \n [85] tensor_1.5                spatstat.data_3.1-2      \n [87] utf8_1.2.4                tidyr_1.3.1              \n [89] generics_0.1.3            data.table_1.15.4        \n [91] httr_1.4.7                htmlwidgets_1.6.4        \n [93] S4Arrays_1.2.0            uwot_0.1.16              \n [95] pkgconfig_2.0.3           gtable_0.3.5             \n [97] lmtest_0.9-40             XVector_0.42.0           \n [99] htmltools_0.5.8.1         dotCall64_1.1-1          \n[101] clue_0.3-65               scales_1.3.0             \n[103] png_0.1-8                 knitr_1.48               \n[105] rjson_0.2.21              reshape2_1.4.4           \n[107] nlme_3.1-165              GlobalOptions_0.1.2      \n[109] zoo_1.8-12                stringr_1.5.1            \n[111] KernSmooth_2.23-24        parallel_4.3.3           \n[113] miniUI_0.1.1.1            pillar_1.9.0             \n[115] vctrs_0.6.5               RANN_2.6.2               \n[117] promises_1.3.0            BiocSingular_1.18.0      \n[119] beachmat_2.18.0           xtable_1.8-4             \n[121] cluster_2.1.6             evaluate_0.24.0          \n[123] cli_3.6.3                 locfit_1.5-9.9           \n[125] compiler_4.3.3            rlang_1.1.4              \n[127] crayon_1.5.3              future.apply_1.11.2      \n[129] labeling_0.4.3            mclust_6.1.1             \n[131] plyr_1.8.9                stringi_1.8.4            \n[133] viridisLite_0.4.2         deldir_2.0-4             \n[135] BiocParallel_1.36.0       munsell_0.5.1            \n[137] lazyeval_0.2.2            spatstat.geom_3.2-9      \n[139] dir.expiry_1.10.0         RcppHNSW_0.6.0           \n[141] sparseMatrixStats_1.14.0  future_1.34.0            \n[143] statmod_1.5.0             shiny_1.9.1              \n[145] ROCR_1.0-11               igraph_2.0.3"
  },
  {
    "objectID": "labs/comparison/comparison_pipelines.html#cell-cycle",
    "href": "labs/comparison/comparison_pipelines.html#cell-cycle",
    "title": "Comparison of all pipelines",
    "section": "4 Cell cycle",
    "text": "4 Cell cycle\nCell cycle predictions are done in a similar way for Seurat and Scanpy using module scores, while in Bioc we are using Cyclone.\nVisualize the phase predictions onto the Seurat umap.\n\ntable(all$Phase)\n\n\n  G1  G2M    S \n2878 1514 2148 \n\ntable(all$phase_bioc)\n\n\n  G1  G2M    S \n3771  734 1551 \n\ntable(all$phase_scpy)\n\n\n  G1  G2M    S \n5010   76 1454 \n\nwrap_plots(\nDimPlot(all, group.by = \"Phase\") + NoAxes(),\nDimPlot(all, group.by = \"phase_bioc\") + NoAxes(),\nDimPlot(all, group.by = \"phase_scpy\") + NoAxes(),\n  ncol = 3 \n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nIn seurat, most T/B-cells look like cycling. With bioc, more mixed. With scanpy also more in B/T-cells, but much more G1 prediction.\nPlot the scores.\n\ncc.scores = c(\"S.Score\", \"G2M.Score\",\"G1.score_bioc\", \"G2M.score_bioc\", \"S.score_bioc\",\"S_score_scpy\", \"G2M_score_scpy\")\n\n\n# copied from pairs help section.\npanel.cor &lt;- function(x, y, digits = 2, prefix = \"\", cex.cor, ...) {\n    usr &lt;- par(\"usr\")\n    on.exit(par(usr))\n    par(usr = c(0, 1, 0, 1))\n    nas = is.na(x) | is.na(y)\n    Cor &lt;- cor(x[!nas], y[!nas], method = \"spearman\") # Remove abs function if desired\n    txt &lt;- paste0(prefix, format(c(Cor, 0.123456789), digits = digits)[1])\n    if(missing(cex.cor)) {\n        cex.cor &lt;- 0.4 / strwidth(txt)\n    }\n    text(0.5, 0.5, txt,\n         cex = 1 + cex.cor) \n} \n\npairs(all@meta.data[,cc.scores], a.action = na.omit,\n      upper.panel = panel.cor,    # Correlation panel\n      lower.panel = panel.smooth)\n\n\n\n\n\n\n\n\nUse spearman cor as the distributions are very different\n\nS and G2M scores in Scanpy and Seurat are correlated to eachother\nS vs G2M are also correlated.\nIn BioC all scores are anticorrelated."
  },
  {
    "objectID": "labs/comparison/comparison_pipelines.html#variable-features",
    "href": "labs/comparison/comparison_pipelines.html#variable-features",
    "title": "Comparison of all pipelines",
    "section": "5 Variable features",
    "text": "5 Variable features\nIn Seurat, FindVariableFeatures is not batch aware unless the data is split into layers by samples, here we have the variable genes created with layers. In Bioc modelGeneVar we used sample as a blocking parameter, e.g calculates the variable genes per sample and combines the variances. Similar in scanpy we used the samples as batch_key.\n\nhvgs = list()\nhvgs$seurat = VariableFeatures(sobj)\nhvgs$bioc = sce@metadata$hvgs\n# scanpy has no strict number on selection, instead uses dispersion cutoff. So select instead top 2000 dispersion genes.\ntmp = rowData(scanpy.sce)\nhvgs_scanpy = rownames(tmp)[tmp$highly_variable]\nhvgs$scanpy = rownames(tmp)[order(tmp$dispersions_norm, decreasing = T)][1:2000]\n\ncmat = make_comb_mat(hvgs)\nprint(cmat)\n\nA combination matrix with 3 sets and 7 combinations.\n  ranges of combination set size: c(211, 861).\n  mode for the combination size: distinct.\n  sets are on rows.\n\nCombination sets are:\n  seurat bioc scanpy code size\n       x    x      x  111  641\n       x    x         110  287\n       x           x  101  211\n            x      x  011  364\n       x              100  861\n            x         010  708\n                   x  001  784\n\nSets are:\n     set size\n  seurat 2000\n    bioc 2000\n  scanpy 2000\n\nUpSet(cmat)\n\n\n\n\n\n\n\n\nSurprisingly low overlap between the methods and many genes that are unique to one pipeline. With discrepancies in the doublet filtering the cells used differ to some extent, but otherwise the variation should be similar. Even if it is estimated in different ways.\nIs the differences more due to the combination of ranks/dispersions or also found within a single dataset?\nOnly Seurat have the dispersions for each individual dataset stored in the object.\nExplore more in the comparison_hvg.qmd script."
  },
  {
    "objectID": "labs/comparison/comparison_pipelines.html#integrations",
    "href": "labs/comparison/comparison_pipelines.html#integrations",
    "title": "Comparison of all pipelines",
    "section": "6 Integrations",
    "text": "6 Integrations\n\nreductions = c(\"umap\",\"umap_cca\", \"umap_harmony\",  \"umap_scanorama\",    \"umap_scanoramaC\", \"umap_bioc\",  \"umap_bioc_mnn\", \"umap_bioc_harmony\", \"umap_bioc_scanorama\", \"umap_scpy\",\"umap_scpy_bbknn\", \"umap_scpy_scanorama\", \"umap_scpy_harmony\"   )\n\ntmp = bioc@reductions$UMAP_on_MNN@cell.embeddings[match(in.all, meta.bioc$cell),]\nrownames(tmp) = colnames(all)\nall[[\"umap_bioc_mnn\"]] = CreateDimReducObject(tmp, key = \"umapbiocmnn_\", assay = \"RNA\")\n\ntmp = bioc@reductions$UMAP_on_Harmony@cell.embeddings[match(in.all, meta.bioc$cell),]\nrownames(tmp) = colnames(all)\nall[[\"umap_bioc_harmony\"]] = CreateDimReducObject(tmp, key = \"umapbiocharmony_\", assay = \"RNA\")\n\ntmp = bioc@reductions$UMAP_on_Scanorama@cell.embeddings[match(in.all, meta.bioc$cell),]\nrownames(tmp) = colnames(all)\nall[[\"umap_bioc_scanorama\"]] = CreateDimReducObject(tmp, key = \"umapbioscanorama_\", assay = \"RNA\")\n\ntmp = scanpy@reductions$X_umap_bbknn@cell.embeddings[match(in.all, meta.scanpy$cell),]\nrownames(tmp) = colnames(all)\nall[[\"umap_scpy_bbknn\"]] = CreateDimReducObject(tmp, key = \"umapscpybbknn_\", assay = \"RNA\")\n\ntmp = scanpy@reductions$X_umap_scanorama@cell.embeddings[match(in.all, meta.scanpy$cell),]\nrownames(tmp) = colnames(all)\nall[[\"umap_scpy_scanorama\"]] = CreateDimReducObject(tmp, key = \"umapscpyscanorama_\", assay = \"RNA\")\n\ntmp = scanpy@reductions$X_umap_harmony@cell.embeddings[match(in.all, meta.scanpy$cell),]\nrownames(tmp) = colnames(all)\nall[[\"umap_scpy_harmony\"]] = CreateDimReducObject(tmp, key = \"umapscpyharmony_\", assay = \"RNA\")\n\n\nplotlist = lapply(reductions, function(x) DimPlot(all, reduction = x, group.by = \"orig.ident\") + NoAxes() + ggtitle(x))\n\nwrap_plots(\n  plotlist,\n  ncol = 4\n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n# remotes::install_github(\"carmonalab/scIntegrationMetrics\", dependencies = FALSE), fails with compiler errors on mac. \n\n\n6.1 Clustering on integrations\nRun clustering on each of the integrations but with same method. Use Seurat, with 30 first components, k=30, and louvain with a few different resolutions.\nFor BBKNN the full integration matrix is not in the reductions, skip for now.\n\n# all seurat integrations.\n\nintegrations = list(\n  seu_cca = \"integrated_cca\",\n  seu_harm = \"harmony\",\n  seu_scan = \"scanorama\",\n  seu_scanC = \"scanoramaC\"\n)\nres = c(0.4,0.6,0.8,1.0)\n\nfor (i in 1:length(integrations)){\n  sname = names(integrations)[i]\n  sobj = FindNeighbors(sobj, reduction = integrations[[sname]], dims = 1:30, verbose = F)\n  for (r in res){\n    sobj = FindClusters(sobj, resolution = r, cluster.name = paste(sname,r,sep=\"_\"), verbose = F)\n  }\n}\n\nmeta.clust = sobj@meta.data[,grepl(\"^seu_\", colnames(sobj@meta.data))]\nmeta.clust = meta.clust[match(in.all, meta.seurat$cell),]\n\n# all bioc\nintegrations = list(\n  bio_mnn = \"MNN\",\n  bio_harm = \"harmony\",\n  bio_scan = \"Scanorama\"\n)\n\nfor (i in 1:length(integrations)){\n  sname = names(integrations)[i]\n  bioc = FindNeighbors(bioc, reduction = integrations[[sname]], dims = 1:30, verbose = F)\n  for (r in res){\n    bioc = FindClusters(bioc, resolution = r, cluster.name = paste(sname,r,sep=\"_\"), verbose = F)\n  }\n}\n\ntmp = bioc@meta.data[,grepl(\"^bio_\", colnames(bioc@meta.data))]\ntmp = tmp[match(in.all, meta.bioc$cell),]\nmeta.clust = cbind(tmp, meta.clust)\n\n# all scanpy\nintegrations = list(\n  scpy_harm = \"X_pca_harmony\",\n  scpy_scan = \"Scanorama\"\n)\n\nfor (i in 1:length(integrations)){\n  sname = names(integrations)[i]\n  scanpy = FindNeighbors(scanpy, reduction = integrations[[sname]], dims = 1:30, verbose = F)\n  for (r in res){\n    scanpy = FindClusters(scanpy, resolution = r, cluster.name = paste(sname,r,sep=\"_\"), verbose = F)\n  }\n}\n\ntmp = scanpy@meta.data[,grepl(\"^scpy_\", colnames(scanpy@meta.data))]\ntmp = tmp[match(in.all, meta.scanpy$cell),]\nmeta.clust = cbind(tmp, meta.clust)\n\nCalculate adjusted Rand index, with mclust package.\n\nari = mat.or.vec(ncol(meta.clust), ncol(meta.clust))\n\nfor (i in 1:ncol(meta.clust)){\n  for (j in 1:ncol(meta.clust)){\n  ari[i,j] = mclust::adjustedRandIndex(meta.clust[,i], meta.clust[,j])\n  }\n}\nrownames(ari) = colnames(meta.clust)\ncolnames(ari) = colnames(meta.clust)\n\nannot = data.frame(Reduce(rbind,strsplit(colnames(meta.clust), \"_\")))\ncolnames(annot) = c(\"Pipe\", \"Integration\", \"Resolution\")\nrownames(annot) = colnames(meta.clust)\nnclust = apply(meta.clust, 2, function(x) length(unique(x)))\nannot$nClust = nclust\npheatmap::pheatmap(ari, annotation_row = annot)\n\n\n\n\n\n\n\n\nAll the scanorama ones are similar, except for the one with counts. Harmony on the bioc object also stands out as quite different.\nNumber of clusters per method:\n\nnclust = apply(meta.clust, 2, function(x) length(unique(x)))\npar(mar = c(3,10,3,3))\nbarplot(nclust, horiz=T, las=2, cex.names=0.6)"
  },
  {
    "objectID": "labs/comparison/comparison_pipelines.html#clustering-on-integrations",
    "href": "labs/comparison/comparison_pipelines.html#clustering-on-integrations",
    "title": "Comparison of all pipelines",
    "section": "6 Clustering on integrations",
    "text": "6 Clustering on integrations\nRun clustering on each of the integrations but with same method. Use Seurat, with 30 first components, k=30, and louvain with a few different resolutions.\n\n# all seurat integrations.\n\nintegrations = list(\n  seu_cca = \"integrated_cca\",\n  seu_harm = \"harmony\",\n  seu_scan = \"scanorama\",\n  seu_scanC = \"scanoramaC\"\n)\nres = c(0.4,0.6,0.8,1.0)\n\nfor (i in 1:length(integrations)){\n  sname = names(integrations)[i]\n  sobj = FindNeighbors(sobj, reduction = integrations[[sname]], dims = 1:30, verbose = F)\n  for (r in res){\n    sobj = FindClusters(sobj, resolution = r, cluster.name = paste(sname,r,sep=\"_\"), verbose = F)\n  }\n}\n\nmeta.clust = sobj@meta.data[,grepl(\"^seu_\", colnames(sobj@meta.data))]\nmeta.clust = meta.clust[match(in.all, meta.seurat$cell),]\n\n# all bioc\nintegrations = list(\n  bio_mnn = \"MNN\",\n  bio_harm = \"harmony\",\n  bio_scan = \"Scanorama\"\n)\n\nfor (i in 1:length(integrations)){\n  sname = names(integrations)[i]\n  bioc = FindNeighbors(bioc, reduction = integrations[[sname]], dims = 1:30, verbose = F)\n  for (r in res){\n    bioc = FindClusters(bioc, resolution = r, cluster.name = paste(sname,r,sep=\"_\"), verbose = F)\n  }\n}\n\ntmp = bioc@meta.data[,grepl(\"^bio_\", colnames(bioc@meta.data))]\ntmp = tmp[match(in.all, meta.bioc$cell),]\nmeta.clust = cbind(tmp, meta.clust)\n\n# all scanpy\nintegrations = list(\n  scpy_harm = \"X_pca_harmony\",\n  scpy_scan = \"Scanorama\"\n)\n\nfor (i in 1:length(integrations)){\n  sname = names(integrations)[i]\n  scanpy = FindNeighbors(scanpy, reduction = integrations[[sname]], dims = 1:30, verbose = F)\n  for (r in res){\n    scanpy = FindClusters(scanpy, resolution = r, cluster.name = paste(sname,r,sep=\"_\"), verbose = F)\n  }\n}\n\ntmp = scanpy@meta.data[,grepl(\"^scpy_\", colnames(scanpy@meta.data))]\ntmp = tmp[match(in.all, meta.scanpy$cell),]\nmeta.clust = cbind(tmp, meta.clust)\n\nFor BBKNN the full integration matrix is not in the reductions, skip for now.\n\nari = mat.or.vec(ncol(meta.clust), ncol(meta.clust))\n\nfor (i in 1:ncol(meta.clust)){\n  for (j in 1:ncol(meta.clust)){\n  ari[i,j] = mclust::adjustedRandIndex(meta.clust[,i], meta.clust[,j])\n  }\n}\nrownames(ari) = colnames(meta.clust)\ncolnames(ari) = colnames(meta.clust)\n\nannot = data.frame(Reduce(rbind,strsplit(colnames(meta.clust), \"_\")))\ncolnames(annot) = c(\"Pipe\", \"Integration\", \"Resolution\")\nrownames(annot) = colnames(meta.clust)\npheatmap::pheatmap(ari, annotation_row = annot)\n\n\n\n\n\n\n\n\nAll the scanorama ones are similar, except for the one with counts. Harmony on the bioc object also stands out as quite different.\nNumber of clusters per method:\n\nnclust = apply(meta.clust, 2, function(x) length(unique(x)))\npar(mar = c(3,10,3,3))\nbarplot(nclust, horiz=T, las=2)"
  },
  {
    "objectID": "labs/comparison/comparison_pipelines.html#clustering-in-the-pipelines",
    "href": "labs/comparison/comparison_pipelines.html#clustering-in-the-pipelines",
    "title": "Comparison of all pipelines",
    "section": "8 Clustering in the pipelines",
    "text": "8 Clustering in the pipelines\nAbove, we did the clustering with the same method on the different integrated spaces. Now we will instead explore the integrated clustering from the different pipelines. We still only use the cells that are in common from all 3 pipelines.\nThe graph clustering is implemented quite differently in the different pipelines.\n\nSeurat - runs detection on SNN, resolution implemented\nScanpy - runs detection on KNN, resolution implemented\nBioc - runs detection on SNN, but different scoring to Seurat, resolution parameter behaves strange. Instead use k to tweak cluster resolution.\n\n\nseu.columns = c(\"RNA_snn_res\",\"kmeans_\",\"hc_\")\nbioc.columns = c(\"louvain_\",\"leiden_\",\"hc_\",\"kmeans_\")\nscpy.columns = c(\"leiden_\",\"hclust_\",\"kmeans_\")\n  \n\nidx = which(rowSums(sapply(seu.columns, grepl, colnames(sobj@meta.data)))&gt;0)\ntmp1 = sobj@meta.data[match(in.all, meta.seurat$cell), idx]\ncolnames(tmp1) = paste0(\"seur_\", colnames(tmp1))\n\nidx = which(rowSums(sapply(bioc.columns, grepl, colnames(bioc@meta.data)))&gt;0)\ntmp2 = bioc@meta.data[match(in.all, meta.bioc$cell), idx]\ncolnames(tmp2) = paste0(\"bioc_\", colnames(tmp2))\n\nmeta.clust2 = cbind(tmp1, tmp2)\n\nidx = which(rowSums(sapply(scpy.columns, grepl, colnames(scanpy@meta.data)))&gt;0)\ntmp2 = scanpy@meta.data[match(in.all, meta.scanpy$cell), idx]\ncolnames(tmp2) = paste0(\"scpy_\", colnames(tmp2))\n\nmeta.clust2 = cbind(meta.clust2, tmp2)\n\nGroup based on adjusted Rand index.\n\nari = mat.or.vec(ncol(meta.clust2), ncol(meta.clust2))\n\nfor (i in 1:ncol(meta.clust2)){\n  for (j in 1:ncol(meta.clust2)){\n  ari[i,j] = mclust::adjustedRandIndex(meta.clust2[,i], meta.clust2[,j])\n  }\n}\nrownames(ari) = colnames(meta.clust2)\ncolnames(ari) = colnames(meta.clust2)\n\n\nm = unlist(lapply(strsplit(colnames(meta.clust2), \"_\"), function(x) x[2]))\np = unlist(lapply(strsplit(colnames(meta.clust2), \"_\"), function(x) x[1]))\nnc = apply(meta.clust2, 2, function(x) length(unique(x)))\n       \nannot = data.frame(Pipe=p, Method=m, nC=nc)\nrownames(annot) = colnames(meta.clust2)\npheatmap::pheatmap(ari, annotation_row = annot)\n\n\n\n\n\n\n\n\nClustering consistency per cell\n\nstats2 = clusterOverlapN(meta.clust2)\n\ncs = colSums(stats2$sumMatrix)\nnames(cs) = colnames(all)\n\n\nall = AddMetaData(all, cs, col.name = \"overlapN.all\")\nall = AddMetaData(all, log10(cs), col.name = \"overlapN.all.log\")\n\n\nFeaturePlot(all, reduction = \"umap_harmony\", features = c(\"overlapN.all\", \"overlapN.all.log\"))\n\n\n\n\n\n\n\n\nScore each method based on how often they place high scoring cells in different clusters.\n\nnclust2 = apply(meta.clust2,2, function(x) length(unique(x)))\n\npar(mfrow = c(2,2))\nplot(nclust2, stats2$scores)\nplot(nclust2, stats2$scores/nclust2)\nplot(nclust2, stats2$probabilities)\n\n\n\n\n\n\n\n\n\nannot$score = stats2$scores\nannot$Name = rownames(annot)\n\nggplot(annot, aes(x=Name, y = score, fill=Pipe)) + geom_bar(stat = \"identity\") + RotatedAxis()\n\n\n\n\n\n\n\nggplot(annot, aes(x=nC, y = score, color=Pipe)) + geom_point()\n\n\n\n\n\n\n\n\n\nsaveRDS(all, \"data/covid/results/merged_all.rds\")"
  },
  {
    "objectID": "labs/comparison/comparison_pipelines.html#clustering-consistency",
    "href": "labs/comparison/comparison_pipelines.html#clustering-consistency",
    "title": "Comparison of all pipelines",
    "section": "7 Clustering consistency",
    "text": "7 Clustering consistency\nCompare all these different clusterings by calculating how often each cell pair is in the same cluster.\n\ncalculateCLProb = function(nC,nS){\n    P = (nS/nC-1) / (nS-1)\n    return(P)\n}\n\nclusterOverlapN &lt;- function(meta.df){\n  # meta.df is a data frame with clustering results in each column \n  dmats = list()\n  probs = mat.or.vec(1,ncol(meta.df))\n  for (i in 1:ncol(meta.df)){\n    tmp = as(outer(meta.df[,i],meta.df[,i], FUN = \"==\")+0, \"dgCMatrix\")\n    diag(tmp) = 0 #score to self to zero.\n    # normalize the counts by probability given number of clusters and cells.\n    prob = calculateCLProb(length(unique(meta.df[,i])),length(meta.df[,i]))\n    probs[i] = prob\n    tmp = tmp /prob\n    dmats[[i]] = tmp\n  }\n  names(dmats) = colnames(meta.df)\n  \n  # add all matrices together, divide by number of matrices.\n  sum.dist = Reduce(\"+\",dmats)/ncol(meta.df)\n  \n  # scores each clustering by adding together the individual consensus scores for the cell pairs that we have. \n  # Divide by number of cells. \n  scores = lapply(dmats, function(x) sum(x*sum.dist)/nrow(meta.df)^2)\n  scores = unlist(scores)\n  names(scores) = colnames(meta.df)\n  return(list(overlapMatrices = dmats, sumMatrix = sum.dist, scores = scores, probabilities = probs))\n}\n\n\nstats = clusterOverlapN(meta.clust)\n\ncs = colSums(stats$sumMatrix)\nnames(cs) = colnames(all)\n\nall = AddMetaData(all, cs, col.name = \"overlapN\")\nall = AddMetaData(all, log10(cs), col.name = \"overlapN.log\")\n\n\nFeaturePlot(all, reduction = \"umap_harmony\", features = c(\"overlapN\", \"overlapN.log\"))\n\n\n\n\n\n\n\n\nFor each cell, we have stats on how often it is in the same cluster as another cell. Ranging from 0 to nclust.\nNow we have a score for each pair of cells of how frequently they cluster together. So we can just multiply the matrices to see if high scoring cells are grouped in that clustering. But we need to adjust for cluster sizes, larger clusters have bigger chance of having a high score. So the scores are normalized by the number of clusters with calculateCLProb\n\nannot$Score = stats$scores\nannot$Name = rownames(annot)\nggplot(annot, aes(x=Name, y=Score, color=Pipe, fill=Integration)) + geom_bar(stat = \"identity\") + RotatedAxis()\n\n\n\n\n\n\n\n\nGenerally high scores for harmony with Scanpy or Seurat, CCA clearly differs. But the scores are biased and only show what is closest to consensus."
  },
  {
    "objectID": "labs/bioc/bioc_05_dge_scoreMarkers.html",
    "href": "labs/bioc/bioc_05_dge_scoreMarkers.html",
    "title": " Differential gene expression",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified.\nIn this tutorial we will cover differential gene expression, which comprises an extensive range of topics and methods. In single cell, differential expresison can have multiple functionalities such as identifying marker genes for cell populations, as well as identifying differentially regulated genes across conditions (healthy vs control). We will also cover controlling batch effect in your test.\nWe can first load the data from the clustering session. Moreover, we can already decide which clustering resolution to use. First let’s define using the louvain clustering to identifying differentially expressed genes.\nsuppressPackageStartupMessages({\n    library(scater)\n    library(scran)\n    # library(venn)\n    library(patchwork)\n    library(ggplot2)\n    library(pheatmap)\n    library(igraph)\n    library(dplyr)\n})\n\ndevtools::source_url(\"https://raw.githubusercontent.com/asabjorklund/single_cell_R_scripts/main/overlap_phyper_v2.R\")\n# download pre-computed data if missing or long compute\nfetch_data &lt;- TRUE\n\n# url for source and intermediate data\npath_data &lt;- \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\npath_file &lt;- \"data/covid/results/bioc_covid_qc_dr_int_cl.rds\"\nif (!dir.exists(dirname(path_file))) dir.create(dirname(path_file), recursive = TRUE)\nif (fetch_data && !file.exists(path_file)) download.file(url = file.path(path_data, \"covid/results/bioc_covid_qc_dr_int_cl.rds\"), destfile = path_file)\nsce &lt;- readRDS(path_file)\nprint(reducedDims(sce))\n\nList of length 17\nnames(17): PCA UMAP tSNE_on_PCA UMAP_on_PCA ... UMAP_on_Scanorama SNN harmony2"
  },
  {
    "objectID": "labs/bioc/bioc_05_dge_scoreMarkers.html#meta-dge_numbers",
    "href": "labs/bioc/bioc_05_dge_scoreMarkers.html#meta-dge_numbers",
    "title": " Differential gene expression",
    "section": "1 ?meta:dge_numbers",
    "text": "1 ?meta:dge_numbers\n?meta:dge_numbers_1\nHowever, findMarker in Scran is implemented so that the tests are run in a pariwise manner, e.g. each cluster is tested agains all the others individually. Then a combined p-value is calculated across all the tests using combineMarkers. So for this method, one large cluster will not influence the results in the same way as FindMarkers in Seurat or rank_genes_groups in Scanpy."
  },
  {
    "objectID": "labs/bioc/bioc_05_dge_scoreMarkers.html#meta-dge_cmg",
    "href": "labs/bioc/bioc_05_dge_scoreMarkers.html#meta-dge_cmg",
    "title": " Differential gene expression",
    "section": "1 Cell marker genes",
    "text": "1 Cell marker genes\nLet us first compute a ranking for the highly differential genes in each cluster. There are many different tests and parameters to be chosen that can be used to refine your results. When looking for marker genes, we want genes that are positively expressed in a cell type and possibly not expressed in others.\nIn the scran function findMarkers t-test, wilcoxon test and binomial test implemented.\n\n# Compute differentiall expression\nmarkers_genes &lt;- scran::findMarkers(\n    x = sce,\n    groups = sce$leiden_k20,\n    test.type = \"wilcox\",\n    lfc = .5,\n    pval.type = \"all\",\n    direction = \"up\"\n)\n\n# List of dataFrames with the results for each cluster\nmarkers_genes\n\nList of length 13\nnames(13): 1 2 3 4 5 6 7 8 9 10 11 12 13\n\n# Visualizing the expression of one\nhead(markers_genes[[\"1\"]])\n\nDataFrame with 6 rows and 15 columns\n          p.value       FDR summary.AUC     AUC.2     AUC.3     AUC.4     AUC.5\n        &lt;numeric&gt; &lt;numeric&gt;   &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\nS100A8          1         1    0.282842  0.982921  0.282842  0.985213  0.983036\nIFI6            1         1    0.262378  0.507736  0.262378  0.504875  0.457423\nRETN            1         1    0.304136  0.416034  0.262345  0.415249  0.415516\nALOX5AP         1         1    0.303393  0.387884  0.259607  0.362574  0.336117\nS100A12         1         1    0.245282  0.875237  0.245282  0.878158  0.875534\nISG15           1         1    0.289798  0.398547  0.273513  0.429643  0.389656\n            AUC.6     AUC.7     AUC.8     AUC.9    AUC.10    AUC.11    AUC.12\n        &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\nS100A8   0.985045  0.854209  0.984766  0.414321  0.959148  0.942246  0.977685\nIFI6     0.462969  0.549265  0.403180  0.316388  0.499180  0.364114  0.443247\nRETN     0.416258  0.400304  0.416099  0.304136  0.411009  0.383941  0.415450\nALOX5AP  0.303393  0.441705  0.250192  0.312316  0.404740  0.453944  0.287193\nS100A12  0.877325  0.839671  0.874472  0.381497  0.870458  0.865589  0.876760\nISG15    0.370755  0.480924  0.339772  0.289798  0.433757  0.299824  0.380235\n           AUC.13\n        &lt;numeric&gt;\nS100A8   0.980941\nIFI6     0.467511\nRETN     0.410955\nALOX5AP  0.335212\nS100A12  0.871185\nISG15    0.353287\n\n\nWe can now select the top 25 overexpressed genes for plotting.\n\n# Colect the top 25 genes for each cluster and put the into a single table\ntop25 &lt;- lapply(names(markers_genes), function(x) {\n    temp &lt;- markers_genes[[x]][1:25, 1:2]\n    temp$gene &lt;- rownames(markers_genes[[x]])[1:25]\n    temp$cluster &lt;- x\n    return(temp)\n})\ntop25 &lt;- as_tibble(do.call(rbind, top25))\ntop25$p.value[top25$p.value == 0] &lt;- 1e-300 # set to very small number if p-value is zero.\ntop25\n\n\n\n  \n\n\n\nWe can plot them as barplots per cluster.\n\npar(mfrow = c(1, 5), mar = c(4, 6, 3, 1))\nfor (i in names(markers_genes)) {\n    barplot(sort(setNames(-log10(top25$p.value), top25$gene)[top25$cluster == i], F),\n        horiz = T, las = 1, main = paste0(i, \" vs. rest\"), border = \"white\", yaxs = \"i\", xlab = \"-log10pval\"\n    )\n    abline(v = c(0, -log10(0.05)), lty = c(1, 2))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can visualize them as a heatmap. Here we are selecting the top 5.\n\ntop25 %&gt;%\n    group_by(cluster) %&gt;%\n    slice_min(p.value, n = 5, with_ties = FALSE) -&gt; top5\n\n\nscater::plotHeatmap(sce[, order(sce$leiden_k20)],\n    features = unique(top5$gene),\n    center = T, zlim = c(-3, 3),\n    colour_columns_by = \"leiden_k20\",\n    show_colnames = F, cluster_cols = F,\n    fontsize_row = 6,\n    color = colorRampPalette(c(\"purple\", \"black\", \"yellow\"))(90)\n)\n\n\n\n\n\n\n\n\nAnother way is by representing the overall group expression and detection rates in a dot-plot.\n\nplotDots(sce, features = unique(top5$gene), group=\"leiden_k20\")\n\n\n\n\n\n\n\n\nNOTE! Perhaps add in t-test and binomial?\n\n1.1 One cluster\nSelect one cluster to visualize and compare for now. Cluster1 has no significant genes.\n\ncl.sel = \"2\"\n\ntmp = markers_genes[[cl.sel]]\n\nplotExpression(sce, features=head(rownames(tmp),9),ncol=3, \n    x=\"leiden_k20\", colour_by=\"leiden_k20\")\n\n\n\n\n\n\n\n\n\n\n1.2 Binomial\n\nmarkers_genesB &lt;- scran::findMarkers(\n    x = sce,\n    groups = sce$leiden_k20,\n    test.type = \"binom\",\n    lfc = .5,\n    pval.type = \"all\",\n    direction = \"up\"\n)\n\n\ntmp = markers_genesB[[cl.sel]]\nplotExpression(sce, features=head(rownames(tmp),9),ncol=3, \n    x=\"leiden_k20\", colour_by=\"leiden_k20\")\n\n\n\n\n\n\n\n\n\n\n1.3 T-test\n\nmarkers_genesT &lt;- scran::findMarkers(\n    x = sce,\n    groups = sce$leiden_k20,\n    test.type = \"t\",\n    lfc = .5,\n    pval.type = \"all\",\n    direction = \"up\"\n)\n\n\ntmp = markers_genesT[[cl.sel]]\nplotExpression(sce, features=head(rownames(tmp),9),ncol=3, \n    x=\"leiden_k20\", colour_by=\"leiden_k20\")\n\n\n\n\n\n\n\n\n\n\n1.4 With blocking\nFor all the tests above there is the option to add a blocking parameter, eg. different batch effects. Lets try the 3 methods but with sample as the blocking parameter.\n\nmarkers_genes.block &lt;- scran::findMarkers(\n    x = sce,\n    groups = sce$leiden_k20,\n    test.type = \"wilcox\",\n    lfc = .5,\n    pval.type = \"all\",\n    direction = \"up\",\n    block = sce$sample\n)\n\n\nmarkers_genesT.block &lt;- scran::findMarkers(\n    x = sce,\n    groups = sce$leiden_k20,\n    test.type = \"t\",\n    lfc = .5,\n    pval.type = \"all\",\n    direction = \"up\",\n    block = sce$sample\n)\n\n\nmarkers_genesB.block &lt;- scran::findMarkers(\n    x = sce,\n    groups = sce$leiden_k20,\n    test.type = \"binom\",\n    lfc = .5,\n    pval.type = \"all\",\n    direction = \"up\",\n    block = sce$sample\n)\n\n\ntop50 = list()\n\ntop50$Wilc = lapply(markers_genes, function(x) rownames(x)[1:50])\ntop50$T = lapply(markers_genesT, function(x) rownames(x)[1:50])\ntop50$Bin = lapply(markers_genesB, function(x) rownames(x)[1:50])\ntop50$WilcB = lapply(markers_genes.block, function(x) rownames(x)[1:50])\ntop50$TB = lapply(markers_genesT.block, function(x) rownames(x)[1:50])\ntop50$BinB = lapply(markers_genesB.block, function(x) rownames(x)[1:50])\n\n\nall.top = unlist(top50, recursive = F)\nall.top = all.top[sort(names(all.top))]\n\n\no = overlap_phyper2(all.top,all.top, bg=nrow(markers_genes[[1]]), nsize = 5, remove.diag = T)"
  },
  {
    "objectID": "labs/bioc/bioc_05_dge_scoreMarkers.html#scoremarkers",
    "href": "labs/bioc/bioc_05_dge_scoreMarkers.html#scoremarkers",
    "title": " Differential gene expression",
    "section": "2 scoreMarkers",
    "text": "2 scoreMarkers\nFrom help section:\nWe do this by realizing that the p-values for these types of comparisons are largely meaningless; individual cells are not meaningful units of experimental replication, while the groups themselves are defined from the data. Thus, by discarding the p-values, we can simplify our marker selection by focusing only on the effect sizes between groups.\n\nmarker.info &lt;- scoreMarkers(sce, sce$leiden_k20)\n\nhead(marker.info[[cl.sel]])\n\nDataFrame with 6 rows and 19 columns\n           self.average other.average self.detected other.detected\n              &lt;numeric&gt;     &lt;numeric&gt;     &lt;numeric&gt;      &lt;numeric&gt;\nAL627309.1   0.00958323   0.007382501    0.00754717     0.00485755\nAL669831.5   0.07951755   0.095087268    0.07547170     0.07457660\nFAM87B       0.00000000   0.000457862    0.00000000     0.00156177\nLINC00115    0.04658059   0.033915500    0.03773585     0.02679335\nFAM41C       0.08049211   0.059562093    0.07044025     0.04304755\nAL645608.3   0.00000000   0.000563350    0.00000000     0.00057802\n           mean.logFC.cohen min.logFC.cohen median.logFC.cohen max.logFC.cohen\n                  &lt;numeric&gt;       &lt;numeric&gt;          &lt;numeric&gt;       &lt;numeric&gt;\nAL627309.1        0.0472783      -0.0833633         0.06080949        0.117636\nAL669831.5       -0.0219147      -0.2288812        -0.00800465        0.144786\nFAM87B           -0.0269628      -0.1580635         0.00000000        0.000000\nLINC00115         0.0760736      -0.1067919         0.07902688        0.251808\nFAM41C            0.0995516      -0.2286314         0.13547578        0.292938\nAL645608.3       -0.0159468      -0.0776198         0.00000000        0.000000\n           rank.logFC.cohen  mean.AUC   min.AUC median.AUC   max.AUC  rank.AUC\n                  &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;  &lt;numeric&gt; &lt;numeric&gt; &lt;integer&gt;\nAL627309.1             1455  0.501348  0.496143   0.501960  0.503774      3234\nAL669831.5             3600  0.500183  0.476813   0.499523  0.533867      2558\nFAM87B                 3593  0.499219  0.493007   0.500000  0.500000      4908\nLINC00115              2219  0.505510  0.491978   0.507006  0.518868      1965\nFAM41C                 1652  0.513641  0.477391   0.517178  0.527700      1448\nAL645608.3             3593  0.499711  0.498364   0.500000  0.500000      4908\n           mean.logFC.detected min.logFC.detected median.logFC.detected\n                     &lt;numeric&gt;          &lt;numeric&gt;             &lt;numeric&gt;\nAL627309.1            0.782157          -0.794033            0.72399206\nAL669831.5            0.178599          -0.610710           -0.00356833\nFAM87B               -0.411385          -1.584963            0.00000000\nLINC00115             0.655948          -0.514369            0.61425476\nFAM41C                0.838723          -0.642304            0.92406702\nAL645608.3           -0.358276          -1.848346            0.00000000\n           max.logFC.detected rank.logFC.detected\n                    &lt;numeric&gt;           &lt;integer&gt;\nAL627309.1        2.60568e+00                 717\nAL669831.5        2.52238e+00                2554\nFAM87B            3.20343e-16                5343\nLINC00115         2.64277e+00                 633\nFAM41C            1.84682e+00                1861\nAL645608.3        3.20343e-16                5343\n\n\n\n2.1 AUC:\nA value of 1 corresponds to upregulation, where all values of our cluster of interest are greater than any value from the other cluster; a value of 0.5 means that there is no net difference in the location of the distributions; and a value of 0 corresponds to downregulation. The AUC is closely related to the U statistic in the Wilcoxon ranked sum test (a.k.a., Mann-Whitney U-test).\n\nchosen &lt;- marker.info[[cl.sel]]\nordered &lt;- chosen[order(chosen$mean.AUC, decreasing=TRUE),]\nhead(ordered[,1:4]) # showing basic stats only, for brevity.\n\nDataFrame with 6 rows and 4 columns\n          self.average other.average self.detected other.detected\n             &lt;numeric&gt;     &lt;numeric&gt;     &lt;numeric&gt;      &lt;numeric&gt;\nCD79A          3.40899      0.431579      0.982390      0.1101662\nMS4A1          2.41687      0.324364      0.928302      0.0929619\nIGHM           3.97828      0.484948      0.889308      0.1137849\nTNFRSF13C      1.94919      0.267831      0.889308      0.0940688\nLINC00926      1.86917      0.239671      0.854088      0.0909818\nCD74           5.62793      2.856010      0.998742      0.7703181\n\n\n\nplotExpression(sce, features=head(rownames(ordered),9),ncol=3, \n    x=\"leiden_k20\", colour_by=\"leiden_k20\")\n\n\n\n\n\n\n\n\n\n\n2.2 Cohen:\nCohen’s d is a standardized log-fold change where the difference in the mean log-expression between groups is scaled by the average standard deviation across groups. In other words, it is the number of standard deviations that separate the means of the two groups. The interpretation is similar to the log-fold change; positive values indicate that the gene is upregulated in our cluster of interest, negative values indicate downregulation and values close to zero indicate that there is little difference. Cohen’s d is roughly analogous to the t -statistic in various two-sample t -tests.\n\nordered &lt;- chosen[order(chosen$mean.logFC.cohen, decreasing=TRUE),]\nhead(ordered[,1:4]) # showing basic stats only, for brevity.\n\nDataFrame with 6 rows and 4 columns\n          self.average other.average self.detected other.detected\n             &lt;numeric&gt;     &lt;numeric&gt;     &lt;numeric&gt;      &lt;numeric&gt;\nCD79A          3.40899      0.431579      0.982390      0.1101662\nIGHM           3.97828      0.484948      0.889308      0.1137849\nMS4A1          2.41687      0.324364      0.928302      0.0929619\nHLA-DRA        5.01704      2.268397      0.993711      0.5879083\nCD74           5.62793      2.856010      0.998742      0.7703181\nTNFRSF13C      1.94919      0.267831      0.889308      0.0940688\n\nplotExpression(sce, features=head(rownames(ordered),9),ncol=3, \n    x=\"leiden_k20\", colour_by=\"leiden_k20\")\n\n\n\n\n\n\n\n\n\n\n2.3 Detected:\nFinally, we also compute the log-fold change in the proportion of cells with detected expression between clusters. This ignores any information about the magnitude of expression, only considering whether any expression is detected at all. Again, positive values indicate that a greater proportion of cells express the gene in our cluster of interest compared to the other cluster. Note that a pseudo-count is added to avoid undefined log-fold changes when no cells express the gene in either group.\n\nordered &lt;- chosen[order(chosen$mean.logFC.detected, decreasing=TRUE),]\nhead(ordered[,1:4]) # showing basic stats only, for brevity.\n\nDataFrame with 6 rows and 4 columns\n       self.average other.average self.detected other.detected\n          &lt;numeric&gt;     &lt;numeric&gt;     &lt;numeric&gt;      &lt;numeric&gt;\nVPREB3     0.908219     0.1220233      0.510692      0.0466771\nIGHD       2.368094     0.2168682      0.810063      0.0607232\nFCRLA      0.512412     0.1032791      0.371069      0.0469162\nPAX5       0.805593     0.1025424      0.519497      0.0469966\nTCL1A      1.213842     0.1031749      0.547170      0.0323989\nFCRL1      0.854202     0.0844183      0.538365      0.0396250\n\nplotExpression(sce, features=head(rownames(ordered),9),ncol=3, \n    x=\"leiden_k20\", colour_by=\"leiden_k20\")"
  },
  {
    "objectID": "labs/bioc/bioc_05_dge_scoreMarkers.html#meta-session",
    "href": "labs/bioc/bioc_05_dge_scoreMarkers.html#meta-session",
    "title": " Differential gene expression",
    "section": "4 Session info",
    "text": "4 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] dplyr_1.1.4                 igraph_2.0.3               \n [3] pheatmap_1.0.12             patchwork_1.2.0            \n [5] scran_1.30.0                scater_1.30.1              \n [7] ggplot2_3.5.1               scuttle_1.12.0             \n [9] SingleCellExperiment_1.24.0 SummarizedExperiment_1.32.0\n[11] Biobase_2.62.0              GenomicRanges_1.54.1       \n[13] GenomeInfoDb_1.38.1         IRanges_2.36.0             \n[15] S4Vectors_0.40.2            BiocGenerics_0.48.1        \n[17] MatrixGenerics_1.14.0       matrixStats_1.4.1          \n\nloaded via a namespace (and not attached):\n [1] bitops_1.0-8              gridExtra_2.3            \n [3] remotes_2.5.0             rlang_1.1.4              \n [5] magrittr_2.0.3            compiler_4.3.3           \n [7] DelayedMatrixStats_1.24.0 vctrs_0.6.5              \n [9] profvis_0.4.0             pkgconfig_2.0.3          \n[11] crayon_1.5.3              fastmap_1.2.0            \n[13] XVector_0.42.0            ellipsis_0.3.2           \n[15] labeling_0.4.3            utf8_1.2.4               \n[17] promises_1.3.0            rmarkdown_2.28           \n[19] sessioninfo_1.2.2         ggbeeswarm_0.7.2         \n[21] purrr_1.0.2               xfun_0.47                \n[23] bluster_1.12.0            zlibbioc_1.48.0          \n[25] cachem_1.1.0              beachmat_2.18.0          \n[27] jsonlite_1.8.8            later_1.3.2              \n[29] DelayedArray_0.28.0       BiocParallel_1.36.0      \n[31] irlba_2.3.5.1             parallel_4.3.3           \n[33] cluster_2.1.6             R6_2.5.1                 \n[35] RColorBrewer_1.1-3        limma_3.58.1             \n[37] pkgload_1.4.0             Rcpp_1.0.13              \n[39] knitr_1.48                usethis_3.0.0            \n[41] httpuv_1.6.15             Matrix_1.6-5             \n[43] tidyselect_1.2.1          abind_1.4-5              \n[45] yaml_2.3.10               viridis_0.6.5            \n[47] codetools_0.2-20          miniUI_0.1.1.1           \n[49] curl_5.2.1                pkgbuild_1.4.4           \n[51] lattice_0.22-6            tibble_3.2.1             \n[53] shiny_1.9.1               withr_3.0.1              \n[55] evaluate_0.24.0           urlchecker_1.0.1         \n[57] pillar_1.9.0              generics_0.1.3           \n[59] RCurl_1.98-1.16           sparseMatrixStats_1.14.0 \n[61] munsell_0.5.1             scales_1.3.0             \n[63] xtable_1.8-4              glue_1.7.0               \n[65] metapod_1.10.0            tools_4.3.3              \n[67] BiocNeighbors_1.20.0      ScaledMatrix_1.10.0      \n[69] locfit_1.5-9.9            fs_1.6.4                 \n[71] cowplot_1.1.3             grid_4.3.3               \n[73] devtools_2.4.5            edgeR_4.0.16             \n[75] colorspace_2.1-1          GenomeInfoDbData_1.2.11  \n[77] beeswarm_0.4.0            BiocSingular_1.18.0      \n[79] vipor_0.4.7               cli_3.6.3                \n[81] rsvd_1.0.5                fansi_1.0.6              \n[83] S4Arrays_1.2.0            viridisLite_0.4.2        \n[85] gtable_0.3.5              digest_0.6.37            \n[87] SparseArray_1.2.2         ggrepel_0.9.6            \n[89] dqrng_0.3.2               farver_2.1.2             \n[91] htmlwidgets_1.6.4         memoise_2.0.1            \n[93] htmltools_0.5.8.1         lifecycle_1.0.4          \n[95] httr_1.4.7                statmod_1.5.0            \n[97] mime_0.12"
  },
  {
    "objectID": "labs/bioc/bioc_05_dge_scoreMarkers.html#compare-for-each-cluster",
    "href": "labs/bioc/bioc_05_dge_scoreMarkers.html#compare-for-each-cluster",
    "title": " Differential gene expression",
    "section": "3 Compare for each cluster",
    "text": "3 Compare for each cluster\nTake all top 50 genes from each of the methods.\n\ntop50 = list()\n\ntop50$Wilc = lapply(markers_genes, function(x) rownames(x)[1:50])\ntop50$T = lapply(markers_genesT, function(x) rownames(x)[1:50])\ntop50$Bin = lapply(markers_genesB, function(x) rownames(x)[1:50])\ntop50$AUC = lapply(marker.info, function(x) rownames(x)[order(x$mean.AUC, decreasing = T)][1:50])\ntop50$cohen = lapply(marker.info, function(x) rownames(x)[order(x$mean.logFC.cohen, decreasing = T)][1:50])\ntop50$det = lapply(marker.info, function(x) rownames(x)[order(x$mean.logFC.detected, decreasing = T)][1:50])\n\n\nall.top = unlist(top50, recursive = F)\n\n\no = overlap_phyper2(all.top,all.top, bg=nrow(marker.info[[1]]), nsize = 5)\n\n\n\n\n\n\n\n\nMore non-unique markers with the scoreMarkers function.\nOne cluster at a time:\n\nfor (n in names(marker.info)){\n  tmp = lapply(top50, function(x) x[[n]])\n  o = overlap_phyper2(tmp,tmp, bg=nrow(marker.info[[1]]), nsize = 10, title = paste0(\"cluster_\",n))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMainly high overlap betwen Wilcox / T-test and AUC / Cohen."
  },
  {
    "objectID": "labs/comparison/clustering_simulations.html",
    "href": "labs/comparison/clustering_simulations.html",
    "title": "Comparison of all pipelines",
    "section": "",
    "text": "With data in place, now we can start loading libraries we will use in this tutorial.\nsuppressPackageStartupMessages({\n    library(Matrix)\n    library(ggplot2)\n    library(patchwork)\n})"
  },
  {
    "objectID": "labs/comparison/clustering_simulations.html#simulate-clusterings",
    "href": "labs/comparison/clustering_simulations.html#simulate-clusterings",
    "title": "Comparison of all pipelines",
    "section": "1 Simulate clusterings",
    "text": "1 Simulate clusterings\nDefine functions:\nCalculating the number of times each pair of cells end up in the same cluster.\n\nclusterOverlap &lt;- function(meta.df){\n  # meta.df is a data frame with clustering results in each column \n  dmats = list()\n  for (i in 1:ncol(meta.df)){\n    tmp = as(outer(meta.df[,i],meta.df[,i], FUN = \"==\")+0, \"dgCMatrix\")\n    diag(tmp) = 0 #score to self to zero.\n    dmats[[i]] = tmp\n  }\n  names(dmats) = colnames(meta.df)\n  sum.dist = Reduce(\"+\",dmats)\n  \n  # scores each clustering by adding together the individual consensus scores for the cell pairs that we have. \n  # Divide by number of cells. \n  scores = lapply(dmats, function(x) sum(x*sum.dist)/nrow(meta.df)^2)\n  scores = unlist(scores)\n  names(scores) = colnames(meta.df)\n  return(list(overlapMatrices = dmats, sumMatrix = sum.dist, scores = scores))\n}\n\n\nnC = 5:20\nnS = 5000\nnI = 50\n\nsim = mat.or.vec(nS,0)\n\nfor (c in nC){\n  for (i in 1:nI){\n    sim = cbind(sim, sample(c,nS, replace = T))\n  }\n}\n\nnclust.sim = apply(sim,2, function(x) length(unique(x)))\n\n\nstats = clusterOverlap(sim)\n\npar(mfrow = c(2,2))\nplot(nclust.sim, stats$scores)\nplot(nclust.sim, stats$scores*nclust.sim)\nplot(nclust.sim, stats$scores*sqrt(nclust.sim))\n\n\n\n\n\n\n\n\n\n1.0.1 Normalize by probability\nMore likely to have 2 cells in the same cluster if there are fewer clusters, hence the scores for lower nC should be lower.\nInstead of scaling by a factor after calculating the scores, do it per clustering so the weight of co-occurence in that clustering is weighted by the probability of 2 cells in the same cluster.\nThe probability of 2 cells ending up in the same cluster 2 times is:\n(nS/nC -1) / (nS - 1)\n\ncalculateCLProb = function(nC,nS){\n    P = (nS/nC-1) / (nS-1)\n    return(P)\n}\n\nclusterOverlapN &lt;- function(meta.df){\n  # meta.df is a data frame with clustering results in each column \n  dmats = list()\n  probs = mat.or.vec(1,ncol(meta.df))\n  for (i in 1:ncol(meta.df)){\n    tmp = as(outer(meta.df[,i],meta.df[,i], FUN = \"==\")+0, \"dgCMatrix\")\n    diag(tmp) = 0 #score to self to zero.\n    # normalize the counts by probability given number of clusters and cells.\n    prob = calculateCLProb(length(unique(meta.df[,i])),length(meta.df[,i]))\n    probs[i] = prob\n    tmp = tmp /prob\n    dmats[[i]] = tmp\n  }\n  names(dmats) = colnames(meta.df)\n  \n  # add all matrices together, divide by number of matrices.\n  sum.dist = Reduce(\"+\",dmats)/ncol(meta.df)\n  \n  # scores each clustering by adding together the individual consensus scores for the cell pairs that we have. \n  # Divide by number of cells. \n  scores = lapply(dmats, function(x) sum(x*sum.dist)/nrow(meta.df)^2)\n  scores = unlist(scores)\n  names(scores) = colnames(meta.df)\n  return(list(overlapMatrices = dmats, sumMatrix = sum.dist, scores = scores, probabilities = probs))\n}\n\n\nstatsN = clusterOverlapN(sim)\nscore.means = sapply(nC, function(x) mean(statsN$scores[nclust.sim == x]))\nscore.means\n\n [1] 1.007884 1.009433 1.010958 1.012149 1.013735 1.015181 1.016591 1.018169\n [9] 1.019725 1.021057 1.022532 1.024177 1.025482 1.027345 1.028581 1.029779\n\n\n\npar(mfrow = c(2,2))\nplot(nclust.sim, statsN$scores)\nplot(nclust.sim, statsN$scores*statsN$probabilities)\nplot(nclust.sim, statsN$scores*nclust.sim)\nplot(nclust.sim, statsN$probabilities)"
  },
  {
    "objectID": "labs/comparison/clustering_simulations.html#thoughts",
    "href": "labs/comparison/clustering_simulations.html#thoughts",
    "title": "Comparison of all pipelines",
    "section": "2 Thoughts",
    "text": "2 Thoughts\nThe scoring of pairs is higher for pairs found in clusterings with high nC, hence when scoring with the same clustering that was included when calculating the scores, the higher nCs will also give somewhat higher scores. With higher number of cells or higher number of iterations, the scores go down somewhat.\nReal clustering is not done with equal size clusters, so the distribution of cluster sizes should also matter, question is if they follow a power law or not."
  },
  {
    "objectID": "labs/comparison/clustering_simulations.html#meta-session",
    "href": "labs/comparison/clustering_simulations.html#meta-session",
    "title": "Comparison of all pipelines",
    "section": "3 Session info",
    "text": "3 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] patchwork_1.2.0 ggplot2_3.5.1   Matrix_1.6-5   \n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.5       cli_3.6.3         knitr_1.48        rlang_1.1.4      \n [5] xfun_0.47         generics_0.1.3    jsonlite_1.8.8    glue_1.7.0       \n [9] colorspace_2.1-1  htmltools_0.5.8.1 scales_1.3.0      fansi_1.0.6      \n[13] rmarkdown_2.28    grid_4.3.3        evaluate_0.24.0   munsell_0.5.1    \n[17] tibble_3.2.1      fastmap_1.2.0     yaml_2.3.10       lifecycle_1.0.4  \n[21] compiler_4.3.3    dplyr_1.1.4       pkgconfig_2.0.3   htmlwidgets_1.6.4\n[25] lattice_0.22-6    digest_0.6.37     R6_2.5.1          tidyselect_1.2.1 \n[29] utf8_1.2.4        pillar_1.9.0      magrittr_2.0.3    withr_3.0.1      \n[33] tools_4.3.3       gtable_0.3.5"
  },
  {
    "objectID": "labs/comparison/comparison_pipelines.html#save",
    "href": "labs/comparison/comparison_pipelines.html#save",
    "title": "Comparison of all pipelines",
    "section": "8 Save",
    "text": "8 Save\n\nsaveRDS(all, \"data/covid/results/merged_all.rds\")"
  },
  {
    "objectID": "labs/seurat/seurat_03_integration_basilisk.html",
    "href": "labs/seurat/seurat_03_integration_basilisk.html",
    "title": " Data Integration",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified.\nIn this tutorial we will look at different ways of integrating multiple single cell RNA-seq datasets. We will explore a few different methods to correct for batch effects across datasets. Seurat uses the data integration method presented in Comprehensive Integration of Single Cell Data, while Scran and Scanpy use a mutual Nearest neighbour method (MNN). Below you can find a list of some methods for single data integration:"
  },
  {
    "objectID": "labs/seurat/seurat_03_integration_basilisk.html#meta-int_prep",
    "href": "labs/seurat/seurat_03_integration_basilisk.html#meta-int_prep",
    "title": " Data Integration",
    "section": "1 Data preparation",
    "text": "1 Data preparation\nLet’s first load necessary libraries and the data saved in the previous lab.\n\nsuppressPackageStartupMessages({\n    library(Seurat)\n    library(ggplot2)\n    library(patchwork)\n    library(basilisk)\n})\n\n# Activate scanorama Python venv\n#reticulate::use_virtualenv(\"/opt/venv/scanorama\")\n#reticulate::py_discover_config()\n\n\n# download pre-computed data if missing or long compute\nfetch_data &lt;- TRUE\n\n# url for source and intermediate data\npath_data &lt;- \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\npath_file &lt;- \"data/covid/results/seurat_covid_qc_dr.rds\"\nif (!dir.exists(dirname(path_file))) dir.create(dirname(path_file), recursive = TRUE)\nif (fetch_data && !file.exists(path_file)) download.file(url = file.path(path_data, \"covid/results/seurat_covid_qc_dr.rds\"), destfile = path_file)\nalldata &lt;- readRDS(path_file)\nprint(names(alldata@reductions))\n\n[1] \"pca\"               \"umap\"              \"tsne\"             \n[4] \"UMAP10_on_PCA\"     \"UMAP_on_ScaleData\"\n\n\nWith Seurat5 we can split the RNA assay into multiple Layers with one count matrix and one data matrix per sample. When we then run FindVariableFeatures on the object it will run it for each of the samples separately, but also compute the overall variable features by combining their ranks.\n\n# get the variable genes from all the datasets without batch information.\nhvgs_old = VariableFeatures(alldata)\n\n# now split the object into layers\nalldata[[\"RNA\"]] &lt;- split(alldata[[\"RNA\"]], f = alldata$orig.ident)\n\n# detect HVGs\nalldata &lt;- FindVariableFeatures(alldata, selection.method = \"vst\", nfeatures = 2000, verbose = FALSE)\n\n# to get the HVGs for each layer we have to fetch them individually\ndata.layers &lt;- Layers(alldata)[grep(\"data.\",Layers(alldata))]\nprint(data.layers)\n\n[1] \"data.covid_1\"  \"data.covid_15\" \"data.covid_16\" \"data.covid_17\"\n[5] \"data.ctrl_5\"   \"data.ctrl_13\"  \"data.ctrl_14\"  \"data.ctrl_19\" \n\nhvgs_per_dataset &lt;- lapply(data.layers, function(x) VariableFeatures(alldata, layer = x) )\nnames(hvgs_per_dataset) = data.layers\n\n# also add in the variable genes that was selected on the whole dataset and the old ones \nhvgs_per_dataset$all &lt;- VariableFeatures(alldata)\nhvgs_per_dataset$old &lt;- hvgs_old\n\ntemp &lt;- unique(unlist(hvgs_per_dataset))\noverlap &lt;- sapply( hvgs_per_dataset , function(x) { temp %in% x } )\npheatmap::pheatmap(t(overlap*1),cluster_rows = F ,\n                   color = c(\"grey90\",\"grey20\"))\n\n\n\n\n\n\n\n\nAs you can see, there are a lot of genes that are variable in just one dataset. There are also some genes in the gene set that was selected using all the data that are not variable in any of the individual datasets. These are most likely genes driven by batch effects.\nA better way to select features for integration is to combine the information on variable genes across the dataset. This is what we have in the all section where the ranks of the variable features in the different datasets is combined.\nFor all downstream integration we will use this set of genes so that it is comparable across the methods. Before doing anything else we need to rerun ScaleData and PCA with that set of genes.\n\nhvgs_all = hvgs_per_dataset$all\n\nalldata = ScaleData(alldata, features = hvgs_all, vars.to.regress = c(\"percent_mito\", \"nFeature_RNA\"))\nalldata = RunPCA(alldata, features = hvgs_all, verbose = FALSE)"
  },
  {
    "objectID": "labs/seurat/seurat_03_integration_basilisk.html#cca",
    "href": "labs/seurat/seurat_03_integration_basilisk.html#cca",
    "title": " Data Integration",
    "section": "2 CCA",
    "text": "2 CCA\nIn Seurat v4 we run the integration in two steps, first finding anchors between datasets with FindIntegrationAnchors() and then running the actual integration with IntegrateData(). Since Seurat v5 this is done in a single command using the function IntegrateLayers(), we specify the name for the integration as integrated_cca.\n\nalldata &lt;- IntegrateLayers(object = alldata, \n                           method = CCAIntegration, orig.reduction = \"pca\", \n                           new.reduction = \"integrated_cca\", verbose = FALSE)\n\nWe should now have a new dimensionality reduction slot (integrated_cca) in the object:\n\nnames(alldata@reductions)\n\n[1] \"pca\"               \"umap\"              \"tsne\"             \n[4] \"UMAP10_on_PCA\"     \"UMAP_on_ScaleData\" \"integrated_cca\"   \n\n\nUsing this new integrated dimensionality reduction we can now run UMAP and tSNE on that object, and we again specify the names of the new reductions so that the old UMAP and tSNE are not overwritten.\n\nalldata &lt;- RunUMAP(alldata, reduction = \"integrated_cca\", dims = 1:30, reduction.name = \"umap_cca\")\nalldata &lt;- RunTSNE(alldata, reduction = \"integrated_cca\", dims = 1:30, reduction.name = \"tsne_cca\")\n\nnames(alldata@reductions)\n\n[1] \"pca\"               \"umap\"              \"tsne\"             \n[4] \"UMAP10_on_PCA\"     \"UMAP_on_ScaleData\" \"integrated_cca\"   \n[7] \"umap_cca\"          \"tsne_cca\"         \n\n\nWe can now plot the unintegrated and the integrated space reduced dimensions.\n\nwrap_plots(\n  DimPlot(alldata, reduction = \"pca\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"PCA raw_data\"),\n  DimPlot(alldata, reduction = \"tsne\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"tSNE raw_data\"),\n  DimPlot(alldata, reduction = \"umap\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"UMAP raw_data\"),\n  \n  DimPlot(alldata, reduction = \"integrated_cca\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"CCA integrated\"),\n  DimPlot(alldata, reduction = \"tsne_cca\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"tSNE integrated\"),\n  DimPlot(alldata, reduction = \"umap_cca\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"UMAP integrated\"),\n  ncol = 3\n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n2.1 Marker genes\nLet’s plot some marker genes for different cell types onto the embedding.\n\n\n\nMarkers\nCell Type\n\n\n\n\nCD3E\nT cells\n\n\nCD3E CD4\nCD4+ T cells\n\n\nCD3E CD8A\nCD8+ T cells\n\n\nGNLY, NKG7\nNK cells\n\n\nMS4A1\nB cells\n\n\nCD14, LYZ, CST3, MS4A7\nCD14+ Monocytes\n\n\nFCGR3A, LYZ, CST3, MS4A7\nFCGR3A+ Monocytes\n\n\nFCER1A, CST3\nDCs\n\n\n\n\nmyfeatures &lt;- c(\"CD3E\", \"CD4\", \"CD8A\", \"NKG7\", \"GNLY\", \"MS4A1\", \"CD14\", \"LYZ\", \"MS4A7\", \"FCGR3A\", \"CST3\", \"FCER1A\")\nFeaturePlot(alldata, reduction = \"umap_cca\", dims = 1:2, features = myfeatures, ncol = 4, order = T) + NoLegend() + NoAxes() + NoGrid()"
  },
  {
    "objectID": "labs/seurat/seurat_03_integration_basilisk.html#meta-dimred_harmony",
    "href": "labs/seurat/seurat_03_integration_basilisk.html#meta-dimred_harmony",
    "title": " Data Integration",
    "section": "3 Harmony",
    "text": "3 Harmony\nAn alternative method for integration is Harmony, for more details on the method, please se their paper Nat. Methods. This method runs the integration on a dimensionality reduction, in most applications the PCA. So first, we prefer to have scaling and PCA with the same set of genes that were used for the CCA integration, which we ran earlier.\nWe can use the same function IntegrateLayers() but intstead specify the method HarmonyIntegration. And as above, we run UMAP on the new reduction from Harmony.\n\nalldata &lt;- IntegrateLayers(\n  object = alldata, method = HarmonyIntegration,\n  orig.reduction = \"pca\", new.reduction = \"harmony\",\n  verbose = FALSE\n)\n\n\nalldata &lt;- RunUMAP(alldata, dims = 1:30, reduction = \"harmony\", reduction.name = \"umap_harmony\")\nDimPlot(alldata, reduction = \"umap_harmony\", group.by = \"orig.ident\") + NoAxes() + ggtitle(\"Harmony UMAP\")"
  },
  {
    "objectID": "labs/seurat/seurat_03_integration_basilisk.html#meta-dimred_scanorama",
    "href": "labs/seurat/seurat_03_integration_basilisk.html#meta-dimred_scanorama",
    "title": " Data Integration",
    "section": "4 Scanorama",
    "text": "4 Scanorama\n\n\n\n\n\n\nImportant\n\n\n\nIf you are running locally using Docker and you have a Mac with ARM chip, the Scanorama reticulate module is known to crash. In this case, you might want to skip this section.\n\n\nAnother integration method is Scanorama (see Nat. Biotech.). This method is implemented in python, but we can run it through the Reticulate package.\nWe will run it with the same set of variable genes, but first we have to create a list of all the objects per sample.\nKeep in mind that for most python tools that uses AnnData format the gene x cell matrix is transposed so that genes are rows and cells are columns.\n\n# get data matrices from all samples, with only the variable genes.\ndata.layers &lt;- Layers(alldata)[grep(\"data.\",Layers(alldata))]\nprint(data.layers)\n\n[1] \"data.covid_1\"  \"data.covid_15\" \"data.covid_16\" \"data.covid_17\"\n[5] \"data.ctrl_5\"   \"data.ctrl_13\"  \"data.ctrl_14\"  \"data.ctrl_19\" \n\nassaylist &lt;- lapply(data.layers, function(x) t(as.matrix(LayerData(alldata, layer = x)[hvgs_all,])))\ngenelist =  rep(list(hvgs_all),length(assaylist))\n\nlapply(assaylist,dim)\n\n[[1]]\n[1]  875 2000\n\n[[2]]\n[1]  549 2000\n\n[[3]]\n[1]  357 2000\n\n[[4]]\n[1] 1057 2000\n\n[[5]]\n[1] 1033 2000\n\n[[6]]\n[1] 1126 2000\n\n[[7]]\n[1]  996 2000\n\n[[8]]\n[1] 1141 2000\n\n\nThen, we use the scanorama function through reticulate. The integrated data is added back into the Seurat object as a new Reduction.\n\ntmp = \"/Users/asabjor/miniconda3/envs/scanpy_2024_nopip\"\n\nintdata = basiliskRun(env=tmp, fun=function(datas, genes) {\n  scanorama &lt;- reticulate::import(\"scanorama\")\n  integrated.data &lt;- scanorama$integrate(datasets_full = datas,\n                                         genes_list = genes )\n  return(integrated.data)\n}, datas = assaylist, genes = genelist, testload=\"scanorama\")\n\nFound 2000 genes among all datasets\n[[0.         0.58287796 0.53501401 0.2384106  0.55082285 0.37142857\n  0.35542857 0.11771429]\n [0.         0.         0.72313297 0.32786885 0.42013553 0.24043716\n  0.25318761 0.15775635]\n [0.         0.         0.         0.24089636 0.46778711 0.52941176\n  0.43137255 0.28851541]\n [0.         0.         0.         0.         0.2536302  0.07284768\n  0.14758751 0.17002629]\n [0.         0.         0.         0.         0.         0.82768635\n  0.64956438 0.33917616]\n [0.         0.         0.         0.         0.         0.\n  0.76305221 0.51270815]\n [0.         0.         0.         0.         0.         0.\n  0.         0.69763365]\n [0.         0.         0.         0.         0.         0.\n  0.         0.        ]]\nProcessing datasets (4, 5)\nProcessing datasets (5, 6)\nProcessing datasets (1, 2)\nProcessing datasets (6, 7)\nProcessing datasets (4, 6)\nProcessing datasets (0, 1)\nProcessing datasets (0, 4)\nProcessing datasets (0, 2)\nProcessing datasets (2, 5)\nProcessing datasets (5, 7)\nProcessing datasets (2, 4)\nProcessing datasets (2, 6)\nProcessing datasets (1, 4)\nProcessing datasets (0, 5)\nProcessing datasets (0, 6)\nProcessing datasets (4, 7)\nProcessing datasets (1, 3)\nProcessing datasets (2, 7)\nProcessing datasets (3, 4)\nProcessing datasets (1, 6)\nProcessing datasets (2, 3)\nProcessing datasets (1, 5)\nProcessing datasets (0, 3)\nProcessing datasets (3, 7)\nProcessing datasets (1, 7)\nProcessing datasets (3, 6)\nProcessing datasets (0, 7)\n\n# Now we create a new dim reduction object in the format that Seurat uses\nintdimred &lt;- do.call(rbind, intdata[[1]])\n\ncolnames(intdimred) &lt;- paste0(\"Scanorama_\", 1:100)\nrownames(intdimred) &lt;- colnames(alldata)\n\n# Add standard deviations in order to draw Elbow Plots in Seurat\nstdevs &lt;- apply(intdimred, MARGIN = 2, FUN = sd)\n\n# Create a new dim red object.\nalldata[[\"scanorama\"]] &lt;- CreateDimReducObject(\n  embeddings = intdimred,\n  stdev      = stdevs,\n  key        = \"Scanorama_\",\n  assay      = \"RNA\")\n\n\n#Here we use all PCs computed from Scanorama for UMAP calculation\nalldata &lt;- RunUMAP(alldata, dims = 1:100, reduction = \"scanorama\",reduction.name = \"umap_scanorama\")\n\n\n DimPlot(alldata, reduction = \"umap_scanorama\", group.by = \"orig.ident\") + NoAxes() + ggtitle(\"Scanorama UMAP\")"
  },
  {
    "objectID": "labs/seurat/seurat_03_integration_basilisk.html#overview-all-methods",
    "href": "labs/seurat/seurat_03_integration_basilisk.html#overview-all-methods",
    "title": " Data Integration",
    "section": "5 Overview all methods",
    "text": "5 Overview all methods\nNow we will plot UMAPS with all three integration methods side by side.\n\np1 &lt;- DimPlot(alldata, reduction = \"umap\", group.by = \"orig.ident\") + ggtitle(\"UMAP raw_data\")\np2 &lt;- DimPlot(alldata, reduction = \"umap_cca\", group.by = \"orig.ident\") + ggtitle(\"UMAP CCA\")\np3 &lt;- DimPlot(alldata, reduction = \"umap_harmony\", group.by = \"orig.ident\") + ggtitle(\"UMAP Harmony\")\np4 &lt;- DimPlot(alldata, reduction = \"umap_scanorama\", group.by = \"orig.ident\")+ggtitle(\"UMAP Scanorama\")\n\nwrap_plots(p1, p2, p3, p4, nrow = 2) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nLook at the different integration results, which one do you think looks the best? How would you motivate selecting one method over the other? How do you think you could best evaluate if the integration worked well?\n\n\nLet’s save the integrated data for further analysis.\n\nsaveRDS(alldata,\"data/covid/results/seurat_covid_qc_dr_int.rds\")"
  },
  {
    "objectID": "labs/seurat/seurat_03_integration_basilisk.html#extra-task",
    "href": "labs/seurat/seurat_03_integration_basilisk.html#extra-task",
    "title": " Data Integration",
    "section": "6 Extra task",
    "text": "6 Extra task\nYou have now done the Seurat integration with CCA which is quite slow. There are other options in the FindIntegrationAnchors() function. Try rerunning the integration with rpca and/or rlsi and create a new UMAP. Compare the results."
  },
  {
    "objectID": "labs/seurat/seurat_03_integration_basilisk.html#meta-session",
    "href": "labs/seurat/seurat_03_integration_basilisk.html#meta-session",
    "title": " Data Integration",
    "section": "7 Session info",
    "text": "7 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] basilisk_1.14.1    patchwork_1.2.0    ggplot2_3.5.1      Seurat_5.1.0      \n[5] SeuratObject_5.0.2 sp_2.1-4          \n\nloaded via a namespace (and not attached):\n  [1] deldir_2.0-4           pbapply_1.7-2          gridExtra_2.3         \n  [4] rlang_1.1.4            magrittr_2.0.3         RcppAnnoy_0.0.22      \n  [7] spatstat.geom_3.2-9    matrixStats_1.4.1      ggridges_0.5.6        \n [10] compiler_4.3.3         dir.expiry_1.10.0      png_0.1-8             \n [13] vctrs_0.6.5            reshape2_1.4.4         stringr_1.5.1         \n [16] pkgconfig_2.0.3        fastmap_1.2.0          labeling_0.4.3        \n [19] utf8_1.2.4             promises_1.3.0         rmarkdown_2.28        \n [22] purrr_1.0.2            xfun_0.47              jsonlite_1.8.8        \n [25] goftest_1.2-3          later_1.3.2            spatstat.utils_3.1-0  \n [28] irlba_2.3.5.1          parallel_4.3.3         cluster_2.1.6         \n [31] R6_2.5.1               ica_1.0-3              stringi_1.8.4         \n [34] RColorBrewer_1.1-3     spatstat.data_3.1-2    reticulate_1.39.0     \n [37] parallelly_1.38.0      lmtest_0.9-40          scattermore_1.2       \n [40] Rcpp_1.0.13            knitr_1.48             tensor_1.5            \n [43] future.apply_1.11.2    zoo_1.8-12             sctransform_0.4.1     \n [46] httpuv_1.6.15          Matrix_1.6-5           splines_4.3.3         \n [49] igraph_2.0.3           tidyselect_1.2.1       abind_1.4-5           \n [52] yaml_2.3.10            spatstat.random_3.2-3  codetools_0.2-20      \n [55] miniUI_0.1.1.1         spatstat.explore_3.2-6 listenv_0.9.1         \n [58] lattice_0.22-6         tibble_3.2.1           plyr_1.8.9            \n [61] basilisk.utils_1.14.1  withr_3.0.1            shiny_1.9.1           \n [64] ROCR_1.0-11            evaluate_0.24.0        Rtsne_0.17            \n [67] future_1.34.0          fastDummies_1.7.4      survival_3.7-0        \n [70] polyclip_1.10-7        fitdistrplus_1.2-1     filelock_1.0.3        \n [73] pillar_1.9.0           KernSmooth_2.23-24     plotly_4.10.4         \n [76] generics_0.1.3         RcppHNSW_0.6.0         munsell_0.5.1         \n [79] scales_1.3.0           globals_0.16.3         xtable_1.8-4          \n [82] RhpcBLASctl_0.23-42    glue_1.7.0             pheatmap_1.0.12       \n [85] lazyeval_0.2.2         tools_4.3.3            data.table_1.15.4     \n [88] RSpectra_0.16-2        RANN_2.6.2             leiden_0.4.3.1        \n [91] dotCall64_1.1-1        cowplot_1.1.3          grid_4.3.3            \n [94] tidyr_1.3.1            colorspace_2.1-1       nlme_3.1-165          \n [97] cli_3.6.3              spatstat.sparse_3.1-0  spam_2.10-0           \n[100] fansi_1.0.6            viridisLite_0.4.2      dplyr_1.1.4           \n[103] uwot_0.1.16            gtable_0.3.5           digest_0.6.37         \n[106] progressr_0.14.0       ggrepel_0.9.6          farver_2.1.2          \n[109] htmlwidgets_1.6.4      htmltools_0.5.8.1      lifecycle_1.0.4       \n[112] httr_1.4.7             mime_0.12              harmony_1.2.1         \n[115] MASS_7.3-60.0.1"
  },
  {
    "objectID": "labs/seurat/seurat_03_integration.html#meta-dimred_harmony",
    "href": "labs/seurat/seurat_03_integration.html#meta-dimred_harmony",
    "title": " Data Integration",
    "section": "3 Harmony",
    "text": "3 Harmony\nAn alternative method for integration is Harmony, for more details on the method, please se their paper Nat. Methods. This method runs the integration on a dimensionality reduction, in most applications the PCA. So first, we prefer to have scaling and PCA with the same set of genes that were used for the CCA integration, which we ran earlier.\nWe can use the same function IntegrateLayers() but intstead specify the method HarmonyIntegration. And as above, we run UMAP on the new reduction from Harmony.\n\nalldata &lt;- IntegrateLayers(\n  object = alldata, method = HarmonyIntegration,\n  orig.reduction = \"pca\", new.reduction = \"harmony\",\n  verbose = FALSE\n)\n\n\nalldata &lt;- RunUMAP(alldata, dims = 1:30, reduction = \"harmony\", reduction.name = \"umap_harmony\")\nDimPlot(alldata, reduction = \"umap_harmony\", group.by = \"orig.ident\") + NoAxes() + ggtitle(\"Harmony UMAP\")"
  },
  {
    "objectID": "labs/seurat/seurat_03_integration.html#meta-dimred_scanorama",
    "href": "labs/seurat/seurat_03_integration.html#meta-dimred_scanorama",
    "title": " Data Integration",
    "section": "4 Scanorama",
    "text": "4 Scanorama\nAnother integration method is Scanorama (see Nat. Biotech.). This method is implemented in python, but we can run it through the Reticulate package.\nWe will run it with the same set of variable genes, but first we have to create a list of all the objects per sample.\nKeep in mind that for most python tools that uses AnnData format the gene x cell matrix is transposed so that genes are rows and cells are columns.\n\n# get data matrices from all samples, with only the variable genes.\ndata.layers &lt;- Layers(alldata)[grep(\"data.\",Layers(alldata))]\nprint(data.layers)\n\n[1] \"data.covid_1\"  \"data.covid_15\" \"data.covid_16\" \"data.covid_17\"\n[5] \"data.ctrl_5\"   \"data.ctrl_13\"  \"data.ctrl_14\"  \"data.ctrl_19\" \n\nassaylist &lt;- lapply(data.layers, function(x) t(as.matrix(LayerData(alldata, layer = x)[hvgs_all,])))\ngenelist =  rep(list(hvgs_all),length(assaylist))\n\nlapply(assaylist,dim)\n\n[[1]]\n[1]  875 2000\n\n[[2]]\n[1]  549 2000\n\n[[3]]\n[1]  357 2000\n\n[[4]]\n[1] 1057 2000\n\n[[5]]\n[1] 1033 2000\n\n[[6]]\n[1] 1126 2000\n\n[[7]]\n[1]  996 2000\n\n[[8]]\n[1] 1141 2000\n\n\nScanorama is implemented in python, but through reticulate we can load python packages and run python functions. In this case we also use the basilisk package for a more clean activation of python environment.\nAt the top of this script, we set the variable condapath to point to the conda environment where scanorama is included.\n\n# run scanorama via basilisk with assaylis and genelist as input.\nintegrated.data = basiliskRun(env=condapath, fun=function(datas, genes) {\n  scanorama &lt;- reticulate::import(\"scanorama\")\n  output &lt;- scanorama$integrate(datasets_full = datas,\n                                         genes_list = genes )\n  return(output)\n}, datas = assaylist, genes = genelist, testload=\"scanorama\")\n\nFound 2000 genes among all datasets\n[[0.         0.58287796 0.53501401 0.2384106  0.55082285 0.37142857\n  0.35542857 0.11771429]\n [0.         0.         0.72313297 0.32786885 0.42013553 0.24043716\n  0.25318761 0.15775635]\n [0.         0.         0.         0.24089636 0.46778711 0.52941176\n  0.43137255 0.28851541]\n [0.         0.         0.         0.         0.2536302  0.07284768\n  0.14758751 0.17002629]\n [0.         0.         0.         0.         0.         0.82768635\n  0.64956438 0.33917616]\n [0.         0.         0.         0.         0.         0.\n  0.76305221 0.51270815]\n [0.         0.         0.         0.         0.         0.\n  0.         0.69763365]\n [0.         0.         0.         0.         0.         0.\n  0.         0.        ]]\nProcessing datasets (4, 5)\nProcessing datasets (5, 6)\nProcessing datasets (1, 2)\nProcessing datasets (6, 7)\nProcessing datasets (4, 6)\nProcessing datasets (0, 1)\nProcessing datasets (0, 4)\nProcessing datasets (0, 2)\nProcessing datasets (2, 5)\nProcessing datasets (5, 7)\nProcessing datasets (2, 4)\nProcessing datasets (2, 6)\nProcessing datasets (1, 4)\nProcessing datasets (0, 5)\nProcessing datasets (0, 6)\nProcessing datasets (4, 7)\nProcessing datasets (1, 3)\nProcessing datasets (2, 7)\nProcessing datasets (3, 4)\nProcessing datasets (1, 6)\nProcessing datasets (2, 3)\nProcessing datasets (1, 5)\nProcessing datasets (0, 3)\nProcessing datasets (3, 7)\nProcessing datasets (1, 7)\nProcessing datasets (3, 6)\nProcessing datasets (0, 7)\n\n# Now we create a new dim reduction object in the format that Seurat uses\nintdimred &lt;- do.call(rbind, integrated.data[[1]])\ncolnames(intdimred) &lt;- paste0(\"Scanorama_\", 1:100)\nrownames(intdimred) &lt;- colnames(alldata)\n\n# Add standard deviations in order to draw Elbow Plots in Seurat\nstdevs &lt;- apply(intdimred, MARGIN = 2, FUN = sd)\n\n# Create a new dim red object.\nalldata[[\"scanorama\"]] &lt;- CreateDimReducObject(\n  embeddings = intdimred,\n  stdev      = stdevs,\n  key        = \"Scanorama_\",\n  assay      = \"RNA\")\n\nTry the same but using counts instead of data.\n\n# get count matrices from all samples, with only the variable genes.\ncount.layers &lt;- Layers(alldata)[grep(\"counts.\",Layers(alldata))]\nprint(count.layers)\n\n[1] \"counts.covid_1\"  \"counts.covid_15\" \"counts.covid_16\" \"counts.covid_17\"\n[5] \"counts.ctrl_5\"   \"counts.ctrl_13\"  \"counts.ctrl_14\"  \"counts.ctrl_19\" \n\nassaylist &lt;- lapply(count.layers, function(x) t(as.matrix(LayerData(alldata, layer = x)[hvgs_all,])))\n\n# run scanorama via basilisk with assaylis and genelist as input.\nintegrated.data = basiliskRun(env=condapath, fun=function(datas, genes) {\n  scanorama &lt;- reticulate::import(\"scanorama\")\n  output &lt;- scanorama$integrate(datasets_full = datas,\n                                         genes_list = genes )\n  return(output)\n}, datas = assaylist, genes = genelist, testload=\"scanorama\")\n\nFound 2000 genes among all datasets\n[[0.         0.48269581 0.50140056 0.248      0.51694095 0.54857143\n  0.45257143 0.22436459]\n [0.         0.         0.72495446 0.55373406 0.29690346 0.18032787\n  0.22222222 0.29973707]\n [0.         0.         0.         0.4789916  0.33053221 0.37815126\n  0.33613445 0.44817927]\n [0.         0.         0.         0.         0.31752178 0.17123936\n  0.1551561  0.26380368]\n [0.         0.         0.         0.         0.         0.73088093\n  0.64181994 0.39526731]\n [0.         0.         0.         0.         0.         0.\n  0.79129663 0.66432954]\n [0.         0.         0.         0.         0.         0.\n  0.         0.67572305]\n [0.         0.         0.         0.         0.         0.\n  0.         0.        ]]\nProcessing datasets (5, 6)\nProcessing datasets (4, 5)\nProcessing datasets (1, 2)\nProcessing datasets (6, 7)\nProcessing datasets (5, 7)\nProcessing datasets (4, 6)\nProcessing datasets (1, 3)\nProcessing datasets (0, 5)\nProcessing datasets (0, 4)\nProcessing datasets (0, 2)\nProcessing datasets (0, 1)\nProcessing datasets (2, 3)\nProcessing datasets (0, 6)\nProcessing datasets (2, 7)\nProcessing datasets (4, 7)\nProcessing datasets (2, 5)\nProcessing datasets (2, 6)\nProcessing datasets (2, 4)\nProcessing datasets (3, 4)\nProcessing datasets (1, 7)\nProcessing datasets (1, 4)\nProcessing datasets (3, 7)\nProcessing datasets (0, 3)\nProcessing datasets (0, 7)\nProcessing datasets (1, 6)\nProcessing datasets (1, 5)\nProcessing datasets (3, 5)\nProcessing datasets (3, 6)\n\n# Now we create a new dim reduction object in the format that Seurat uses\n# The scanorama output has 100 dimensions.\nintdimred &lt;- do.call(rbind, integrated.data[[1]])\ncolnames(intdimred) &lt;- paste0(\"Scanorama_\", 1:100)\nrownames(intdimred) &lt;- colnames(alldata)\n\n# Add standard deviations in order to draw Elbow Plots in Seurat\nstdevs &lt;- apply(intdimred, MARGIN = 2, FUN = sd)\n\n# Create a new dim red object.\nalldata[[\"scanoramaC\"]] &lt;- CreateDimReducObject(\n  embeddings = intdimred,\n  stdev      = stdevs,\n  key        = \"Scanorama_\",\n  assay      = \"RNA\")\n\n\n#Here we use all PCs computed from Scanorama for UMAP calculation\nalldata &lt;- RunUMAP(alldata, dims = 1:100, reduction = \"scanorama\",reduction.name = \"umap_scanorama\")\nalldata &lt;- RunUMAP(alldata, dims = 1:100, reduction = \"scanoramaC\",reduction.name = \"umap_scanoramaC\")\n\n\np1 = DimPlot(alldata, reduction = \"umap_scanorama\", group.by = \"orig.ident\") + NoAxes() + ggtitle(\"Scanorama UMAP\")\np2 = DimPlot(alldata, reduction = \"umap_scanoramaC\", group.by = \"orig.ident\") + NoAxes() + ggtitle(\"ScanoramaC UMAP\")\n\nwrap_plots(p1,p2)"
  },
  {
    "objectID": "labs/bioc/bioc_03_integration.html#fastmnn",
    "href": "labs/bioc/bioc_03_integration.html#fastmnn",
    "title": " Data Integration",
    "section": "2 fastMNN",
    "text": "2 fastMNN\nThe mutual nearest neighbors (MNN) approach within the scran package utilizes a novel approach to adjust for batch effects. The fastMNN() function returns a representation of the data with reduced dimensionality, which can be used in a similar fashion to other lower-dimensional representations such as PCA. In particular, this representation can be used for downstream methods such as clustering. The BNPARAM can be used to specify the specific nearest neighbors method to use from the BiocNeighbors package. Here we make use of the Annoy library via the BiocNeighbors::AnnoyParam() argument. We save the reduced-dimension MNN representation into the reducedDims slot of our sce object.\n\nmnn_out &lt;- batchelor::fastMNN(sce, subset.row = hvgs, batch = factor(sce$sample), k = 20, d = 50)\n\n\n\n\n\n\n\nCaution\n\n\n\nfastMNN() does not produce a batch-corrected expression matrix.\n\n\nWe will take the reduced dimension in the new mnn_out object and add it into the original sce object.\n\nmnn_dim &lt;- reducedDim(mnn_out, \"corrected\")\nreducedDim(sce, \"MNN\") &lt;- mnn_dim\n\nWe can observe that a new assay slot is now created under the name MNN.\n\nreducedDims(sce)\n\nList of length 9\nnames(9): PCA UMAP tSNE_on_PCA UMAP_on_PCA ... KNN UMAP_on_Graph MNN\n\n\nThus, the result from fastMNN() should solely be treated as a reduced dimensionality representation, suitable for direct plotting, TSNE/UMAP, clustering, and trajectory analysis that relies on such results.\n\nset.seed(42)\nsce &lt;- runTSNE(sce, dimred = \"MNN\", n_dimred = 50, perplexity = 30, name = \"tSNE_on_MNN\")\nsce &lt;- runUMAP(sce, dimred = \"MNN\", n_dimred = 50, ncomponents = 2, name = \"UMAP_on_MNN\")\n\nWe can now plot the unintegrated and the integrated space reduced dimensions.\n\nwrap_plots(\n    plotReducedDim(sce, dimred = \"PCA\", colour_by = \"sample\", point_size = 0.6) + ggplot2::ggtitle(label = \"PCA\"),\n    plotReducedDim(sce, dimred = \"tSNE_on_PCA\", colour_by = \"sample\", point_size = 0.6) + ggplot2::ggtitle(label = \"tSNE_on_PCA\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_PCA\", colour_by = \"sample\", point_size = 0.6) + ggplot2::ggtitle(label = \"UMAP_on_PCA\"),\n    plotReducedDim(sce, dimred = \"MNN\", colour_by = \"sample\", point_size = 0.6) + ggplot2::ggtitle(label = \"MNN\"),\n    plotReducedDim(sce, dimred = \"tSNE_on_MNN\", colour_by = \"sample\", point_size = 0.6) + ggplot2::ggtitle(label = \"tSNE_on_MNN\"),\n    plotReducedDim(sce, dimred = \"UMAP_on_MNN\", colour_by = \"sample\", point_size = 0.6) + ggplot2::ggtitle(label = \"UMAP_on_MNN\"),\n    ncol = 3\n) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nLet’s plot some marker genes for different cell types onto the embedding.\n\n\n\nMarkers\nCell Type\n\n\n\n\nCD3E\nT cells\n\n\nCD3E CD4\nCD4+ T cells\n\n\nCD3E CD8A\nCD8+ T cells\n\n\nGNLY, NKG7\nNK cells\n\n\nMS4A1\nB cells\n\n\nCD14, LYZ, CST3, MS4A7\nCD14+ Monocytes\n\n\nFCGR3A, LYZ, CST3, MS4A7\nFCGR3A+ Monocytes\n\n\nFCER1A, CST3\nDCs\n\n\n\n\nplotlist &lt;- list()\nfor (i in c(\"CD3E\", \"CD4\", \"CD8A\", \"NKG7\", \"GNLY\", \"MS4A1\", \"CD14\", \"LYZ\", \"MS4A7\", \"FCGR3A\", \"CST3\", \"FCER1A\")) {\n    plotlist[[i]] &lt;- plotReducedDim(sce, dimred = \"UMAP_on_MNN\", colour_by = i, by_exprs_values = \"logcounts\", point_size = 0.6) +\n        scale_fill_gradientn(colours = colorRampPalette(c(\"grey90\", \"orange3\", \"firebrick\", \"firebrick\", \"red\", \"red\"))(10)) +\n        ggtitle(label = i) + theme(plot.title = element_text(size = 20))\n}\nwrap_plots(plotlist = plotlist, ncol = 3)"
  },
  {
    "objectID": "labs/comparison/comparison_dge.html",
    "href": "labs/comparison/comparison_dge.html",
    "title": "Comparison of DGE results",
    "section": "",
    "text": "force_rerun = FALSE\nif (force_rerun){\n  reticulate::use_condaenv(\"/Users/asabjor/miniconda3/envs/scanpy_2024_nopip/\")\n  reticulate::py_config()\n}\nLoad libraries\nsuppressPackageStartupMessages({\n    library(Seurat)\n    library(Matrix)\n    library(ggplot2)\n    library(patchwork)\n    library(scran)\n    library(basilisk)\n    library(basilisk.utils)\n    library(zellkonverter)\n    library(ComplexHeatmap)\n})\n\ndevtools::source_url(\"https://raw.githubusercontent.com/asabjorklund/single_cell_R_scripts/main/overlap_phyper_v2.R\")\nLoad data\npath_results &lt;- \"data/covid/results\"\nall = readRDS(file.path(path_results,\"merged_all.rds\"))\nwrap_plots(\n    DimPlot(all, group.by = \"orig.ident\", reduction = \"umap_harmony\") + NoAxes() + ggtitle(\"Seurat harmony\"),\n    DimPlot(all, group.by = \"orig.ident\", reduction = \"umap_bioc_harmony\") + NoAxes() + ggtitle(\"Bioc harmony\"),\n    DimPlot(all, group.by = \"orig.ident\", reduction = \"umap_scpy_harmony\") + NoAxes() + ggtitle(\"Scanpy harmony\"),\n    ncol = 3\n)\nSelect one clustering and use that for DGE detection in all the different methods. Use clustering with Seurat and louvain_0.5.\nsel.clust = \"RNA_snn_res.0.5\"\nall$clusters = all[[sel.clust]]\n\n\nwrap_plots(\n    DimPlot(all, group.by = sel.clust, reduction = \"umap_harmony\", label = T) + NoAxes() + ggtitle(\"Seurat harmony\"),\n    DimPlot(all, group.by = sel.clust, reduction = \"umap_bioc_harmony\", label = T) + NoAxes() + ggtitle(\"Bioc harmony\"),\n    DimPlot(all, group.by = sel.clust, reduction = \"umap_scpy_harmony\", label = T) + NoAxes() + ggtitle(\"Scanpy harmony\"),\n    ncol = 3\n) + plot_layout(guides=\"collect\")\n# Also merge the layers\nall &lt;- JoinLayers(object = all, layers = c(\"data\",\"counts\"))"
  },
  {
    "objectID": "labs/comparison/comparison_dge.html#seurat-dge",
    "href": "labs/comparison/comparison_dge.html#seurat-dge",
    "title": "Comparison of DGE results",
    "section": "1 Seurat DGE",
    "text": "1 Seurat DGE\nRun seurat FindAllMarkers with Wilcoxon, both the new implementation (wilcox) and similar to seurat v4 (wilcox_limma), MAST and T-test.\n\noutfile = file.path(path_results, \"seurat_dge.rds\")\nif (file.exists(outfile) & !force_rerun ){ \n  markersS = readRDS(outfile)\n}else {\n\n  markersS = list()\n  markersS$wilc = FindAllMarkers(all.sub, test.use = \"wilcox\", only.pos = T)\n  markersS$wilcL = FindAllMarkers(all.sub, test.use = \"wilcox_limma\", only.pos = T)\n  markersS$Ttest = FindAllMarkers(all.sub, test.use = \"t\", only.pos = T)\n  markersS$mast = FindAllMarkers(all.sub, test.use = \"MAST\", only.pos = T)\n  saveRDS(markersS, file = outfile)\n}"
  },
  {
    "objectID": "labs/comparison/comparison_dge.html#bioc-dge",
    "href": "labs/comparison/comparison_dge.html#bioc-dge",
    "title": "Comparison of DGE results",
    "section": "2 Bioc DGE",
    "text": "2 Bioc DGE\nFor Bioc no need to do the subsampling, if not for speed. Still use the subsampled object for comparable numbers of cells.\nRun with Wilcoxon, T-test and Binomial test.\n\nsce = as.SingleCellExperiment(all.sub)\n\n\noutfile = file.path(path_results, \"bioc_dge.rds\")\nif (file.exists(outfile) & !force_rerun ){ \n  markersB = readRDS(outfile)\n}else {\n\n  markersB = list()\n  markersB$wilc&lt;- scran::findMarkers( sce, groups = sce$clusters,\n    test.type = \"wilcox\", pval.type = \"all\",     direction = \"up\")\n\n  markersB$Ttest&lt;- scran::findMarkers( sce, groups = sce$clusters,\n    test.type = \"t\", pval.type = \"all\",     direction = \"up\")\n\n  markersB$binom&lt;- scran::findMarkers( sce, groups = sce$clusters,\n    test.type = \"binom\", pval.type = \"all\",     direction = \"up\")\n  \n  saveRDS(markersB, file = outfile)\n}"
  },
  {
    "objectID": "labs/comparison/comparison_dge.html#scanpy-dge",
    "href": "labs/comparison/comparison_dge.html#scanpy-dge",
    "title": "Comparison of DGE results",
    "section": "3 Scanpy DGE",
    "text": "3 Scanpy DGE\nFirst, save SCE as an h5ad matrix. with zellkonverter. Then run python code with basilisk.\n## Python code example\nimport scanpy \n\nfpath = \"/Users/asabjor/courses/course_git/temp/workshop-scRNAseq/labs/comparison/data/covid/results/merged_all.h5ad\"\nadata = scanpy.read_h5ad(fpath)\n\nprint(adata.shape)\nprint(adata.X[1:10,1:10])\n\nscanpy.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\nscanpy.pp.log1p(adata)\nprint(adata.shape)\nprint(adata.X[1:10,1:10])\n    \nscanpy.tl.rank_genes_groups(adata, 'clusters', method='t-test', key_added = \"Ttest\")\nprint(adata.uns.Ttest.names[1:10])\nscanpy$tl$rank_genes_groups(adata, 'clusters', method='t-test_overestim_var', key_added = \"Ttest_o\")\nscanpy$tl$rank_genes_groups(adata, 'clusters', method='wilcoxon', key_added = \"wilc\")\nscanpy$pp$scale(adata) # scale before logreg.\nscanpy$tl$rank_genes_groups(adata, 'clusters', method='logreg', key_added = \"logreg\")\n\n\noutfile = file.path(path_results, \"scanpy_dge.rds\")\nif (file.exists(outfile) & !force_rerun ){ \n  dge.scanpy = readRDS(outfile)\n}else {\nzellkonverter::writeH5AD(sce, file.path(path_results,\"merged_all.h5ad\"))\nreticulate::py_config()\n\npenv = \"/Users/asabjor/miniconda3/envs/scanpy_2024_nopip\"\ndge.scanpy = basiliskRun(env=penv, fun=function(fpath) {\n    scanpy &lt;- reticulate::import(\"scanpy\")\n    adata = scanpy$read_h5ad(fpath)\n    \n    output = list()\n    output$shape1 = adata$shape\n    output$head1 = adata$X[1:10,1:10]\n    print(adata$shape)\n    print(adata$X[1:10,1:10])\n    scanpy$pp$normalize_per_cell(adata, counts_per_cell_after=1e4)\n    scanpy$pp$log1p(adata)\n    print(adata$shape)\n    print(adata$X[1:10,1:10])\n    output$shape2 = adata$shape\n    output$head2 = adata$X[1:10,1:10]\n    \n    scanpy$tl$rank_genes_groups(adata, 'clusters', method='t-test', key_added = \"Ttest\")\n    print(adata$uns['Ttest']['names'])\n    scanpy$tl$rank_genes_groups(adata, 'clusters', method='t-test_overestim_var', key_added = \"Ttest_o\")\n    scanpy$tl$rank_genes_groups(adata, 'clusters', method='wilcoxon', key_added = \"wilc\")\n    scanpy$pp$scale(adata) # scale before logreg.\n    scanpy$tl$rank_genes_groups(adata, 'clusters', method='logreg', key_added = \"logreg\")\n    output$Ttest = adata$uns['Ttest']\n    return(adata$uns)\n    \n}, fpath = \"/Users/asabjor/courses/course_git/temp/workshop-scRNAseq/labs/comparison/data/covid/results/merged_all.h5ad\",  testload=\"scanpy\")\nreticulate::py_config()\n\nsaveRDS(dge.scanpy, file = outfile)\n}\n\nParse all scanpy results into dataframes\n\nscanpy.tests = names(dge.scanpy)[-1:-2]\nclusters = sort(unique(all$clusters))\n\n# convert from lists to dfs.\nmarkers_scanpy = list()\nfor (tname in scanpy.tests){\n  x = dge.scanpy[[tname]]\n  markers_scanpy[[tname]] = list()\n  for (i in 1:length(clusters)){ \n    df = data.frame(Reduce(cbind, lapply(x[-1:-2], function(y) { y[,i]})))\n    colnames(df) = names(x[-1:-2])\n    rownames(df) = x$names[,i]\n    markers_scanpy[[tname]][[i]] = df\n  }\n}"
  },
  {
    "objectID": "labs/comparison/comparison_dge.html#meta-session",
    "href": "labs/comparison/comparison_dge.html#meta-session",
    "title": "Comparison of DGE results",
    "section": "6 Session info",
    "text": "6 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] grid      stats4    stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] pheatmap_1.0.12             ComplexHeatmap_2.18.0      \n [3] zellkonverter_1.12.1        basilisk.utils_1.14.1      \n [5] basilisk_1.14.1             scran_1.30.0               \n [7] scuttle_1.12.0              SingleCellExperiment_1.24.0\n [9] SummarizedExperiment_1.32.0 Biobase_2.62.0             \n[11] GenomicRanges_1.54.1        GenomeInfoDb_1.38.1        \n[13] IRanges_2.36.0              S4Vectors_0.40.2           \n[15] BiocGenerics_0.48.1         MatrixGenerics_1.14.0      \n[17] matrixStats_1.4.1           patchwork_1.2.0            \n[19] ggplot2_3.5.1               Matrix_1.6-5               \n[21] Seurat_5.1.0                SeuratObject_5.0.2         \n[23] sp_2.1-4                   \n\nloaded via a namespace (and not attached):\n  [1] RcppAnnoy_0.0.22          splines_4.3.3            \n  [3] later_1.3.2               bitops_1.0-8             \n  [5] filelock_1.0.3            tibble_3.2.1             \n  [7] polyclip_1.10-7           fastDummies_1.7.4        \n  [9] lifecycle_1.0.4           doParallel_1.0.17        \n [11] edgeR_4.0.16              globals_0.16.3           \n [13] lattice_0.22-6            MASS_7.3-60.0.1          \n [15] magrittr_2.0.3            limma_3.58.1             \n [17] plotly_4.10.4             rmarkdown_2.28           \n [19] remotes_2.5.0             yaml_2.3.10              \n [21] metapod_1.10.0            httpuv_1.6.15            \n [23] sctransform_0.4.1         spam_2.10-0              \n [25] sessioninfo_1.2.2         pkgbuild_1.4.4           \n [27] spatstat.sparse_3.1-0     reticulate_1.39.0        \n [29] cowplot_1.1.3             pbapply_1.7-2            \n [31] RColorBrewer_1.1-3        pkgload_1.4.0            \n [33] abind_1.4-5               zlibbioc_1.48.0          \n [35] Rtsne_0.17                purrr_1.0.2              \n [37] RCurl_1.98-1.16           circlize_0.4.16          \n [39] GenomeInfoDbData_1.2.11   ggrepel_0.9.6            \n [41] irlba_2.3.5.1             listenv_0.9.1            \n [43] spatstat.utils_3.1-0      goftest_1.2-3            \n [45] RSpectra_0.16-2           spatstat.random_3.2-3    \n [47] dqrng_0.3.2               fitdistrplus_1.2-1       \n [49] parallelly_1.38.0         DelayedMatrixStats_1.24.0\n [51] leiden_0.4.3.1            codetools_0.2-20         \n [53] DelayedArray_0.28.0       shape_1.4.6.1            \n [55] tidyselect_1.2.1          farver_2.1.2             \n [57] ScaledMatrix_1.10.0       spatstat.explore_3.2-6   \n [59] jsonlite_1.8.8            GetoptLong_1.0.5         \n [61] BiocNeighbors_1.20.0      ellipsis_0.3.2           \n [63] progressr_0.14.0          iterators_1.0.14         \n [65] ggridges_0.5.6            survival_3.7-0           \n [67] foreach_1.5.2             tools_4.3.3              \n [69] ica_1.0-3                 Rcpp_1.0.13              \n [71] glue_1.7.0                gridExtra_2.3            \n [73] SparseArray_1.2.2         xfun_0.47                \n [75] usethis_3.0.0             dplyr_1.1.4              \n [77] withr_3.0.1               fastmap_1.2.0            \n [79] bluster_1.12.0            fansi_1.0.6              \n [81] digest_0.6.37             rsvd_1.0.5               \n [83] R6_2.5.1                  mime_0.12                \n [85] colorspace_2.1-1          scattermore_1.2          \n [87] tensor_1.5                spatstat.data_3.1-2      \n [89] utf8_1.2.4                tidyr_1.3.1              \n [91] generics_0.1.3            data.table_1.15.4        \n [93] httr_1.4.7                htmlwidgets_1.6.4        \n [95] S4Arrays_1.2.0            uwot_0.1.16              \n [97] pkgconfig_2.0.3           gtable_0.3.5             \n [99] lmtest_0.9-40             XVector_0.42.0           \n[101] htmltools_0.5.8.1         profvis_0.4.0            \n[103] dotCall64_1.1-1           clue_0.3-65              \n[105] scales_1.3.0              png_0.1-8                \n[107] knitr_1.48                rjson_0.2.21             \n[109] reshape2_1.4.4            curl_5.2.1               \n[111] nlme_3.1-165              cachem_1.1.0             \n[113] GlobalOptions_0.1.2       zoo_1.8-12               \n[115] stringr_1.5.1             KernSmooth_2.23-24       \n[117] parallel_4.3.3            miniUI_0.1.1.1           \n[119] pillar_1.9.0              vctrs_0.6.5              \n[121] RANN_2.6.2                urlchecker_1.0.1         \n[123] promises_1.3.0            BiocSingular_1.18.0      \n[125] beachmat_2.18.0           xtable_1.8-4             \n[127] cluster_2.1.6             evaluate_0.24.0          \n[129] cli_3.6.3                 locfit_1.5-9.9           \n[131] compiler_4.3.3            rlang_1.1.4              \n[133] crayon_1.5.3              future.apply_1.11.2      \n[135] labeling_0.4.3            fs_1.6.4                 \n[137] plyr_1.8.9                stringi_1.8.4            \n[139] viridisLite_0.4.2         deldir_2.0-4             \n[141] BiocParallel_1.36.0       munsell_0.5.1            \n[143] lazyeval_0.2.2            devtools_2.4.5           \n[145] spatstat.geom_3.2-9       dir.expiry_1.10.0        \n[147] RcppHNSW_0.6.0            sparseMatrixStats_1.14.0 \n[149] future_1.34.0             statmod_1.5.0            \n[151] shiny_1.9.1               ROCR_1.0-11              \n[153] memoise_2.0.1             igraph_2.0.3"
  },
  {
    "objectID": "labs/comparison/comparison_dge.html#compare-significant-genes.",
    "href": "labs/comparison/comparison_dge.html#compare-significant-genes.",
    "title": "Comparison of DGE results",
    "section": "4 Compare significant genes.",
    "text": "4 Compare significant genes.\nCreate a list object with all the significant dge genes.\n\nFDR from BioC findMarkers is the BH-adjusted p-value.\nBioC scoreMarkers does not provide p-values in the same way, cannot compare here.\nAdjusted p-value from scanpy is BH-adjusted p-value. Scanpy logreg does not have p-values.\nP-values in Seurat are identical to scanpy for wilcoxon, so also BH.\n\nSet cutoff for significant at 0.01.\n\npval.cut = 0.01\n\nsign.scanpy = lapply(markers_scanpy, function(x) { \n  tmp = lapply(x, function(y) y[y$pvals_adj &lt;= pval.cut,])\n  names(tmp) = as.character(clusters) \n  return(tmp)\n  })\n\nsign.dge = unlist(sign.scanpy, recursive = F)\nnames(sign.dge) = sub(\"Ttest_o\", \"TtestO\", names(sign.dge))\nnames(sign.dge) = paste0(\"scpy_\", names(sign.dge))\n\n\nsign.bioc = lapply(markersB, function(x) { \n  lapply(x, function(y) y[y$FDR &lt;= pval.cut,])\n  })\nnames(sign.bioc) = paste0(\"bioc_\", names(sign.bioc))\nsign.bioc = unlist(sign.bioc, recursive = F)\n\n\nsign.seu = lapply(markersS, function(x){\n  x = x[x$p_val_adj &lt;= pval.cut,]\n  x = split(x, x$cluster)\n  x = lapply(x, function(y) { \n    rownames(y) = y$gene\n    return(y) })    \n  x\n})\n\n\nnames(sign.seu) = paste0(\"seu_\", names(sign.seu))\nsign.seu = unlist(sign.seu, recursive = F)\n\n\nsign.dge = c(sign.dge, sign.seu, sign.bioc)\n\nsaveRDS(sign.dge, file = file.path(path_results, \"all_sign_dge.Rds\"))\n\nNumber of dge genes\n\nnG = unlist(lapply(sign.dge, nrow))\npipeline = Reduce(rbind,strsplit(names(nG),\"_\"))[,1]\npar(mar = c(8,5,2,2))\nbarplot(nG, las = 2, col = factor(pipeline), main = \"Number of significant genes\", cex.names = 0.5)\n\n\n\n\n\n\n\n\nTurns out that the two different wilcoxon runs in Seurat have identical results, so remove one. sign.dge\nOverlap of DGE genes is calculated with a phyper test using all expressed genes as the background. The heatmaps display the -log10(p-value) from the phyper test.\n\nsign.dge = sign.dge[!grepl(\"wilcL\",names(sign.dge))]\n\ntmp = lapply(sign.dge, rownames)\no = overlap_phyper2(tmp, tmp, nsize = 5, bg = nrow(all), remove.diag = T)\n\n\n\n\n\n\n\n\nCluster and plot with annotation.\n\nn = names(sign.dge)\n\ns = Reduce(rbind, strsplit(n, \"[\\\\._]\"))\nannot.tests = data.frame(s)\ncolnames(annot.tests) = c(\"Pipe\",\"Test\",\"Cluster\")\nrownames(annot.tests) = names(sign.dge)\nannot.tests$TestName = paste(s[,1],s[,2], sep= \"_\")\n\npheatmap(-log10(o$P[1:100,1:100]+1e-230),annotation_col = annot.tests)\n\n\n\n\n\n\n\n\nVery different number of genes, so unfair comparison."
  },
  {
    "objectID": "labs/comparison/comparison_dge.html#compare-top-genes",
    "href": "labs/comparison/comparison_dge.html#compare-top-genes",
    "title": "Comparison of DGE results",
    "section": "5 Compare top genes",
    "text": "5 Compare top genes\nCreate a list object with all the dge genes as ranked lists to compare top X genes.\n\nranked.scanpy = lapply(markers_scanpy, function(x) { \n  tmp = lapply(x, rownames)\n  names(tmp) = clusters\n  return(tmp)\n  })\n\nnames(ranked.scanpy) = paste0(\"scpy_\", names(ranked.scanpy))\nnames(ranked.scanpy) = sub(\"Ttest_o\", \"TtestO\", names(ranked.scanpy))\nranked.scanpy = unlist(ranked.scanpy, recursive = F)\n\n\ngetRankSeu = function(x) {\n  x = split(x, x$cluster)\n  lapply(x, function(y) { \n    y$gene } )\n}\n\nrank.seu = lapply(markersS, getRankSeu)\nnames(rank.seu) = paste0(\"seu_\", names(rank.seu))\nrank.seu = unlist(rank.seu, recursive = F)\n\n# remove wilcoxonL\nrank.seu = rank.seu[!grepl(\"wilcL\", names(rank.seu))]\n\n\nrank.bioc = lapply(markersB, function(x) { \n  lapply(x, rownames)\n})\nnames(rank.bioc) = paste0(\"bioc_\", names(rank.bioc))\nrank.bioc = unlist(rank.bioc, recursive = F)\n\n\nall.rank = c(ranked.scanpy, rank.seu, rank.bioc )\n\nsaveRDS(all.rank, file = file.path(path_results, \"all_ranked_dge.Rds\"))\n\n\n5.1 Top 50\n\ntopG = lapply(all.rank, function(x) x[1:50])\n\no = overlap_phyper2(topG,topG, nsize = 5, bg = nrow(all), remove.diag = T)\n\n\n\n\n\n\n\npheatmap(-log10(o$P[1:100,1:100]+1e-320),annotation_col = annot.tests)\n\n\n\n\n\n\n\n\nClearly more unique genes per cluster with the bioC methods.\nMerge all top genes from all clusters.\nOBS! Skip using background, all overlaps are significant, so we will get a false scale of the p-values, but is more visual.\n\ntmp = split(topG, annot.tests$TestName)\ntmp = lapply(tmp, unlist)\ntmp = lapply(tmp, unique)\n\no = overlap_phyper2(tmp,tmp, remove.diag = T)\n\n\n\n\n\n\n\n# with grouping.\npheatmap(-log10(o$P + 1e-320)[1:length(tmp), 1:length(tmp)], display_numbers = o$M[1:length(tmp), 1:length(tmp)])\n\n\n\n\n\n\n\n\nBinomial test stands out, but some similarty to the other bioC tests. Also, all unique genes per cluster.\n\n\n5.2 Top 200\nSame but with top200 genes.\n\ntopG = lapply(all.rank, function(x) x[1:200])\ntmp = split(topG, annot.tests$TestName)\ntmp = lapply(tmp, unlist)\ntmp = lapply(tmp, unique)\n\no = overlap_phyper2(tmp,tmp, remove.diag = T)\n\n\n\n\n\n\n\n# with grouping.\npheatmap(-log10(o$P + 1e-320)[1:length(tmp), 1:length(tmp)], display_numbers = o$M[1:length(tmp), 1:length(tmp)])\n\n\n\n\n\n\n\n\nBioC stands out from the rest. Wilcoxon in Seurat and Scanpy are different."
  },
  {
    "objectID": "labs/comparison/comparison_pipelines.html#load-data",
    "href": "labs/comparison/comparison_pipelines.html#load-data",
    "title": "Comparison of all pipelines",
    "section": "1 Load data",
    "text": "1 Load data\nOBS! Zellkonverter installs conda env with basilisk! Takes a while to run first time!!\n\npath_results &lt;- \"data/covid/results\"\nif (!dir.exists(path_results)) dir.create(path_results, recursive = T)\n\npath_seurat = \"../seurat/data/covid/results/\"\npath_bioc = \"../bioc/data/covid/results/\"\npath_scanpy = \"../scanpy/data/covid/results/\"\n\n# fetch the files with qc and dimred for each \n\n# seurat\nsobj = readRDS(file.path(path_seurat,\"seurat_covid_qc_dr_int_cl.rds\"))\n\n# bioc\nsce = readRDS(file.path(path_bioc,\"bioc_covid_qc_dr_int_cl.rds\"))\nbioc = as.Seurat(sce)\n\n# scanpy\nscanpy.sce = readH5AD(file.path(path_scanpy, \"scanpy_covid_qc_dr_scanorama_cl.h5ad\"))\nscanpy = as.Seurat(scanpy.sce, counts = NULL, data = \"X\") # only have the var.genes data that is scaled."
  },
  {
    "objectID": "labs/scanpy/scanpy_06_celltyping.html#section-2",
    "href": "labs/scanpy/scanpy_06_celltyping.html#section-2",
    "title": " Celltype prediction",
    "section": "8 Session info",
    "text": "8 Session info\n\n\nClick here\n\n\nsc.logging.print_versions()\n\n-----\nanndata     0.10.8\nscanpy      1.10.3\n-----\nCoreFoundation              NA\nFoundation                  NA\nPIL                         10.4.0\nPyObjCTools                 NA\nannoy                       NA\nanyio                       NA\nappnope                     0.1.4\narray_api_compat            1.8\narrow                       1.3.0\nasciitree                   NA\nasttokens                   NA\nattr                        24.2.0\nattrs                       24.2.0\nbabel                       2.14.0\nbrotli                      1.1.0\ncelltypist                  1.6.3\ncertifi                     2024.08.30\ncffi                        1.17.1\ncharset_normalizer          3.3.2\ncloudpickle                 3.0.0\ncolorama                    0.4.6\ncomm                        0.2.2\ncycler                      0.12.1\ncython_runtime              NA\ncytoolz                     0.12.3\ndask                        2024.9.0\ndateutil                    2.9.0\ndebugpy                     1.8.5\ndecorator                   5.1.1\ndefusedxml                  0.7.1\nexceptiongroup              1.2.2\nexecuting                   2.1.0\nfastjsonschema              NA\nfbpca                       NA\nfqdn                        NA\ngoogle                      NA\ngseapy                      1.1.3\nh5py                        3.11.0\nidna                        3.10\nigraph                      0.11.6\nintervaltree                NA\nipykernel                   6.29.5\nisoduration                 NA\njedi                        0.19.1\njinja2                      3.1.4\njoblib                      1.4.2\njson5                       0.9.25\njsonpointer                 3.0.0\njsonschema                  4.23.0\njsonschema_specifications   NA\njupyter_events              0.10.0\njupyter_server              2.14.2\njupyterlab_server           2.27.3\nkiwisolver                  1.4.7\nlegacy_api_wrap             NA\nleidenalg                   0.10.2\nllvmlite                    0.43.0\nmarkupsafe                  2.1.5\nmatplotlib                  3.9.2\nmatplotlib_inline           0.1.7\nmpl_toolkits                NA\nmsgpack                     1.1.0\nnatsort                     8.4.0\nnbformat                    5.10.4\nnumba                       0.60.0\nnumcodecs                   0.13.0\nnumpy                       1.26.4\nobjc                        10.3.1\noverrides                   NA\npackaging                   24.1\npandas                      1.5.3\nparso                       0.8.4\npatsy                       0.5.6\npickleshare                 0.7.5\nplatformdirs                4.3.6\nprometheus_client           NA\nprompt_toolkit              3.0.47\npsutil                      6.0.0\npure_eval                   0.2.3\npycparser                   2.22\npydev_ipython               NA\npydevconsole                NA\npydevd                      2.9.5\npydevd_file_utils           NA\npydevd_plugins              NA\npydevd_tracing              NA\npygments                    2.18.0\npynndescent                 0.5.13\npyparsing                   3.1.4\npythonjsonlogger            NA\npytz                        2024.2\nreferencing                 NA\nrequests                    2.32.3\nrfc3339_validator           0.1.4\nrfc3986_validator           0.1.1\nrpds                        NA\nscanorama                   1.7.4\nscipy                       1.14.1\nsend2trash                  NA\nsession_info                1.0.0\nsix                         1.16.0\nsklearn                     1.5.2\nsniffio                     1.3.1\nsocks                       1.7.1\nsortedcontainers            2.4.0\nsparse                      0.15.4\nsphinxcontrib               NA\nstack_data                  0.6.2\nstatsmodels                 0.14.3\ntexttable                   1.7.0\nthreadpoolctl               3.5.0\ntlz                         0.12.3\ntoolz                       0.12.1\ntorch                       2.4.0.post101\ntorchgen                    NA\ntornado                     6.4.1\ntqdm                        4.66.5\ntraitlets                   5.14.3\ntyping_extensions           NA\numap                        0.5.6\nuri_template                NA\nurllib3                     2.2.3\nwcwidth                     0.2.13\nwebcolors                   24.8.0\nwebsocket                   1.8.0\nyaml                        6.0.2\nzarr                        2.18.3\nzipp                        NA\nzmq                         26.2.0\nzoneinfo                    NA\nzstandard                   0.23.0\n-----\nIPython             8.27.0\njupyter_client      8.6.3\njupyter_core        5.7.2\njupyterlab          4.2.5\nnotebook            7.2.2\n-----\nPython 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:17:34) [Clang 14.0.6 ]\nmacOS-14.7-x86_64-i386-64bit\n-----\nSession information updated at 2024-10-22 09:56"
  },
  {
    "objectID": "labs/comparison/comparison_pipelines.html#umaps",
    "href": "labs/comparison/comparison_pipelines.html#umaps",
    "title": "Comparison of all pipelines",
    "section": "2 Umaps",
    "text": "2 Umaps\n\nwrap_plots(\n    DimPlot(sobj, group.by = \"orig.ident\") + NoAxes() + ggtitle(\"Seurat\"),\n    DimPlot(bioc, group.by = \"sample\") + NoAxes() + ggtitle(\"Bioc\"),\n    DimPlot(scanpy, group.by = \"sample\", reduction = \"X_umap_uncorr\") + NoAxes() + ggtitle(\"Scanpy\"),\n    ncol = 3\n)\n\n\n\n\n\n\n\n\nSettings for umap differs:\n\nSeurat\n\ndims = 1:30,\nn.neighbors = 30,\nn.epochs = 200,\nmin.dist = 0.3,\nlearning.rate = 1,\nspread = 1\n\nBioc\n\nn_dimred = 30,\nn_neighbors = 15,\n\nspread = 1,\nmin_dist = 0.01,\nn_epchs = 200\n\nScanpy\n\ndoes not use the umap algorithm for neigbors, feeds the KNN to the umap function.\nn_pcs = 30,\nn_neighbors = 20,\nmin_dist=0.5,\nspread=1.0,"
  },
  {
    "objectID": "labs/comparison/comparison_pipelines.html#celltype-prediction.",
    "href": "labs/comparison/comparison_pipelines.html#celltype-prediction.",
    "title": "Comparison of all pipelines",
    "section": "9 Celltype prediction.",
    "text": "9 Celltype prediction.\nDone only for sample ctrl13, load all the results.\n\nsobj = readRDS(file.path(path_seurat, \"seurat_covid_qc_dr_int_cl_ct-ctrl13.rds\"))\n\nsce = readRDS(file.path(path_bioc, \"bioc_covid_qc_dr_int_cl_ct-ctrl13.rds\"))\nbioc = as.Seurat(sce)\nbioc$scmap_cluster = sce$scmap_cluster #for some reason that column dissappears when running as.Seurat.\n\nscanpy.sce = readH5AD(file.path(path_scanpy, \"scanpy_covid_qc_dr_int_cl_ct-ctrl13.h5ad\"))\nscanpy = as.Seurat(scanpy.sce, counts = NULL, data = \"X\")\n\nMerge into one object.\n\nmeta.seurat = sobj@meta.data\nmeta.scanpy = scanpy@meta.data\nmeta.bioc = bioc@meta.data\n\nmeta.bioc$cell = rownames(meta.bioc)\nmeta.scanpy$cell = sapply(rownames(meta.scanpy), function(x) substr(x,1,nchar(x)-2))\nmeta.seurat$cell = unlist(lapply(strsplit(rownames(meta.seurat),\"_\"), function(x) x[3]))\n\nin.all = intersect(intersect(meta.scanpy$cell, meta.seurat$cell), meta.bioc$cell)\n\ntmp1 = meta.bioc[match(in.all, meta.bioc$cell),]\ncolnames(tmp1) = paste0(colnames(tmp1),\"_bioc\")\ntmp2 = meta.scanpy[match(in.all, meta.scanpy$cell),]\ncolnames(tmp2) = paste0(colnames(tmp2),\"_scpy\")\n\nall = sobj[,match(in.all, meta.seurat$cell)]\n\nmeta.all = cbind(all@meta.data, tmp1,tmp2)\nall@meta.data = meta.all\n\nCelltype predictions:\n\nSeurat\n\npredicted.id - TransferData\nsingler.immune - singleR with DatabaseImmuneCellExpressionData\nsingler.hpca- singleR with HumanPrimaryCellAtlasData\nsingler.ref- singleR with own ref.\npredicted.celltype.l1, predicted.celltype.l2, predicted.celltype.l3 - Azimuth predictions at different levels.\n\nBioc\n\nscmap_cluster - scMap with clusters in ref data\nscmap_cell - scMap with all cells in ref data\nsingler.immune - singleR with DatabaseImmuneCellExpressionData\nsingler.hpca- singleR with HumanPrimaryCellAtlasData\nsingler.ref- singleR with own ref.\n\nScanpy\n\nlabel_trans - label transfer with scanorama\nlouvain - bbknn\npredicted_labels, majority_voting - celltypist “Immune_All_High” reference\npredicted_labels_ref, majority_voting_ref - celltypist with own reference\n\n\n\nall.pred = c(\"label_trans_scpy\",   \"louvain_scpy\",  \"predicted_labels_scpy\", \"majority_voting_scpy\", \"predicted_labels_ref_scpy\",  \"majority_voting_ref_scpy\",      \"scmap_cluster_bioc\", \"scmap_cell_bioc\",  \"singler.immune_bioc\", \"singler.hpca_bioc\",  \"singler.ref_bioc\", \"predicted.id\", \"singler.immune\", \"singler.hpca\",  \"singler.ref\", \"predicted.celltype.l1\", \"predicted.celltype.l2\", \"predicted.celltype.l3\")\n\nct = all@meta.data[,all.pred]\n\nplots = sapply(all.pred, function(x) DimPlot(all, reduction = \"umap_harmony\", group.by = x) + NoAxes() + ggtitle(x))\n\nwrap_plots(plots[1:9], ncol = 3)\n\n\n\n\n\n\n\nwrap_plots(plots[10:18], ncol = 3)\n\n\n\n\n\n\n\n\nSubset for ones that are using same ref:\n\nref.pred = c(\"label_trans_scpy\",   \"louvain_scpy\", \"predicted_labels_ref_scpy\",  \"majority_voting_ref_scpy\",      \"scmap_cluster_bioc\", \"scmap_cell_bioc\",   \"singler.ref_bioc\", \"predicted.id\",  \"singler.ref\")\n\nlev = sort(unique(unlist(apply(all@meta.data[,ref.pred],2,unique))))\nlev = unique(sub(\"cells\",\"cell\",lev))\n\n#\"FCGR3A+ Monocytes\" -&gt; \"ncMono\"\n#\"CD14+ Monocytes\" -&gt; \"cMono\"\n#\"Dendritic cell\" -&gt; \"cDC\"\nlev = lev[!grepl(\"\\\\+\",lev)]\nlev = lev[!grepl(\"Dendr\",lev)]\n\nfor (rp in ref.pred){\n  tmp = as.character(all@meta.data[,rp])\n  tmp = sub(\"cells\",\"cell\",tmp)\n  tmp = sub(\"FCGR3A\\\\+ Monocytes\",\"ncMono\",tmp)\n  tmp = sub(\"CD14\\\\+ Monocytes\",\"cMono\",tmp)\n  tmp = sub(\"Dendritic cell\",\"cDC\",tmp)\n  tmp[is.na(tmp)] = \"unassigned\"\n  all@meta.data[,paste0(rp,\"2\")] = factor(tmp,levels = lev)\n  \n}\n\ncoldef =  RColorBrewer::brewer.pal(9,\"Paired\")\nnames(coldef) = lev\nplots2 = sapply(paste0(ref.pred,\"2\"), function(x) DimPlot(all, reduction = \"umap_harmony\", group.by = x) + NoAxes() + ggtitle(x) + scale_color_manual(values = coldef))\n\n\nwrap_plots(plots2, ncol = 3) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\nref.pred2 = paste0(ref.pred,\"2\")\n\ntmp = all@meta.data[,ref.pred2]\n\ncounts = sapply(ref.pred2,function(x) table(tmp[,x]))\ncounts = reshape2::melt(counts)\n\n\nggplot(counts, aes(x=Var2, y=value, fill=Var1)) + geom_bar(stat = \"identity\") + RotatedAxis()"
  },
  {
    "objectID": "labs/comparison/comparison_hvg.html",
    "href": "labs/comparison/comparison_hvg.html",
    "title": "Comparison of HVGs",
    "section": "",
    "text": "With data in place, now we can start loading libraries we will use in this tutorial.\nsuppressPackageStartupMessages({\n    library(Seurat)\n    library(zellkonverter)\n    library(Matrix)\n    library(ggplot2)\n    library(patchwork)\n    library(scran)\n    library(ComplexHeatmap)\n    library(basilisk)\n})\n\ndevtools::source_url(\"https://raw.githubusercontent.com/asabjorklund/single_cell_R_scripts/main/overlap_phyper_v2.R\")"
  },
  {
    "objectID": "labs/comparison/comparison_hvg.html#load-data",
    "href": "labs/comparison/comparison_hvg.html#load-data",
    "title": "Comparison of HVGs",
    "section": "1 Load data",
    "text": "1 Load data\nOBS! Zellkonverter installs conda env with basilisk! Takes a while to run first time!!\n\npath_results &lt;- \"data/covid/results\"\nif (!dir.exists(path_results)) dir.create(path_results, recursive = T)\n\npath_seurat = \"../seurat/data/covid/results/\"\npath_bioc = \"../bioc/data/covid/results/\"\npath_scanpy = \"../scanpy/data/covid/results/\"\n\npath_results &lt;- \"data/covid/results\"\nif (!dir.exists(path_results)) dir.create(path_results, recursive = T)\n\npath_seurat = \"../seurat/data/covid/results/\"\npath_bioc = \"../bioc/data/covid/results/\"\npath_scanpy = \"../scanpy/data/covid/results/\"\n\n# fetch the files with qc and dimred for each \n\n# seurat\nsobj = readRDS(file.path(path_seurat,\"seurat_covid_qc_dr_int_cl.rds\"))\n\n# bioc\nsce = readRDS(file.path(path_bioc,\"bioc_covid_qc_dr_int_cl.rds\"))\nbioc = as.Seurat(sce)\n\n# scanpy\nscanpy.sce = readH5AD(file.path(path_scanpy, \"scanpy_covid_qc_dr_scanorama_cl.h5ad\"))\nscanpy = as.Seurat(scanpy.sce, counts = NULL, data = \"X\") # only have the var.genes data that is scaled."
  },
  {
    "objectID": "labs/comparison/comparison_hvg.html#umaps",
    "href": "labs/comparison/comparison_hvg.html#umaps",
    "title": "Comparison of HVGs",
    "section": "2 Umaps",
    "text": "2 Umaps\n\nwrap_plots(\n    DimPlot(sobj, group.by = \"orig.ident\") + NoAxes() + ggtitle(\"Seurat\"),\n    DimPlot(bioc, group.by = \"sample\") + NoAxes() + ggtitle(\"Bioc\"),\n    DimPlot(scanpy, group.by = \"sample\", reduction = \"X_umap_uncorr\") + NoAxes() + ggtitle(\"Scanpy\"),\n    ncol = 3\n)\n\n\n\n\n\n\n\n\nCreate one dataset with the cells that are present in all samples. Also add in umap from all 3 pipelines.\n\nmeta.seurat = sobj@meta.data\nmeta.scanpy = scanpy@meta.data\nmeta.bioc = bioc@meta.data\n\nmeta.bioc$cell = rownames(meta.bioc)\nmeta.scanpy$cell = sapply(rownames(meta.scanpy), function(x) substr(x,1,nchar(x)-2))\nmeta.seurat$cell = unlist(lapply(strsplit(rownames(meta.seurat),\"_\"), function(x) x[3]))\n\n\nin.all = intersect(intersect(meta.scanpy$cell, meta.seurat$cell), meta.bioc$cell)\n\ntmp1 = meta.bioc[match(in.all, meta.bioc$cell),]\ncolnames(tmp1) = paste0(colnames(tmp1),\"_bioc\")\ntmp2 = meta.scanpy[match(in.all, meta.scanpy$cell),]\ncolnames(tmp2) = paste0(colnames(tmp2),\"_scpy\")\n\nall = sobj[,match(in.all, meta.seurat$cell)]\n\nmeta.all = cbind(all@meta.data, tmp1,tmp2)\nall@meta.data = meta.all\n\nReductions(all)\n\n [1] \"pca\"               \"umap\"              \"tsne\"             \n [4] \"UMAP10_on_PCA\"     \"UMAP_on_ScaleData\" \"integrated_cca\"   \n [7] \"umap_cca\"          \"tsne_cca\"          \"harmony\"          \n[10] \"umap_harmony\"      \"scanorama\"         \"scanoramaC\"       \n[13] \"umap_scanorama\"    \"umap_scanoramaC\"  \n\ntmp = bioc@reductions$UMAP_on_PCA@cell.embeddings[match(in.all, meta.bioc$cell),]\nrownames(tmp) = colnames(all)\nall[[\"umap_bioc\"]] = CreateDimReducObject(tmp, key = \"umapbioc_\", assay = \"RNA\")\ntmp = scanpy@reductions$X_umap_uncorr@cell.embeddings[match(in.all, meta.scanpy$cell),]\nrownames(tmp) = colnames(all)\nall[[\"umap_scpy\"]] = CreateDimReducObject(tmp, key = \"umapscpy_\", assay = \"RNA\")\n\nReductions(all)\n\n [1] \"pca\"               \"umap\"              \"tsne\"             \n [4] \"UMAP10_on_PCA\"     \"UMAP_on_ScaleData\" \"integrated_cca\"   \n [7] \"umap_cca\"          \"tsne_cca\"          \"harmony\"          \n[10] \"umap_harmony\"      \"scanorama\"         \"scanoramaC\"       \n[13] \"umap_scanorama\"    \"umap_scanoramaC\"   \"umap_bioc\"        \n[16] \"umap_scpy\""
  },
  {
    "objectID": "labs/comparison/comparison_hvg.html#variable-features",
    "href": "labs/comparison/comparison_hvg.html#variable-features",
    "title": "Comparison of HVGs",
    "section": "3 Variable features",
    "text": "3 Variable features\nIn Seurat, FindVariableFeatures is not batch aware unless the data is split into layers by samples, here we have the variable genes created with layers. In Bioc modelGeneVar we used sample as a blocking parameter, e.g calculates the variable genes per sample and combines the variances. Similar in scanpy we used the samples as batch_key.\n\nhvgs = list()\nhvgs$seurat = VariableFeatures(sobj)\nhvgs$bioc = sce@metadata$hvgs\n# scanpy has no strict number on selection, instead uses dispersion cutoff. So select instead top 2000 dispersion genes.\nscpy.hvg = rowData(scanpy.sce)\nhvgs_scanpy = rownames(scpy.hvg)[scpy.hvg$highly_variable]\nhvgs$scanpy = rownames(scpy.hvg)[order(scpy.hvg$dispersions_norm, decreasing = T)][1:2000]\n\ncmat = make_comb_mat(hvgs)\nprint(cmat)\n\nA combination matrix with 3 sets and 7 combinations.\n  ranges of combination set size: c(211, 861).\n  mode for the combination size: distinct.\n  sets are on rows.\n\nCombination sets are:\n  seurat bioc scanpy code size\n       x    x      x  111  641\n       x    x         110  287\n       x           x  101  211\n            x      x  011  364\n       x              100  861\n            x         010  708\n                   x  001  784\n\nSets are:\n     set size\n  seurat 2000\n    bioc 2000\n  scanpy 2000\n\nUpSet(cmat)\n\n\n\n\n\n\n\n\nSurprisingly low overlap between the methods and many genes that are unique to one pipeline. With discrepancies in the doublet filtering the cells used differ to some extent, but otherwise the variation should be similar. Even if it is estimated in different ways.\nIs the differences more due to the combination of ranks/dispersions or also found within a single dataset?\nOnly Seurat have the dispersions for each individual dataset stored in the object.\n\n3.1 Compare dispersions\nRecalculate for bioc.\n\nvar.out &lt;- modelGeneVar(sce, block = sce$sample)\nhvgs_bioc &lt;- getTopHVGs(var.out, n = 2000)\ncutoff &lt;- rownames(var.out) %in% hvgs_bioc\n\n\n# Merge all hvg info\n\nall.genes = intersect(rownames(var.out), rownames(scpy.hvg))\n\nscpy.hvg = rowData(scanpy.sce)\ncolnames(scpy.hvg) = paste0(colnames(scpy.hvg), \"_scpy\")\ncolnames(var.out) = paste0(colnames(var.out), \"_bioc\")\n\nseu.hvg = HVFInfo(all) \n\nall.hvg = cbind(seu.hvg[all.genes,], scpy.hvg[all.genes,], var.out[all.genes,])\n\n\npar(mfrow=c(2,2))\nplot(all.hvg$means_scpy, all.hvg$dispersions_norm_scpy, main = \"Scanpy\",pch = 16, cex = 0.4)\npoints(all.hvg$means_scpy[all.hvg$highly_variable_scpy], all.hvg$dispersions_norm_scpy[all.hvg$highly_variable_scpy], col=\"red\",pch = 16, cex = 0.4)\n\nplot(all.hvg$mean_bioc, all.hvg$bio_bioc, pch = 16, cex = 0.4, main = \"Bioc\")\npoints(all.hvg$mean_bioc[rownames(all.hvg) %in% hvgs$bioc], all.hvg$bio_bioc[rownames(all.hvg) %in% hvgs$bioc], col = \"red\", pch = 16, cex = .6)\n\n   \nplot(log1p(all.hvg$mean), all.hvg$variance.standardized, main = \"Seurat\",pch = 16, cex = 0.4)    \npoints(log1p(all.hvg$mean)[match(hvgs$seurat, rownames(all.hvg))], all.hvg$variance.standardized[match(hvgs$seurat, rownames(all.hvg))],col=\"red\",pch = 16, cex = 0.4)    \n\n\n\n\n\n\n\n\nScanpy uses min_disp=0.5, min_mean=0.0125, max_mean=3, so the highly expressed ones are not included.\n\nall.hvg$mean_log = log1p(all.hvg$mean)\nsel.means = c(\"mean\",\"mean_log\",\"means_scpy\", \"mean_bioc\")\npairs(all.hvg[,sel.means])\n\n\n\n\n\n\n\nsel.disp = c(\"variance.standardized\",\"dispersions_norm_scpy\", \"bio_bioc\",\"total_bioc\")\npairs(all.hvg[,sel.disp])\n\n\n\n\n\n\n\n\nDifference in what cells were used, and also in how the dispersions across the samples is combined into one value. Do for just one sample instead."
  },
  {
    "objectID": "labs/comparison/comparison_hvg.html#meta-session",
    "href": "labs/comparison/comparison_hvg.html#meta-session",
    "title": "Comparison of HVGs",
    "section": "6 Session info",
    "text": "6 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] grid      stats4    stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] pheatmap_1.0.12             basilisk_1.14.1            \n [3] ComplexHeatmap_2.18.0       scran_1.30.0               \n [5] scuttle_1.12.0              SingleCellExperiment_1.24.0\n [7] SummarizedExperiment_1.32.0 Biobase_2.62.0             \n [9] GenomicRanges_1.54.1        GenomeInfoDb_1.38.1        \n[11] IRanges_2.36.0              S4Vectors_0.40.2           \n[13] BiocGenerics_0.48.1         MatrixGenerics_1.14.0      \n[15] matrixStats_1.4.1           patchwork_1.2.0            \n[17] ggplot2_3.5.1               Matrix_1.6-5               \n[19] zellkonverter_1.12.1        Seurat_5.1.0               \n[21] SeuratObject_5.0.2          sp_2.1-4                   \n\nloaded via a namespace (and not attached):\n  [1] fs_1.6.4                  spatstat.sparse_3.1-0    \n  [3] bitops_1.0-8              devtools_2.4.5           \n  [5] httr_1.4.7                RColorBrewer_1.1-3       \n  [7] doParallel_1.0.17         profvis_0.4.0            \n  [9] tools_4.3.3               sctransform_0.4.1        \n [11] utf8_1.2.4                R6_2.5.1                 \n [13] lazyeval_0.2.2            uwot_0.1.16              \n [15] GetoptLong_1.0.5          urlchecker_1.0.1         \n [17] withr_3.0.1               gridExtra_2.3            \n [19] progressr_0.14.0          cli_3.6.3                \n [21] Cairo_1.6-2               spatstat.explore_3.2-6   \n [23] fastDummies_1.7.4         labeling_0.4.3           \n [25] spatstat.data_3.1-2       ggridges_0.5.6           \n [27] pbapply_1.7-2             parallelly_1.38.0        \n [29] sessioninfo_1.2.2         limma_3.58.1             \n [31] generics_0.1.3            shape_1.4.6.1            \n [33] ica_1.0-3                 spatstat.random_3.2-3    \n [35] dplyr_1.1.4               ggbeeswarm_0.7.2         \n [37] fansi_1.0.6               abind_1.4-5              \n [39] lifecycle_1.0.4           yaml_2.3.10              \n [41] edgeR_4.0.16              SparseArray_1.2.2        \n [43] Rtsne_0.17                promises_1.3.0           \n [45] dqrng_0.3.2               crayon_1.5.3             \n [47] dir.expiry_1.10.0         miniUI_0.1.1.1           \n [49] lattice_0.22-6            beachmat_2.18.0          \n [51] cowplot_1.1.3             pillar_1.9.0             \n [53] knitr_1.48                metapod_1.10.0           \n [55] rjson_0.2.21              future.apply_1.11.2      \n [57] codetools_0.2-20          leiden_0.4.3.1           \n [59] glue_1.7.0                data.table_1.15.4        \n [61] remotes_2.5.0             vctrs_0.6.5              \n [63] png_0.1-8                 spam_2.10-0              \n [65] gtable_0.3.5              cachem_1.1.0             \n [67] xfun_0.47                 S4Arrays_1.2.0           \n [69] mime_0.12                 survival_3.7-0           \n [71] iterators_1.0.14          statmod_1.5.0            \n [73] bluster_1.12.0            ellipsis_0.3.2           \n [75] fitdistrplus_1.2-1        ROCR_1.0-11              \n [77] nlme_3.1-165              usethis_3.0.0            \n [79] filelock_1.0.3            RcppAnnoy_0.0.22         \n [81] irlba_2.3.5.1             vipor_0.4.7              \n [83] KernSmooth_2.23-24        colorspace_2.1-1         \n [85] ggrastr_1.0.2             tidyselect_1.2.1         \n [87] compiler_4.3.3            curl_5.2.1               \n [89] BiocNeighbors_1.20.0      basilisk.utils_1.14.1    \n [91] DelayedArray_0.28.0       plotly_4.10.4            \n [93] scales_1.3.0              lmtest_0.9-40            \n [95] stringr_1.5.1             digest_0.6.37            \n [97] goftest_1.2-3             spatstat.utils_3.1-0     \n [99] rmarkdown_2.28            XVector_0.42.0           \n[101] htmltools_0.5.8.1         pkgconfig_2.0.3          \n[103] sparseMatrixStats_1.14.0  fastmap_1.2.0            \n[105] rlang_1.1.4               GlobalOptions_0.1.2      \n[107] htmlwidgets_1.6.4         shiny_1.9.1              \n[109] DelayedMatrixStats_1.24.0 farver_2.1.2             \n[111] zoo_1.8-12                jsonlite_1.8.8           \n[113] BiocParallel_1.36.0       BiocSingular_1.18.0      \n[115] RCurl_1.98-1.16           magrittr_2.0.3           \n[117] GenomeInfoDbData_1.2.11   dotCall64_1.1-1          \n[119] munsell_0.5.1             Rcpp_1.0.13              \n[121] reticulate_1.39.0         stringi_1.8.4            \n[123] zlibbioc_1.48.0           MASS_7.3-60.0.1          \n[125] plyr_1.8.9                pkgbuild_1.4.4           \n[127] parallel_4.3.3            listenv_0.9.1            \n[129] ggrepel_0.9.6             deldir_2.0-4             \n[131] splines_4.3.3             tensor_1.5               \n[133] circlize_0.4.16           locfit_1.5-9.9           \n[135] igraph_2.0.3              spatstat.geom_3.2-9      \n[137] RcppHNSW_0.6.0            reshape2_1.4.4           \n[139] ScaledMatrix_1.10.0       pkgload_1.4.0            \n[141] evaluate_0.24.0           foreach_1.5.2            \n[143] httpuv_1.6.15             RANN_2.6.2               \n[145] tidyr_1.3.1               purrr_1.0.2              \n[147] polyclip_1.10-7           future_1.34.0            \n[149] clue_0.3-65               scattermore_1.2          \n[151] rsvd_1.0.5                xtable_1.8-4             \n[153] RSpectra_0.16-2           later_1.3.2              \n[155] viridisLite_0.4.2         tibble_3.2.1             \n[157] memoise_2.0.1             beeswarm_0.4.0           \n[159] cluster_2.1.6             globals_0.16.3"
  },
  {
    "objectID": "labs/comparison/comparison_hvg.html#for-one-sample",
    "href": "labs/comparison/comparison_hvg.html#for-one-sample",
    "title": "Comparison of HVGs",
    "section": "4 For one sample",
    "text": "4 For one sample\nCompare for ctrl.13 sample, Seurat uses variance.standardized to rank the genes. Bioc uses the bio slot with estimated biological variation.\nUse default settings in all the different methods.\n\n4.1 Seurat\nTry the different methods implemented in Seurat. From the help section:\n* “vst”: First, fits a line to the relationship of log(variance) and log(mean) using local polynomial regression (loess). Then standardizes the feature values using the observed mean and expected variance (given by the fitted line). Feature variance is then calculated on the standardized values after clipping to a maximum (see clip.max parameter).\n* “mean.var.plot” (mvp): First, uses a function to calculate average expression (mean.function) and dispersion (dispersion.function) for each feature. Next, divides features into num.bin (deafult 20) bins based on their average expression, and calculates z-scores for dispersion within each bin. The purpose of this is to identify variable features while controlling for the strong relationship between variability and average expression\n* “dispersion” (disp): selects the genes with the highest dispersion values\nFeature selection for individual datasets\nIn each dataset, we next aimed to identify a subset of features (e.g., genes) exhibiting high variability across cells, and therefore represent heterogeneous features to prioritize for downstream analysis. Choosing genes solely based on their log-normalized single-cell variance fails to account for the mean-variance relationship that is inherent to single-cell RNA-seq. Therefore, we first applied a variance-stabilizing transformation to correct for this [Mayer et al., 2018, Hafemeister and Satija, 2019]. To learn the mean-variance relationship from the data, we computed the mean and variance of each gene using the unnormalized data (i.e., UMI or counts matrix), and applied -transformation to both. We then fit a curve to predict the variance of each gene as a function of its mean, by calculating a local fitting of polynomials of degree 2 (R function loess, span = 0.3). This global fit provided us with a regularized estimator of variance given the mean of a feature. As such, we could use it to standardize feature counts without removing higher-than-expected variation.\n\nctrl = all[,all$orig.ident == \"ctrl_13\"]\nctrl@assays$RNA@meta.data[,1:ncol(ctrl@assays$RNA@meta.data)] = NULL\n\nctrl = FindVariableFeatures(ctrl)\nhvg_seu = list()\nhvg_seu$vst = VariableFeatures(ctrl)\n#top20 &lt;- head(hvg_seu$vst, 20)\n#LabelPoints(plot = VariableFeaturePlot(ctrl), points = top20, repel = TRUE)\n\n\nctrl = FindVariableFeatures(ctrl, selection.method = \"mean.var.plot\")\nhvg_seu$mvp = VariableFeatures(ctrl)\n#top20 &lt;- head(hvg_seu$mvp, 20)\n#LabelPoints(plot = VariableFeaturePlot(ctrl), points = top20, repel = TRUE)\n\nctrl = FindVariableFeatures(ctrl, selection.method = \"dispersion\")\nhvg_seu$disp = VariableFeatures(ctrl)\n#top20 &lt;- head(hvg_seu$disp, 20)\n#LabelPoints(plot = VariableFeaturePlot(ctrl), points = top20, repel = TRUE)\n\ncmat = make_comb_mat(hvg_seu)\ncmat\n\nA combination matrix with 3 sets and 7 combinations.\n  ranges of combination set size: c(32, 1295).\n  mode for the combination size: distinct.\n  sets are on rows.\n\nCombination sets are:\n  vst mvp disp code size\n    x   x    x  111  395\n    x   x       110   32\n    x        x  101  278\n        x    x  011  554\n    x           100 1295\n        x       010  152\n             x  001  773\n\nSets are:\n   set size\n   vst 2000\n   mvp 1133\n  disp 2000\n\n\nVery little overlap between the methods within Seurat. Mvp and disp are calculated on data, while vst is done on counts.\n\nvinfo.seu = ctrl@assays$RNA@meta.data\nrownames(vinfo.seu) = rownames(ctrl)\n\nvinfo.seu$vg_vst = rownames(vinfo.seu) %in% hvg_seu$vst\nvinfo.seu$vg_disp = rownames(vinfo.seu) %in% hvg_seu$disp\nvinfo.seu$vg_mvp = rownames(vinfo.seu) %in% hvg_seu$mvp\nvinfo.seu$vg_vst_disp = vinfo.seu$vg_vst + vinfo.seu$vg_disp == 2\nvinfo.seu$hvinfo = ifelse(vinfo.seu$vg_vst, \"VST\",NA)\nvinfo.seu$hvinfo[vinfo.seu$vg_mvp] = \"MVP\"\nvinfo.seu$hvinfo[vinfo.seu$vg_disp] = \"DISP\"\nvinfo.seu$hvinfo[vinfo.seu$vg_vst_disp] = \"VST/DISP\"\n\n\nmeans = colnames(vinfo.seu)[grepl(\"mean\", colnames(vinfo.seu))]\ndisp = c(\"vf_vst_counts.ctrl_13_variance.standardized\",\"vf_disp_data.ctrl_13_mvp.dispersion.scaled\",\"vf_mvp_data.ctrl_13_mvp.dispersion.scaled\")\npairs(vinfo.seu[,means])\n\n\n\n\n\n\n\nplot_hvg = function(df, m,d,vg, log=FALSE){\n  top20 = head(vg,20)\n  p = ggplot(df, aes(x=.data[[m]], y=.data[[d]], color=hvinfo)) + geom_point() + theme_classic() \n  if (log) { p = p + scale_x_log10()}\n  LabelPoints(plot = p, points = top20, repel = TRUE)\n}\n\np1S =   plot_hvg(vinfo.seu, \"vf_vst_counts.ctrl_13_mean\", \"vf_vst_counts.ctrl_13_variance.standardized\",hvg_seu$vst, log=TRUE)\np2S =   plot_hvg(vinfo.seu, \"vf_disp_data.ctrl_13_mvp.mean\", \"vf_disp_data.ctrl_13_mvp.dispersion.scaled\",hvg_seu$disp)\np3S =   plot_hvg(vinfo.seu, \"vf_mvp_data.ctrl_13_mvp.mean\", \"vf_mvp_data.ctrl_13_mvp.dispersion.scaled\",hvg_seu$mvp)\nwrap_plots(p1S,p2S,p3S,   ncol=2)\n\n\n\n\n\n\n\n\n\n\n4.2 Bioc\n\nctrl.sce = as.SingleCellExperiment(ctrl)\nvar.out &lt;- modelGeneVar(ctrl.sce)\nhvgs_bioc &lt;- getTopHVGs(var.out, n = 2000)\n\nvar.out.df = data.frame(var.out)\nvar.out.df$hvg = rownames(var.out) %in% hvgs_bioc\np = ggplot(var.out.df, aes(x=mean, y=total, colour = hvg)) + geom_point() + theme_classic()\n\ntop20 &lt;- head(hvgs_bioc, 20)\npB = LabelPoints(plot = p, points = top20, repel = TRUE)\npB\n\n\n\n\n\n\n\np2 = ggplot(var.out.df, aes(x=mean, y=bio, colour = hvg)) + geom_point() + theme_classic()\n\ntop20 &lt;- head(hvgs_bioc, 20)\npB2 = LabelPoints(plot = p2, points = top20, repel = TRUE)\npB2\n\n\n\n\n\n\n\n\n\n\n4.3 Scanpy\nRun with both seurat and seurat_v3\nCannot run seurat_v3 with normalized data:\nUserWarning: `flavor='seurat_v3'` expects raw count data, but non-integers were found.\n  warnings.warn(\nFor the dispersion-based methods (flavor=‘seurat’ Satija et al. [2015] and flavor=‘cell_ranger’ Zheng et al. [2017]), the normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected.\nFor flavor=‘seurat_v3’/‘seurat_v3_paper’ [Stuart et al., 2019], a normalized variance for each gene is computed. First, the data are standardized (i.e., z-score normalization per feature) with a regularized standard deviation. Next, the normalized variance is computed as the variance of each gene after the transformation. Genes are ranked by the normalized variance. Only if batch_key is not None, the two flavors differ: For flavor=‘seurat_v3’, genes are first sorted by the median (across batches) rank, with ties broken by the number of batches a gene is a HVG. For flavor=‘seurat_v3_paper’, genes are first sorted by the number of batches a gene is a HVG, with ties broken by the median (across batches) rank.\n\npenv = \"/Users/asabjor/miniconda3/envs/scanpy_2024_nopip\"\nhvg.scanpy = basiliskRun(env=penv, fun=function(counts) {\n    scanpy &lt;- reticulate::import(\"scanpy\")\n    ad = reticulate::import(\"anndata\")\n    adata = ad$AnnData(counts)\n    print(adata$X[1:10,1:10])\n    var1 = scanpy$pp$highly_variable_genes(adata, flavor = \"seurat_v3\", inplace=FALSE)\n    scanpy$pp$normalize_per_cell(adata, counts_per_cell_after=1e4)\n    scanpy$pp$log1p(adata)\n    print(adata$X[1:10,1:10])\n    scanpy$pp$highly_variable_genes(adata)\n    return(list(disp=adata$var, vst=var1))\n    \n}, counts = t(ctrl@assays$RNA@layers$counts.ctrl_13),  testload=\"scanpy\")\n\n\n#flavor\n#Literal['seurat', 'cell_ranger', 'seurat_v3', 'seurat_v3_paper'] (default: 'seurat')\n\n\nrownames(hvg.scanpy$disp) = rownames(ctrl)\nrownames(hvg.scanpy$vst) = rownames(ctrl)\n\n\ntop20 &lt;- head(rownames(hvg.scanpy$disp)[order(hvg.scanpy$disp$dispersions_norm, decreasing = T)], 20)\np = ggplot(hvg.scanpy$disp, aes(x=means, y=dispersions_norm, colour = highly_variable)) + geom_point() + theme_classic() + ggtitle(\"Scanpy disp\")\n\npS = LabelPoints(plot = p, points = top20, repel = TRUE)\npS \n\n\n\n\n\n\n\ntop20 &lt;- head(rownames(hvg.scanpy$vst)[order(hvg.scanpy$vst$variances_norm, decreasing = T)], 20)\np2 = ggplot(hvg.scanpy$vst, aes(x=means, y=variances_norm, colour = highly_variable)) + geom_point() + theme_classic() + ggtitle(\"Scanpy vst\") + scale_x_log10()\n\npS2 = LabelPoints(plot = p2, points = top20, repel = TRUE)\npS2\n\n\n\n\n\n\n\n\n\n\n4.4 All together\n\ncolnames(hvg.scanpy$disp) = paste0(colnames(hvg.scanpy$disp), \"_scpyD\")\ncolnames(hvg.scanpy$vst) = paste0(colnames(hvg.scanpy$vst), \"_scpyV\")\ncolnames(var.out) = paste0(colnames(var.out), \"_bioc\")\nctrl.hvg = cbind(vinfo.seu, var.out, hvg.scanpy$disp, hvg.scanpy$vst)\n\n\nwrap_plots(p1S + ggtitle(\"Seurat vst\"),p2S + ggtitle(\"Seurat disp\"),p3S + ggtitle(\"Seurat mvp\"),pB + ggtitle(\"Bioc total\"),pS,pS2, ncol=3)\n\n\n\n\n\n\n\n\n\nsel = c(\"vf_vst_counts.ctrl_13_variance.standardized\",\"vf_disp_data.ctrl_13_mvp.dispersion.scaled\",\"bio_bioc\", \"dispersions_norm_scpyD\", \"variances_norm_scpyV\")\n\npairs(ctrl.hvg[,sel])\n\n\n\n\n\n\n\n\n\nsel = c(\"vf_vst_counts.ctrl_13_mean_log\",\"vf_disp_data.ctrl_13_mvp.mean\", \"mean_bioc\", \"means_scpyD\", \"means_scpyV_log\")\n\nctrl.hvg$vf_vst_counts.ctrl_13_mean_log = log1p(ctrl.hvg$vf_vst_counts.ctrl_13_mean)\nctrl.hvg$means_scpyV_log = log1p(ctrl.hvg$means_scpyV)\n\npairs(ctrl.hvg[,sel])\n\n\n\n\n\n\n\n\nMean Bioc stands out the most, mvp in seurat and scanpy is identical.\n\nhvgs = hvg_seu\nhvgs$scpyD = rownames(hvg.scanpy$disp)[hvg.scanpy$disp$highly_variable_scpyD]\nhvgs$scpyV = rownames(hvg.scanpy$disp)[hvg.scanpy$vst$highly_variable_scpyV]\nhvgs$bioc = hvgs_bioc\n\no = overlap_phyper2(hvgs,hvgs, remove.diag = T)\n\n\n\n\n\n\n\n\nVst clearly stands out as more different, but is also based on counts instead of lognorm data.\nThe variance estimate for the same sample is quite different. Even the top variable genes are not the same and are very different gene groups.\n\n\n4.5 Top genes\nExplore top genes with vst, bioc and scanpy\n\ntopG = lapply(hvgs, head, 10)\n\n# same for mvp and dips, remove one.\ntopG$mvp = NULL\n\n# sort for scanpy.\ntopG$scpyD = rownames(ctrl.hvg)[order(ctrl.hvg$dispersions_norm_scpyD, decreasing = T)][1:10]\ntopG$scpyV = rownames(ctrl.hvg)[order(ctrl.hvg$variances_norm_scpyV, decreasing = T)][1:10]\n\n\nprint(topG)\n\n$vst\n [1] \"IGKC\"   \"IGLC2\"  \"LYPD2\"  \"PTGDS\"  \"PPBP\"   \"IGHM\"   \"IGLC3\"  \"S100A9\"\n [9] \"CD79A\"  \"S100A8\"\n\n$disp\n [1] \"S100A9\" \"S100A8\" \"CAVIN2\" \"IGKC\"   \"GNLY\"   \"C1QA\"   \"FTL\"    \"LYZ\"   \n [9] \"IGLC2\"  \"PPBP\"  \n\n$scpyD\n [1] \"CAVIN2\" \"C1QA\"   \"IFI27\"  \"IGLC3\"  \"LYPD2\"  \"GNG11\"  \"S100B\"  \"IGLC2\" \n [9] \"CLU\"    \"PF4\"   \n\n$scpyV\n [1] \"IGKC\"   \"IGLC2\"  \"LYPD2\"  \"PTGDS\"  \"PPBP\"   \"IGHM\"   \"IGLC3\"  \"S100A9\"\n [9] \"CD79A\"  \"S100A8\"\n\n$bioc\n [1] \"GNLY\"    \"NKG7\"    \"S100A9\"  \"LYZ\"     \"S100A8\"  \"CCL5\"    \"HLA-DRA\"\n [8] \"CST3\"    \"CTSS\"    \"FGFBP2\" \n\ntable(unlist(topG))\n\n\n   C1QA  CAVIN2    CCL5   CD79A     CLU    CST3    CTSS  FGFBP2     FTL   GNG11 \n      2       2       1       2       1       1       1       1       1       1 \n   GNLY HLA-DRA   IFI27    IGHM    IGKC   IGLC2   IGLC3   LYPD2     LYZ    NKG7 \n      2       1       1       2       3       4       3       3       2       1 \n    PF4    PPBP   PTGDS  S100A8  S100A9   S100B \n      1       3       2       4       4       1 \n\n\nVST with seurat or scanpy are the same. Some difference in the dispersion ones.\n\nsmall.leg &lt;- theme(legend.text = element_text(size=3), legend.key.size = unit(0.5,\"point\"))\n\nFeaturePlot(ctrl, reduction = \"umap_harmony\", features = unique(unlist(topG)), order = T, ncol=5) + small.leg\n\n\n\n\n\n\n\nVlnPlot(ctrl, features = unique(unlist(topG)), ncol = 5)\n\n\n\n\n\n\n\n\nMore clear celltype genes in the Bioc selection. More B-cell genes for Scanpy/Seurat.\nExpression levels of the variable genes.\n\nctrl.hvg$nC = rowSums(ctrl@assays$RNA@layers$counts.ctrl_13)\nctrl.hvg$meanE = rowMeans(ctrl@assays$RNA@layers$data.ctrl_13)\nctrl.hvg$vg_bioc = rownames(ctrl.hvg) %in% hvgs_bioc\n\nwrap_plots(\nggplot(ctrl.hvg, aes(x=nC, fill=vg_vst)) + geom_histogram( alpha=0.5, position=\"identity\") + scale_x_log10() + NoLegend() + ggtitle(\"VST\"),\nggplot(ctrl.hvg, aes(x=nC, fill=vg_disp)) + geom_histogram( alpha=0.5, position=\"identity\") + scale_x_log10() + NoLegend() + ggtitle(\"Disp\"), \nggplot(ctrl.hvg, aes(x=nC, fill=highly_variable_scpyD)) + geom_histogram( alpha=0.5, position=\"identity\") + scale_x_log10() + NoLegend() + ggtitle(\"Scanpy Disp\"),\nggplot(ctrl.hvg, aes(x=nC, fill=highly_variable_scpyV)) + geom_histogram( alpha=0.5, position=\"identity\") + scale_x_log10() + NoLegend() + ggtitle(\"Scanpy VST\"),\nggplot(ctrl.hvg, aes(x=nC, fill=vg_bioc)) + geom_histogram( alpha=0.5, position=\"identity\") + scale_x_log10() + NoLegend() + ggtitle(\"Bioc\"),\n\nggplot(ctrl.hvg, aes(x=meanE, fill=vg_vst)) + geom_histogram( alpha=0.5, position=\"identity\") + scale_x_log10() + NoLegend() + ggtitle(\"VST\"), \nggplot(ctrl.hvg, aes(x=meanE, fill=vg_disp)) + geom_histogram( alpha=0.5, position=\"identity\") + scale_x_log10() + NoLegend() + ggtitle(\"Disp\"), \nggplot(ctrl.hvg, aes(x=meanE, fill=highly_variable_scpyD)) + geom_histogram( alpha=0.5, position=\"identity\") + scale_x_log10() + NoLegend() + ggtitle(\"Scanpy Disp\"), \nggplot(ctrl.hvg, aes(x=meanE, fill=highly_variable_scpyV)) + geom_histogram( alpha=0.5, position=\"identity\") + scale_x_log10() + NoLegend() + ggtitle(\"Scanpy VST\"), \nggplot(ctrl.hvg, aes(x=meanE, fill=vg_bioc)) + geom_histogram( alpha=0.5, position=\"identity\") + scale_x_log10() + NoLegend()+ ggtitle(\"Bioc\"), \nncol =5\n)\n\n\n\n\n\n\n\n\nBioC distibution shifted to more highly expressed genes.\nDispersion gives the extremely higly expressed genes."
  },
  {
    "objectID": "labs/comparison/comparison_hvg.html#discussion",
    "href": "labs/comparison/comparison_hvg.html#discussion",
    "title": "Comparison of HVGs",
    "section": "5 Discussion",
    "text": "5 Discussion\n\nSeurat v3 paper suggests vst on counts, but then uses the variable genes in lognorm space\nFrom SC best practices, suggests scry for HVG selection.\n\nlog-transformation is not possible for exact zeros, analysts often add a small pseudo count, e.g., 1 (log1p), to all normalized counts before log transforming the data. Choosing the pseudo count, however, is arbitrary and can introduce biases to the transformed data. This arbitrariness has then also an effect on the feature selection as the observed variability depends on the chosen pseudo count. A small pseudo count value close to zero is increasing the variance of genes with zero counts [Townes et al., 2019].\nFrom Townes paper:\nZero inflation is an artifact of log normalization\nSelect a few of the top genes and compare. IGKC,IGLC2 is top gene with vst, CAVIN2, C1QA with dispersion, GNLY,NKG7 with bioc, S100A9 ranks high in all, but most in bioc.\n\nselG = c(\"IGKC\",\"IGLC2\",\"CAVIN2\",\"C1QA\",\"GNLY\",\"NKG7\", \"S100A9\" )\n\n# violin plots for each slot.\nVlnPlot(ctrl,selG, ncol = 7, layer = \"counts\", group.by = \"orig.ident\")\n\n\n\n\n\n\n\nVlnPlot(ctrl,selG, ncol = 7, layer = \"data\", group.by = \"orig.ident\")\n\n\n\n\n\n\n\nVlnPlot(ctrl,selG, ncol = 7, layer = \"scale.data\", group.by = \"orig.ident\")\n\n\n\n\n\n\n\n\nTry normalizing and logtrans with different pseudocounts and scale factors.\nDefault scale factor is 10K\n\nselG = c(\"IGKC\",\"IGLC2\",\"CAVIN2\",\"C1QA\",\"GNLY\",\"NKG7\", \"S100A9\" )\n\nC = ctrl@assays$RNA@layers$counts.ctrl_13\nrownames(C) = rownames(ctrl)\nhist(colSums(C),100)\n\n\n\n\n\n\n\nC.norm = C/colSums(C)\n# do log1p or log(x+1e-6)\n# scale factor 15K or 4k\n\nLayerData(ctrl,\"norm_15k_1p\") = log1p(C.norm*15000)\nLayerData(ctrl,\"norm_4k_1p\") = log1p(C.norm*4000)\nLayerData(ctrl,\"norm_15k_e6p\") = log(C.norm*15000 + 1e-6)\nLayerData(ctrl,\"norm_4k_e6p\") = log(C.norm*4000 + 1e-6)\n\n# violin plots for each slot.\nVlnPlot(ctrl,selG, ncol = 7, layer = \"norm_15k_1p\", group.by = \"orig.ident\")\n\n\n\n\n\n\n\nVlnPlot(ctrl,selG, ncol = 7, layer = \"norm_4k_1p\", group.by = \"orig.ident\")\n\n\n\n\n\n\n\nVlnPlot(ctrl,selG, ncol = 7, layer = \"norm_15k_e6p\", group.by = \"orig.ident\")\n\n\n\n\n\n\n\nVlnPlot(ctrl,selG, ncol = 7, layer = \"norm_4k_e6p\", group.by = \"orig.ident\")"
  },
  {
    "objectID": "labs/seurat/test_basilisk.html",
    "href": "labs/seurat/test_basilisk.html",
    "title": " Data Integration",
    "section": "",
    "text": "Note\n\n\n\nCode chunks run R commands unless otherwise specified.\nIn this tutorial we will look at different ways of integrating multiple single cell RNA-seq datasets. We will explore a few different methods to correct for batch effects across datasets. Seurat uses the data integration method presented in Comprehensive Integration of Single Cell Data, while Scran and Scanpy use a mutual Nearest neighbour method (MNN). Below you can find a list of some methods for single data integration:"
  },
  {
    "objectID": "labs/seurat/test_basilisk.html#meta-int_prep",
    "href": "labs/seurat/test_basilisk.html#meta-int_prep",
    "title": " Data Integration",
    "section": "1 Data preparation",
    "text": "1 Data preparation\nLet’s first load necessary libraries and the data saved in the previous lab.\n\nsuppressPackageStartupMessages({\n    library(Seurat)\n    library(ggplot2)\n    library(patchwork)\n    library(basilisk)\n})\n\ncondapath = \"/Users/asabjor/miniconda3/envs/scanpy_2024_nopip\"\n\n\n# download pre-computed data if missing or long compute\nfetch_data &lt;- TRUE\n\n# url for source and intermediate data\npath_data &lt;- \"https://export.uppmax.uu.se/naiss2023-23-3/workshops/workshop-scrnaseq\"\npath_file &lt;- \"data/covid/results/seurat_covid_qc_dr.rds\"\nif (!dir.exists(dirname(path_file))) dir.create(dirname(path_file), recursive = TRUE)\nif (fetch_data && !file.exists(path_file)) download.file(url = file.path(path_data, \"covid/results/seurat_covid_qc_dr.rds\"), destfile = path_file)\nalldata &lt;- readRDS(path_file)\nprint(names(alldata@reductions))\n\n[1] \"pca\"               \"umap\"              \"tsne\"             \n[4] \"UMAP10_on_PCA\"     \"UMAP_on_ScaleData\"\n\n\nWith Seurat5 we can split the RNA assay into multiple Layers with one count matrix and one data matrix per sample. When we then run FindVariableFeatures on the object it will run it for each of the samples separately, but also compute the overall variable features by combining their ranks.\n\n# get the variable genes from all the datasets without batch information.\nhvgs_old = VariableFeatures(alldata)\n\n# now split the object into layers\nalldata[[\"RNA\"]] &lt;- split(alldata[[\"RNA\"]], f = alldata$orig.ident)\n\n# detect HVGs\nalldata &lt;- FindVariableFeatures(alldata, selection.method = \"vst\", nfeatures = 2000, verbose = FALSE)\n\n# to get the HVGs for each layer we have to fetch them individually\ndata.layers &lt;- Layers(alldata)[grep(\"data.\",Layers(alldata))]\nprint(data.layers)\n\n[1] \"data.covid_1\"  \"data.covid_15\" \"data.covid_16\" \"data.covid_17\"\n[5] \"data.ctrl_5\"   \"data.ctrl_13\"  \"data.ctrl_14\"  \"data.ctrl_19\" \n\nhvgs_per_dataset &lt;- lapply(data.layers, function(x) VariableFeatures(alldata, layer = x) )\nnames(hvgs_per_dataset) = data.layers\n\n# also add in the variable genes that was selected on the whole dataset and the old ones \nhvgs_per_dataset$all &lt;- VariableFeatures(alldata)\nhvgs_per_dataset$old &lt;- hvgs_old\n\ntemp &lt;- unique(unlist(hvgs_per_dataset))\noverlap &lt;- sapply( hvgs_per_dataset , function(x) { temp %in% x } )\npheatmap::pheatmap(t(overlap*1),cluster_rows = F ,\n                   color = c(\"grey90\",\"grey20\"))\n\n\n\n\n\n\n\n\nAs you can see, there are a lot of genes that are variable in just one dataset. There are also some genes in the gene set that was selected using all the data that are not variable in any of the individual datasets. These are most likely genes driven by batch effects.\nA better way to select features for integration is to combine the information on variable genes across the dataset. This is what we have in the all section where the ranks of the variable features in the different datasets is combined.\nFor all downstream integration we will use this set of genes so that it is comparable across the methods. Before doing anything else we need to rerun ScaleData and PCA with that set of genes.\n\nhvgs_all = hvgs_per_dataset$all\n\nalldata = ScaleData(alldata, features = hvgs_all, vars.to.regress = c(\"percent_mito\", \"nFeature_RNA\"))\nalldata = RunPCA(alldata, features = hvgs_all, verbose = FALSE)"
  },
  {
    "objectID": "labs/seurat/test_basilisk.html#cca",
    "href": "labs/seurat/test_basilisk.html#cca",
    "title": " Data Integration",
    "section": "2 CCA",
    "text": "2 CCA\nIn Seurat v4 we run the integration in two steps, first finding anchors between datasets with FindIntegrationAnchors() and then running the actual integration with IntegrateData(). Since Seurat v5 this is done in a single command using the function IntegrateLayers(), we specify the name for the integration as integrated_cca.\n\nalldata &lt;- IntegrateLayers(object = alldata, \n                           method = CCAIntegration, orig.reduction = \"pca\", \n                           new.reduction = \"integrated_cca\", verbose = FALSE)\n\nWe should now have a new dimensionality reduction slot (integrated_cca) in the object:\n\nnames(alldata@reductions)\n\n[1] \"pca\"               \"umap\"              \"tsne\"             \n[4] \"UMAP10_on_PCA\"     \"UMAP_on_ScaleData\"\n\n\nUsing this new integrated dimensionality reduction we can now run UMAP and tSNE on that object, and we again specify the names of the new reductions so that the old UMAP and tSNE are not overwritten.\n\nalldata &lt;- RunUMAP(alldata, reduction = \"integrated_cca\", dims = 1:30, reduction.name = \"umap_cca\")\nalldata &lt;- RunTSNE(alldata, reduction = \"integrated_cca\", dims = 1:30, reduction.name = \"tsne_cca\")\n\nnames(alldata@reductions)\n\nWe can now plot the unintegrated and the integrated space reduced dimensions.\n\nwrap_plots(\n  DimPlot(alldata, reduction = \"pca\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"PCA raw_data\"),\n  DimPlot(alldata, reduction = \"tsne\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"tSNE raw_data\"),\n  DimPlot(alldata, reduction = \"umap\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"UMAP raw_data\"),\n  \n  DimPlot(alldata, reduction = \"integrated_cca\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"CCA integrated\"),\n  DimPlot(alldata, reduction = \"tsne_cca\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"tSNE integrated\"),\n  DimPlot(alldata, reduction = \"umap_cca\", group.by = \"orig.ident\")+NoAxes()+ggtitle(\"UMAP integrated\"),\n  ncol = 3\n) + plot_layout(guides = \"collect\")\n\n\n2.1 Marker genes\nLet’s plot some marker genes for different cell types onto the embedding.\n\n\n\nMarkers\nCell Type\n\n\n\n\nCD3E\nT cells\n\n\nCD3E CD4\nCD4+ T cells\n\n\nCD3E CD8A\nCD8+ T cells\n\n\nGNLY, NKG7\nNK cells\n\n\nMS4A1\nB cells\n\n\nCD14, LYZ, CST3, MS4A7\nCD14+ Monocytes\n\n\nFCGR3A, LYZ, CST3, MS4A7\nFCGR3A+ Monocytes\n\n\nFCER1A, CST3\nDCs\n\n\n\n\nmyfeatures &lt;- c(\"CD3E\", \"CD4\", \"CD8A\", \"NKG7\", \"GNLY\", \"MS4A1\", \"CD14\", \"LYZ\", \"MS4A7\", \"FCGR3A\", \"CST3\", \"FCER1A\")\nFeaturePlot(alldata, reduction = \"umap_cca\", dims = 1:2, features = myfeatures, ncol = 4, order = T) + NoLegend() + NoAxes() + NoGrid()"
  },
  {
    "objectID": "labs/seurat/test_basilisk.html#meta-dimred_harmony",
    "href": "labs/seurat/test_basilisk.html#meta-dimred_harmony",
    "title": " Data Integration",
    "section": "3 Harmony",
    "text": "3 Harmony\nAn alternative method for integration is Harmony, for more details on the method, please se their paper Nat. Methods. This method runs the integration on a dimensionality reduction, in most applications the PCA. So first, we prefer to have scaling and PCA with the same set of genes that were used for the CCA integration, which we ran earlier.\nWe can use the same function IntegrateLayers() but intstead specify the method HarmonyIntegration. And as above, we run UMAP on the new reduction from Harmony.\n\nalldata &lt;- IntegrateLayers(\n  object = alldata, method = HarmonyIntegration,\n  orig.reduction = \"pca\", new.reduction = \"harmony\",\n  verbose = FALSE\n)\n\n\nalldata &lt;- RunUMAP(alldata, dims = 1:30, reduction = \"harmony\", reduction.name = \"umap_harmony\")\nDimPlot(alldata, reduction = \"umap_harmony\", group.by = \"orig.ident\") + NoAxes() + ggtitle(\"Harmony UMAP\")"
  },
  {
    "objectID": "labs/seurat/test_basilisk.html#meta-dimred_scanorama",
    "href": "labs/seurat/test_basilisk.html#meta-dimred_scanorama",
    "title": " Data Integration",
    "section": "4 Scanorama",
    "text": "4 Scanorama\nAnother integration method is Scanorama (see Nat. Biotech.). This method is implemented in python, but we can run it through the Reticulate package.\nWe will run it with the same set of variable genes, but first we have to create a list of all the objects per sample.\nKeep in mind that for most python tools that uses AnnData format the gene x cell matrix is transposed so that genes are rows and cells are columns.\n\n# get data matrices from all samples, with only the variable genes.\ndata.layers &lt;- Layers(alldata)[grep(\"data.\",Layers(alldata))]\nprint(data.layers)\n\n[1] \"data.covid_1\"  \"data.covid_15\" \"data.covid_16\" \"data.covid_17\"\n[5] \"data.ctrl_5\"   \"data.ctrl_13\"  \"data.ctrl_14\"  \"data.ctrl_19\" \n\nassaylist &lt;- lapply(data.layers, function(x) t(as.matrix(LayerData(alldata, layer = x)[hvgs_all,])))\ngenelist =  rep(list(hvgs_all),length(assaylist))\n\nlapply(assaylist,dim)\n\n[[1]]\n[1]  875 2000\n\n[[2]]\n[1]  549 2000\n\n[[3]]\n[1]  357 2000\n\n[[4]]\n[1] 1057 2000\n\n[[5]]\n[1] 1033 2000\n\n[[6]]\n[1] 1126 2000\n\n[[7]]\n[1]  996 2000\n\n[[8]]\n[1] 1141 2000\n\n\nScanorama is implemented in python, but through reticulate we can load python packages and run python functions. In this case we also use the basilisk package for a more clean activation of python environment.\nAt the top of this script, we set the variable condapath to point to the conda environment where scanorama is included.\n\n# run scanorama via basilisk with assaylis and genelist as input.\nintegrated.data = basiliskRun(env=condapath, fun=function(datas, genes) {\n  scanorama &lt;- reticulate::import(\"scanorama\")\n  output &lt;- scanorama$integrate(datasets_full = datas,\n                                         genes_list = genes )\n  return(output)\n}, datas = assaylist, genes = genelist, testload=\"scanorama\")\n\n\n# Now we create a new dim reduction object in the format that Seurat uses\nintdimred &lt;- do.call(rbind, integrated.data[[1]])\ncolnames(intdimred) &lt;- paste0(\"Scanorama_\", 1:100)\nrownames(intdimred) &lt;- colnames(alldata)\n\n# Add standard deviations in order to draw Elbow Plots in Seurat\nstdevs &lt;- apply(intdimred, MARGIN = 2, FUN = sd)\n\n# Create a new dim red object.\nalldata[[\"scanorama\"]] &lt;- CreateDimReducObject(\n  embeddings = intdimred,\n  stdev      = stdevs,\n  key        = \"Scanorama_\",\n  assay      = \"RNA\")\n\nTry the same but using counts instead of data.\n\n# get count matrices from all samples, with only the variable genes.\ncount.layers &lt;- Layers(alldata)[grep(\"counts.\",Layers(alldata))]\nprint(count.layers)\nassaylist &lt;- lapply(count.layers, function(x) t(as.matrix(LayerData(alldata, layer = x)[hvgs_all,])))\n\n# run scanorama via basilisk with assaylis and genelist as input.\nintegrated.data = basiliskRun(env=condapath, fun=function(datas, genes) {\n  scanorama &lt;- reticulate::import(\"scanorama\")\n  output &lt;- scanorama$integrate(datasets_full = datas,\n                                         genes_list = genes )\n  return(output)\n}, datas = assaylist, genes = genelist, testload=\"scanorama\")\n\n# Now we create a new dim reduction object in the format that Seurat uses\n# The scanorama output has 100 dimensions.\nintdimred &lt;- do.call(rbind, integrated.data[[1]])\ncolnames(intdimred) &lt;- paste0(\"Scanorama_\", 1:100)\nrownames(intdimred) &lt;- colnames(alldata)\n\n# Add standard deviations in order to draw Elbow Plots in Seurat\nstdevs &lt;- apply(intdimred, MARGIN = 2, FUN = sd)\n\n# Create a new dim red object.\nalldata[[\"scanoramaC\"]] &lt;- CreateDimReducObject(\n  embeddings = intdimred,\n  stdev      = stdevs,\n  key        = \"Scanorama_\",\n  assay      = \"RNA\")\n\n\n#Here we use all PCs computed from Scanorama for UMAP calculation\nalldata &lt;- RunUMAP(alldata, dims = 1:100, reduction = \"scanorama\",reduction.name = \"umap_scanorama\")\nalldata &lt;- RunUMAP(alldata, dims = 1:100, reduction = \"scanoramaC\",reduction.name = \"umap_scanoramaC\")\n\n\np1 = DimPlot(alldata, reduction = \"umap_scanorama\", group.by = \"orig.ident\") + NoAxes() + ggtitle(\"Scanorama UMAP\")\np2 = DimPlot(alldata, reduction = \"umap_scanoramaC\", group.by = \"orig.ident\") + NoAxes() + ggtitle(\"ScanoramaC UMAP\")\n\nwrap_plots(p1,p2)"
  },
  {
    "objectID": "labs/seurat/test_basilisk.html#overview-all-methods",
    "href": "labs/seurat/test_basilisk.html#overview-all-methods",
    "title": " Data Integration",
    "section": "5 Overview all methods",
    "text": "5 Overview all methods\nNow we will plot UMAPS with all three integration methods side by side.\n\np1 &lt;- DimPlot(alldata, reduction = \"umap\", group.by = \"orig.ident\") + ggtitle(\"UMAP raw_data\")\np2 &lt;- DimPlot(alldata, reduction = \"umap_cca\", group.by = \"orig.ident\") + ggtitle(\"UMAP CCA\")\np3 &lt;- DimPlot(alldata, reduction = \"umap_harmony\", group.by = \"orig.ident\") + ggtitle(\"UMAP Harmony\")\np4 &lt;- DimPlot(alldata, reduction = \"umap_scanorama\", group.by = \"orig.ident\")+ggtitle(\"UMAP Scanorama\")\n\nwrap_plots(p1, p2, p3, p4, nrow = 2) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\nDiscuss\n\n\n\nLook at the different integration results, which one do you think looks the best? How would you motivate selecting one method over the other? How do you think you could best evaluate if the integration worked well?\n\n\nLet’s save the integrated data for further analysis.\n\nsaveRDS(alldata,\"data/covid/results/seurat_covid_qc_dr_int.rds\")"
  },
  {
    "objectID": "labs/seurat/test_basilisk.html#extra-task",
    "href": "labs/seurat/test_basilisk.html#extra-task",
    "title": " Data Integration",
    "section": "6 Extra task",
    "text": "6 Extra task\nYou have now done the Seurat integration with CCA which is quite slow. There are other options in the FindIntegrationAnchors() function. Try rerunning the integration with rpca and/or rlsi and create a new UMAP. Compare the results."
  },
  {
    "objectID": "labs/seurat/test_basilisk.html#meta-session",
    "href": "labs/seurat/test_basilisk.html#meta-session",
    "title": " Data Integration",
    "section": "7 Session info",
    "text": "7 Session info\n\n\nClick here\n\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS/LAPACK: /Users/asabjor/miniconda3/envs/seurat5/lib/libopenblasp-r0.3.27.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] sv_SE.UTF-8/sv_SE.UTF-8/sv_SE.UTF-8/C/sv_SE.UTF-8/sv_SE.UTF-8\n\ntime zone: Europe/Stockholm\ntzcode source: system (macOS)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] basilisk_1.14.1    patchwork_1.2.0    ggplot2_3.5.1      Seurat_5.1.0      \n[5] SeuratObject_5.0.2 sp_2.1-4          \n\nloaded via a namespace (and not attached):\n  [1] deldir_2.0-4           pbapply_1.7-2          gridExtra_2.3         \n  [4] rlang_1.1.4            magrittr_2.0.3         RcppAnnoy_0.0.22      \n  [7] spatstat.geom_3.2-9    matrixStats_1.4.1      ggridges_0.5.6        \n [10] compiler_4.3.3         dir.expiry_1.10.0      png_0.1-8             \n [13] vctrs_0.6.5            reshape2_1.4.4         stringr_1.5.1         \n [16] pkgconfig_2.0.3        fastmap_1.2.0          utf8_1.2.4            \n [19] promises_1.3.0         rmarkdown_2.28         purrr_1.0.2           \n [22] xfun_0.47              jsonlite_1.8.8         goftest_1.2-3         \n [25] later_1.3.2            spatstat.utils_3.1-0   irlba_2.3.5.1         \n [28] parallel_4.3.3         cluster_2.1.6          R6_2.5.1              \n [31] ica_1.0-3              stringi_1.8.4          RColorBrewer_1.1-3    \n [34] spatstat.data_3.1-2    reticulate_1.39.0      parallelly_1.38.0     \n [37] lmtest_0.9-40          scattermore_1.2        Rcpp_1.0.13           \n [40] knitr_1.48             tensor_1.5             future.apply_1.11.2   \n [43] zoo_1.8-12             sctransform_0.4.1      httpuv_1.6.15         \n [46] Matrix_1.6-5           splines_4.3.3          igraph_2.0.3          \n [49] tidyselect_1.2.1       abind_1.4-5            yaml_2.3.10           \n [52] spatstat.random_3.2-3  codetools_0.2-20       miniUI_0.1.1.1        \n [55] spatstat.explore_3.2-6 listenv_0.9.1          lattice_0.22-6        \n [58] tibble_3.2.1           plyr_1.8.9             basilisk.utils_1.14.1 \n [61] withr_3.0.1            shiny_1.9.1            ROCR_1.0-11           \n [64] evaluate_0.24.0        Rtsne_0.17             future_1.34.0         \n [67] fastDummies_1.7.4      survival_3.7-0         polyclip_1.10-7       \n [70] fitdistrplus_1.2-1     filelock_1.0.3         pillar_1.9.0          \n [73] KernSmooth_2.23-24     plotly_4.10.4          generics_0.1.3        \n [76] RcppHNSW_0.6.0         munsell_0.5.1          scales_1.3.0          \n [79] globals_0.16.3         xtable_1.8-4           glue_1.7.0            \n [82] pheatmap_1.0.12        lazyeval_0.2.2         tools_4.3.3           \n [85] data.table_1.15.4      RSpectra_0.16-2        RANN_2.6.2            \n [88] leiden_0.4.3.1         dotCall64_1.1-1        cowplot_1.1.3         \n [91] grid_4.3.3             tidyr_1.3.1            colorspace_2.1-1      \n [94] nlme_3.1-165           cli_3.6.3              spatstat.sparse_3.1-0 \n [97] spam_2.10-0            fansi_1.0.6            viridisLite_0.4.2     \n[100] dplyr_1.1.4            uwot_0.1.16            gtable_0.3.5          \n[103] digest_0.6.37          progressr_0.14.0       ggrepel_0.9.6         \n[106] htmlwidgets_1.6.4      htmltools_0.5.8.1      lifecycle_1.0.4       \n[109] httr_1.4.7             mime_0.12              MASS_7.3-60.0.1"
  }
]